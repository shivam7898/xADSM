# Dimension Reduction {#c34}

```{r 'C34', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
invisible(lapply(f_getPathR(A12pKbl), knitr::read_chunk)) #wwww
```

## Dimensions {.tabset .tabset-fade}

> 10 or 11 Dimensions are enough for the Universe. How many are needed for your data!


### Multicollinearity {.unlisted .unnumbered}

```{definition 'Multicollinearity'}
\textcolor{pink}{Multicollinearity} is a condition where some of the predictor variables are strongly correlated with each other. 
```

- Problems: Multicollinearity
  - Multicollinearity leads to instability in the solution space, leading to possible incoherent results, such as in multiple regression, where a multicollinear set of predictors can result in a regression which is significant overall, even when none of the individual variables is significant. 
  - Even if such instability is avoided, inclusion of variables which are highly correlated tends to overemphasize a particular component of the model, as the component is essentially being double counted.
- Problems: Too many variables
  - The sample size needed to fit a multivariate function grows exponentially with the number of variables.
  - The use of too many predictor variables to model a relationship with a response variable can unnecessarily complicate the interpretation of the analysis, and violates the principle of parsimony
    - i.e. keep the number of predictors to such a size that would be easily interpreted. 
  - Also, retaining too many variables may lead to overfitting
    - i.e. generality of the findings is hindered because new data do not behave the same as the training data for all the variables.

### Parsimony {.unlisted .unnumbered}

```{definition 'Principle-of-Parsimony'}
\textcolor{pink}{Principle of parsimony} is the problem-solving principle that "entities should not be multiplied beyond necessity". 
```

- It is inaccurately paraphrased as "the simplest explanation is usually the best one". 
- It advocates that when presented with competing hypotheses about the same prediction, one should select the solution with the fewest assumptions, and that this is not meant to be a way of choosing between hypotheses that make different predictions. 

### Overfitting & Underfitting {.unlisted .unnumbered}

```{definition 'Overfitting'}
\textcolor{pink}{Overfitting} is the production of an analysis that corresponds too closely to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.
```


```{definition 'Underfitting'}
\textcolor{pink}{Underfitting} occurs when a statistical model cannot adequately capture the underlying structure of the data. 
```

- An overfitted model is a statistical model that contains more parameters than can be justified by the data.
  - The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.
  - over-fitting occurs when a model begins to "memorize" training data rather than "learning" to generalize from a trend.
- Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.
- Overfitting Example
  - If the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. Such a model, though, will typically fail severely when making predictions.
  - A noisy linear dataset can be fitted by a polynomial function also which would give a perfect fit. However, the linear function (in this case) would be better in extrapolating beyond the fitted data.
- To decrease the chance or amount of overfitting
  - model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout
  - To explicitly penalize overly complex models 
  - To evaluate the model performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter. 

## Dimension-reduction methods 

- These use the correlation structure among the predictor variables to accomplish the following: 
  - To reduce the number of predictor items. 
  - To help ensure that these predictor items are independent. 
  - To provide a framework for interpretability of the results. 

- Dimension-reduction methods: 
  - PCA
  - Factor analysis 
  - User-defined composites 
  
## PCA

```{definition 'PCA'}
\textcolor{pink}{Principal components analysis (PCA)} seeks to explain the correlation structure of a set of predictor variables ${m}$, using a smaller set of linear combinations of these variables, called components ${k}$. PCA acts solely on the predictor variables, and ignores the target variable.
```


- Suppose that the original variables ${\{X_1, X_2, \ldots, X_m\}}$ form a coordinate system in m-dimensional space. 
  - Let each variable ${X_i}$ represent an ${n \times 1}$ vector, where ${n}$ is number of records.
  - The principal components represent a new coordinate system, found by rotating the original system along the directions of maximum variability.
- Analysis
  - Standardize the data, so that the mean for each variable is zero, and the standard deviation is one.
  - ${X_i \to Z_i = \frac{X_i - {\mu}_i}{{\sigma}_{ii}}}$
  - The covariance is a measure of the degree to which two variables vary together.
  - A positive covariance indicates that, when one variable increases, the other tends to increase, while a negative covariance indicates that, when one variable increases, the other tends to decrease. 
  - ${{\sigma}_{ii}^2}$ denotes the variance of ${X_i}$. 
    - If ${X_i}$ and ${X_j}$ are independent, then ${{\sigma}_{ij}^2 = 0}$; but reverse may not be TRUE i.e. ${{\sigma}_{ij}^2 = 0}$ does not imply that ${X_i}$ and ${X_j}$ are independent. 
    - Note that the covariance measure is not scaled, so that changing the units of measure would change the value of the covariance.
    - The correlation coefficient ${r_{ij}} = \frac{{\sigma}_{ij}^2}{{\sigma}_{ii}{\sigma}_{jj}}$ avoids this difficulty by scaling the covariance by each of the standard deviations.
    - Then, the correlation matrix is denoted as ${\rho}$

```{definition 'Eigenvalues'}
Let \textcolor{pink}{$\mathbf{B}$} be an $m \times m$ matrix, and let \textcolor{pink}{$\mathbf{I}$} be the $m \times m$ identity matrix. Then the scalars $\{\lambda_1, \lambda_2, \ldots, \lambda_m\}$ are said to be the \textcolor{pink}{eigenvalues of $\mathbf{B}$} if they satisfy $|\mathbf{B} - \lambda \mathbf{I}| = 0$, where $|\mathbf{Q}|$ denotes the determinant of Q.
```

```{definition 'Eigenvector'}
Let \textcolor{pink}{$\mathbf{B}$} be an $m \times m$ matrix, and let ${\lambda}$ be an eigenvalue of $\mathbf{B}$. Then nonzero $m \times 1$ vector \textcolor{pink}{$\overrightarrow{e}$} is said to be an \textcolor{pink}{eigenvector of B}, if $\mathbf{B} \overrightarrow{e} = ùúÜ\overrightarrow{e}$.
```

- The total variability in the standardized set of predictors equals the sum of the variances of the Z-vectors, which equals the sum of the variances of the components, which equals the sum of the eigenvalues, which equals the numer of predictors
  - i.e. $\sum_{i=1}^m {\text{Var}({Y_i})} = \sum_{i=1}^m {\text{Var}({Z_i})} = \sum_{i=1}^m {\lambda_i} = m$
- The partial correlation between a given component and a given predictor variable is a function of an eigenvector and an eigenvalue. Specifically, $\text{Corr}(Y_i, Z_j) = e_{ij}\sqrt{\lambda_i}$, where $\{ (\lambda_1, e_1), (\lambda_2, e_2), \ldots, (\lambda_m, e_m)\}$ are the eigenvalue‚Äìeigenvector pairs for the correlation matrix $\rho$, and we note that $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_m$. In other words, the eigenvalues are ordered by size. (A partial correlation coefficient is a correlation coefficient that takes into account the effect of all the other variables.)
- The proportion of the total variability in Z that is explained by the $i^{\text{th}}$ principal component is the ratio of the $i^{\text{th}}$ eigenvalue to the number of variables, that is, the ratio $\frac{\lambda_i}{m}$.

## Data Housing

\textcolor{pink}{Please import the "C34-cadata.txt".} 

- Source: http://lib.stat.cmu.edu/datasets/houses.zip
- About: [20640, 9]
  - It provides census information from all the block groups from the 1990 California census. 
  - For this data set, a block group has an average of 1425.5 people living in an area that is geographically compact. 
  - Block groups were excluded that contained zero entries for any of the variables. 
  - Variables: median house value (Target), median income, housing median age, total rooms, total bedrooms, population, households, latitude, and longitude.


```{r 'C34-Housing', include=FALSE, eval=FALSE}
# #read_table() can handle double space delimited file.
tbl <- read_table(paste0(.z$XL, "C34-cadata.txt"), skip = 27, 
                col_names = c("median_house_value", "median_income", "housing_median_age", 
                              "total_rooms", "total_bedrooms", "population", "households", 
                              "latitude", "longitude"))
stopifnot(!"problems" %in% names(attributes(tbl)))
#problems(tbl)
#attr(tbl, "problems") <- NULL
attr(tbl, "spec") <- NULL
xxC34Housing <- tbl
f_setRDS(xxC34Housing)
```

```{r 'C34-GetHousing', include=FALSE}
bb <- aa <- xxC34Housing <- f_getRDS(xxC34Housing)  #wwww
```

## Partition Train & Test Data

```{r 'C34-Partition'}
set.seed(3)  #wwww
# #sample() and its varaints can be used for Partitioning of Dataset
if(FALSE) {
  # #This approach is difficult to extend for 3 or more splits
  # #Further, Using floor() multiple times might result in loss of a row 
  train_idx <- sample.int(n = nrow(bb), size = floor(0.8 * nrow(bb)), replace = FALSE)
  train_idx <- sample(seq_len(nrow(bb)), size = floor(0.8 * nrow(bb)), replace = FALSE)  
  train_bb <- bb[train_idx, ]
  test_bb <- bb[-train_idx, ]
}
#
# #For 3 or more splits
#brk_bb = c(train = 0.8, test = 0.1, validate = 0.1)
brk_bb = c(train = 0.9, test = 0.1)
idx_bb = sample(cut(seq_len(nrow(bb)), nrow(bb) * cumsum(c(0, brk_bb)), labels = names(brk_bb)))
#
# #Splits by Fixed Numbers not Percentages
if(FALSE) {
  brk_bb = c(train = 18570, test = nrow(bb))
  idx_bb = sample(cut(seq_len(nrow(bb)), c(0, brk_bb), labels = names(brk_bb)))
}
# #List of Multiple Tibbles
part_l = split(bb, idx_bb)
#
# #nrow(), ncol(), dim() can be applied
vapply(part_l, nrow, FUN.VALUE = integer(1))
stopifnot(identical(nrow(bb), sum(vapply(part_l, nrow, FUN.VALUE = integer(1)))))
```

## Basics

```{r 'C34-Basic', include=FALSE}
xsyw <- part_l$train  #wwww
# #Separate Working Names and Display Names
if(FALSE) paste0(names(xsyw), collapse = ", ")
c_xsyw <- c("h_value", "income", "h_age", "rooms", "bedrooms", 
               "population", "households", "latitude", "longitude")
names(c_xsyw) <- c("House Value (Median)", "Income (Median)", "House Age (Median)", 
        "Rooms (Total)", "Bedrooms (Total)", "Population", "Households", "Latitude", "Longitude")
names(xsyw) <- c_xsyw
#
c_zsyw <- c_xsyw
names(c_zsyw) <- c("Target", "Income", "Age", "Rooms", "Beds", "Pop", "Houses", "Lat", "Long")
#
# #Scaling
zsyw <- xsyw %>% mutate(across(everything(), ~ as.vector(scale(.))))
# #Predictors Only
xw <- xsyw %>% select(-1)
zw <- zsyw %>% select(-1)
# #Long
f_wl(xsyw, zsyw, xw, zw)
#
# #Summary
xsyg <- xsyl %>% group_by(Keys) %>% 
  summarise(Min = min(Values), Max = max(Values), SD = sd(Values), 
            Mean = mean(Values), Median = median(Values), Mode = f_getMode(Values),
            Unique = length(unique(Values)), isNA = sum(is.na(Values)))
# #Relabel with Verification Before and After
levels(xsyg$Keys)
levels(xsyg$Keys) <- names(c_xsyw)
levels(xsyg$Keys)
#
# #Long to Wide: For Reference Only
if(FALSE) { zsyw <- zsyl %>% 
  group_by(Keys) %>% 
  mutate(ID = row_number()) %>% 
  pivot_wider(names_from = Keys, values_from = Values)
}
```

```{r 'C34T01', echo=FALSE}
# #Print Kable Table
hh <- xsyg %>% 
  #mutate(across(where(is.numeric), round, digits = 4)) %>% 
  mutate(across(where(is.numeric), format, digits = 3, 
                #nsmall = 0, replace.zero = TRUE, zero.print = "0", 
                drop0trailing = TRUE, scientific = FALSE)) 
#
cap_hh <- paste0("(C34T01)", "[", nrow(xsyw), ", ", ncol(xsyw), "] ", "Houses: Training Basics") 
f_pKbl(x = hh, caption = cap_hh, debug = FALSE)
```

## Boxplot

```{r 'C34-BoxScaledHouse', include=FALSE}
hh <- zsyl
levels(hh$Keys) <- names(c_xsyw)
#
ttl_hh <- "Houses: BoxPlots (Scaled)"
cap_hh <- "C34P01"
sub_hh <- NULL 
lgd_hh  <- NULL
```

```{r 'C34-ScaleBox', include=FALSE}
# #IN: hh(Keys, Values), 
C34 <- hh %>% { ggplot(data = ., mapping = aes(x = Keys, y = Values, fill = Keys)) +
    geom_boxplot() +
    k_gglayer_box +
    scale_y_continuous(breaks = breaks_pretty()) + 
    coord_flip() +
    theme(legend.position = 'none') +
    labs(x = NULL, y = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P01","-Houses-Box-Z", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P01, device = "png", dpi = 144) 
}
```

```{r 'C34P01', echo=FALSE, fig.cap="Houses: Boxplot (Scaled)"}
knitr::include_graphics(paste0(.z$PX, "C34P01","-Houses-Box-Z", ".png"))
```

## Normality

Note that normality of the data is not strictly required to perform non-inferential PCA but that strong departures from normality may diminish the observed correlations. As data mining applications usually do not involve inference, we will not worry about normality.

## Predictors SPLOM

- Rooms, bedrooms, population, and households all appear to be positively correlated. 
- Latitude and longitude appear to be negatively correlated. 
  - Scatter plot between them looks like the State of California
- House Median Age appears to be correlated the least with the other predictors

```{r 'C34-Predictors', include=FALSE}
hh <- zw
#hh <- hh[1:100, ]
labels_hh <- names(c_zsyw)[-1] #names(hh)
#
ttl_hh <- "Houses: SPLOM (8)"
cap_hh <- "C34P02"
sub_hh <- NULL 
lgd_hh  <- NULL 
```

```{r 'C34-SPLOM', include=FALSE}
# #Assumes Column 1 has Target Variable (Factor)
C34 <- hh %>% { 
  ggpairs(data = ., mapping = aes(alpha = I(0.1)), columnLabels = labels_hh,
          upper = list(continuous = wrap("cor", size = 5, alpha = 1))) +
    labs(caption = cap_hh, subtitle = sub_hh, title = ttl_hh) 
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P02", "-House-SPLOM-z", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P02, device = "png", dpi = 72, width = k_width, height = k_height) 
}
```

```{r 'C34P02', echo=FALSE, fig.cap="Houses: SPLOM (8)", out.width='100%'}
knitr::include_graphics(paste0(.z$PX, "C34P02", "-House-SPLOM-z", ".png"))
```

## Predictors Corplot

```{r 'C34-PredCor', include=FALSE, eval=FALSE}
# #Correlation Matrix | Table (Long) | Tibble
hh <- cor(zw) %>% as.table() %>% as_tibble(.name_repair = 'unique') %>% 
  #filter(...1 != ...2) %>% 
  filter(!duplicated(paste0(pmax(as.character(...1), as.character(...2)), 
                            pmin(as.character(...1), as.character(...2))))) %>% 
  mutate(across(where(is.character), factor, levels = c_zsyw[-1], labels = names(c_zsyw)[-1]))
#
ttl_hh <- "Houses: Corrplot (8)"
cap_hh <- "C34P03"
```

```{r 'C34-CorPlot', include=FALSE, eval=FALSE}
# #IN: hh (Correlation Tibble Long, Triangle with Diagonal) 
C34 <- hh %>% {ggplot(., aes(y = ...1, x = ...2, fill = n)) + 
    geom_tile(color = "white") + 
    geom_text(aes(label = round(n, 2)), color = "black", size = 4) +
    coord_fixed() +
    scale_fill_distiller(palette = "BrBG", direction = 1, limits = c(-1, 1)) +
    guides(fill = guide_colourbar(barwidth = 0.5, barheight = 15)) +
    theme(axis.title = element_blank(), 
          axis.line = element_blank(), 
          axis.ticks = element_blank(),
          panel.grid.major = element_blank(), 
          panel.border = element_blank()) +
	  labs(subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34-PredCor-A', ref.label=c('C34-PredCor', 'C34-CorPlot'), include=FALSE}
#
```

```{r 'C34P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P03","-House-Corplot-z", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P03, device = "png", dpi = 144) 
}
```

```{r 'C34P03', echo=FALSE, fig.cap="Houses: Corrplot (8)"}
knitr::include_graphics(paste0(.z$PX, "C34P03","-House-Corplot-z", ".png"))
```

## Correlation {.tabset .tabset-fade}

### Correlation Matrix {.unlisted .unnumbered}

```{r 'C34T02', echo=FALSE, cache=FALSE}
# #cor() produces a Matrix of Correlations: Redundant Triangle (Upper or Lower) and Diagonal
# #Print Kable Table
hh <- cor(zw)
cap_hh <- paste0("(C34T02) ", "Houses: Correlation Matrix") 
f_pKblM(x = hh, caption = cap_hh, negPos = c(-0.5, 0.5))
```

### Matrices {.unlisted .unnumbered}

- \textcolor{pink}{cor()} : Correlation Function produces a Matrix
  - Matirces has HUGE number of problems but unfortunately some function output is in that form
  - names() does NOT work on Matrices but colnames() works, even though names() is superior to colnames() in all other aspects
  - Symmatrical Matrix. Diagonal and one of the Triangles (Upper or Lower) are redundant
  - Too many decimal printing

```{r 'C34-Matrices'}
# #cor() produces a Matrix
ii <- cor(zw)
str(ii)
ii
#
# #We can eliminate Lower Triangle and Diagonal. However NA does not print well with format()
# #outcome of upper.tri() is easily compared to as.table(). lower.tri() will need extra step
#
# #Take advatage of Matrix Triangle and Set to 0 for later handling by format()
# #IF we remove the diagonal then dimensions gets haywire i.e. 8 to 7 columns left [28] elements
# #IF we keep the diagonal then dimensions gets haywire i.e. 8 to 9 columns left [36] elements
# #So, cannot use NA, has to use ZERO (So that, later, format can replace it.)
#
# #However, we finally went ahead with dplyr solution which handled NA separately from format()
# #Thus eliminating need of assigning 0, NA are being used for redundant triangle and diagonal
#
kk <- ii #ii is FULL 8x8 Matrix
kk[upper.tri(kk, diag = TRUE)] <- NA 
mm  <- kk %>% as.table() %>% as_tibble(.name_repair = 'unique') %>% drop_na() %>% 
  filter(n > 0.5 | n < -0.5) %>% arrange(desc(abs(n)))
#
kk <- ii #ii is FULL 8x8 Matrix
nn  <- kk %>% as.table() %>% as_tibble(.name_repair = 'unique') %>% 
    filter(...1 != ...2) %>% 
    filter(!duplicated(paste0(pmax(as.character(...1), as.character(...2)), 
                              pmin(as.character(...1), as.character(...2))))) %>%
  filter(n > 0.5 | n < -0.5) %>% arrange(desc(abs(n)))
stopifnot(identical(mm, nn))
#
# #However, above are long because of the usage of as.table(). For Wide:
ll <- kk %>% #as.table() %>% 
  as_tibble() %>% 
  mutate(ID = row_number()) 
# #Get Names
oo <- names(ll)
ll %>% mutate(across(-ID, ~ ifelse(ID <= match(cur_column(), oo), NA, .x))) %>% select(-ID)
```

### f_pKblM() {.unlisted .unnumbered}

```{r 'A12B-pKblM', eval=FALSE}
#
```

### f_pKbl() {.unlisted .unnumbered}

```{r 'A12A-pKbl', eval=FALSE}
#
```

## PCA {.tabset .tabset-fade}

- There are two general methods to perform PCA in R :
  - Spectral decomposition which examines the covariances / correlations between variables
    - \textcolor{pink}{princomp()}
      -  It uses divisor $N$ for the covariance matrix.
    - Names: "sdev, loadings, center, scale, n.obs, scores, call"
  - Singular value decomposition (SVD) which examines the covariances / correlations between individuals
    - SVD has slightly better numerical accuracy
    - \textcolor{pink}{prcomp()}
      - Unlike princomp, variances are computed with the usual divisor $N - 1$.
      - \textcolor{pink}{cov()} also uses $N - 1$
    - Names: "sdev, rotation, center, scale, x"
- Output contains
  - SD of Principal Components
  - rotation / loadings: the matrix of variable loadings (columns are eigenvectors)
  - x / scores: The coordinates of the individuals (observations) on the principal components.
- Understanding the result
  - PCA was carried out on the eight predictors in the house data set.
  - PCA was carried out on the eight predictors in the house data set. The \textcolor{pink}{component matrix} is shown in Table \@ref(tab:C34T03). 
  - Each of the columns in Table represents one of the compnonents $Y_i = e_i^T\mathbf{Z}$.
  - The cell entries are called the \textcolor{pink}{component weights}, and represent the partial correlation between the variable and the component.
    - As the component weights are correlations, they range between one and negative one.
    - NOTE: Sign may differ from the Book. 
    - Eigenvalues are given by ${s}^2$ as shown in Table \@ref(tab:C34T04) 
    - First Eigenvalue is 3.9 and there are 8 predictor variables, thus, first component (PC1) explains $3.9/8 \approx 48\%$ of the variance
      - i.e. this single component by itself carries about half of the information in all eight predictors.
      - In general, the first principal component may be viewed as the single best summary of the correlations among the predictors. Specifically, this particular linear combination of the variables accounts for more variability than any other linear combination. 
      - The second principal component is the second-best linear combination of the variables, on the condition that it is orthogonal to the first principal component. It is derived from the variability that is left over, once the first component has been accounted for.



```{definition 'Orthogonal'}
Two vectors are \textcolor{pink}{orthogonal} if they are mathematically independent, have no correlation, and are at right angles to each other. 
```



### Component Matrix {.unlisted .unnumbered}

```{r 'C34-PCA', include=FALSE}
# #Perform PCA #wwww
ii <- princomp(zw)
pca_zw <- prcomp(zw)
#
# #Focus on prcomp()
names(pca_zw)
#pca_zw$rotation
#summary(pca_zw)
#names(summary(pca_zw))
summary(pca_zw)$importance
#
str(head(pca_zw$x))
dim(pca_zw$x)
```

```{r 'C34T03', echo=FALSE}
# #Print Kable Table
hh <- pca_zw$rotation 
#
cap_hh <- paste0("(C34T03) ", "Houses: PCA Component Matrix") 
f_pKblM(x = hh, caption = cap_hh, isTri = FALSE)
```

### Code {.unlisted .unnumbered}

```{r 'C34-PCA-A', eval=TRUE, ref.label=c('C34-PCA')}
#
```

## Orthogonality of PCA {.tabset .tabset-fade}

### Image {.unlisted .unnumbered}

```{r 'C34-PredCorPCA', include=FALSE}
# #Correlation Matrix | Table (Long) | Tibble
hh <- cor(pca_zw$x) %>% as.table() %>% as_tibble(.name_repair = 'unique') %>% 
  #filter(...1 != ...2) %>% 
  filter(!duplicated(paste0(pmax(as.character(...1), as.character(...2)), 
                            pmin(as.character(...1), as.character(...2))))) #%>% 
  #mutate(across(where(is.character), factor, levels = c_zsyw[-1], labels = names(c_zsyw)[-1]))
#
ttl_hh <- "Houses: PCA Corrplot - ALL are ZERO"
cap_hh <- "C34P04"
```

```{r 'C34-PredCorPCA-A', ref.label=c('C34-PredCorPCA', 'C34-CorPlot'), include=FALSE}
#
```

```{r 'C34P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P04","-House-Corplot-PCA", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P04, device = "png", dpi = 144) 
}
```

```{r 'C34P04', echo=FALSE, fig.cap="Houses: PCA Corrplot - ALL are ZERO"}
knitr::include_graphics(paste0(.z$PX, "C34P04","-House-Corplot-PCA", ".png"))
```

### Code {.unlisted .unnumbered}

```{r 'C34-PredCorPCA-B', eval=FALSE, ref.label=c('C34-PredCorPCA')}
#
```

## ScreePlot {.tabset .tabset-fade}

### Image {.unlisted .unnumbered}

```{r 'C34-Scree', include=FALSE}
pca_eigen <- summary(pca_zw)$importance %>% t() %>% as_tibble(rownames = "PCA") %>% 
  rename(SD = 2, pVar = 3, pVarCum = 4) %>% 
  mutate(EigenVal = SD^2, pVarManual = EigenVal/sum(EigenVal), 
         isOne = ifelse(EigenVal > 1, "Yes", "No"),
         isNinty = ifelse(pVarCum < 0.9, "Yes", "No"))
hh <- pca_eigen
#
ttl_hh <- "Houses: PCA Eigenvalue ScreePlot"
cap_hh <- "C34P05"
y_hh <- "Eigenvalue"
```

```{r 'C34-ScreePlot', include=FALSE}
# #IN: hh 
C34 <- hh %>% {ggplot(., aes(x = PCA, y = EigenVal)) + 
    geom_point(aes(color = isOne), size = 3) +
    geom_line(aes(group = 1)) +
    geom_hline(aes(yintercept = 1), color = '#440154FF', linetype = "dashed") +
    annotate("segment", x = 3.5, xend = 3.1, y = 1.6, 
                    yend = 1.2, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("segment", x = 4.5, xend = 4.1, y = 1.3, 
                    yend = 0.9, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("segment", x = 5.5, xend = 5.1, y = 0.6, 
                    yend = 0.2, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    geom_text(data = tibble(x = c(3.5, 4.5, 5.5), y = c(1.7, 1.4, 0.7), 
              labels = c("Eigenvalue Criterion", "Screeplot Criterion", "Elbow Point")), 
              aes(x=x, y=y, label=labels), check_overlap = TRUE) + 
    scale_fill_distiller(palette = "BrBG") +
    #coord_fixed() +
    theme(legend.position = 'none') +
	  labs(y = y_hh, subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P05","-House-PCA-Scree", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P05, device = "png", dpi = 144) 
}
```

```{r 'C34P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C34P05","-House-PCA-Scree", ".png"))
```

```{r 'C34-CumVar', include=FALSE}
ttl_hh <- "Houses: PCA Proportion of Variance Explained"
cap_hh <- "C34P06"
y_hh <- NULL
```

```{r 'C34-CumVarPlot', include=FALSE}
# #IN: hh 
C34 <- hh %>% {ggplot(., aes(x = PCA, y = pVarCum)) + 
    geom_point(aes(color = isNinty), size = 3) +
    geom_line(aes(group = 1)) +
    geom_hline(aes(yintercept = 0.9), color = '#440154FF', linetype = "dashed") +
    scale_fill_distiller(palette = "BrBG") +
    scale_y_continuous(limits = c(0, 1), labels=percent) + 
    theme(legend.position = 'none') +
    labs(y = y_hh,
         subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P06","-House-PCA-Var", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P06, device = "png", dpi = 144) 
}
```

```{r 'C34P06', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C34P06","-House-PCA-Var", ".png"))
```

```{r 'C34P0506', echo=FALSE, ref.label=c('C34P05', 'C34P06'), fig.cap="House: PCA Screeplot with Variance"}
#
```


```{r 'C34T04', echo=FALSE}
# #Print Kable Table
hh <- pca_eigen %>% select(-pVarManual) %>% 
  rename(exp_Variance = 3, cum_Var = 4, isEigenOne = 6, isVarNinty = 7) %>% 
  #mutate(across(where(is.numeric), round, digits = 4)) %>% 
  add_row(summarise(., across(1, ~"Total")), summarise(., across(EigenVal, sum))) %>% 
  mutate(across(where(is.numeric), format, digits = 3, 
                #nsmall = 0, replace.zero = TRUE, zero.print = "0", 
                drop0trailing = TRUE, scientific = FALSE)) 
#
cap_hh <- paste0("(C34T04) ", "Houses: PCA Eigenvalues & Variance") 
f_pKbl(x = hh, caption = cap_hh, debug = FALSE)
```

### Code {.unlisted .unnumbered}

```{r 'C34-Scree-A', eval=FALSE, ref.label=c('C34-Scree', 'C34-ScreePlot', 'C34-CumVar', 'C34-CumVarPlot')}
#
```

## How many Components 

- In our example Single Component (PC1) can account for approximately half of the variability. But, all 8 accounts for 100% variability. So between 1 and 8 where is the cut-off
  - Criteria
    - The Eigenvalue Criterion 
    - The Proportion of Variance Explained Criterion 
    - The Minimum Communality Criterion - Deferred by Book
    - The Scree Plot Criterion

- The Eigenvalue Criterion
  - Refer Figure \@ref(fig:C34P0506) and Table \@ref(tab:C34T04)
  - Sum of the eigenvalues represents the number of variables entered into the PCA i.e. 8
  - An eigenvalue of 1 would then mean that the component would explain about "one variable worth" of the variability. 
  - Therefore, the eigenvalue criterion states that only components with eigenvalues greater than 1 should be retained. 
  - Note that, if there are fewer than 20 variables, the eigenvalue criterion tends to recommend extracting too few components, while, if there are more than 50 variables, this criterion may recommend extracting too many.
  - Thus in example: 3 can be retained. PC4 has value around 0.8 so it may or may not be retained.

- The Proportion of Variance Explained Criterion
  - We can define how much of the total variability that we would like the principal components to account for and then selects them acccordingly
  - Thus in example: 3 can be retained. PC4 will be selected if more than 90% should be accounted
  
- The Scree Plot Criterion
  - \textcolor{pink}{Elbow Point:} The maximum number of components that should be extracted is just before where the plot first begins to straighten out into a horizontal line. 
  - 
  

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C34-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, c32churn, xxB16Cars, xxB18Churn, brk_bb, hh, idx_bb, part_l,
          xxC34Housing, C34P01, cap_hh, lgd_hh, loc_png, names_bb, sub_hh, ttl_hh, xsyg, xsyl, 
          zsyl, C34P02, mm, nn, oo, zsyw, c_xsyw, C34P03, C34P04, xl, xsyw, xw, zl, zw, c_zsyw, 
          labels_hh, C34P05, C34P06, pca_zw, y_hh)
```

```{r 'C34-Validation', include=TRUE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
