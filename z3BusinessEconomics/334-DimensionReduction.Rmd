# Dimension Reduction {#c34}

```{r 'C34', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
invisible(lapply(f_getPathR(A12pKbl), knitr::read_chunk)) #wwww
```

## Dimensions {.tabset .tabset-fade}

> 10 or 11 Dimensions are enough for the Universe. How many are needed for your data!


### Multicollinearity {.unlisted .unnumbered}

```{definition 'Multicollinearity'}
\textcolor{pink}{Multicollinearity} is a condition where some of the predictor variables are strongly correlated with each other. 
```

- Problems: Multicollinearity
  - Multicollinearity leads to instability in the solution space, leading to possible incoherent results, such as in multiple regression, where a multicollinear set of predictors can result in a regression which is significant overall, even when none of the individual variables is significant. 
  - Even if such instability is avoided, inclusion of variables which are highly correlated tends to overemphasize a particular component of the model, as the component is essentially being double counted.
- Problems: Too many variables
  - The sample size needed to fit a multivariate function grows exponentially with the number of variables.
  - The use of too many predictor variables to model a relationship with a response variable can unnecessarily complicate the interpretation of the analysis, and violates the principle of parsimony
    - i.e. keep the number of predictors to such a size that would be easily interpreted. 
  - Also, retaining too many variables may lead to overfitting
    - i.e. generality of the findings is hindered because new data do not behave the same as the training data for all the variables.

### Parsimony {.unlisted .unnumbered}

```{definition 'Principle-of-Parsimony'}
\textcolor{pink}{Principle of parsimony} is the problem-solving principle that "entities should not be multiplied beyond necessity". 
```

- It is inaccurately paraphrased as "the simplest explanation is usually the best one". 
- It advocates that when presented with competing hypotheses about the same prediction, one should select the solution with the fewest assumptions, and that this is not meant to be a way of choosing between hypotheses that make different predictions. 

### Overfitting & Underfitting {.unlisted .unnumbered}

```{definition 'Overfitting'}
\textcolor{pink}{Overfitting} is the production of an analysis that corresponds too closely to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.
```


```{definition 'Underfitting'}
\textcolor{pink}{Underfitting} occurs when a statistical model cannot adequately capture the underlying structure of the data. 
```

- An overfitted model is a statistical model that contains more parameters than can be justified by the data.
  - The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.
  - over-fitting occurs when a model begins to "memorize" training data rather than "learning" to generalize from a trend.
- Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.
- Overfitting Example
  - If the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. Such a model, though, will typically fail severely when making predictions.
  - A noisy linear dataset can be fitted by a polynomial function also which would give a perfect fit. However, the linear function (in this case) would be better in extrapolating beyond the fitted data.
- To decrease the chance or amount of overfitting
  - model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout
  - To explicitly penalize overly complex models 
  - To evaluate the model performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter. 

## Dimension-reduction methods 

- These use the correlation structure among the predictor variables to accomplish the following: 
  - To reduce the number of predictor items. 
  - To help ensure that these predictor items are independent. 
  - To provide a framework for interpretability of the results. 

- Dimension-reduction methods: 
  - PCA
  - Factor analysis 
  - User-defined composites 
  
## PCA

```{definition 'PCA'}
\textcolor{pink}{Principal components analysis (PCA)} seeks to explain the correlation structure of a set of predictor variables ${m}$, using a smaller set of linear combinations of these variables, called components ${k}$. PCA acts solely on the predictor variables, and ignores the target variable.
```


- Suppose that the original variables ${\{X_1, X_2, \ldots, X_m\}}$ form a coordinate system in m-dimensional space. 
  - Let each variable ${X_i}$ represent an ${n \times 1}$ vector, where ${n}$ is number of records.
  - The principal components represent a new coordinate system, found by rotating the original system along the directions of maximum variability.
- Analysis
  - Standardize the data, so that the mean for each variable is zero, and the standard deviation is one.
  - ${X_i \to Z_i = \frac{X_i - {\mu}_i}{{\sigma}_{ii}}}$
  - The covariance is a measure of the degree to which two variables vary together.
  - A positive covariance indicates that, when one variable increases, the other tends to increase, while a negative covariance indicates that, when one variable increases, the other tends to decrease. 
  - ${{\sigma}_{ii}^2}$ denotes the variance of ${X_i}$. 
    - If ${X_i}$ and ${X_j}$ are independent, then ${{\sigma}_{ij}^2 = 0}$; but reverse may not be TRUE i.e. ${{\sigma}_{ij}^2 = 0}$ does not imply that ${X_i}$ and ${X_j}$ are independent. 
    - Note that the covariance measure is not scaled, so that changing the units of measure would change the value of the covariance.
    - The correlation coefficient ${r_{ij}} = \frac{{\sigma}_{ij}^2}{{\sigma}_{ii}{\sigma}_{jj}}$ avoids this difficulty by scaling the covariance by each of the standard deviations.
    - Then, the correlation matrix is denoted as ${\rho}$

```{definition 'Eigenvalues'}
Let \textcolor{pink}{$\mathbf{B}$} be an $m \times m$ matrix, and let \textcolor{pink}{$\mathbf{I}$} be the $m \times m$ identity matrix. Then the scalars $\{\lambda_1, \lambda_2, \ldots, \lambda_m\}$ are said to be the \textcolor{pink}{eigenvalues of $\mathbf{B}$} if they satisfy $|\mathbf{B} - \lambda \mathbf{I}| = 0$, where $|\mathbf{Q}|$ denotes the determinant of Q.
```

```{definition 'Eigenvector'}
Let \textcolor{pink}{$\mathbf{B}$} be an $m \times m$ matrix, and let ${\lambda}$ be an eigenvalue of $\mathbf{B}$. Then nonzero $m \times 1$ vector \textcolor{pink}{$\overrightarrow{e}$} is said to be an \textcolor{pink}{eigenvector of B}, if $\mathbf{B} \overrightarrow{e} = ùúÜ\overrightarrow{e}$.
```

- The total variability in the standardized set of predictors equals the sum of the variances of the Z-vectors, which equals the sum of the variances of the components, which equals the sum of the eigenvalues, which equals the numer of predictors
  - i.e. $\sum_{i=1}^m {\text{Var}({Y_i})} = \sum_{i=1}^m {\text{Var}({Z_i})} = \sum_{i=1}^m {\lambda_i} = m$
- The partial correlation between a given component and a given predictor variable is a function of an eigenvector and an eigenvalue. Specifically, $\text{Corr}(Y_i, Z_j) = e_{ij}\sqrt{\lambda_i}$, where $\{ (\lambda_1, e_1), (\lambda_2, e_2), \ldots, (\lambda_m, e_m)\}$ are the eigenvalue‚Äìeigenvector pairs for the correlation matrix $\rho$, and we note that $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_m$. In other words, the eigenvalues are ordered by size. (A partial correlation coefficient is a correlation coefficient that takes into account the effect of all the other variables.)
- The proportion of the total variability in Z that is explained by the $i^{\text{th}}$ principal component is the ratio of the $i^{\text{th}}$ eigenvalue to the number of variables, that is, the ratio $\frac{\lambda_i}{m}$.

## Data Housing

\textcolor{pink}{Please import the "C34-cadata.txt".} 

- Source: http://lib.stat.cmu.edu/datasets/houses.zip
- About: [20640, 9]
  - It provides census information from all the block groups from the 1990 California census. 
  - For this data set, a block group has an average of 1425.5 people living in an area that is geographically compact. 
  - Block groups were excluded that contained zero entries for any of the variables. 
  - Variables: median house value (Target), median income, housing median age, total rooms, total bedrooms, population, households, latitude, and longitude.


```{r 'C34-Housing', include=FALSE, eval=FALSE}
# #read_table() can handle double space delimited file.
tbl <- read_table(paste0(.z$XL, "C34-cadata.txt"), skip = 27, 
                col_names = c("median_house_value", "median_income", "housing_median_age", 
                              "total_rooms", "total_bedrooms", "population", "households", 
                              "latitude", "longitude"))
stopifnot(!"problems" %in% names(attributes(tbl)))
#problems(tbl)
#attr(tbl, "problems") <- NULL
attr(tbl, "spec") <- NULL
xxC34Housing <- tbl
f_setRDS(xxC34Housing)
```

```{r 'C34-GetHousing', include=FALSE}
bb <- aa <- xxC34Housing <- f_getRDS(xxC34Housing)  #wwww
```

## Partition Train & Test Data

```{r 'C34-Partition'}
set.seed(3)  #wwww
# #sample() and its variants can be used for Partitioning of Dataset
if(FALSE) {
  # #This approach is difficult to extend for 3 or more splits
  # #Further, Using floor() multiple times might result in loss of a row 
  train_idx <- sample.int(n = nrow(bb), size = floor(0.8 * nrow(bb)), replace = FALSE)
  train_idx <- sample(seq_len(nrow(bb)), size = floor(0.8 * nrow(bb)), replace = FALSE)  
  train_bb <- bb[train_idx, ]
  test_bb <- bb[-train_idx, ]
}
#
# #For 3 or more splits
#brk_bb = c(train = 0.8, test = 0.1, validate = 0.1)
brk_bb = c(train = 0.9, test = 0.1)
idx_bb = sample(cut(seq_len(nrow(bb)), nrow(bb) * cumsum(c(0, brk_bb)), labels = names(brk_bb)))
#
# #Splits by Fixed Numbers not Percentages
if(FALSE) {
  brk_bb = c(train = 18570, test = nrow(bb))
  idx_bb = sample(cut(seq_len(nrow(bb)), c(0, brk_bb), labels = names(brk_bb)))
}
# #List of Multiple Tibbles
part_l = split(bb, idx_bb)
#
# #nrow(), ncol(), dim() can be applied
vapply(part_l, nrow, FUN.VALUE = integer(1))
stopifnot(identical(nrow(bb), sum(vapply(part_l, nrow, FUN.VALUE = integer(1)))))
```

## Basics

```{r 'C34-Basic', include=FALSE}
xsyw <- part_l$train  #wwww
# #Separate Working Names and Display Names
if(FALSE) paste0(names(xsyw), collapse = ", ")
c_xsyw <- c("h_value", "income", "h_age", "rooms", "bedrooms", 
               "population", "households", "latitude", "longitude")
names(c_xsyw) <- c("House Value (Median)", "Income (Median)", "House Age (Median)", 
        "Rooms (Total)", "Bedrooms (Total)", "Population", "Households", "Latitude", "Longitude")
names(xsyw) <- c_xsyw
#
c_zsyw <- c_xsyw
names(c_zsyw) <- c("Target", "Income", "Age", "Rooms", "Beds", "Pop", "Houses", "Lat", "Long")
#
# #Scaling
zsyw <- xsyw %>% mutate(across(everything(), ~ as.vector(scale(.))))
# #Predictors Only
xw <- xsyw %>% select(-1)
zw <- zsyw %>% select(-1)
# #Long
f_wl(xsyw, zsyw, xw, zw)
#
# #Summary
xsyg <- xsyl %>% group_by(Keys) %>% 
  summarise(Min = min(Values), Max = max(Values), SD = sd(Values), 
            Mean = mean(Values), Median = median(Values), Mode = f_getMode(Values),
            Unique = length(unique(Values)), isNA = sum(is.na(Values)))
# #Relabel with Verification Before and After
levels(xsyg$Keys)
levels(xsyg$Keys) <- names(c_xsyw)
levels(xsyg$Keys)
#
# #Long to Wide: For Reference Only
if(FALSE) { zsyw <- zsyl %>% 
  group_by(Keys) %>% 
  mutate(ID = row_number()) %>% 
  pivot_wider(names_from = Keys, values_from = Values)
}
```

```{r 'C34T01', echo=FALSE}
# #Print Kable Table
hh <- xsyg %>% 
  #mutate(across(where(is.numeric), round, digits = 4)) %>% 
  mutate(across(where(is.numeric), format, digits = 3, 
                #nsmall = 0, replace.zero = TRUE, zero.print = "0", 
                drop0trailing = TRUE, scientific = FALSE)) 
#
cap_hh <- paste0("(C34T01)", "[", nrow(xsyw), ", ", ncol(xsyw), "] ", "Houses: Training Basics") 
f_pKbl(x = hh, caption = cap_hh, debug = FALSE)
```

## Boxplot

```{r 'C34-BoxScaledHouse', include=FALSE}
hh <- zsyl
levels(hh$Keys) <- names(c_xsyw)
#
ttl_hh <- "Houses: BoxPlots (Scaled)"
cap_hh <- "C34P01"
sub_hh <- NULL 
lgd_hh  <- NULL
```

```{r 'C34-ScaleBox', include=FALSE}
# #IN: hh(Keys, Values), 
C34 <- hh %>% { ggplot(data = ., mapping = aes(x = Keys, y = Values, fill = Keys)) +
    geom_boxplot() +
    k_gglayer_box +
    scale_y_continuous(breaks = breaks_pretty()) + 
    coord_flip() +
    theme(legend.position = 'none') +
    labs(x = NULL, y = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P01","-Houses-Box-Z", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P01, device = "png", dpi = 144) 
}
```

```{r 'C34P01', echo=FALSE, fig.cap="Houses: Boxplot (Scaled)"}
knitr::include_graphics(paste0(.z$PX, "C34P01","-Houses-Box-Z", ".png"))
```

## Normality

Note that normality of the data is not strictly required to perform non-inferential PCA but that strong departures from normality may diminish the observed correlations. As data mining applications usually do not involve inference, we will not worry about normality.

## Predictors SPLOM

- Rooms, bedrooms, population, and households all appear to be positively correlated. 
- Latitude and longitude appear to be negatively correlated. 
  - Scatter plot between them looks like the State of California
- House Median Age appears to be correlated the least with the other predictors

```{r 'C34-Predictors', include=FALSE}
hh <- zw
#hh <- hh[1:100, ]
labels_hh <- names(c_zsyw)[-1] #names(hh)
#
ttl_hh <- "Houses: SPLOM (8)"
cap_hh <- "C34P02"
sub_hh <- NULL 
lgd_hh  <- NULL 
```

```{r 'C34-SPLOM', include=FALSE, eval=FALSE}
# #IN: hh, labels_hh
C34 <- hh %>% { 
  ggpairs(data = ., mapping = aes(alpha = I(0.1)), columnLabels = labels_hh,
          upper = list(continuous = wrap("cor", size = 5, alpha = 1))) +
    labs(caption = cap_hh, subtitle = sub_hh, title = ttl_hh) 
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34-SPLOM-zw', include=FALSE, ref.label=c('C34-SPLOM')}
#
```

```{r 'C34P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P02", "-House-SPLOM-z", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P02, device = "png", dpi = 72, width = k_width, height = k_height) 
}
```

```{r 'C34P02', echo=FALSE, out.width='100%', fig.cap="Houses: SPLOM (8)"}
knitr::include_graphics(paste0(.z$PX, "C34P02", "-House-SPLOM-z", ".png"))
```

## Predictors Corplot

```{r 'C34-PredCor', include=FALSE, eval=FALSE}
# #Correlation Matrix | Table (Long) | Tibble
hh <- cor(zw) %>% as.table() %>% as_tibble(.name_repair = 'unique') %>% 
  #filter(...1 != ...2) %>% 
  filter(!duplicated(paste0(pmax(as.character(...1), as.character(...2)), 
                            pmin(as.character(...1), as.character(...2))))) %>% 
  mutate(across(where(is.character), factor, levels = c_zsyw[-1], labels = names(c_zsyw)[-1]))
#
ttl_hh <- "Houses: Corrplot (8)"
cap_hh <- "C34P03"
```

```{r 'C34-CorPlot', include=FALSE, eval=FALSE}
# #IN: hh (Correlation Tibble Long, Triangle with Diagonal) 
C34 <- hh %>% { ggplot(., aes(y = ...1, x = ...2, fill = n)) + 
    geom_tile(color = "white") + 
    geom_text(aes(label = round(n, 2)), color = "black", size = 4) +
    coord_fixed() +
    scale_fill_distiller(palette = "BrBG", direction = 1, limits = c(-1, 1)) +
    guides(fill = guide_colourbar(barwidth = 0.5, barheight = 15)) +
    theme(axis.title = element_blank(), 
          axis.line = element_blank(), 
          axis.ticks = element_blank(),
          panel.grid.major = element_blank(), 
          panel.border = element_blank()) +
	  labs(subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34-PredCor-A', ref.label=c('C34-PredCor', 'C34-CorPlot'), include=FALSE}
#
```

```{r 'C34P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P03","-House-Corplot-z", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P03, device = "png", dpi = 144) 
}
```

```{r 'C34P03', echo=FALSE, fig.cap="Houses: Corrplot (8)"}
knitr::include_graphics(paste0(.z$PX, "C34P03","-House-Corplot-z", ".png"))
```

## Correlation {.tabset .tabset-fade}

### Correlation Matrix {.unlisted .unnumbered}

```{r 'C34T02', echo=FALSE}
# #cor() produces a Matrix of Correlations: Redundant Triangle (Upper or Lower) and Diagonal
# #Print Kable Table
hh <- cor(zw)
cap_hh <- paste0("(C34T02) ", "Houses: Correlation Matrix") 
f_pKblM(x = hh, caption = cap_hh, negPos = c(-0.5, 0.5))
```

### Matrices {.unlisted .unnumbered}

- \textcolor{pink}{cor()} : Correlation Function produces a Matrix
  - Matirces has HUGE number of problems but unfortunately some function output is in that form
  - names() does NOT work on Matrices but colnames() works, even though names() is superior to colnames() in all other aspects
  - Symmatrical Matrix. Diagonal and one of the Triangles (Upper or Lower) are redundant
  - Too many decimal printing

```{r 'C34-Matrices'}
# #cor() produces a Matrix
ii <- cor(zw)
str(ii)
ii
#
# #We can eliminate Lower Triangle and Diagonal. However NA does not print well with format()
# #outcome of upper.tri() is easily compared to as.table(). lower.tri() will need extra step
#
# #Take advatage of Matrix Triangle and Set to 0 for later handling by format()
# #IF we remove the diagonal then dimensions gets haywire i.e. 8 to 7 columns left [28] elements
# #IF we keep the diagonal then dimensions gets haywire i.e. 8 to 9 columns left [36] elements
# #So, cannot use NA, has to use ZERO (So that, later, format can replace it.)
#
# #However, we finally went ahead with dplyr solution which handled NA separately from format()
# #Thus eliminating need of assigning 0, NA are being used for redundant triangle and diagonal
#
kk <- ii #ii is FULL 8x8 Matrix
kk[upper.tri(kk, diag = TRUE)] <- NA 
mm  <- kk %>% as.table() %>% as_tibble(.name_repair = 'unique') %>% drop_na() %>% 
  filter(n > 0.5 | n < -0.5) %>% arrange(desc(abs(n)))
#
kk <- ii #ii is FULL 8x8 Matrix
nn  <- kk %>% as.table() %>% as_tibble(.name_repair = 'unique') %>% 
    filter(...1 != ...2) %>% 
    filter(!duplicated(paste0(pmax(as.character(...1), as.character(...2)), 
                              pmin(as.character(...1), as.character(...2))))) %>%
  filter(n > 0.5 | n < -0.5) %>% arrange(desc(abs(n)))
stopifnot(identical(mm, nn))
#
# #However, above are long because of the usage of as.table(). For Wide:
ll <- kk %>% #as.table() %>% 
  as_tibble() %>% 
  mutate(ID = row_number()) 
# #Get Names
oo <- names(ll)
ll %>% mutate(across(-ID, ~ ifelse(ID <= match(cur_column(), oo), NA, .x))) %>% select(-ID)
```

### f_pKblM() {.unlisted .unnumbered}

```{r 'C34F-pKblM', eval=FALSE, ref.label=c('A12B-pKblM')}
#
```

### f_pKbl() {.unlisted .unnumbered}

```{r 'C34F-pKbl', eval=FALSE, ref.label=c('A12A-pKbl')}
#
```

## PCA {.tabset .tabset-fade}

- There are two general methods to perform PCA in R :
  - Spectral decomposition which examines the covariances / correlations between variables
    - \textcolor{pink}{princomp()}
      -  It uses divisor $N$ for the covariance matrix.
    - Names: "sdev, loadings, center, scale, n.obs, scores, call"
  - Singular value decomposition (SVD) which examines the covariances / correlations between individuals
    - SVD has slightly better numerical accuracy
    - \textcolor{pink}{prcomp()}
      - Unlike princomp, variances are computed with the usual divisor $N - 1$.
      - \textcolor{pink}{cov()} also uses $N - 1$
      - Use sdev or sqrt(Eigenvalues) to convert Eigenvectors into Loadings (For BOOK /psych Comparison)
    - Names: "sdev, rotation, center, scale, x"
- Output contains
  - SD of Principal Components
  - rotation / loadings: the matrix of variable loadings (columns are eigenvectors)
  - x / scores: The coordinates of the individuals (observations) on the principal components.
- Understanding the result
  - PCA was carried out on the eight predictors in the house data set.
  - PCA was carried out on the eight predictors in the house data set. The \textcolor{pink}{component matrix} is shown in Table \@ref(tab:C34T03). 
  - Each of the columns in Table represents one of the compnonents $Y_i = e_i^T\mathbf{Z}$.
  - The cell entries are called the \textcolor{pink}{component weights}, and represent the partial correlation between the variable and the component.
    - As the component weights are correlations, they range between one and negative one.
    - NOTE: Sign may differ from the Book. 
    - Eigenvalues are given by ${s}^2$ as shown in Table \@ref(tab:C34T04) 
    - First Eigenvalue is 3.9 and there are 8 predictor variables, thus, first component (PC1) explains $3.9/8 \approx 48\%$ of the variance
      - i.e. this single component by itself carries about half of the information in all eight predictors.
      - In general, the first principal component may be viewed as the single best summary of the correlations among the predictors. Specifically, this particular linear combination of the variables accounts for more variability than any other linear combination. 
      - The second principal component is the second-best linear combination of the variables, on the condition that it is orthogonal to the first principal component. It is derived from the variability that is left over, once the first component has been accounted for.


```{definition 'Orthogonal'}
Two vectors are \textcolor{pink}{orthogonal} if they are mathematically independent, have no correlation, and are at right angles to each other. 
```

### Component Matrix {.unlisted .unnumbered}

```{r 'C34-PCA'}
# #Perform PCA by prcomp() #wwww
#ii <- princomp(zw)
pca_zw <- prcomp(zw)
#
names(pca_zw)
#
# #Principal components have "loadings" i.e. $rotation and "scores" i.e. $x
# #Loadings specify the weight that each variable contributes to the principal component.
# #Scores show the value each sample has on each principal component.
#
dim(pca_zw$rotation)
dim(pca_zw$x)
#
# #Matrix Multiplication i.e. %*% of original variables with loadings gives scores
bb <- as.matrix(zw) %*% pca_zw$rotation
all.equal(bb, pca_zw$x)
identical(round(bb, 5), round(pca_zw$x, 5))
#
summary(pca_zw)$importance
#
pca_eigen <- summary(pca_zw)$importance %>% t() %>% as_tibble(rownames = "PCA") %>% 
  rename(SD = 2, pVar = 3, pVarCum = 4) %>% 
  mutate(EigenVal = SD^2, pVarManual = EigenVal/sum(EigenVal), 
         isOne = ifelse(EigenVal > 1, "Yes", "No"),
         isNinty = ifelse(pVarCum < 0.9, "Yes", "No"))
```

```{r 'C34T03', echo=FALSE}
# #Print Kable Table
hh <- pca_zw$rotation 
rownames(hh) <- names(c_xsyw)[-1]
#
cap_hh <- paste0("(C34T03) ", "Houses: PCA Component Matrix") 
f_pKblM(x = hh, caption = cap_hh, isTri = FALSE, debug = TRUE)
```

### Code {.unlisted .unnumbered}

```{r 'C34-PCA-A', eval=TRUE, ref.label=c('C34-PCA')}
#
```

### Loadings and Eigenvectors {.unlisted .unnumbered}

```{r 'C34-Loadings'}
#wwww
# #Component Matrix of prcomp() does not match with either BOOK or psych::principal()
# #prcomp() rotation contains eigenvectors not loadings. 
# #Loadings = Eigenvectors * sqrt(Eigenvalues) = Eigenvectors * sdev
#
# #psych::principal()
psy_zw <- principal(zw, nfactors = ncol(zw), rotate = 'none', scores = TRUE)
names(psy_zw)
#
#psy_zw$loadings
#
# #To Match them Multiply by SD = sqrt(Eigenvalues)
sd_pca <- summary(pca_zw)$sdev
eigen_pca <- sd_pca ^ 2
#
# #Multiply PC1 column with sqrt(Eigenvalue) of PC1 i.e. SD and so on
load_pca <- t(t(pca_zw$rotation) * sd_pca)
#
round(load_pca, 3)
round(psy_zw$loadings, 3)
```



## Orthogonality of PCA {.tabset .tabset-fade}

### Image {.unlisted .unnumbered}

```{r 'C34-PredCorPCA', include=FALSE}
# #Correlation Matrix | Table (Long) | Tibble
hh <- cor(pca_zw$x) %>% as.table() %>% as_tibble(.name_repair = 'unique') %>% 
  #filter(...1 != ...2) %>% 
  filter(!duplicated(paste0(pmax(as.character(...1), as.character(...2)), 
                            pmin(as.character(...1), as.character(...2))))) #%>% 
  #mutate(across(where(is.character), factor, levels = c_zsyw[-1], labels = names(c_zsyw)[-1]))
#
ttl_hh <- "Houses: PCA Corrplot - ALL are ZERO"
cap_hh <- "C34P04"
```

```{r 'C34-PredCorPCA-A', ref.label=c('C34-PredCorPCA', 'C34-CorPlot'), include=FALSE}
#
```

```{r 'C34P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P04","-House-Corplot-PCA", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P04, device = "png", dpi = 144) 
}
```

```{r 'C34P04', echo=FALSE, fig.cap="Houses: PCA Corrplot - ALL are ZERO"}
knitr::include_graphics(paste0(.z$PX, "C34P04","-House-Corplot-PCA", ".png"))
```

### Code {.unlisted .unnumbered}

```{r 'C34-PredCorPCA-B', eval=FALSE, ref.label=c('C34-PredCorPCA')}
#
```

## ScreePlot {.tabset .tabset-fade}

### Image {.unlisted .unnumbered}

```{r 'C34-Scree', include=FALSE}
hh <- pca_eigen
#
ttl_hh <- "Houses: PCA Eigenvalue ScreePlot"
cap_hh <- "C34P05"
y_hh <- "Eigenvalue"
```

```{r 'C34-ScreePlot', include=FALSE}
# #IN: hh 
C34 <- hh %>% { ggplot(., aes(x = PCA, y = EigenVal)) + 
    geom_point(aes(color = isOne), size = 3) +
    geom_line(aes(group = 1)) +
    geom_hline(aes(yintercept = 1), color = '#440154FF', linetype = "dashed") +
    annotate("segment", x = 3.5, xend = 3.1, y = 1.6, 
                    yend = 1.2, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("segment", x = 4.5, xend = 4.1, y = 1.3, 
                    yend = 0.9, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("segment", x = 5.5, xend = 5.1, y = 0.6, 
                    yend = 0.2, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    geom_text(data = tibble(x = c(3.5, 4.5, 5.5), y = c(1.7, 1.4, 0.7), 
              labels = c("Eigenvalue Criterion", "Screeplot Criterion", "Elbow Point")), 
              aes(x=x, y=y, label=labels), check_overlap = TRUE) + 
    scale_fill_distiller(palette = "BrBG") +
    #coord_fixed() +
    theme(legend.position = 'none') +
	  labs(y = y_hh, subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P05","-House-PCA-Scree", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P05, device = "png", dpi = 144) 
}
```

```{r 'C34P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C34P05","-House-PCA-Scree", ".png"))
```

```{r 'C34-CumVar', include=FALSE}
ttl_hh <- "Houses: PCA Proportion of Variance Explained"
cap_hh <- "C34P06"
y_hh <- NULL
```

```{r 'C34-CumVarPlot', include=FALSE}
# #IN: hh 
C34 <- hh %>% { ggplot(., aes(x = PCA, y = pVarCum)) + 
    geom_point(aes(color = isNinty), size = 3) +
    geom_line(aes(group = 1)) +
    geom_hline(aes(yintercept = 0.9), color = '#440154FF', linetype = "dashed") +
    annotate("segment", x = 4, xend = 4, y = 0.83, 
                    yend = 0.93, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    geom_text(data = tibble(x = 5.5, y = 0.8, labels = c("Proportion of Variance Covered Criterion")), 
              aes(x=x, y=y, label=labels), check_overlap = TRUE) + 
    scale_fill_distiller(palette = "BrBG") +
    scale_y_continuous(limits = c(0, 1), labels = percent) + 
    theme(legend.position = 'none') +
    labs(y = y_hh,
         subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P06","-House-PCA-Var", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P06, device = "png", dpi = 144) 
}
```

```{r 'C34P06', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C34P06","-House-PCA-Var", ".png"))
```

```{r 'C34P0506', echo=FALSE, ref.label=c('C34P05', 'C34P06'), fig.cap="House: PCA Screeplot with Variance"}
#
```


```{r 'C34T04', echo=FALSE}
# #Print Kable Table
hh <- pca_eigen %>% select(-pVarManual) %>% 
  rename(exp_Variance = 3, cum_Var = 4, isEigenOne = 6, isVarNinty = 7) %>% 
  #mutate(across(where(is.numeric), round, digits = 4)) %>% 
  add_row(summarise(., across(1, ~"Total")), summarise(., across(EigenVal, sum))) %>% 
  mutate(across(where(is.numeric), format, digits = 3, 
                #nsmall = 0, replace.zero = TRUE, zero.print = "0", 
                drop0trailing = TRUE, scientific = FALSE)) 
#
cap_hh <- paste0("(C34T04) ", "Houses: PCA Eigenvalues & Variance") 
f_pKbl(x = hh, caption = cap_hh, debug = FALSE)
```

### Code {.unlisted .unnumbered}

```{r 'C34-Scree-A', eval=FALSE, ref.label=c('C34-Scree', 'C34-ScreePlot', 'C34-CumVar', 'C34-CumVarPlot')}
#
```

## How many Components 

- In our example Single Component (PC1) can account for approximately half of the variability. But, all 8 accounts for 100% variability. So between 1 and 8 where is the cut-off
  - Criteria
    - The Eigenvalue Criterion 
    - The Proportion of Variance Explained Criterion 
    - The Minimum Communality Criterion - Deferred by Book
    - The Scree Plot Criterion

- The Eigenvalue Criterion
  - Refer Figure \@ref(fig:C34P0506) and Table \@ref(tab:C34T04)
  - Sum of the eigenvalues represents the number of variables entered into the PCA i.e. 8
  - An eigenvalue of 1 would then mean that the component would explain about "one variable worth" of the variability. 
  - Therefore, the eigenvalue criterion states that only components with eigenvalues greater than 1 should be retained. 
  - Note that, if there are fewer than 20 variables, the eigenvalue criterion tends to recommend extracting too few components, while, if there are more than 50 variables, this criterion may recommend extracting too many.
  - Thus in example: 3 can be retained. PC4 has value around 0.8 so it may or may not be retained.

- The Proportion of Variance Explained Criterion
  - We can define how much of the total variability that we would like the principal components to account for and then selects them acccordingly
  - Thus in example: 3 can be retained. PC4 will be selected if more than 90% should be accounted
  
- The Scree Plot Criterion
  - \textcolor{pink}{Elbow Point:} The maximum number of components that should be extracted is just before where the plot first begins to straighten out into a horizontal line.
  

## Factor Scores

- Modified Table \@ref(tab:C34T03) as \@ref(tab:C34T05) 
- To investigate the relationship between PC3 and PC4, and their constituent variables, we next consider \textcolor{pink}{factor scores}. Factor scores are estimated values of the factors for each observation, and are based on factor analysis.

```{r 'C34T05', echo=FALSE}
# #Print Kable Table
hh <- pca_zw$rotation[, 1:4] 
hh[abs(hh) < 0.15] <- NA
rownames(hh) <- names(c_zsyw)[-1]
#
cap_hh <- paste0("(C34T05) ", "Houses: PCA Eigenvectors upto PC4") 
f_pKblM(x = hh, caption = cap_hh, isTri = FALSE, dig = 3, debug = FALSE)
```

```{r 'C34-PC3', include=FALSE}
hh <- zw %>% select(income, h_age) %>% add_column(PC3 = pca_zw$x[, 3]) %>% 
  rename(Income = 1, Age = 2)
#hh <- hh[1:100, ]
labels_hh <- names(hh) #names(c_zsyw)[-1] #names(hh)
#
ttl_hh <- "Houses: SPLOM PC3 Factor Scores"
cap_hh <- "C34P07"
sub_hh <- NULL 
lgd_hh  <- NULL 
```

```{r 'C34-SPLOM-PC3', include=FALSE, ref.label=c('C34-SPLOM')}
#
```

```{r 'C34P07-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P07", "-House-SPLOM-PC3", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P07, device = "png", dpi = 144) 
}
```

```{r 'C34P07', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C34P07", "-House-SPLOM-PC3", ".png"))
```

```{r 'C34-PC4', include=FALSE}
hh <- zw %>% select(income, h_age) %>% add_column(PC4 = pca_zw$x[, 4]) %>% 
  rename(Income = 1, Age = 2)
#hh <- hh[1:100, ]
labels_hh <- names(hh) #names(c_zsyw)[-1] #names(hh)
#
ttl_hh <- "Houses: SPLOM PC4 Factor Scores"
cap_hh <- "C34P08"
sub_hh <- NULL 
lgd_hh  <- NULL 
```

```{r 'C34-SPLOM-PC4', include=FALSE, ref.label=c('C34-SPLOM')}
#
```

```{r 'C34P08-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P08", "-House-SPLOM-PC4", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P08, device = "png", dpi = 144) 
}
```

```{r 'C34P08', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C34P08", "-House-SPLOM-PC4", ".png"))
```


```{r 'C34P0708', echo=FALSE, ref.label=c('C34P07', 'C34P08'), fig.cap="Correlation Matrices of Factor Scores of PC3 and PC4"}
#
```

- Consider the left side of Figure \@ref(fig:C34P0708)
  - The strong negative correlation between component 3 and median income is strikingly evident
  - But the relationship between component 3 and housing median age is rather amorphous. It would be difficult to estimate the correlation between component 3 and housing median age as being 0.413, with only the scatter plot to guide us. 
- Similarly for the right side of Figure \@ref(fig:C34P0708)
  - The relationship between component 4 and housing median age is crystal clear
  - while the relationship between component 4 and median income is not entirely clear, reflecting its lower positive correlation of 0.374. 
  - We conclude, therefore, that the component weight of 0.413 for housing median age in component 3 is not of practical significance, and similarly for the component weight for median income in component 4.
- This discussion leads us to the following criterion for assessing the component weights. 
  - For a component weight to be considered of practical significance, it should exceed $\pm 0.5$ in magnitude. 
  - Note that the component weight represents the correlation between the component and the variable; thus, the squared component weight represents the amount of the total variability of the variable that is explained by the component. 
  - Thus, this threshold value of $\pm 0.5$ requires that at least 25% of the variance of the variable be explained by a particular component.
- Thus, the Table \@ref(tab:C34T03) is modified further as \@ref(tab:C34T06) 
  - NOTE: Sign differ from the Book but that is OK.
  - NOTE: Valeus should be matching when eigenvectors are converted to loadings
    - Note that the partition of the variables among the four components is \textcolor{pink}{mutually exclusive}, meaning that no variable is shared (after suppression) by any two components
    - and \textcolor{pink}{exhaustive}, meaning that all eight variables are contained in the four components.

```{r 'C34T06', echo=FALSE}
# #Print Kable Table
#hh <- pca_zw$rotation[, 1:4] 
hh <- load_pca[, 1:4] 
hh[abs(hh) < 0.5] <- NA
rownames(hh) <- names(c_zsyw)[-1]
#
cap_hh <- paste0("(C34T06) ", "Houses: PCA Loadings (not Eigenvectors) upto PC4") 
f_pKblM(x = hh, caption = cap_hh, isTri = FALSE, dig = 3, debug = FALSE)
```

## Matrix {.tabset .tabset-fade}

### Create {.unlisted .unnumbered}

```{r 'C34-Matrix'}
# #Create a Matrix
# #Default: The Matrix is Filled Column by Column
matrix(1:9, nrow = 3)
#
# #Change Behavour to fill the matrix by Row
matrix(1:9, nrow = 3, byrow = TRUE)
```

### Multiply Vector {.unlisted .unnumbered}

```{r 'C34-MultiplyVec'}
# #names() does not work on Matrix
mm <- matrix(1:9, ncol = 3, byrow = TRUE)
rownames(mm) <- tail(letters, 3)
colnames(mm) <- head(letters, 3)
mm
#
vv <- 1:3
#
# #Matrix Multipley (Deprecated)
# #Multiply the Vector with Matrix Rows i.e. x * 1, y * 2, z * 3
ii <- diag(vv) %*% mm #loss of rownames because it is taken from Left Side Matrix
#
# #Multiply the Vector with Matrix Columns i.e. a * 1, b * 2, c * 3
jj <- mm %*% diag(vv) #loss of colnames because it is taken from Right Side Matrix
#
# #Check Attributes
attributes(ii)$dimnames
attributes(jj)$dimnames
#
# #Add Missing RowNames or ColNames 
rownames(ii) <- rownames(mm)
colnames(jj) <- colnames(mm)
#
# #Coercing by as.integer() will produce a vector not matrix. Use eiher mode() or class()
#ii[] <- as.integer(ii) #Even using [] is NOT coercing to integer
class(ii) <- "integer"
mode(jj) <- "integer" 
#
# #Print
ii
str(ii)
jj
str(jj)
#
# #Equivalent: sweep() (Deprecated). SLOW, However it keeps the dimnames.  
swp_ii <- sweep(mm, MARGIN = 1, vv, `*`)
swp_jj <- sweep(mm, MARGIN = 2, vv, `*`)
#
# #Recommended:
# #Equivalent: R Recycle Vector Column-wise. So double-transpose is needed if multiplying on jj.
# #Double Transpose is FASTEST & retains dimnames. Bonus: This is Commutative.
rec_ii <- mm * vv
com_ii <- vv * mm 
rec_jj <- t(t(mm) * vv)
com_jj <- t(vv * t(mm))
all(identical(rec_ii, com_ii), identical(rec_jj, com_jj)) #Commutative
#
all(identical(ii, swp_ii), identical(jj, swp_jj), identical(ii, rec_ii), identical(jj, rec_jj))
```

### Square Elements {.unlisted .unnumbered}

```{r 'C34-MatSquare'}
mm <- matrix(1:9, ncol = 3, byrow = TRUE)
rownames(mm) <- tail(letters, 3)
colnames(mm) <- head(letters, 3)
mm
#
# #Square Each Element of the Matrix
mm ** 2 # ** operator is highly obscure and is actually parsed as ^ so use that not this one
mm ^ 2
stopifnot(identical(mm ^ 2, mm ** 2))
```


## Communalities

```{definition 'Communality'}
PCA does not extract all the variance from the variables, but only that proportion of the variance that is shared by several variables. \textcolor{pink}{Communality} represents the proportion of variance of a particular variable that is shared with other variables. Communality values are calculated as the sum of squared component weights, for a given variable.
```

- The communalities represent the overall importance of each of the variables in the PCA as a whole.
- Communalities that are very low for a particular variable should be an indication that the particular variable might not participate in the PCA solution (i.e., might not be a member of any of the principal components).
- Communalities less than 0.5 can be considered to be too low, as this would mean that the variable shares less than half of its variability in common with the other variables. 
  - Now, if we want to keep the variable housing median age as an active part of the analysis, then, extracting only three components would not be adequate, as housing median age shares only 35% of its variance with the other variables. To keep this variable in the analysis, we would need to extract the fourth component, which lifts the communality for housing median age over the 50% threshold. 
- Minimum Communality Criterion
  - Enough components should be extracted so that the communalities for each of these variables exceed a certain threshold (e.g., 50%)

```{r 'C34T07', echo=FALSE}
# #Print Kable Table
hh <- load_pca[, 1:4] 
# #Add New Columns by Squaring and Sum
hh <- cbind(hh, Comm_PC3 = rowSums(hh[, 1:3]^2), Comm_PC4 = rowSums(hh^2))
#hh[abs(hh) < 0.4] <- NA
rownames(hh) <- names(c_zsyw)[-1]
#
cap_hh <- paste0("(C34T07) ", "Houses: PCA Loadings with Communalities") 
f_pKblM(x = hh, caption = cap_hh, negPos = c(-0.5, 0.5), isTri = FALSE, dig = 2, debug = FALSE)
```

## Decision on How many Components

- The Eigenvalue Criterion recommended 3 components, but did not absolutely reject the 4 component. Also, for small numbers of variables, this criterion can underestimate the best number of components to extract.
- The Proportion of Variance Explained Criterion stated that we needed to use 4 components if we wanted to account for >90% of the variability. 
- The Scree Plot Criterion said not to exceed 4 components. 
- The Minimum Communality Criterion stated that, if we wanted to keep housing median age in the analysis, we had to extract 4 components.
- Conclusion: PC4 is included.

- Test Dataset
  - We can perform PCA on Test dataset also and that should provide us similar pattern of PC1 to PC4 in terms of eigenvectors and loadings (not exactly same but similar). This should be taken as the confirmation that PCA of Training Data can be applied on Test Data
  - If the results differ too much then it should be taken as the indication of traning data being not a representation of test data. 

## Factor Analysis

Factor Analysis (FA) is related to PCA but the two methods have different goals.

Principal components seek to identify orthogonal linear combinations of the variables, to be used either for descriptive purposes or to substitute a smaller number of uncorrelated components for the original variables. 

In contrast, factor analysis represents a model for the data, and as such is more elaborate. 

The \textcolor{pink}{factor analysis model} hypothesizes that the response vector ${\{X_1, X_2, \ldots, X_m\}}$ can be modeled as linear combinations of a smaller set of ${k}$ unobserved, "latent" random variables ${\{F_1, F_2, \ldots, F_k\}}$ called \textcolor{pink}{common factors}, along with an error term $\mathbf{\epsilon} = {\{\epsilon_1, \epsilon_2, \ldots, \epsilon_m\}}$. Specifically, the factor analysis model is :

\begin{equation}
  \underset{m \times 1}{\mathbf{X - \mu}} = \underset{m \times k}{\mathbf{L}} \, \underset{k \times 1}{\mathbf{F}} + \underset{m \times 1}{\mathbf{\epsilon}}
  (\#eq:fa)
\end{equation}


Where $\underset{m \times 1}{\mathbf{X - \mu}}$ is the response vector, centered by the mean vector, $\underset{m \times k}{\mathbf{L}}$ is the matrix of factor loadings, with $l_{ij}$ representing the factor loading of the $i^{\text{th}}$ variable on the $j^{\text{th}}$ factor, $\underset{k \times 1}{\mathbf{F}}$ represents the vector of unobservable common factors, and $\underset{m \times 1}{\mathbf{\epsilon}}$ represents the error vector. 

The factor analysis model differs from other models, such as the linear regression model, in that the predictor variables ${\{F_1, F_2, \ldots, F_k\}}$ are unobservable. Because so many terms are unobserved, further assumptions must be made before we may uncover the factors from the observed responses alone. 

These assumptions are that $E(\mathbf{F}) = \mathbf{0}, \text{Cov}(\mathbf{F}) = \mathbf{I}, E(\mathbf{\epsilon}) = \mathbf{0}, \text{Cov}(\mathbf{\epsilon})$ is a diagonal matrix. 

Unfortunately, the factor solutions provided by factor analysis are invariant to transformations. Hence, the factors uncovered by the model are in essence nonunique, without further constraints. This indistinctness provides the motivation for factor rotation.

## UCI Data Repository

- Repository of many standard datasets
  - Archieved: https://archive.ics.uci.edu/ml/datasets.php
  - Beta: https://archive-beta.ics.uci.edu/ml/datasets
  - The data is generally in ".data" format which is a CSV File
  - Details are generally in ".names" format which is a Text File

## Data Adult {.tabset .tabset-fade}

\textcolor{pink}{Please import the "C34-adult.csv".} 

- Source: https://archive-beta.ics.uci.edu/ml/datasets/adult
- About: Train [32561, 15] & Test[16281, 15] = Total[48842, 15]
  - The intended task is to find the set of demographic characteristics that can best predict whether or not the individual has an income of over 50000 dollars per year. 
- Steps [(External) Luke Perich](https://rpubs.com/s3589539/503258 "https://rpubs.com")
  - Merged Train and Test for easy and simultaneous cleaning. Source Column attached for easy separation later.
  - 'fnlwgt' (stands for final weight) - Dropped
    - It has no predictive power since it is a feature aimed to allocate similar weights to people with similar demographic characteristics.
  - 'income' 
    - Edited "." in Text and Modified to 0 (<= 50K) and 1 (>50K) as Factor for Summary
  - 'Education' is dropped 
    - It is just a label on 'education_num' (number of years of education). (Not Tested "ForLater")
  - 'marital_status'
    - Number of Levels reduced by merging 3 types of Married. Two of them have small count.
  - 'age' - num - Min 17 to Max 90 All Numbers are Present
  - 'education_num' - num - Min 1 to Max 16 All Numbers are Present
  - 'hours_per_week' - num - 
    - 3 Hours are missing but that can happen 
    - cannot remove 99 hours because 98 hours and other nearby hours are also present in the data
  - 'capital_gain' & 'capital_loss' - Both removed
    - After 41310 dollars, There is directly 99999 dollars which cannot be correct. It should be set to Median if it is kept
    - 44807 observations have 0 capital gain
    - 46560 observations have 0 capital loss
  - 'workclass', 'occupation', 'native_country'
    - These 3 contain Question Mark which have been converted to NA but not removed from dataset to have the possibility of imputation or to keep their other column variable information for analysis
  - 'native_country'
    - With huge bias towards US there is no point in having so many countries or regions even.
    - Changed to Binary Factors of USA and Other


```{r 'C34-GetAdult', include=FALSE}
bb <- aa <- xxC34Adult <- f_getRDS(xxC34Adult)  #wwww
```

### Import {.unlisted .unnumbered}

```{r 'C34-Adult', eval=FALSE, include=FALSE}
tbl <- read_csv(paste0(.z$XL, "C34-adult-train.csv"),
                col_names = c("age", "workclass", "fnlwgt", "education", "education_num",
                              "marital_status", "occupation", "relationship", "race", "sex",
                              "capital_gain", "capital_loss", "hours_per_week", "native_country",
                              "Income"), 
                show_col_types = FALSE)
attr(tbl, "problems") <- NULL
attr(tbl, "spec") <- NULL
tbl_aa <- tbl
# # 1st row needs to be skippe in Test
tbl <- read_csv(paste0(.z$XL, "C34-adult-test.csv"), skip = 1, 
                col_names = c("age", "workclass", "fnlwgt", "education", "education_num",
                              "marital_status", "occupation", "relationship", "race", "sex",
                              "capital_gain", "capital_loss", "hours_per_week", "native_country",
                              "Income"), 
                show_col_types = FALSE)
attr(tbl, "problems") <- NULL
attr(tbl, "spec") <- NULL
tbl_bb <- tbl
```

### Processing {.unlisted .unnumbered}

```{r 'C34-Process', eval=FALSE}
# #Merge Tibbles with ID Names in Column
# #NA Introduced by changing Question Mark to NA
aa <- bind_rows(Train = tbl_aa, Test = tbl_bb, .id = 'source') 
#
bb <- aa %>% 
  select(-c(fnlwgt, education, capital_gain, capital_loss)) %>% 
  mutate(Income = ifelse(Income == "<=50K" | Income == "<=50K.", "0", "1")) %>% 
  mutate(across(c(workclass, occupation, native_country), ~na_if(., "?"))) %>% 
  mutate(native_country = ifelse(str_detect(native_country, 
            paste0(c("United-", "Outlying-US"), collapse = "|")), "USA", "Other")) %>% 
  mutate(across(where(is.character), ~ factor(., levels = unique(.)))) %>% 
  mutate(marital_status = 
    fct_collapse(marital_status, 
      Married = c("Married-civ-spouse", "Married-spouse-absent", "Married-AF-spouse"))) 
#
xxC34Adult <- bb
f_setRDS(xxC34Adult)
```

### Merge Factor Levels {.unlisted .unnumbered}

```{r 'C34-Merge', eval=FALSE}
# #Levels of each Factor Variable
#lapply(aa[, sapply(aa, is.factor)], levels)
#levels(aa$marital_status)
summary(aa$marital_status)
ii <- aa %>% select(marital_status) %>% 
  mutate(marital_status = 
    fct_collapse(marital_status, 
      Married = c("Married-civ-spouse", "Married-spouse-absent", "Married-AF-spouse")))
#summary(ii$marital_status)
```

### Check Numeric {.unlisted .unnumbered}

```{r 'C34-Numeric', eval=FALSE}
# #Check Numeric Columns by summary()
if(TRUE) aa %>% select(!where(is.numeric)) %>% summary()
if(TRUE) sort(unique(aa$age))
if(TRUE) length(sort(unique(aa$age)))
if(TRUE) aa %>% count(age) %>% mutate(PROP = n/sum(n)) #%>% arrange(desc(n)) %>% head(10)
#
# #Find Missing Numbers in a Sequence of Numbers
#ii <- unique(aa$age) 
ii <- unique(aa$hours_per_week)
jj <- min(ii):max(ii)
jj[!jj %in% ii]
#
# #Equivalent
setdiff(jj, ii)
```

### Search String {.unlisted .unnumbered}

```{r 'C34-StrSearch', eval=FALSE}
# #To Search For Question Mark in All Factor Columns, Question Mark needs to be escaped
# #The Backslash used for escaping itself needs to be escaped using Backslash
aa %>% rowwise() %>%
  mutate(find_me = any(str_detect(c_across(where(is.factor)), 
                                  regex("\\?", ignore_case = TRUE)), na.rm = TRUE)) %>% 
  filter(find_me)
#
# #To Get the Column Names containing a String i.e. '?'
which(vapply(aa, function(x) any(stri_detect(x, regex = "\\?", max_count = 1)), logical(1)))
```

### Replace Mutiple Partial Matches {.unlisted .unnumbered}

```{r 'C34-StrReplace', eval=FALSE}
# #Search and Replace for Multiple Partial Matches 
# #NOTE: "|" should be used to collpase NOT " | "
# #NOTE: Question Marks Replaced as Other
aa %>% mutate(native_country = ifelse(str_detect(native_country, 
            paste0(c("United-", "Outlying-US"), collapse = "|")), "USA", "Other")) %>% 
  count(native_country)
```

### Check Factor {.unlisted .unnumbered}

```{r 'C34-Factor', eval=FALSE}
# #Check Factor Columns by summary()
if(TRUE) aa %>% select(!where(is.factor)) %>% summary()
ii <- factor(aa$native_country)
if(TRUE) levels(ii)
if(TRUE) nlevels(ii)
aa %>% count(native_country) #%>% tail(10) 
```

## Correlation Matrix 

Note that the correlations, although statistically significant in several cases, are overall much weaker than the correlations from the 'houses' data set. A weaker correlation structure should pose more of a challenge for the dimension-reduction method. 

NOTE: While the Book created 'Net Captial', I have skipped that because Capital Gain and Capital Loss Columns have extremely high number of zeroes. Further, an ID column 'fnlwgt' was also dropped.


```{r 'C34-PartAdult', include=FALSE}
#wwww
# #Select Numeric Variables and Break the Original Training Dataset into further Train & Test 
bb <- xxC34Adult %>% filter(source == "Train") %>% 
  select(age, edu = education_num, hours = hours_per_week, Income) 
set.seed(3)  
brk_bb = c(train = 25000, test = nrow(bb))
idx_bb = sample(cut(seq_len(nrow(bb)), c(0, brk_bb), labels = names(brk_bb)))
# #List of Multiple Tibbles
part_l = split(bb, idx_bb)
#
# #nrow(), ncol(), dim() can be applied
vapply(part_l, nrow, FUN.VALUE = integer(1))
stopifnot(identical(nrow(bb), sum(vapply(part_l, nrow, FUN.VALUE = integer(1)))))
#
# #Training Data
adl_xsyw <- part_l$train 
adl_xw <- adl_xsyw %>% select(-Income)
# #Scaling
adl_zsyw <- adl_xsyw %>% mutate(across(where(is.numeric), ~ as.vector(scale(.))))
adl_zw <- adl_zsyw %>% select(-Income)
# #Long
f_wl(adl_xw, adl_zw)
```

```{r 'C34T08', echo=FALSE}
hh <- cor(adl_zw)
cap_hh <- paste0("(C34T08) ", "Adult: Correlation Matrix") 
f_pKblM(x = hh, caption = cap_hh, negPos = c(-0.5, 0.5), dig = 3, debug = FALSE)
```

Factor analysis requires a certain level of correlation in order to function appropriately. The following tests have been developed to ascertain whether there exists sufficiently high correlation to perform factor analysis.

- Note, however, that statistical tests in the context of huge data sets can be misleading. With huge sample sizes, even the smallest effect sizes become statistically significant. This is why data mining methods rely on cross-validation methodologies, not statistical inference.

- The proportion of variability within the standardized predictor variables which is shared in common, and therefore might be caused by underlying factors, is measured by the \textcolor{pink}{Kaiser‚ÄìMeyer‚ÄìOlkin (KMO) Measure of Sampling Adequacy}. 
  - Values of the KMO statistic less than 0.50 indicate that factor analysis may not be appropriate. 
-  \textcolor{pink}{Bartlett Test of Sphericity} tests the null hypothesis that the correlation matrix is an identity matrix, that is, that the variables are really uncorrelated. 
  - The statistic reported is the p-value, so that very small values would indicate evidence against the null hypothesis, that is, the variables really are correlated. 
  - For p-values much larger than 0.10, there is insufficient evidence that the variables are correlated, and so factor analysis may not be suitable.
  - It compares an observed correlation matrix to the identity matrix. 
    - Essentially it checks to see if there is a certain redundancy between the variables that we can summarize with a few number of factors. 
    - The null hypothesis of the test is that the variables are orthogonal, i.e. not correlated. 
    - If the numbers in the matrix represent correlation coefficients, like Identity Matrix, it means that each variable is perfectly orthogonal (i.e. "uncorrelated") to every other variable and thus a data reduction technique like PCA or factor analysis would not be able to "compress" the data in any meaningful way. 

- The KMO statistic has a value of 0.52, which is not less than 0.5, meaning that this test does not find the level of correlation to be too low for factor analysis. 
- The p-value for Bartlett Test of Sphericity rounds to zero, so that the null hypothesis that no correlation exists among the variables is rejected. We therefore proceed with the factor analysis.

## KMO Test

```{r 'C34-KMO'}
# #KMO Test: Measure of Sampling Adequacy (MSA) 
KMO(cor(adl_zw))
```


## Bartlett Test of Sphericity

- \textcolor{orange}{Caution:} It is NOT same as 'Bartlett Test for Equality of Variances'
- This tests requires multivariate normality. If this condition is not met, KMO can still be used. 


```{r 'C34-BartSphere'}
# #Bartlett Test of Sphericity
bartsph <- cortest.bartlett(cor(adl_zw), n = nrow(adl_zw))
if(bartsph$p.value < 0.05) {
  cat("Null Rejected. Variables are Correlated. Dimension Reduction can be performed.\n")
} else {
  cat("Failed to reject the Null. Uncorrelated Variables. No benefit in Dimension Reduction.\n")
}
```

## FA {.tabset .tabset-fade}

- To allow us to view the results using a scatter plot, we decide a priori to extract only two factors.
- The following factor analysis is performed using the \textcolor{pink}{principal axis factoring} option. 

  - In principal axis factoring, an iterative procedure is used to estimate the communalities and the factor solution. 
  - This particular analysis required 152 such iterations before reaching convergence. 
  - The eigenvalues and the proportions of the variance explained by each factor are shown in Table

- \textcolor{pink}{fa()}
  - Trying with 'pa' as given in Book, even though 'pa' produces warnings whereas 'minres' does not
- \textcolor{orange}{Warning:} The estimated weights for the factor scores are probably incorrect.  Try a different factor score estimation method.
  - It is coming most probably because age and edu might be serially correlated i.e. 
    - As the Age is increasing, education might also increasing with a lag. Age values might be shifted forward in time with Education values
- \textcolor{orange}{Warning:} An ultra-Heywood case was detected.  Examine the results carefully
  - communality > 1
  - Heywood cases should be treated as invalid. 
  - Try to reduce the number of factors, try other initial communalities (in PAF method), try to drop variables with low KMO, check multicollinearity
  - These are encountered typically when there are too few variables to support the requested number of factors.
    - Both Warnings go away when requested factors were reduced from 3 to 2

```{conjecture 'CorMat'}
\textcolor{brown}{Error in if (prod(R2) < 0) { : missing value where TRUE/FALSE needed}}
```

- Add 'SMC = FALSE' in fa()

- NOTE
  - Cummulative Variance is only 48% i.e. less than half is explained by first two factors extracted.
    - In contrast, the Housing data had ~76% explained by first two factors because there the correlation was strong
  
### pa {.unlisted .unnumbered}

```{r 'C34-FA'}
adl_fa <- fa(adl_zw, nfactors = 2, #ncol(adl_zw)
   fm = "pa", rotate = "none", SMC = FALSE)
# #Loadings
adl_fa$loadings
#
# #Values
adl_fa$values
#
# #Communalities
adl_fa$communalities
```

### pa {.unlisted .unnumbered}

```{r 'C34-FA-PA'}
# #Warnings
# #The estimated weights for the factor scores are probably incorrect.  
# #Try a different factor score estimation method.
# #An ultra-Heywood case was detected.  Examine the results carefully
fa(adl_zw, nfactors = 2, #ncol(adl_zw)
   fm = "pa", rotate = "none", SMC = FALSE)
```

### With cor() {.unlisted .unnumbered}

```{r 'C34-FA-Cor'}
# #Warnings
# #The estimated weights for the factor scores are probably incorrect.  
# #Try a different factor score estimation method.
# #An ultra-Heywood case was detected.  Examine the results carefully
fa(cor(adl_zw), nfactors = 2, #ncol(adl_zw)
   fm = "pa", rotate = "none", n.obs = 25000, SMC = FALSE)
```

### minres {.unlisted .unnumbered}

```{r 'C34-FA-MinRes'}
# #No Warning
fa(adl_zw, nfactors = ncol(adl_zw), fm = "minres", rotate = "none")
```

## Factor Rotation

To assist in the interpretation of the factors, factor rotation may be performed. Factor rotation corresponds to a transformation (usually orthogonal) of the coordinate axes, leading to a different set of factor loadings.

The sharpest focus occurs when each variable has high factor loadings on a single factor, with low-to-moderate loadings on the other factors.

- "ForLater" Figure 4.6 Page 115 - Biplot

- No significant difference was observed with different Rotations, unlike BOOK.


```{r 'C34-BiPlot', include=FALSE, eval=FALSE}
# #This does not look good
# #Works but really bad. Limited Options. Arrow Colour is matched with points Not visible
#hh <- adl_fa$scores#[1:10, ]
biplot.psych(adl_fa, cex = 1, group = adl_zsyw[["Income"]], 
             col = viridis(2), 
             pch=c(21, 24)[adl_zsyw[["Income"]]], main="x")
#
```


```{r 'C34-Rotation'}
# #Huge number of Rotations are available including "none", "varimax", "quartimax", "equamax" ...
# #No significant difference observed
fa(adl_zw, nfactors = 2, fm = "pa", rotate = "none", SMC = FALSE)$loadings
#
fa(adl_zw, nfactors = 2, fm = "pa", rotate = "varimax", SMC = FALSE)$loadings
#
fa(adl_zw, nfactors = 2, fm = "pa", rotate = "quartimax", SMC = FALSE)$loadings
#
fa(adl_zw, nfactors = 2, fm = "pa", rotate = "equamax", SMC = FALSE)$loadings
```

## User Defined Composites


- User Defined Composites or Summated Scales
  - A user-defined composite is simply a linear combination of the variables, which combines several variables together into a single composite measure.
  - The simplest user-defined composite is simply the mean of the variables.
  - When compared to the use of individual variables, user-defined composites provide a way to diminish the effect of measurement error. 
    - Measurement error refers to the disparity between the observed variable values, and the "true" variable value. Measurement error contributes to the background error noise, interfering with the ability of models to accurately process the signal provided by the data, with the result that truly significant relationships may be missed. 
    - User-defined composites reduce measurement error by combining multiple variables into a single measure.
  - Appropriately constructed user-defined composites allow the analyst to represent the manifold aspects of a particular concept using a single measure. 
    - Thus, user-defined composites enable the analyst to embrace the range of model characteristics, while retaining the benefits of a parsimonious model.



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C34-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, c32churn, xxB16Cars, xxB18Churn, brk_bb, hh, idx_bb, part_l,
          xxC34Housing, C34P01, cap_hh, lgd_hh, loc_png, names_bb, sub_hh, ttl_hh, xsyg, xsyl, 
          zsyl, C34P02, mm, nn, oo, zsyw, c_xsyw, C34P03, C34P04, xl, xsyw, xw, zl, zw, c_zsyw, 
          labels_hh, C34P05, C34P06, pca_zw, y_hh, C34P07, C34P08, pca_eigen, com_ii, com_jj,
          eigen_pca, load_pca, psy_zw, rec_ii, rec_jj, sd_pca, swp_ii, swp_jj, vv, adl_fa, adl_xl,
          adl_xsyw, adl_xw, adl_zl, adl_zsyw, adl_zw, bartsph, xxC34Adult)
```

```{r 'C34-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
