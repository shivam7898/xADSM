# Continuous Probability Distributions {#c07}

```{r 'C07', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- This chapter covers 
  - [Sample](#sample-c07 "c07")
  - "ForLater" - Sampling Distribution of $\overline{p}$, Properties of Point Estimators, Other Sampling Methods

## Definitions (Ref)

```{r 'C07D01', comment="", echo=FALSE, results='asis'}
f_getDef("Elements")
```

```{r 'C07D02', comment="", echo=FALSE, results='asis'}
f_getDef("Variable")
```

```{r 'C07D03', comment="", echo=FALSE, results='asis'}
f_getDef("Population")
```

```{r 'C07D04', comment="", echo=FALSE, results='asis'}
f_getDef("Sample")
```

```{r 'C07D05', comment="", echo=FALSE, results='asis'}
f_getDef("Parameter-vs-Statistic")
```

## Sample {#sample-c07 .tabset .tabset-fade}

The sample contains only a portion of the population. Some sampling error is to be expected. So, the sample results provide only \textcolor{pink}{estimates} of the values of the corresponding population characteristics.

```{definition 'Sampled-Population'}
The \textcolor{pink}{sampled population} is the population from which the sample is drawn.
```

```{definition 'Frame'}
\textcolor{pink}{Frame} is a list of the elements that the sample will be selected from.
```


```{definition 'Target-Population'}
The \textcolor{pink}{target population} is the population we want to make inferences about. Generally (adn preferably), it will be same as 'Sampled-Population', but it may differ also.
```


```{definition 'SRS'}
A \textcolor{pink}{simple random sample (SRS)} is a set of $k$ objects in a population of $N$ objects where all possible samples are equally likely to happen. The number of such different simple random samples is $C_k^N$
```

```{definition 'Sampling-without-Replacement'}
\textcolor{pink}{Sampling without replacement:} Once an element has been included in the sample, it is removed from the population and cannot be selected a second time.
```

```{definition 'Sampling-with-Replacement'}
\textcolor{pink}{Sampling with replacement:} Once an element has been included in the sample, it is returned to the population. A previously selected element can be selected again and therefore may appear in the sample more than once.
```

- Infinite Population
  - Sometimes the population is infinitely large or the elements of the population are being generated by an ongoing process for which there is no limit on the number of elements that can be generated. 
  - Thus, it is not possible to develop a list of all the elements in the population. This is considered the \textcolor{pink}{infinite population} case. 
  - With an infinite population, we cannot select a 'simple random sample' because we cannot construct a frame consisting of all the elements. 
  - In the infinite population case, statisticians recommend selecting what is called a 'random sample'.
  
```{definition 'Random-Sample'}
A \textcolor{pink}{random sample} of size $n$ from an infinite population is a sample selected such that the following two conditions are satisfied. Each element selected comes from the same population. Each element is selected independently. The second condition prevents selection bias.
```

Random sample vs. SRS

- Random sample: every element of the population has a (nonzero) probability of being drawn.
  - each element does not necessarily have an equal chance of being chosen.
- SRS: every element of the population has the same (nonzero) probability of being drawn. 
  - SRS is thus a special case of a random sample.
  - SRS is a subset of a statistical population in which each member of the subset has an equal probability of being chosen.

- Elaboration of the Two conditions for Random Sample
  - Example: Consider a production line designed to fill boxes of a breakfast cereal.
    - Each element selected comes from the same population. 
      - To ensure this, the boxes must be selected at approximately the same point in time. 
      - This way the inspector avoids the possibility of selecting some boxes when the process is operating properly and other boxes when the process is not operating properly.
    - Each element is selected independently.
      - It is satisfied by designing the production process so that each box of cereal is filled independently.
  - Example: Consider the population of customers arriving at a fast-food restaurant. 
    - McDonald's, implemented a random sampling procedure for this situation. 
    - The sampling procedure was based on the fact that some customers presented discount coupons.
    - Whenever a customer presented a discount coupon, the next customer served was asked to complete a customer profile questionnaire. Because arriving customers presented discount coupons randomly and independently of other customers, this sampling procedure ensured that customers were selected independently.



## Point Estimation  {.tabset .tabset-fade}

```{definition 'Proportion'}
A \textcolor{pink}{population proportion $P$}, is a parameter that describes a percentage value associated with a population. It is given by $P = \frac{X}{N}$, where $X$ is the count of successes in the population, and $N$ is the size of the population. It is estimated through \textcolor{pink}{sample proportion $\overline{p} = \frac{x}{n}$}, where $x$ is the count of successes in the sample, and $n$ is the size of the sample obtained from the population.
```

```{definition 'Point-Estimation'}
To estimate the value of a \textcolor{pink}{population parameter}, we compute a corresponding characteristic of the sample, referred to as a \textcolor{pink}{sample statistic}. This process is called \textcolor{pink}{point estimation}.
```

```{definition 'Point-Estimator'}
A sample statistic is the \textcolor{pink}{point estimator} of the corresponding population parameter. For example, $\overline{x}, s, s^2, s_{xy}, r_{xy}$ sample statics are point estimators for corresponding population parameters of $\mu$ (mean), $\sigma$ (standard deviation), $\sigma^2$ (variance), $\sigma_{xy}$ (covariance), $\rho_{xy}$ (correlation)
```

```{definition 'Point-Estimate'}
The numerical value obtained for the sample statistic is called the \textcolor{pink}{point estimate}.
```

## Sampling Distributions

```{definition 'Sampling-Distribution'}
The \textcolor{pink}{sampling distribution of $\overline{x}$} is the probability distribution of all possible values of the sample mean $\overline{x}$.
```

Suppose, from a Population, we take a sample of size $n$ and calculate point estimate mean $\overline{x}_{1}$. Further, we can select another random sample from the Population and get another point estimate mean  $\overline{x}_{2}$. If we repeat this process for 500 times, we will have a frame of $\{\overline{x}_{1}, \overline{x}_{2}, \ldots, \overline{x}_{500}\}$.

If we consider the process of selecting a simple random sample as an experiment, the sample mean $\overline{x}$ is the numerical description of the outcome of the experiment. Thus, the sample mean $\overline{x}$ is a random variable. As a result, just like other random variables, $\overline{x}$ has a mean or expected value, a standard deviation, and a probability distribution. Because the various possible values of $\overline{x}$ are the result of different simple random samples, the probability distribution of $\overline{x}$ is called the \textcolor{pink}{sampling distribution of $\overline{x}$}. Knowledge of this sampling distribution and its properties will enable us to make probability statements about how close the sample mean $\overline{x}$ is to the population mean $\mu$.

Just as with other probability distributions, the sampling distribution of $\overline{x}$ has an expected value or mean, a standard deviation, and a characteristic shape or form.

### Mean

- Expected Value of $\overline{x}$
  - The mean of the $\overline{x}$ random variable is the expected value of $\overline{x}$. 
  - Let $E(\overline{x})$ represent the expected value of $\overline{x}$ and $\mu$ represent the mean of the population from which we are selecting a simple random sample. Then, \textcolor{pink}{$E(\overline{x}) = \mu$} 
  - When the expected value of a point estimator equals the population parameter, we say the point estimator is \textcolor{pink}{unbiased}. Thus, \textcolor{pink}{$\overline{x}$ is an unbiased estimator of the population mean  $\mu$}.

### Standard Deviation

- Standard Deviation of $\overline{x}$, \textcolor{pink}{$\sigma_{\overline{x}}$} is given by \@ref(eq:sigma-x-bar)
  - $\sqrt{\frac{N - n}{N-1}}$ is commonly referred to as the \textcolor{pink}{finite population correction factor}. With large population, it approaches 1
  - Thus, $\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}}$ becomes good approximation when the sample size is less than or equal to 5% of the population size; that is, \textcolor{pink}{$n/N \leq 0.05$}.
  - To further emphasize the difference between $\sigma_{\overline{x}}$ and $\sigma$, we refer to the standard deviation of $\overline{x}$, $\sigma_{\overline{x}}$, as the \textcolor{pink}{standard error of the mean}. In general, the term \textcolor{pink}{standard error} refers to the standard deviation of a point estimator. 
  - The standard error of the mean is helpful in determining how far the sample mean may be from the population mean.

\begin{equation}
  \begin{align}
    \text{Finite Population:} \sigma_{\overline{x}} &= \sqrt{\frac{N - n}{N-1}}\left(\frac{\sigma}{\sqrt{n}} \right) \\
    \text{Infinite Population:} \sigma_{\overline{x}} &= \frac{\sigma}{\sqrt{n}}
  \end{align}
  (\#eq:sigma-x-bar)
\end{equation}

### Form 

Form of the Sampling Distribution of $\overline{x}$

- When the population has a normal distribution, the sampling distribution of $\overline{x}$ is normally distributed for any sample size. 

- When the population from which we are selecting a random sample does not have a normal distribution, \textcolor{pink}{the central limit theorem} is helpful in identifying the shape of the sampling distribution of $\overline{x}$.

```{definition 'Central-Limit-Theorem'}
\textcolor{pink}{Central Limit Theorem:} In selecting random samples of size $n$ from a population, the sampling distribution of the sample mean  $\overline{x}$ can be approximated by a normal distribution as the sample size becomes large.
```

How large the sample size needs to be before the central limit theorem applies and we can assume that the shape of the sampling distribution is approximately normal

- For most applications, the sampling distribution of $\overline{x}$ can be approximated by a normal distribution whenever the sample is size 30 or more. 
- In cases where the population is highly skewed or outliers are present, samples of size 50 may be needed. 
- Finally, if the population is discrete, the sample size needed for a normal approximation often depends on the population proportion.


## Ex: EAI {.unlisted .unnumbered .tabset .tabset-fade}

Task of developing a profile of the company’s 2500 managers. The characteristics to be identified include the mean annual salary for the managers and the proportion of managers having completed a training.

- Population 
  - Population Size N = 2500 managers
  - Training: 1500/2500 managers have completed Training 
  - Salary: ${\mathcal {N}}_{(\mu = 51800,\, \sigma =4000)}$
  - Proportion of the population that completed the training program $p = \frac{1500}{2500} = 0.60$

- Suppose that a sample of 30 managers will be used. i.e. $n=30$ and 19 Yes for Training
  - Suppose, sample have ${\mathcal {N}}_{(\overline{x} = 51814,\, s =3348)}$
  - Also, $\overline{p} = \frac{x}{n} = \frac{19}{30} = 0.63$

- If 500 such samples are taken, where each have their own $\overline{x}$
  - Then their expected value $E(\overline{x}) = \mu = 51800$
  - Standard Error $\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}} = \frac{4000}{\sqrt{30}} = 730.3$

- Suppose the director believes the sample mean $\overline{x}$ will be an acceptable estimate of the population mean $\mu$ if the sample mean is within \$500 of the population mean. 
  - However, it is not possible to guarantee that the sample mean will be within \$500 of the population
  - We can reframe the request in probability terms i.e.
    - What is the probability that the sample mean computed using a simple random sample of 30 EAI managers will be within \$500 of the population mean
    - i.e. Probability that $\overline{x} \in [51300, 52300] $
      - For $z = \frac{\overline{x} - \mu}{\sigma_{\overline{x}}}$
      - For $\overline{x} = 52300 \Rightarrow z = \frac{52300 - 51800}{730.30} = 0.68$
      - For $\overline{x} = 51300 \Rightarrow z = \frac{51300 - 51800}{730.30} = -0.68$
  - $P_{(51300 \leq \overline{x} \leq 52300)} = P_{(\overline{x} \leq 52300)} - P_{(\overline{x} \leq 51300)} = P_{(z \leq 0.68)} - P_{(z \leq -0.68)} = 0.7517 - 0.2483 = 0.5034$
    - A simple random sample of 30 EAI managers has a 0.5034 probability of providing a sample mean $\overline{x}$ that is within \$500 of the population mean. 
      - Thus, there is a $1 − 0.5034 = 0.4966$ probability that the difference between $\overline{x}$ and $\mu$ will be more than \$500. 
      - In other words, a simple random sample of 30 EAI managers has roughly a 50–50 chance of providing a sample mean within the allowable $500. Perhaps a larger sample size should be considered. 
      - Let us explore this possibility by considering the relationship between the sample size and the sampling distribution of $\overline{x}$.

- Impact of $n = 100$ in place of $n =30$
  - First note that $E(\overline{x}) = \mu$ regardless of the sample size. Thus, the mean of all possible values of $\overline{x}$ is equal to the population mean $\mu$ regardless of the sample size $n$. 
  - However, standard error is reduced to $\sigma_{\overline{x}} = \frac{4000}{\sqrt{100}} = 400$
  - For $\overline{x} = 52300 \Rightarrow z = \frac{52300 - 51800}{400} = 1.25$
  - For $\overline{x} = 51300 \Rightarrow z = \frac{51300 - 51800}{400} = -1.25$
  - Thus $P_{(51300 \leq \overline{x} \leq 52300)} = P_{(\overline{x} \leq 52300)} - P_{(\overline{x} \leq 51300)} = P_{(z \leq 1.25)} - P_{(z \leq -1.25)} = 0.8944 - 0.1056 = 0.7888$
  - Thus, by increasing the sample size from 30 to 100 EAI managers, we increase the probability of obtaining a sample mean within \$500 of the population mean from 0.5034 to 0.7888.

\textcolor{orange}{Caution:} Here, we took advantage of the fact that the population mean $\mu$ and the population standard deviation $\sigma$ were known. However, usually these values will be unknown. 

## "ForLater" {.unlisted .unnumbered}


Properties of Point Estimators  

Three properties of good point estimators: unbiased, efficiency, and consistency.

$\theta = \text{the population parameter of interest}$
$\hat{\theta} = \text{the sample statistic or point estimator of } \theta$

- Unbiased
  - If the expected value of the sample statistic is equal to the population parameter being estimated, the sample statistic is said to be an \textcolor{pink}{unbiased estimator} of the population parameter
- Efficiency
  - When sampling from a normal population, the standard error of the sample mean is less than the standard error of the sample median. Thus, the sample mean is more efficient than the sample median.
- Consistency
  - A point estimator is consistent if the values of the point estimator tend to become closer to the population parameter as the sample size becomes larger. 

Other Sampling Methods

- Stratified Random Sampling
- Cluster Sampling
- Systematic Sampling
- Convenience Sampling
- Judgment Sampling

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C07-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C07-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects
f_()
#
difftime(Sys.time(), k_start)
```

****
