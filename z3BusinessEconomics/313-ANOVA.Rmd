# ANOVA {#c13}

```{r 'C13', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Experimental Design and Analysis of Variance"
  - "ForLater" - Exercises, Fisher LSD Onwards 

## Introduction

Example: Chemitech: Comparision of theree methods of assembly A, B, C in terms of most assemblies per week

- In this experiment
  - assembly method is the \textcolor{pink}{independent variable or factor}. 
  - Because three assembly methods correspond to this factor, we say that three treatments are associated with this experiment; each treatment corresponds to one of the three assembly methods. 
    - The three assembly methods or \textcolor{pink}{treatments} define the three \textcolor{pink}{populations of interest}
  - This is an example of a single-factor experiment; it involves one categorical factor (method of assembly)
  - For each population the \textcolor{pink}{dependent or response variable} is the number of filtration systems assembled per week, and the primary statistical objective of the experiment is to determine whether the mean number of units produced per week is the same for all three populations (methods).
- Suppose a random sample of three employees is selected from all assembly workers. 
  - The three randomly selected workers are the \textcolor{pink}{experimental units}. 
  - The experimental design that we will use is called a \textcolor{pink}{completely randomized design}.       - This type of design requires that each of the three assembly methods or treatments be assigned randomly to one of the experimental units or workers. 

```{definition 'Randomization'}
\textcolor{pink}{Randomization} is the process of assigning the treatments to the experimental units at random.
```

- Suppose that instead of selecting just three workers at random we selected 15 workers and then randomly assigned each of the three treatments to 5 of the workers. 
  - Because each method of assembly is assigned to 5 workers, we say that five \textcolor{pink}{replicates} have been obtained.
- As given in data, sample means for A, B, C are : $\{{\overline{x}}_1 = 62, {\overline{x}}_1 = 66, {\overline{x}}_1 = 52\}$
  - From these data, method B appears to result in higher production rates than either of the other methods.

```{r 'C13-Chemitech'}
xxChemitech <- tibble(A = c(58, 64, 55, 66, 67), 
                      B = c(58, 69, 71, 64, 68), 
                      C = c(48, 57, 59, 47, 49))
aa <- xxChemitech
# #Summary
aa %>% 
    pivot_longer(everything(), names_to = "key", values_to = "value") %>% 
    group_by(key) %>% 
    summarise(across(value, 
                     list(Count = length, Mean = mean, SD = sd, Variance = var), 
                     .names = "{.fn}"))

```


## Hypothesis

- The real issue is \textcolor{pink}{whether the three sample means observed are different enough} for us to conclude that the means of the populations corresponding to the three methods of assembly are different.
  - Let $\{{\mu}_1, {\mu}_2, {\mu}_3\}$ denote mean number of units produced per week using methods A, B and C
  - we want to use the sample means to test the following hypotheses:
  
```{r 'C13D01', comment="", echo=FALSE, results='asis'}
f_getDef("Hypo-ANOVA") #dddd
```

If ${H_0}$ is rejected, we cannot conclude that all population means are different. Rejecting ${H_0}$ means that at least two population means have different values.

## Assumptions for Analysis of Variance
  
Three assumptions are required to use analysis of variance. 

1. \textcolor{pink}{For each population, the response variable is normally distributed.}
    - Implication: In the example, the number of units produced per week (response variable) must be normally distributed for each assembly method. 
2. \textcolor{pink}{The variance of the response variable, ${\sigma}^2$, is the same for all of the populations.} 
    - Implication: In the example, the variance of the number of units produced per week must be the same for each assembly method. 
3. \textcolor{pink}{The observations must be independent.} 
    - Implication: In the example, the number of units produced per week for each employee must be independent of the number of units produced per week for any other employee.

If the sample sizes are equal, analysis of variance is not sensitive to departures from the assumption of normally distributed populations.

## Conceptual Overview

If the means for the three populations are equal, we would expect the three sample means to be close together. The more the sample means differ, the stronger the evidence we have for the conclusion that the population means differ. In other words, if the variability among the sample means is "small", it supports ${H_0}$; if the variability among the sample means is "large", it supports ${H_a}$.

If the null hypothesis is true, we can use the variability among the sample means to develop an estimate of ${\sigma}^2$. 

First, note that if the assumptions for analysis of variance are satisfied and the null hypothesis is true, each sample will have come from the same normal distribution with mean ${\mu}$ and variance ${\sigma}^2$. 

Recall that the sampling distribution of the sample mean ${\overline{x}}$ for a simple random sample of size ${n}$ from a normal population will be normally distributed with mean ${\mu}$ and variance ${\sigma}_{{\overline{x}}}^2 = \frac{{\sigma}^2}{n}$.

In this case, the mean and variance of the three sample mean values $\{{\overline{x}}_1 = 62, {\overline{x}}_1 = 66, {\overline{x}}_1 = 52\}$ can be used to estimate the mean and variance of the sampling distribution. 

When the sample sizes are equal, as in this example, the best estimate of the mean of the sampling distribution of ${\overline{x}}$ is the mean or average of the sample means. 

In this example, an estimate of the mean of the sampling distribution of ${\overline{x}}$ is $(62 + 66 + 52)/3 = 60$. We refer to this estimate as the \textcolor{pink}{overall sample mean}. Refer equation \@ref(eq:mean-all-n-anv)

An estimate of the variance of the sampling distribution of ${\overline{x}}$, ${\sigma}_{{\overline{x}}}^2$, is provided by the variance of the three sample means.

$${s}_{\overline{x}}^2 = \frac{(62 - 60)^2 + (66 - 60)^2 + (52 - 60)^2}{3 - 1} = 52$$
Because ${\sigma}^2 = n {\sigma}_{{\overline{x}}}^2$, the estimate can be given by 

$$E_{{\sigma}^2} = n E_{{\sigma}_{{\overline{x}}}^2} = n {s}_{\overline{x}}^2 = 5 * 52 = 260$$

The $n {s}_{\overline{x}}^2$ is referred as \textcolor{pink}{between-treatments estimate of ${\sigma}^2$.} It is based on the assumption that the null hypothesis is true. In this case, each sample comes from the same population, and there is only one sampling distribution of ${\overline{x}}$.

In contrast, when the population means are not equal, the between-treatments estimate will overestimate the population variance ${\sigma}^2$.

The variation within each of the samples also has an effect on the conclusion we reach in analysis of variance. When a simple random sample is selected from each population, each of the sample variances provides an unbiased estimate of ${\sigma}^2$. Hence, we can combine or pool the individual estimates of ${\sigma}^2$ into one overall estimate. The estimate of ${\sigma}^2$ obtained in this way is called the \textcolor{pink}{pooled or within-treatments estimate of ${\sigma}^2$}. 

Because each sample variance provides an estimate of ${\sigma}^2$ based only on the variation within each sample, the within-treatments estimate of ${\sigma}^2$ is not affected by whether the population means are equal. When the sample sizes are equal, the within-treatments estimate of ${\sigma}^2$ can be obtained by computing the average of the individual sample variances $\{27.5, 26.5, 31\}$. 

For this exmple we obtain:

$$\text{Within-treatments estimate of } {\sigma}^2 = \frac{27.5 + 26.5 + 31}{3} = 28.33$$
Remember, that the between-treatments approach provides a good estimate of ${\sigma}^2$ only if the null hypothesis is true; if the null hypothesis is false, the between-treatments approach overestimates ${\sigma}^2$. The within-treatments approach provides a good estimate of ${\sigma}^2$ in either case. 

Thus, if the null hypothesis is true, the two estimates will be similar and their ratio will be close to 1. 

If the null hypothesis is false, the between-treatments estimate will be larger than the within-treatments estimate, and their ratio will be large. 

## ANOVA

\textcolor{pink}{Analysis of Variance and the Completely Randomized Design}


```{definition 'Hypo-ANOVA'}
\textcolor{pink}{$\text{\{ANOVA\}} {H_0} : {\mu}_1 = {\mu}_2 = \dots = {\mu}_k \iff {H_a}: \text{Not all population means are equal}$}
```

where ${\mu}_j$ is mean of the $j^{\text{th}}$ population. We assume that a simple random sample of size ${n}_j$ has been selected from each of the ${k}$ populations or treatments. For the resulting sample data, let

\begin{equation}
  \begin{align}
  {x}_{ij} &= \text{value of observation } i \text{ for treatment } j \\
{n}_{j} &= \text{number of observations for treatment } j \\
{\overline{x}}_{j} &= \text{sample mean for treatment } j \\
{s}_{j}^2 &= \text{sample variance for treatment } j \\
{s}_{j} &= \text{sample e standard deviation for treatment } j
  \end{align}
\end{equation}

The formulas for the sample mean and sample variance for treatment ${j}$ are given in equations \@ref(eq:mean-anv) and \@ref(eq:var-anv)


\begin{equation}
  {\overline{x}}_j =  \frac{\sum_{i=1}^{n_j}{x}_{ij}}{{n}_j}
  (\#eq:mean-anv)
\end{equation}

\begin{equation}
  {s}_j^2 = \frac{\sum_{i=1}^{n_j}{\left({x}_{ij} - {\overline{x}}_j\right)^2}}{{n}_j - 1}
  (\#eq:var-anv)
\end{equation}

The overall sample mean, denoted ${\overline{\overline{x}}}$, is the sum of all the observations divided by the total number of observations. 


\begin{equation}
  {\bar{\bar{x}}} = \frac{\sum_{j=1}^k{\sum_{i=1}^{{n}_j}{{x}_{ij}}}}{{n}_T}
  (\#eq:mean-all-anv)
\end{equation}

Where

\begin{equation}
  {n}_T = {n}_1 + {n}_2 + \cdots + {n}_k
  (\#eq:n-anv)
\end{equation}

If the size of each sample is {n}, the equation \@ref(eq:n-anv) becomes ${n}_T = kn$, and the equation \@ref(eq:mean-all-anv) reduces to \@ref(eq:mean-all-n-anv)

\begin{equation}
  {\bar{\bar{x}}} = \frac{\sum_{j=1}^k{{\overline{x}}_{j}}}{k}
  (\#eq:mean-all-n-anv)
\end{equation}

Thus, whenever the sample sizes are the same, the overall sample mean is just the average of the ${k}$ sample means.

Thus, in the example, from \@ref(eq:mean-all-n-anv), ${\bar{\bar{x}}} = \frac{62 + 66 + 52}{3} = 60$

## MSTR

\textcolor{pink}{Between-Treatments Estimate of Population Variance}

The between-treatments estimate of ${\sigma}^2$ is called the \textcolor{pink}{mean square due to treatments} and is denoted \textcolor{pink}{$\text{MSTR}$}. Refer equation \@ref(eq:mstr-anv)

\begin{equation}
  \text{MSTR} = \frac{\text{SSTR}}{k - 1} = \frac{\sum_{j=1}^{k}{n}_j\left({\overline{x}}_j - {\bar{\bar{x}}} \right)^2}{k - 1}
  (\#eq:mstr-anv)
\end{equation}

The numerator in equation \@ref(eq:mstr-anv) is called the \textcolor{pink}{sum of squares due to treatments} and is denoted \textcolor{pink}{$\text{SSTR}$}. The denominator, ${k âˆ’ 1}$, represents the degrees of freedom associated with SSTR. Refer equation \@ref(eq:sstr-anv)

\begin{equation}
  \text{SSTR} = \sum_{j=1}^{k}{n}_j\left({\overline{x}}_j - {\bar{\bar{x}}} \right)^2
  (\#eq:sstr-anv)
\end{equation}

If ${H_0}$ is true, MSTR provides an unbiased estimate of ${\sigma}^2$. However, if the means of the ${k}$ populations are not equal, MSTR is not an unbiased estimate of ${\sigma}^2$; in fact, in that case, MSTR should overestimate ${\sigma}^2$.

In the example:

- \@ref(eq:sstr-anv) $\text{SSTR} = 5(62 - 60)^2 + 5(66 - 60)^2 + 5(52 - 60)^2 = 520$
- \@ref(eq:mstr-anv) $\text{MSTR} = \frac{520}{3 - 1} = 260$

## MSE

\textcolor{pink}{Within-Treatments Estimate of Population Variance}

The within-treatments estimate of ${\sigma}^2$ is called the \textcolor{pink}{mean square due to error} and is denoted \textcolor{pink}{$\text{MSE}$}. Refer equation \@ref(eq:mse-anv)

\begin{equation}
  \text{MSE} = \frac{\text{SSE}}{{n}_T - k} = \frac{\sum_{j=1}^{k}{({n}_j - 1){s}_j^2}}{{n}_T - k}
  (\#eq:mse-anv)
\end{equation}

The numerator in equation \@ref(eq:mse-anv) is called the \textcolor{pink}{sum of squares due to error} and is denoted \textcolor{pink}{$\text{SSE}$}. The denominator, ${{n}_T - k}$ is referred to as the degrees of freedom associated with SSE. Refer equation \@ref(eq:sse-anv)


\begin{equation}
  \text{SSE} = \sum_{j=1}^{k}{({n}_j - 1){s}_j^2}
  (\#eq:sse-anv)
\end{equation}

In the example:

- \@ref(eq:sse-anv) $\text{SSE} = (5 - 1)27.5 + (5 - 1)26.5 + (5 - 1)31 = 340$
- \@ref(eq:mse-anv) $\text{MSE} = \frac{340}{15 - 3} = 28.33$

## F test

\textcolor{pink}{Comparing the Variance Estimates}

If the null hypothesis is true, $\text{MSTR}$ and $\text{MSE}$ provide two independent, unbiased estimates of ${\sigma}^2$. 

Refer [Chapter: Variance](#c11 "c11").

We know that for normal populations, the sampling distribution of the ratio of two independent estimates of ${\sigma}^2$ follows an F distribution. Hence, if the null hypothesis is true and the ANOVA assumptions are valid, the sampling distribution of $\frac{\text{MSTR}}{\text{MSE}}$ is an F distribution with numerator degrees of freedom equal to ${k - 1}$ and denominator degrees of freedom equal to ${{n}_T - k}$. 

In other words, if the null hypothesis is true, the value of MSTR/MSE should appear to have been selected from this F distribution. However, if the null hypothesis is false, the value of $\frac{\text{MSTR}}{\text{MSE}}$ will be inflated because MSTR overestimates ${\sigma}^2$. Hence, we will reject ${H_0}$ if the resulting value of $\frac{\text{MSTR}}{\text{MSE}}$ appears to be too large to have been selected from an F distribution with ${k - 1}$ numerator degrees of freedom and ${{n}_T - k}$ denominator degrees of freedom. 

Because the decision to reject ${H_0}$ is based on the value of $\frac{\text{MSTR}}{\text{MSE}}$, the test statistic used to test for the equality of ${k}$ population means is given by equation \@ref(eq:f-anv)

\textcolor{pink}{Test Statistic for the Equality of ${k}$ Population Means :}


\begin{equation}
  F = \frac{\text{MSTR}}{\text{MSE}}
  (\#eq:f-anv)
\end{equation}


Because we will only reject the null hypothesis for large values of the test statistic, the p-value is the upper tail area of the F distribution to the right of the test statistic ${F}$.

- \textcolor{pink}{aov()}
  - Terms
    - Df degrees of freedom 
      - for the independent variable (levels - 1)
      - and for the residuals (total observations - 1 - levels)
    - Sum Sq shows the sum of squares 
      - SSTR: total variation between the group means 
      - SSE: and the overall mean
    - Mean Sq shows the mean of the sum of squares 
      - MSTR (Between): sum of squares / degrees of freedom for each parameter
      - MSE (Within): mean square of the residuals
    - F-value is the test statistic from the F test. 
      - Mean square of each independent variable / mean square of the residuals. 
      - The larger the F value, the more likely it is that the variation caused by the independent variable is real and not due to chance.
    - Pr(>F) is the p-value of the F-statistic. 
      - likelihood that the F-value calculated from the test would have occurred if the null hypothesis of no difference among group means were true.
- In the Example
  - Total Variance = Between or MSTR + Within or MSE
  - First Line (Column): $\text{DOF}_{(k-1)} = 2, \text{SSTR} = 520, \text{MSTR} = 260$
  - Residuals (Within) : $\text{DOF}_{(n-k)} = 12, \text{SSE} = 340, \text{MSE} = 28.33$
  - \@ref(eq:f-anv) $F = \frac{\text{MSTR}}{\text{MSE}} = 9.18$
- Calculate ${}^U\!P_{(F)}$
  - ${}^U\!P_{F = 9.18} = 0.0038$
    - `pf(q = 9.18, df1 = 2, df2 = 12, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^U\!P_{(F)} < {\alpha} \to {H_0}$ is rejected i.e. the means are different
  - The test provides sufficient evidence to conclude that the means of the three populations are not equal.
  - Analysis of variance supports the conclusion that the population mean number of units produced per week for the three assembly methods are not equal

## ANOVA Table

The sum of squares associated with the source of variation referred to as "Total" is called the \textcolor{pink}{total sum of squares (SST)}. SST divided by its degrees of freedom ${n}_T - 1$ is nothing more than the overall sample variance that would be obtained if we treated the entire set of 15 observations as one data set. With the entire data set as one sample. Refer equation \@ref(eq:sst-anv)


\begin{equation}
  \text{SST} = \text{SSTR} + \text{SSE} = \sum_{j=1}^k{\sum_{i=1}^{{n}_j}{\left( {x}_{ij} - \bar{\bar{x}}\right)^2}}
  (\#eq:sst-anv)
\end{equation}

The degrees of freedom associated with this total sum of squares is the sum of the degrees of freedom associated with the sum of squares due to treatments and the sum of squares due to error i.e. ${n}_T - 1 = (k - 1) + ({n}_T - k)$.

In other words, SST can be partitioned into two sums of squares: the sum of squares due to treatments and the sum of squares due to error. Note also that the degrees of freedom corresponding to SST, ${n}_T - 1$, can be partitioned into the degrees of freedom corresponding to SSTR, $k - 1$, and the degrees of freedom corresponding to SSE, ${n}_T - k$. 

The analysis of variance can be viewed as the process of partitioning the total sum of squares and the degrees of freedom into their corresponding sources: treatments and error. Dividing the sum of squares by the appropriate degrees of freedom provides the variance estimates, the F value, and the p-value used to test the hypothesis of equal population means.

The square root of MSE provides the best estimate of the population standard deviation ${\sigma}$. This estimate of ${\sigma}$ on the computer output is \textcolor{pink}{Pooled StDev}.


## Code

```{r 'C13-ANOVA'}
str(aa)
bb <- aa %>% pivot_longer(everything(), names_to = "key", values_to = "value")
# 
# #ANOVA
bb_anova <- aov(formula = value ~ key, data = bb)
#names(bb_anova)
#bb_anova
#
# #
model.tables(ii_aov, type = "means")
#
# #Summary
summary(bb_anova)
```








## Summary

Analysis of variance (ANOVA) can be used to test for differences among means of several populations or treatments. 

The completely randomized design and the randomized block design are used to draw conclusions about differences in the means of a single factor. The primary purpose of blocking in the randomized block design is to remove extraneous sources of variation from the error term. Such blocking provides a better estimate of the true error variance and a better test to determine whether the population or treatment means of the factor differ significantly. 

The basis for the statistical tests used in analysis of variance and experimental design is the development of two independent estimates of the population variance ${\sigma}^2$. In the single-factor case, one estimator is based on the variation between the treatments; this estimator provides an unbiased estimate of ${\sigma}^2$ only if the means $\{{\mu}_1, {\mu}_2, \ldots, {\mu}_k\}$ are all equal. A second estimator of ${\sigma}^2$ is based on the variation of the observations within each sample; this estimator will always provide an unbiased estimate of ${\sigma}^2$. 

By computing the ratio of these two estimators (the F statistic), it is determined whether to reject the null hypothesis that the population or treatment means are equal.

In all the experimental designs considered, the partitioning of the sum of squares and degrees of freedom into their various sources enabled us to compute the appropriate values for the analysis of variance calculations and tests. 

Further, Fisher LSD procedure and the Bonferroni adjustment can be used to perform pairwise comparisons to determine which means are different.

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C13-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C13-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
