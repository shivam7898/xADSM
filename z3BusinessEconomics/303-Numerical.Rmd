# Numerical Measures {#c03}

```{r 'C03', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
invisible(lapply(f_getPathR(A09isPrime, A10getUtil), knitr::read_chunk))
```

## Overview

- This chapter covers 
  - Side Topic: [Number Theory](#numbers-c03 "c03"), [Primes](#primes-c03 "c03"), [Benchmark](#benchmark-c03 "c03"), Vectorising & Compiling & Profiling Functions,
  - Measures of Location: [Mean](mean-c03 "c03"), Weighted Mean, [Median](#median-c03 "c03"), Geometric Mean, [Mode](#mode-c03 "c03"), [Percentiles](#percentiles-c03 "c03"), Quantiles
  - Measures of Variability: Range, IQR, [Variance](#variance-c03 "c03"), [Standard Deviation](#sd-c03 "c03")
  - Shape [Skewness](#skewness-c03 "c03"), [Kurtosis]({#kurtosis-c03 "c03")
  - Relative Location: [z-scores](#z-scores-c03 "c03"), Chebyshev Theorem, [Empirical Rule](#empirical-c03 "c03"), [Outliers](#outliers-c03 "c03")
  - Two Variables: [Covariance](#covariance-c03 "c03"), [Correlation Coefficient](#correlation-c03 "c03")
  - Graphs: [BoxPlot](#boxplot-c03 "c03"), [Scatterplot](#covariance-c03 "c03")
  - "ForLater" - Sum Mean Formulae, Skewness

## Definitions (Ref)

```{r 'C03D01', comment="", echo=FALSE, results='asis'}
f_getDef("Parameter-vs-Statistic") #dddd
```

## Number Theory {#numbers-c03 .tabset .tabset-fade}

```{definition 'Number'}
A \textcolor{pink}{number} is a mathematical object used to count, measure, and label. Their study or usage is called \textcolor{pink}{arithmetic}, a term which may also refer to \textcolor{pink}{number theory}, the study of the properties of numbers. 
```

Individual numbers can be represented by symbols, called \textcolor{pink}{numerals}; for example, "5" is a numeral that represents the 'number five'. 

As only a relatively small number of symbols can be memorized, basic numerals are commonly organized in a \textcolor{pink}{numeral system}, which is an organized way to represent any number. The most common numeral system is the \textcolor{pink}{Hindu–Arabic numeral system}, which allows for the representation of any number using a combination of ten fundamental numeric symbols, called \textcolor{pink}{digits}.

\textcolor{pink}{Counting} is the process of determining the number of elements of a finite set of objects, i.e., determining the size of a set. \textcolor{pink}{Enumeration} refers to uniquely identifying the elements of a set by assigning a number to each element. 

\textcolor{pink}{Measurement} is the quantification of attributes of an object or event, which can be used to compare with other objects or events.

```{r 'C03-Sets-A', include=FALSE}
# #NOT Working in Bookdown
# # For Infinity '\u221e'
cat(paste0(" \u2115 (0,1,2,...) \u2282 \u2124 (-1,0,1) \u2282 \u211a (-0.5,+0.5) \u2282 \u211d (\u221a2, e(\u2091), \u03c0) \u2282 \u2102 (a +i b) "))
```

### Sets {.unlisted .unnumbered}

> Formally, $\mathbb{N} \to \mathbb{Z} \to \mathbb{Q} \to \mathbb{R} \to \mathbb{C}$  
> Practically, $\mathbb{N} \subset \mathbb{Z} \subset \mathbb{Q} \subset \mathbb{R} \subset \mathbb{C}$

The \textcolor{pink}{natural numbers $\mathbb{N}$} are those numbers used for counting and ordering. ISO standard begin the natural numbers with 0, corresponding to the \textcolor{pink}{non-negative integers $\mathbb{N} = \{0, 1, 2, 3, \ldots \}$}, whereas others start with 1, corresponding to the \textcolor{pink}{positive integers $\mathbb{N^*} = \{1, 2, 3, \ldots \}$}

The set of \textcolor{pink}{integers $\mathbb{Z}$} consists of zero (${0}$), the positive natural numbers $\{1, 2, 3, \ldots \}$ and their additive inverses (the negative integers). Thus i.e., \textcolor{pink}{$\mathbb{Z} = \{\ldots, -3, -2, -1, 0, 1, 2, 3, \ldots \}$}. An integer is colloquially defined as a number that can be written without a fractional component. 

\textcolor{pink}{Rational numbers $\mathbb{Q}$} are those which can be expressed as the quotient or fraction p/q of two integers, a numerator p and a non-zero denominator q. Thus, \textcolor{pink}{Rational Numbers $\mathbb{Q} = \{1, 2, 3, \ldots \}$}

A \textcolor{pink}{real number} is a value of a continuous quantity that can represent a distance along a line. The real numbers include all the rational numbers $\mathbb{Q}$, and all the irrational numbers. Thus, \textcolor{pink}{Real Numbers $\mathbb{R} = \mathbb{Q} \cup \{\sqrt{2}, \sqrt{3}, \ldots\} \cup \{ \pi, e, \phi, \ldots \}$}

The \textcolor{pink}{complex numbers $\mathbb{C}$} contain numbers which are expressed in the form $a + ib$, where ${a}$ and ${b}$ are real numbers. These have two components the real numbers and a specific element denoted by ${i}$ (imaginary unit) which satisfies the equation \textcolor{pink}{$i^2 = −1$}.

### Pi {.unlisted .unnumbered}

The number \textcolor{pink}{Pi $\pi = 3.14159\ldots$} is defined as the ratio of circumference of a circle to its diameter.

\begin{equation} 
  \pi = \int _{-1}^{1} \frac{dx}{\sqrt{1- x^2}}
  (\#eq:pi)
\end{equation} 

\begin{equation} 
  e^{i\varphi}=\cos \varphi +i\sin \varphi 
  (\#eq:euler-formula)
\end{equation} 

\begin{equation} 
  e^{i\pi} + 1 =0
  (\#eq:euler-identity)
\end{equation} 

```{r 'C03-PI'}
# #Read OIS File for 20000 PI digits including integral (3) and fractional (14159...)
# #md5sum = "daf0b33a67fd842a905bb577957a9c7f"
tbl <- read_delim(file = paste0(.z$XL, "PI-OIS-b000796.txt"), 
  delim = " ", col_names = c("POS", "VAL"), col_types = list(POS = "i", VAL = "i"))
attr(tbl, "spec") <- NULL
attr(tbl, "problems") <- NULL
xxPI <- tbl
f_setRDS(xxPI)
```

### e {.unlisted .unnumbered}

Euler Number \textcolor{pink}{$e = 2.71828\ldots$}, is the base of the natural logarithm. 

\begin{equation} 
  e = \lim_{n \to \infty} \left(1 + \frac{1}{n} \right)^{n} = \sum \limits_{n=0}^{\infty} \frac{1}{n!}
  (\#eq:euler-number-e)
\end{equation} 

### Phi {.unlisted .unnumbered}

Two quantities are in the \textcolor{pink}{golden ratio $\varphi = 1.618\ldots$} if their ratio is the same as the ratio of their sum to the larger of the two quantities.

\begin{equation} 
  \varphi^2 - \varphi -1 =0 \\
  \varphi = \frac{1+\sqrt{5}}{2}
  (\#eq:golden-ratio-phi)
\end{equation} 

### Groups {.unlisted .unnumbered}

```{definition 'Prime'}
A \textcolor{pink}{prime number} is a natural number greater than 1 that is not a product of two smaller natural numbers. A natural number greater than 1 that is not prime is called a 'composite number'. 1 is neither a Prime nor a composite, it is a 'Unit'. Thus, by definition, Negative Integers and Zero cannot be Prime.
```

```{definition 'Parity-Odd-Even'}
\textcolor{pink}{Parity} is the property of an integer $\mathbb{Z}$ of whether it is even or odd. It is \textcolor{pink}{even} if the integer is divisible by 2 with no remainders left and it is \textcolor{pink}{odd} otherwise. Thus, -2, 0, +2 are even but -1, 1 are odd. Numbers ending with 0, 2, 4, 6, 8 are even. Numbers ending with 1, 3, 5, 7, 9 are odd.
```

```{definition 'Positive-Negative'}
An integer $\mathbb{Z}$ is \textcolor{pink}{positive} if it is greater than zero, and \textcolor{pink}{negative} if it is less than zero. Zero is defined as neither negative nor positive. 
```

```{definition 'Mersenne-Primes'}
\textcolor{pink}{Mersenne primes} are those prime number that are of the form \textcolor{pink}{$(2^n -1)$}; that is, $\{3, 7, 31, 127, \ldots \}$
```


Mersenne primes:

- $\{3, 7, 31, 127, 8191, 131071, 524287, 2147483647, 2305843009213693951, \ldots \}$
- $\{3 (2^{nd}), 7(4^{th}), 31(11^{th}), 127(31^{st}), 8191 (1028^{th}), 131071 (12,251^{th}), 524287 (43,390^{th}), \ldots \}$  
  - Mersenne primes with their position in List of Primes
- \textcolor{pink}{$2147483647 = (2^{231} − 1)$}
  - It is 105,097,565$^{th}$ Prime, $8^{th}$ Mersenne prime and is one of only four known double Mersenne primes.
  - It represents the largest value that a signed 32-bit integer field can hold. 

## Primes {#primes-c03 .tabset .tabset-fade}

### Empty Vector {.unlisted .unnumbered}

```{r 'C03-EmptyVector'}
# #Create empty Vector with NA
aa <- rep(NA_integer_, 10)
print(aa)
```

### f_isPrime() {.unlisted .unnumbered}

```{r 'C03F-isPrime', ref.label=c('A09A-isPrime', 'A09B-isPrimeV', 'A09C-isPrimeC'), eval=FALSE }
#
```

### Primes {.unlisted .unnumbered}

```{r 'C03-ListPrime', eval=FALSE}
# #There are 4 Primes in First 10, 25 in 100, 168 in 1000, 1229 in 10000.
# # Using Vectorise Version, get all the Primes
aa <- 1:10
bb <- aa[f_isPrimeV(aa)]
ii <- f_getPrimeUpto(10)
stopifnot(identical(bb, ii))
# #
xxPrime10 <- c(2, 3, 5, 7) |> as.integer()
# #
xxPrime100 <- c(2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 
               53, 59, 61, 67, 71, 73, 79, 83, 89, 97)  |> as.integer()
#
# #Generate List of ALL Primes till 524287 (i.e. Total 43,390 Primes)
xxPrimes <- f_getPrimeUpto(524287L)
# #Save as RDS
f_setRDS(xxPrimes)
```

### Large Integers {.unlisted .unnumbered}

```{r 'C03-LargeNumbers'}
# #NOTE: Assigning 2147483647L causes the Chunk to throw Warnings even with 'eval=FALSE'.
if(FALSE){
# #Assignment of 2305843009213693951L is NOT possible without Warning
# #Even within non-executing Block or with 'eval=FALSE' or suppressWarnings() or tryCatch()
# #It cannot be stored as integer, thus it is automatically converted to double
  #bb <- 2305843009213693951L
# #Warning: non-integer value 2305843009213693951L qualified with L; using numeric value 
# #NOTE that the value changed. It is explicitly NOT a prime anymore.
  #print(aa, digits = 20)
# #[1] 2305843009213693952
#
# #Assignment of 2147483647L is possible and direct printing in console works BUT
# #Its printing will also throw Warnings that are difficult to handle
# #Avoid Printing. Even within non-executing Block, it is affecting R Bookdown.
  aa <- 2147483647L
  #print(aa)
}
```

### f_getPrime() {.unlisted .unnumbered}

```{r 'C03F-getPrimeUpto', ref.label=c('A09D-getPrimeUpto'), eval=FALSE}
#
```

### Benchmark {#benchmark-c03 .unlisted .unnumbered}

```{r 'C03-Benchmark', eval=FALSE}
# #Compare any number of functions
result <- microbenchmark(
  sum(1:100)/length(1:100), 
  mean(1:100),
  #times = 1000,
  check = 'identical'
)
# #Print Table
print(result)
##Unit: microseconds
##                     expr min    lq    mean median     uq    max neval cld
## sum(1:100)/length(1:100) 1.2 1.301 1.54795 1.5005 1.6005  7.501   100  a 
##              mean(1:100) 5.9 6.001 6.56989 6.1010 6.2010 28.001   100   b
#
# #Boxplot of Benchmarking Result
#autoplot(result)
# #Above testcase showed a surprising result of sum()/length() being much faster than mean()
#
# #Or Compare Plot Rendering
if(FALSE) microbenchmark(print(jj), print(kk), print(ll), times = 2)
```

### Sum-Mean {.unlisted .unnumbered}

"ForLater" - Include `rowsum(), rowSums(), colSums(), rowMeans(), colMeans()` in this also.

```{r 'C03-SumMean'}
# #Conclusion: use mean() because precision is difficult to achieve compared to speed
#
# #sum()/length() is faster than mean()
# #However, mean() does double pass, so it would be more accurate
# #mean.default() and var() compute means with an additional pass and so are more accurate
# #e.g. the variance of a constant vector is (almost) always zero 
# #and the mean of such a vector will be equal to the constant value to machine precision.
aa <- 1:100
#
microbenchmark(
  sum(aa)/length(aa), 
  mean(aa),
  mean.default(aa),
  .Internal(mean(aa)),
  #times = 1000,
  check = 'identical'
)
# #rnorm() generates random deviates of given length
set.seed(3)
aa <- rnorm(1e7)
str(aa)
#
# #NOTE manual calculation and mean() is NOT matching
identical(sum(aa)/length(aa), mean(aa))
#
# #There is a slight difference
sum(aa)/length(aa) - mean(aa)
```

### Remove Objects  {.unlisted .unnumbered}

```{r 'C03-rmPattern', eval=FALSE}
if(FALSE) {
  # #Remove all objects matching a pattern
  rm(list = ls(pattern = "f_"))
}
```

### Options Memory {.unlisted .unnumbered}

```{r 'C03-Memory'}
# #Check the Current Options Value
getOption("expressions")
if(FALSE) {
  # #Change Value
  # #NOTE it did not help when recursive function failed
  # #Error: node stack overflow
  # #Error during wrapup: node stack overflow
  # #Error: no more error handlers available ...
  options(expressions=10000)
}
```

### Vectorize() {.unlisted .unnumbered}

```{r 'C03-Vectorize', eval=FALSE}
# #To Vectorise a Function
f_isPrimeV <- Vectorize(f_isPrime)
```

### Compiling {.unlisted .unnumbered}

```{r 'C03-Compile', eval=FALSE}
# #To Pre-Compile a Function for faster performance
f_isPrimeC <- cmpfun(f_isPrime)
```

### Profiling {.unlisted .unnumbered}

```{r 'C03-Profile', eval=FALSE}
# #To Profile a Function Calls for improvements
Rprof("file.out")
f_isPrime(2147483647L)
#f_getPrimesUpto(131071L)
Rprof(NULL)
summaryRprof("file.out")
```

### Legacy A {.unlisted .unnumbered}

```{r 'C03-isPrime-NOT', eval=FALSE}
# #Functions to check for PRIME - All of them have various problems
# #"-3L -2L -1L 0L 1L 8L" FALSE "2L 3L ... 524287L 2147483647L" TRUE
isPrime_a <- function(x) {
  # #Fails for "2147483647L" Error: cannot allocate vector of size 8.0 Gb
  if (x == 2L) {
    return(TRUE)
  } else if (any(x %% 2:(x-1) == 0)) {
    return(FALSE)
  } else return(TRUE)
}

isPrime_b <- function(x){
  # #Comparison of Division and Integer Division by 1,2,...,x
  # #Fails for "2147483647L" Error: cannot allocate vector of size 16.0 Gb
  # #Fails for "-ve and zero" Error: missing value where TRUE/FALSE needed
  # vapply(x, function(y) sum(y / 1:y == y %/% 1:y), integer(1L)) == 2L
  if(sum(x / 1:x == x %/% 1:x) == 2) {
    return(TRUE) 
  } else return(FALSE)
}

isPrime_c <- function(x) {
  # #RegEx Slowest: Iit converts -ve values and coerce non-integers which may result in bugs
  x <- abs(as.integer(x))
  if(x > 8191L) {
    print("Do not run this with large values. RegEx is really slow.")
    stop()
  }
  !grepl('^1?$|^(11+?)\\1+$', strrep('1', x))
}

isPrime_d <- function(x) {
  # #Fails for "1" & returns TRUE
  # #Fails for "-ve and zero" Error: NA/NaN argument
  if(x == 2L || all(x %% 2L:max(2, floor(sqrt(x))) != 0)) {
    return(TRUE)
  } else return(FALSE)
}

isPrime_e <- function(x) {
  # #Fails for "-ve and zero" Error: NA/NaN argument
  # #This is the most robust which can be improved by conditional check for positive integers
  # #However, this checks the number against ALL Smaller values including non-primes
  if(x == 2L || all(x %% 2L:ceiling(sqrt(x)) != 0)) {
    # # "seq.int(3, ceiling(sqrt(x)), 2)" is slower
    return(TRUE)
  } else {
    ## (any(x %% 2L:ceiling(sqrt(x)) == 0))
    ## (any(x %% seq.int(3, ceiling(sqrt(x)), 2) == 0))
    ## NOTE Further, if sequence starts from 3, add 2 also as a Prime Number
    return(FALSE)
  }
}
```

### Legacy B {.unlisted .unnumbered}

```{r 'C03-getPrime-NOT', eval=FALSE}
# #131071 (12,251th), 524287 (43,390th), 2147483647 (105,097,565th)
aa <- 1:131071
# #Following works but only till 524287L, Memory Overflow ERROR for 2147483647L
bb <- aa[f_isPrimeV(aa)]

getPrimeUpto_a <- function(x){
  # #Extremely slow, cannot go beyond 8191L in benchmark testing
  if(x < 2) return("ERROR")
  y <- 2:x
  primes <- rep(2L, x)
  j <- 1L
  for (i in y) {
    if (!any(i %% primes == 0)) {
      j <- j + 1L
      primes[j] <- i
	  #cat(paste0("i=", i, ", j=", j, ", Primes= ",paste0(head(primes, j), collapse = ", ")))
    }
	#cat("\n")
  }
  result <- head(primes, j)
  #str(result)
  #cat(paste0("Head: ", paste0(head(result), collapse = ", "), "\n"))
  #cat(paste0("Tail: ", paste0(tail(result), collapse = ", "), "\n"))
  return(result)
}

getPrimeUpto_b <- function(x){
# #https://stackoverflow.com/questions/3789968/
  # #This is much faster even from the "aa[f_isPrimeV(aa)]"
    if(x < 2) return("ERROR")
    y <- 2:x
    i <- 1
    while (y[i] <= sqrt(x)) {
        y <-  y[y %% y[i] != 0 | y == y[i]]
        i <- i+1
    }
	result <- y
    #str(result)
    #cat(paste0("Head: ", paste0(head(result), collapse = ", "), "\n"))
    #cat(paste0("Tail: ", paste0(tail(result), collapse = ", "), "\n"))
    return(result)
}

getPrimeUpto_c <- function(x) {
  # #Problems and Slow
  # #Returns a Vetor of Primes till the Number i.e. f_getPrimesUpto(7) = (2, 3, 5, 7)
  # #NOTE: f_getPrimesUpto(1) and f_getPrimesUpto(2) both return "2"
  if(!is.integer(x)) {
    cat("Error! Integer required. \n")
    stop()
  } else if(!identical(1L, length(x))) {
    cat("Error! Unit length vector required. \n")
    stop()
  } else if(x <= 0L) {
    cat("Error! Positive Integer required. \n")
    stop()
  } else if(x > 2147483647) {
    cat(paste0("Doubles are stored as approximation. Prime will not be calculated for value higher than '2147483647' \n"))
    stop()
  }
  
  # #Cannot create vector of length 2147483647L and also not needed that many
  # #ceiling(sqrt(7L)) return 3, however we need length 4 (2, 3, 5, 7)
  # #So, added extra 10
  #primes <- rep(NA_integer_, 10L + sqrt(2L))
  primes <- rep(2L, 10L + sqrt(2L))
  j <- 1L
  primes[j] <- 2L
  #
  i <- 2L
  while(i <= x) {
    # #na.omit() was the slowest step, so changed all NA to 2L in the primes
    #k <- na.omit(primes[primes <= ceiling(sqrt(i))])
    k <- primes[primes <= ceiling(sqrt(i))]
    if(all(as.logical(i %% k))) {
      j <- j + 1
      primes[j] <- i
    }  
    # #Increment with INTEGER Addition
    i = i + 1L
  }
  result <- primes[complete.cases(primes)]
  str(result)
  cat(paste0("Head: ", paste0(head(result), collapse = ", "), "\n"))
  cat(paste0("Tail: ", paste0(tail(result), collapse = ", "), "\n"))
  return(result)
}

getPrimeUpto_d <- function(n = 10L, i = 2L, primes = c(2L), bypass = TRUE){
  # #Using Recursion is NOT a good solution
  # #Function to return N Primes upto 1000 Primes (7919) or Max Value reaching 10000.
  if(i > 10000){
    cat("Reached 10000 \n")
    return(primes)
  }
  if(bypass) {
    maxN <- 1000L
    if(!is.integer(n)) {
      cat("Error! Integer required. \n")
      stop()
    } else if(!identical(1L, length(n))) {
      cat("Error! Unit length vector required. \n")
      stop()
    } else if(n <= 0L) {
      cat("Error! Positive Integer required. \n")
      stop()
    } else if(n > maxN) {
      cat(paste0("Error! This will calculate only upto ", maxN, " prime Numebers. \n"))
      stop()
    }
  }
  if(length(primes) < n) {
    if(all(as.logical(i %% primes[primes <= ceiling(sqrt(i))]))) {
      # #Coercing 0 to FALSE, Non-zero Values to TRUE
      # # "i %% 2L:ceiling(sqrt(i))" checks i agains all integers till sqrt(i)
      # # "primes[primes <= ceiling(sqrt(i))]" checks i against only the primes till sqrt(i)
      # #However, the above needs hardcoded 2L as prime so the vector is never empty
      # #Current Number is Prime, so include it in the vector and check the successive one
      f_getPrime(n, i = i+1, primes = c(primes, i), bypass = FALSE)
    } else {
      # #Current Number is NOT Prime, so check the successive one
      f_getPrime(n, i = i+1, primes = primes, bypass = FALSE)
    }
  } else {
    # #Return the vector when it reaches the count
    return(primes)
  }
}
```

## Measures of Location

### Mean {#mean-c03 .tabset .tabset-fade}

```{definition 'Mean'}
Given a data set ${X=\{x_1,x_2,\ldots,x_n\}}$, the \textcolor{pink}{mean ${\overline{x}}$} is the sum of all of the values ${x_1,x_2,\ldots,x_n}$ divided by the count ${n}$.
```

- Refer equation \@ref(eq:mean)
  - Sample mean is denoted by \textcolor{pink}{${\overline{x}}$} (x bar) and Population mean is denoted by \textcolor{pink}{${\mu}$}. 
  - Mean is the most commonly used measure of central location, even though it is influenced by extreme values.

\begin{equation} 
  \overline{x} = \frac{1}{n}\left (\sum_{i=1}^n{x_i}\right ) = \frac{x_1+x_2+\cdots +x_n}{n}
  (\#eq:mean)
\end{equation} 

In the mean calculation, normally each ${x_i}$ is given equal importance or weightage of ${1/n}$. However, in some instances the mean is computed by giving each observation a weight that reflects its relative importance. A mean computed in this manner is referred to as the \textcolor{pink}{weighted mean}, as given in equation \@ref(eq:mean-weighted)

\begin{equation} 
  \overline{x} = \frac{\sum_{i=1}^n{w_ix_i}}{\sum_{i=1}^n{w_i}}
  (\#eq:mean-weighted)
\end{equation} 

\textcolor{orange}{Caution:} Unit of mean is same as unit of the variable e.g. cost_per_kg thus 'w' would be 'kg'.

#### Mean {.unlisted .unnumbered}

```{r 'C03-Mean'}
aa <- 1:10
# #Mean of First 10 Numbers
mean(aa)
```

#### More {.unlisted .unnumbered}

```{r 'C03-MeanMore'}
aa <- 1:10
# #Mean of First 10 Numbers
ii <- mean(aa)
print(ii)
jj <- sum(aa)/length(aa)
stopifnot(identical(ii, jj))
#
# #Mean of First 10 Prime Numbers (is neither Prime nor Integer)
mean(f_getRDS(xxPrimes)[1:10])
#
# #Mean of First 100 Digits of PI
f_getRDS(xxPI)[1:100, ] %>% pull(VAL) %>% mean()
```

#### Weighted Mean {.unlisted .unnumbered}

```{r 'C03-WeightedMean'}
aa <- tibble(Purchase = 1:5, cost_per_kg = c(3, 3.4, 2.8, 2.9, 3.25), 
             kg = c(1200, 500, 2750, 1000, 800))
# #NOTE that unit of mean is same as unit of the variable e.g. cost_per_kg thus 'w' would be 'kg'
(ii <- sum(aa$cost_per_kg * aa$kg)/sum(aa$kg))
jj <- with(aa, sum(cost_per_kg * kg)/sum(kg))
kk <- weighted.mean(x = aa$cost_per_kg, w = aa$kg)
stopifnot(all(identical(ii, jj), identical(ii, kk)))
```

### Median {#median-c03 .tabset .tabset-fade}

```{definition 'Median'}
\textcolor{pink}{Median} of a population is any value such that at most half of the population is less than the proposed median and at most half is greater than the proposed median.  
```

- Refer equation \@ref(eq:median)
  - The median is the value in the middle when the data is sorted
  - For an odd number of observations, the median is the middle value.
  - For an even number of observations, the median is the average of the two middle values.
  - Although the mean is the more commonly used measure of central location, whenever a data set contains extreme values, the median is preferred.
    - \textcolor{pink}{The mean and median are different concepts and answer different questions.} 
      - Ex: Income - nearly always reported as median, but if we are looking the the 'spending power of whole community' it may no not be right.
  - The median is well-defined for any ordered data, and is independent of any distance metric. 
    - The median can thus be applied to classes which are ranked but not numerical (ordinal), although the result might be halfway between classes if there is an even number of cases. 

\begin{equation} 
  \begin{align} 
    \text{if n is odd, } median(x) & = x_{(n+1)/2} \\
    \text{if n is even, } median(x) & = \frac{x_{(n/2)}+x_{(n/2)+1}}{2}
  \end{align}
  (\#eq:median)
\end{equation} 

#### Median {.unlisted .unnumbered}

```{r 'C03-Median'}
aa <- 1:10 
# #Median of First 10 Numbers
median(aa)
```

#### More {.unlisted .unnumbered}

```{r 'C03-MedianMore'}
aa <- 1:10 
# #Median of First 10 Numbers
median(aa)
#
# #Median of First 10 Prime Numbers (is NOT prime)
median(f_getRDS(xxPrimes)[1:10])
#
# #Median of First 100 Digits of PI
f_getRDS(xxPI)[1:100, ] %>% pull(VAL) %>% median()
```

### Geometric Mean {.tabset .tabset-fade}

```{definition 'Geometric-Mean'}
The \textcolor{pink}{geometric mean $\overline{x}_g$} is a measure of location that is calculated by finding the n^{th} root of the product of ${n}$ values. 
```

- Refer equation \@ref(eq:geometric-mean)
  - The geometric mean applies only to positive numbers
  - The geometric mean is often used for a set of numbers whose values are meant to be multiplied together or are exponential in nature
  - For all positive data sets containing at least one pair of unequal values, the harmonic mean is always the least of the three means, while the arithmetic mean is always the greatest of the three and the geometric mean is always in between.

\begin{equation} 
  \overline{x}_g = \left(\prod _{i=1}^{n} x_i\right)^{\frac{1}{n}} = \sqrt[{n}]{x_1 x_2 \ldots x_n}
  (\#eq:geometric-mean)
\end{equation} 

#### Geometric Mean {.unlisted .unnumbered }

```{r 'C03-GeometricMean'}
aa <- 1:10
# #Geometric Mean of of First 10 Numbers
exp(mean(log(aa)))
```

#### More {.unlisted .unnumbered }

```{r 'C03-GMmore'}
aa <- 1:10
# #Geometric Mean of of First 10 Numbers
ii <- exp(mean(log(aa)))
jj <- prod(aa)^(1/length(aa))
stopifnot(identical(ii, jj))
#
# #Geometric Mean of First 10 Prime Numbers 
exp(mean(log(f_getRDS(xxPrimes)[1:10])))
```

### Mode {#mode-c03 .tabset .tabset-fade}

```{definition 'Mode'}
The \textcolor{pink}{mode} is the value that occurs with greatest frequency.
```

- The median makes sense when there is a linear order on the possible values. Unlike median, the concept of mode makes sense for any random variable assuming values from a vector space.

#### Mode {.unlisted .unnumbered }

```{r 'C03-Mode'}
# #Mode of First 100 Digits of PI
bb <- f_getRDS(xxPI)[1:100, ] %>% pull(VAL)
f_getMode(bb)
```

#### More {.unlisted .unnumbered }

```{r 'C03-ModeMore'}
# #Mode of First 100 Digits of PI
bb <- f_getRDS(xxPI)[1:100, ]
#
# #Get Frequency
bb %>% count(VAL)
#
# #Get Mode
bb %>% pull(VAL) %>% f_getMode()
```

#### f_getMode() {.unlisted .unnumbered }

```{r 'C03F-getMode', ref.label=c('A10A-getMode')}
#
```

### Percentiles {#percentiles-c03 .tabset .tabset-fade}

```{definition 'Percentile'}
A \textcolor{pink}{percentile} provides information about how the data are spread over the interval from the smallest value to the largest value. For a data set containing ${n}$ observations, the $p^{th}$ percentile divides the data into two parts: approximately p% of the observations are less than the $p^{th}$ percentile, and approximately (100 – p)% of the observations are greater than the $p^{th}$ percentile. 
```

- Refer equation \@ref(eq:percentiles)
  - Percentile is the value which divides the data into two groups when it is sorted
  - \textcolor{pink}{Quartiles} are specific percentiles of 25%, 50% and 75%
  - \textcolor{pink}{Median} is 50% percentile
  - \textcolor{orange}{Caution:} Excel "PERCENTILE.EXC" calculations match with `type =6` option of `quantile()`, default is `type =7`
  
\begin{equation} 
  L_p = \frac{p}{100}(n+1)
  (\#eq:percentiles)
\end{equation}

#### Percentiles {.unlisted .unnumbered}

```{r 'C03-Percentile'}
# #First 100 Digits of PI
bb <- f_getRDS(xxPI)[1:100, ]
#
# #50% Percentile of Digits i.e. Median
quantile(bb$VAL, 0.5)
```

#### More {.unlisted .unnumbered}

```{r 'C03-PercentileMore'}
# #First 100 Digits of PI
bb <- f_getRDS(xxPI)[1:100, ]
#
# #50% Percentile of Digits i.e. Median
ii <- quantile(bb$VAL, 0.5)
print(ii)
jj <- median(bb$VAL)
stopifnot(identical(unname(ii), jj))
# 
# #All Quartiles
quantile(bb$VAL, seq(0, 1, 0.25))
# #summary()
summary(bb$VAL)
#
# #To Match with Excel "PERCENTILE.EXC" use type=6 in place of default type=7
quantile(bb$VAL, seq(0, 1, 0.25), type = 6)
```

## Measures of Variability
In addition to measures of location, it is often desirable to consider measures of variability, or dispersion.

- Range `range()`
  - (Largest value - Smallest value) i.e. `max() - min()`
  - Range is based on only two of the observations and thus is highly influenced by extreme values.
- Interquartile Range (IQR) `IQR()`
  - The difference between the third quartile, and the first quartile
  - It overcomes the dependency on extreme values
- Mean Absolute Error (MAE)
  - $MAE = \frac{\sum |x_i - \overline{x}|}{n}$

### Variance {#variance-c03 .tabset .tabset-fade}

```{definition 'Variance'}
The \textcolor{pink}{variance $({\sigma}^2)$} is based on the difference between the value of each observation ${x_i}$ and the mean ${\overline{x}}$. The average of the squared deviations is called the variance. 
```

- Refer equation \@ref(eq:variance)
  - Sample Variance is denoted by \textcolor{pink}{$s^2$} and Population Variance is denoted by \textcolor{pink}{$\sigma^2$}
  - The variance is a measure of variability that utilizes all the data.
  - The difference between each ${x_i}$ and the mean ($\overline{x}, \mu$) is called a \textcolor{pink}{deviation about the mean} i.e. ($x_i - \overline{x}$). Sum of deviation about the mean is always zero i.e. $\sum (x_i - \overline{x}) =0$
  - In the computation of the variance, the deviations about the mean are squared.

\begin{equation} 
  \begin{align} 
    \sigma^2 &= \frac{1}{n} \sum _{i=1}^{n} \left(x_i - \mu \right)^2 \\
    s^2 &= \frac{1}{n-1} \sum _{i=1}^{n} \left(x_i - \overline{x} \right)^2
  \end{align}
  (\#eq:variance)
\end{equation} 

### Standard Deviation {#sd-c03 .tabset .tabset-fade}

```{definition 'Standard-Deviation'}
The \textcolor{pink}{standard deviation ($s, \sigma$)} is defined to be the positive square root of the variance. It is a measure of the amount of variation or dispersion of a set of values.
```

- Refer equation \@ref(eq:sd)
  - Standard deviation for sample is denoted by \textcolor{pink}{${s}$} and for Population by \textcolor{pink}{${\sigma}$}
  - The \textcolor{pink}{coefficient of variation} is a relative measure of variability. It measures the standard deviation relative to the mean. It is given in percentage as $100 \times \sigma / \mu$

\begin{equation} 
  \begin{align} 
     \sigma &= \sqrt{\frac{1}{N} \sum_{i=1}^N \left(x_i - \mu\right)^2} \\
    {s} &= \sqrt{\frac{1}{N-1} \sum_{i=1}^N \left(x_i - \overline{x}\right)^2}
  \end{align}
  (\#eq:sd)
\end{equation} 

## Measures of Distribution Shape 

### Skewness {#skewness-c03 .tabset .tabset-fade} 

```{definition 'Skewness'}
\textcolor{pink}{Skewness $(\tilde{\mu}_{3})$} is a measure of the shape of a data distribution. It is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean.
```

```{definition 'Tails'}
A \textcolor{pink}{tail} refers to the tapering sides at either end of a distribution curve.
```


- Data skewed to the left result in negative skewness; a symmetric data distribution results in zero skewness; and data skewed to the right result in positive skewness. 
- $\tilde{\mu}_{3}$ is the $3^{rd}$ standardized moment
  - Side topic: A \textcolor{pink}{standardized moment} of a probability distribution is a moment (normally a higher degree central moment) that is normalized. The normalization is typically a division by an expression of the standard deviation which renders the moment scale invariant. 
    - $\tilde{\mu}_{1} = 0$, because the first moment about the mean is always zero. 
    - $\tilde{\mu}_{2} = 1$, because the second moment about the mean is equal to the variance ${\sigma}^2$. 
    - $\tilde{\mu}_{3}$ is a measure of skewness
    - $\tilde{\mu}_{4}$ refers to the Kurtosis 


Refer figure \@ref(fig:C03P060405) 

- No skew: (symmetric)
  - A unimodal distribution with zero value of skewness does not imply that this distribution is symmetric necessarily. However, a symmetric unimodal or multimodal distribution always has zero skewness.   - The normal distribution has a skewness of zero. But reverse may not be true.
- Negative skew: (left-skewed, left-tailed, or skewed to the left)
  - The left tail is longer, thus the 'left' refers to the left tail being drawn out
  - The curve itself appears to be leaning to the right i.e. the mass of the distribution is concentrated on the right of the figure
- Positive skew: (right-skewed, right-tailed, or skewed to the right)
  - The right tail is longer, thus the 'right' refers to the right tail being drawn out 
  - The curve itself appears to be leaning to the left i.e. the mass of the distribution is concentrated on the left of the figure
- Relationship of mean and median
  - The skewness is not directly related to the relationship between the mean and median: a distribution with negative skew can have its mean greater than or less than the median, and likewise for positive skew
  - However, generally the skew can be calculated as $({\mu} -{\nu})/\sigma$, where ${\nu}$ is median
- Application:
  - Skewness indicates the direction and relative magnitude of deviation from the normal distribution.
  - It indicates the direction of outliers
  - With pronounced skewness, standard statistical inference procedures such as a confidence interval for a mean will be not only incorrect, in the sense that the true coverage level will differ from the nominal (e.g., 95%) level, but they will also result in unequal error probabilities on each side. 

Skewness is given by the equation \@ref(eq:skewness), which is being shown here because it ~~looked cool~~ has deep meaning

\begin{equation} 
 Skew = \frac{\tfrac {1}{n}\sum_{i=1}^{n}(x_{i}-{\overline {x}})^{3}}{\left[\tfrac {1}{n-1}\sum_{i=1}^{n}(x_{i}-{\overline {x}})^{2} \right]^{3/2}}
  (\#eq:skewness)
\end{equation} 


#### Charts {.unlisted .unnumbered}

```{r 'C03-LoadDistributions-A', include=FALSE, ref.label=c('C03-LoadDistributions')}
#
```

```{r 'C03-DensityNormal', include=FALSE, eval=FALSE}
hh <- tibble(ee = xxNormal)
title_hh <- "Normal Distribution (Symmetrical)"
caption_hh <- "C03P04"
```

```{r 'C03-DensityNormal-A', ref.label=c('C03-DensityNormal', 'C03-Density'), include=FALSE}
#
```

```{r 'C03P04-Save', include=FALSE}
#ggsave(paste0(.z$PX, "C03P04", "_Normal_Symmetric", ".png"), plot = eval(parse(text = caption_hh)))
loc_png <- paste0(.z$PX, "C03P04", "_Normal_Symmetric", ".png")
if(!file.exists(loc_png)) ggsave(loc_png, plot = eval(parse(text = caption_hh)))
```

```{r 'C03P04', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C03P04", "_Normal_Symmetric", ".png"))
```

```{r 'C03-DensityExp', include=FALSE, eval=FALSE}
hh <- tibble(ee = xxExp)
title_hh <- "Exponential Distribution (Positive Skew, Right Tail)"
caption_hh <- "C03P05" 
```

```{r 'C03-DensityExp-A', ref.label=c('C03-DensityExp', 'C03-Density'), include=FALSE}
#
```

```{r 'C03P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C03P05", "_Exponential_Positive", ".png")
if(!file.exists(loc_png)) ggsave(loc_png, plot = eval(parse(text = caption_hh)))
```

```{r 'C03P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C03P05", "_Exponential_Positive", ".png"))
```

```{r 'C03-DensityBeta', include=FALSE, eval=FALSE}
hh <- tibble(ee = xxBeta)
title_hh <- "Beta Distribution (Negative Skew, Left Tail)"
caption_hh <- "C03P06"
```

```{r 'C03-DensityBeta-A', ref.label=c('C03-DensityBeta', 'C03-Density'), include=FALSE}
#
```

```{r 'C03P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C03P06", "_Beta_Negative", ".png")
if(!file.exists(loc_png)) ggsave(loc_png, plot = eval(parse(text = caption_hh)))
```

```{r 'C03P06', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C03P06", "_Beta_Negative", ".png"))
```

```{r 'C03P060405', echo=FALSE, ref.label=c('C03P06', 'C03P04', 'C03P05') ,fig.cap="(Left Tail, Negative) Beta, Normal Distribution, Exponential (Positive, Right Tail)", out.width='33%'}
#
```

#### skewness() {.unlisted .unnumbered}

```{r 'C03-Skewness_Gen'}
# #Skewness Calculation: Package "e1071" (Package "moments" deprecated)
even_skew <- c(49, 50, 51)
pos_skew <- c(even_skew, 60)
neg_skew <- c(even_skew, 40)
skew_lst <- list(even_skew, pos_skew, neg_skew)
# #Mean, Median, SD
cat(paste0("Mean (neg, even, pos): ", 
           paste0(vapply(skew_lst, mean, numeric(1)), collapse = ", "), "\n"))
cat(paste0("Median (neg, even, pos): ", 
           paste0(vapply(skew_lst, median, numeric(1)), collapse = ", "), "\n"))
cat(paste0("SD (neg, even, pos): ", paste0(
           round(vapply(skew_lst, sd, numeric(1)), 1), collapse = ", "), "\n"))
#
cat(paste0("Skewness (neg, even, pos): ", paste0(
           round(vapply(skew_lst, e1071::skewness, numeric(1)), 1), collapse = ", "), "\n"))
cat(paste0("Kurtosis (neg, even, pos): ", paste0(
           round(vapply(skew_lst, e1071::kurtosis, numeric(1)), 1), collapse = ", "), "\n"))
```

#### Normal Exp Beta {.unlisted .unnumbered}

```{r 'C03-SkewnessNormal'}
# #Skewness Calculation: Package "e1071" (Package "moments" deprecated)
dis_lst <- list(xxNormal, xxExp, xxBeta)
#
# #Skewness: Normal has value close to 3 Kurtosis (=0 excess Kurtosis)
# #Skewness "e1071" has Type = 3 as default. Its Type = 1 matches "moments"
# #Practically, Normal has (small) NON-Zero Positive Skewness
skew_e_t3 <- vapply(dis_lst, e1071::skewness, numeric(1))
skew_e_t2 <- vapply(dis_lst, e1071::skewness, type = 2, numeric(1))
skew_e_t1 <- vapply(dis_lst, e1071::skewness, type = 1, numeric(1))
skew_mmt <-  vapply(dis_lst, moments::skewness, numeric(1))
stopifnot(identical(round(skew_e_t1, 10), round(skew_mmt, 10)))
cat(paste0("e1071: Type = 1 Skewness (Normal, Exp, Beta): ", 
           paste0(round(skew_e_t1, 4), collapse = ", "), "\n"))
cat(paste0("e1071: Type = 2 Skewness (Normal, Exp, Beta): ", 
           paste0(round(skew_e_t2, 4), collapse = ", "), "\n"))
cat(paste0("e1071: Type = 3 Skewness (Normal, Exp, Beta): ", 
           paste0(round(skew_e_t3, 4), collapse = ", "), "\n"))
#
# #Formula: (sigma_ (x_i - mu)^3) /(n * sd^3)
bb <- xxNormal
skew_man <- sum({bb - mean(bb)}^3) / {length(bb) * sd(bb)^3}
cat(paste0("(Manual) Skewness of Normal: ", round(skew_man, 4), 
           " (vs. e1071 Type 3 = ", round(skew_e_t3[1], 4), ") \n"))
```

#### Distributions {.unlisted .unnumbered}

```{r 'C03-Distributions', eval=FALSE}
set.seed(3)
nn <- 10000L
# #Normal distribution is symmetrical
xxNormal <- rnorm(n = nn, mean = 0, sd = 1)
# #The exponential distribution is positive skew
xxExp <- rexp(n = nn, rate = 1)
# #The beta distribution with hyper-parameters α=5 and β=2 is negative skew
xxBeta <- rbeta(n = nn, shape1 = 5, shape2 = 2)
#
# #Save
f_setRDS(xxNormal)
f_setRDS(xxExp)
f_setRDS(xxBeta)
#f_getRDS(xxNormal)
```

```{r 'C03-LoadDistributions', eval=FALSE}
# #Get the Distributions
xxNormal <- f_getRDS(xxNormal)
xxExp <- f_getRDS(xxExp)
xxBeta <- f_getRDS(xxBeta)
```

#### Density {.unlisted .unnumbered}

```{r 'C03-Density', eval=FALSE}
# #Density Curve
# #Assumes 'hh' has data in 'ee'. In: caption_hh
#Basics
mean_hh <- mean(hh$ee)
sd_hh <- sd(hh$ee)
#
skew_hh <- skewness(hh$ee)
kurt_hh <- kurtosis(hh$ee)
# #Get Quantiles and Ranges of mean +/- sigma 
q05_hh <- quantile(hh[[1]], .05)
q95_hh <- quantile(hh[[1]], .95)
density_hh <- density(hh[[1]])
density_hh_tbl <- tibble(x = density_hh$x, y = density_hh$y)
sig3r_hh <- density_hh_tbl %>% filter(x >= {mean_hh + 3 * sd_hh})
sig3l_hh <- density_hh_tbl %>% filter(x <= {mean_hh - 3 * sd_hh})
sig2r_hh <- density_hh_tbl %>% filter(x >= {mean_hh + 2 * sd_hh}, {x < mean_hh + 3 * sd_hh})
sig2l_hh <- density_hh_tbl %>% filter(x <= {mean_hh - 2 * sd_hh}, {x > mean_hh - 3 * sd_hh})
sig1r_hh <- density_hh_tbl %>% filter(x >= {mean_hh + sd_hh}, {x < mean_hh + 2 * sd_hh})
sig1l_hh <- density_hh_tbl %>% filter(x <= {mean_hh - sd_hh}, {x > mean_hh - 2 * sd_hh})
sig0r_hh <- density_hh_tbl %>% filter(x > mean_hh, {x < mean_hh + 1 * sd_hh})
sig0l_hh <- density_hh_tbl %>% filter(x < mean_hh, {x > mean_hh - 1 * sd_hh})
#
# #Change x-Axis Ticks interval
xbreaks_hh <- seq(-3, 3)
xpoints_hh <- mean_hh + xbreaks_hh * sd_hh
#
# # Latex Labels 
xlabels_hh <- c(TeX(r'($\,\,\mu - 3 \sigma$)'), TeX(r'($\,\,\mu - 2 \sigma$)'), 
                TeX(r'($\,\,\mu - 1 \sigma$)'), TeX(r'($\mu$)'), TeX(r'($\,\,\mu + 1 \sigma$)'), 
                TeX(r'($\,\,\mu + 2 \sigma$)'), TeX(r'($\,\,\mu + 3\sigma$)'))
#
C03 <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) + 
  geom_density(alpha = 0.2, colour = "#21908CFF") + 
  geom_area(data = sig3l_hh, aes(x = x, y = y), fill = '#440154FF') + 
  geom_area(data = sig3r_hh, aes(x = x, y = y), fill = '#440154FF') + 
  geom_area(data = sig2l_hh, aes(x = x, y = y), fill = '#3B528BFF') + 
  geom_area(data = sig2r_hh, aes(x = x, y = y), fill = '#3B528BFF') + 
  geom_area(data = sig1l_hh, aes(x = x, y = y), fill = '#21908CFF') + 
  geom_area(data = sig1r_hh, aes(x = x, y = y), fill = '#21908CFF') + 
  geom_area(data = sig0l_hh, aes(x = x, y = y), fill = '#5DC863FF') + 
  geom_area(data = sig0r_hh, aes(x = x, y = y), fill = '#5DC863FF') + 
  scale_x_continuous(breaks = xpoints_hh, labels = xlabels_hh) + 
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), 
        axis.ticks = element_blank(), 
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
        axis.line.y = element_blank(), axis.title.y = element_blank(), axis.text.y = element_blank()) + 
  labs(x = "x", y = "Density", 
       subtitle = paste0("Mean = ", round(mean_hh, 3), "; SD = ", round(sd_hh, 3), "; Skewness = ", round(skew_hh, 3), "; Kurtosis = ", round(kurt_hh, 3)), 
        caption = caption_hh, title = title_hh)
}
assign(caption_hh, C03)
rm(C03)
```

### Kurtosis {#kurtosis-c03 .tabset .tabset-fade} 

```{definition 'Kurtosis'}
\textcolor{pink}{Kurtosis $(\tilde{\mu}_{4})$} is a measure of the "tailedness" of the probability distribution of a real-valued random variable. Like skewness, kurtosis describes the shape of a probability distribution. For ${\mathcal {N}}_{(\mu,\, \sigma)}$, kurtosis is 3 and \textcolor{pink}{excess kurtosis} is 0 (i.e. subtract 3).
```

Distributions with zero excess kurtosis are called \textcolor{pink}{mesokurtic}. The most prominent example of a mesokurtic distribution is the normal distribution. The kurtosis of any univariate normal distribution is 3. 

Distributions with kurtosis less than 3 are said to be \textcolor{pink}{platykurtic}. It means the distribution produces fewer and less extreme outliers than does the normal distribution. An example of a platykurtic distribution is the uniform distribution, which does not produce outliers.

Distributions with kurtosis greater than 3 are said to be \textcolor{pink}{leptokurtic}. An example of a leptokurtic distribution is the Laplace distribution, which has tails that asymptotically approach zero more slowly than a Gaussian, and therefore produces more outliers than the normal distribution. 

Kurtosis is the average (or expected value) of the standardized data raised to the fourth power. Any standardized values that are less than 1 (i.e., data within one standard deviation of the mean, where the "peak" would be), contribute virtually nothing to kurtosis, since raising a number that is less than 1 to the fourth power makes it closer to zero. The only data values that contribute to kurtosis in any meaningful way are those outside the region of the peak; i.e., the outliers. Therefore, kurtosis measures outliers only; it measures nothing about the "peak". 

The sample kurtosis is a useful measure of whether there is a problem with outliers in a data set. Larger kurtosis indicates a more serious outlier problem. 

```{r 'C03-Kurtosis'}
# #Kurtosis Calculation: Package "e1071" (Package "moments" deprecated)
dis_lst <- list(xxNormal, xxExp, xxBeta)
#
# #Kurtosis: Normal has value close to 3 Kurtosis (=0 excess Kurtosis)
# #Kurtosis "e1071" has Type = 3 as default. Its Type = 1 matches "moments" with difference of 3
kurt_e_t3 <- vapply(dis_lst, e1071::kurtosis, numeric(1))
kurt_e_t2 <- vapply(dis_lst, e1071::kurtosis, type = 2, numeric(1))
kurt_e_t1 <- vapply(dis_lst, e1071::kurtosis, type = 1, numeric(1))
kurt_mmt <-  vapply(dis_lst, moments::kurtosis, numeric(1))
stopifnot(identical(round(kurt_e_t1, 10), round(kurt_mmt - 3, 10)))
cat(paste0("e1071: Type = 1 Kurtosis (Normal, Exp, Beta): ", 
           paste0(round(kurt_e_t1, 4), collapse = ", "), "\n"))
cat(paste0("e1071: Type = 2 Kurtosis (Normal, Exp, Beta): ", 
           paste0(round(kurt_e_t2, 4), collapse = ", "), "\n"))
cat(paste0("e1071: Type = 3 Kurtosis (Normal, Exp, Beta): ", 
           paste0(round(kurt_e_t3, 4), collapse = ", "), "\n"))
#
# #Formula: (sigma_ (x_i - mu)^4) /(n * sd^4)
bb <- xxNormal
kurt_man <- {sum({bb - mean(bb)}^4) / {length(bb) * sd(bb)^4}} - 3
cat(paste0("(Manual) Kurtosis of Normal: ", round(kurt_man, 4), 
           " (vs. e1071 Type 3 = ", round(kurt_e_t3[1], 4), ") \n"))
```

## Relative Location

### z-Scores {#z-scores-c03 .tabset .tabset-fade}

Measures of relative location help us determine how far a particular value is from the mean. By using both the mean and standard deviation, we can determine the relative location of any observation. 

```{definition 'TheSample'}
A sample of \textcolor{pink}{${n}$} observations given by ${X=\{x_1,x_2,\ldots,x_n\}}$ have a sample mean \textcolor{pink}{${\overline{x}}$} and the sample standard deviation, \textcolor{pink}{${s}$}.
```

```{definition 'z-Scores'}
The \textcolor{pink}{z-score, ${z_i}$}, can be interpreted as the number of standard deviations ${x_i}$ is from the mean ${\overline{x}}$. It is associated with each ${x_i}$. The z-score is often called the \textcolor{pink}{standardized value} or \textcolor{pink}{standard score}. 
```

- Refer equation \@ref(eq:z-scores) (Similar to equation \@ref(eq:z-val))
  -  For example, $z_1 = 1.2$  would indicate that ${x_1}$ is 1.2 standard deviations greater than the sample mean. Similarly, $z_2 = -0.5$ would indicate that ${x_2}$ is 0.5 standard deviation less than the sample mean. 
  - A z-score greater than zero occurs for observations with a value greater than the mean, and a z-scoreless than zero occurs for observations with a value less than the mean. 
  - A z-score of zero indicates that the value of the observation is equal to the mean.
  - The z-score for any observation can be interpreted as a measure of the relative location of the observation in a data set.
  - The process of converting a value for a variable to a z-score is often referred to as a \textcolor{pink}{z transformation} or \textcolor{pink}{scaling}.

\begin{equation} 
  z_i = \frac{x_i - \overline{x}}{s}
  (\#eq:z-scores)
\end{equation} 

NOTE:  "Z statistic" is a special case of "Z critical" because $\sigma/\sqrt{n}$ is the 'standard error of the sample mean' which means that it is a standard deviation. Rather than (eg) a known population standard deviation or even just sample standard deviation, per CLT, it is the standard deviation of the sample mean. The 'critical Z' (i.e. standard score) is something than can always be computed ("a general case") whenever there is a mean and standard deviation; it translates X into a Z variable with zero mean and unit variance. (it "imposes normality" when the data may not be normal!). The "Z statistic" similarly standardizes as a special case where it is standardizing the sample mean.

```{definition 't-statistic'}
Computing a z-score requires knowing the mean ${\mu}$ and standard deviation ${\sigma}$ of the complete population to which a data point belongs. If one only has a sample of observations from the population, then the analogous computation with sample mean ${\overline{x}}$ and sample standard deviation ${s}$ yields the \textcolor{pink}{t-statistic}. 
```

\textcolor{orange}{Caution:} 

- Scaling does influence the interpretation of the parameters when doing many statistical analyses (regression, PCA etc) so the decision to scale should be based on how you want to interpret your parameters. 
  - Although the shapes of distributions are unchanged by scaling, the distributions themselves are definitely changed. 
  - Ex: After scaling a Poisson distribution, it would no longer be a Poisson distribution
  - However, scaling will not change the underlying distribution of the variable nor will it influence (positively or negatively) the violations of model assumptions.


```{r 'C03-Original', include=FALSE}
xxflights <- f_getRDS(xxflights)
bb <- na.omit(xxflights$air_time)
hh <- tibble(ee = bb)
title_hh <- "Flights: Air Time (Original)"
caption_hh <- "C03P07" 
```

```{r 'C03-Original-A', ref.label=c('C03-Original', 'C03-Histogram'), include=FALSE}
#
```

```{r 'C03P07-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C03P07", "_Flights_Original", ".png")
if(!file.exists(loc_png)){
ggsave(loc_png, plot = C03P07)
}
```

```{r 'C03P07', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C03P07", "_Flights_Original", ".png"))
```

```{r 'C03-Scaled'}
xxflights <- f_getRDS(xxflights)
bb <- na.omit(xxflights$air_time)
# Scaling
ii <- {bb - mean(bb)} / sd(bb)
str(ii)
# #scale() gives a Matrix with original mean and sd as its attribute
jj <- scale(bb)
str(jj)
stopifnot(identical(as.vector(ii), as.vector(jj)))
#
hh <- tibble(ee = as.vector(jj))
title_hh <- "Flights: Air Time (Scaled)"
caption_hh <- "C03P08" #iiii
```

```{r 'C03-Scaled-A', ref.label=c('C03-Scaled', 'C03-Histogram'), include=FALSE}
#
```

```{r 'C03P08-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C03P08", "_Flights_Scaled", ".png")
if(!file.exists(loc_png)){
ggsave(loc_png, plot = C03P08)
}
```

```{r 'C03P08', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C03P08", "_Flights_Scaled", ".png")) #iiii
```

#### Image {.unlisted .unnumbered}

```{r 'C03P0708', echo=FALSE, ref.label=c('C03P07', 'C03P08') ,fig.cap="Before and After Scaling"}
#
```

#### Histogram {.unlisted .unnumbered}

```{r 'C03-Histogram', eval=FALSE}
# #hh$ee title_hh caption_hh
#
C03 <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) +
  geom_histogram(bins = 50, alpha = 0.4, fill = '#FDE725FF') + 
  geom_vline(aes(xintercept = mean(.data[["ee"]])), color = '#440154FF') +
  annotate(geom = "text", x = mean(.[[1]]), y = -Inf, 
           label = TeX(r'($\bar{x}$)', output = "character"), 
           color = '#440154FF', hjust = -2, vjust = -2.5, parse = TRUE) +
  coord_cartesian(ylim = c(0, 35000)) +
  theme(plot.title.position = "panel") + 
  labs(x = "x", y = "Frequency", 
       subtitle = paste0("(Mean= ", round(mean(.[[1]]), 3), 
                         "; SD= ", round(sd(.[[1]]), 3),
                         ")"), 
      caption = caption_hh, title = title_hh)
}
assign(caption_hh, C03)
rm(C03)
```

#### Annotate  {.unlisted .unnumbered}

```{r 'C03-Annotate', eval=FALSE}
if(FALSE){
# #check_overlap = TRUE works for de-blurring. However, it still checks each point thus slow
geom_text(aes(label = TeX(r'($\bar{x}$)', output = "character"), 
              x = mean(.data[["ee"]]), y = -Inf),
          color = '#440154FF', hjust = -2, vjust = -2.5, parse = TRUE, check_overlap = TRUE) 
# #Create your own dataset
geom_text(data = tibble(x = mean(.[[1]]), y = -Inf, 
                        label = TeX(r'($\bar{x}$)', output = "character")), 
          aes(x = x, y = y, label = label), 
          color = '#440154FF', hjust = -2, vjust = -2.5, parse = TRUE ) 
# #Or Equivalent
ggplot2::annotate(geom = "text", x = mean(.[[1]]), y = -Inf, 
                  label = TeX(r'($\bar{x}$)', output = "character"), 
                  color = '#440154FF', hjust = -2, vjust = -2.5, parse = TRUE) 
#
ggpp::annotate(geom = "text", x = mean(.[[1]]), y = -Inf, 
               label = TeX(r'($\bar{x}$)', output = "character"), 
               color = '#440154FF', hjust = -2, vjust = -2.5, parse = TRUE) 
}
```

#### Colours {.unlisted .unnumbered}

```{r 'C03-Colours'}
# #List All Colour Names in R
str(colors())
# #Packages: viridis, scales, viridisLite
# #Show N Colours with Max. Contrast
q_colors <- 5
# #Display Colours
if(FALSE) show_col(viridis_pal()(q_colors))
# #Get the Viridis i.e. "D" palette Hex Values for N Colours
v_colors <-  viridis(q_colors, option = "D")
v_colors
```

### Chebyshev Theorem {.tabset .tabset-fade}

```{definition 'Chebyshev-Theorem'}
\textcolor{pink}{Chebyshev Theorem} can be used to make statements about the proportion of data values that must be within a specified number of standard deviations ${\sigma}$, of the mean ${\mu}$.
```

- Refer to \@ref(def:Chebyshev-Theorem)
  - \textcolor{pink}{Chebyshev Theorem:} At least $(1-1/z^2)$ of the data values must be within z standard deviations of the mean, where z is any value greater than 1. 
    - Thus, at least 75% of the data values must be within $\overline{x} \pm 2s$, 89% within $\overline{x} \pm 3s$, and 94% $\overline{x} \pm 4s$.
  - Chebyshev theorem can be applied to any data set regardless of the shape of the distribution of the data. 
  - Ex: Test scores of 100 students have $(\mu = 70, \sigma = 5)$
  - How many students had test scores between 60 and 80
    - From equation \@ref(eq:z-scores), $z_{60} = \frac{60 - 70}{5} = -2$
    - Similarly, $z_{80} = \frac{80 - 70}{5} = +2$
    - According to theorem \@ref(def:Chebyshev-Theorem), values that must be within ${z}$ standard deviation are
      - ${(1-1/z^2) = (1 - 1/2^2) = 0.75 = 75\%}$
      - i.e. 75 students must have test scores between 60 and 80
  - How many students had test scores between 58 and 82
    - $z_{58} = -2.4, z_{82} = +2.4$
    - ${(1 - 1/2.4^2) \approx 0.826 \approx 83\%}$
      - i.e. 83 students must have test scores between 58 and 82

### Empirical Rule {#empirical-c03 .tabset .tabset-fade}

```{definition 'Empirical-Rule'}
\textcolor{pink}{Empirical rule} is used to compute the percentage of data values that must be within one, two, and three standard deviations ${\sigma}$ of the mean ${\mu}$ for a normal distribution. These probabilities are Pr(x) 68.27%, 95.45%, and 99.73%.
```

- According to the \textcolor{pink}{empirical rule}, for a Normal distribution
  - $Pr({\mu} - 1{\sigma} \leq {X} \leq {\mu} + 1{\sigma}) \approx 68.27\%$
  - $Pr({\mu} - 2{\sigma} \leq {X} \leq {\mu} + 2{\sigma}) \approx 95.45\%$ i.e. mostly
  - $Pr({\mu} - 3{\sigma} \leq {X} \leq {\mu} + 3{\sigma}) \approx 99.73\%$ i.e. almost all data values


## Outliers {#outliers-c03}


```{definition 'Outliers'}
Sometimes unusually large or unusually small values are called \textcolor{pink}{outliers}. It is a data point that differs significantly from other observations.
```

- Reasons
  - Outliers can occur by chance in any distribution, but they often indicate either measurement error or that the population has a heavy-tailed distribution. A frequent cause of outliers is a mixture of two distributions, which may be two distinct sub-populations.
  - In most larger samplings of data, some data points will be further away from the sample mean than what is deemed reasonable. However, a small number of outliers is to be expected (and not due to any anomalous condition). 
  - Estimators capable of coping with outliers are said to be \textcolor{pink}{robust}: the median is a robust statistic of central tendency, while the mean is not. However, the mean is generally a more precise estimator.
- Keeping vs. Removing Outliers
  - data value that has been incorrectly recorded /included - should be removed
  - unusual data value that has been recorded correctly and belongs in the data set - should be kept
- Standardized values (z-scores) can be used to identify outliers. 
  - [Empirical Rule](#empirical-c03 "c03") allows us to conclude that for normal distribution, almost all the data values will be within three standard deviations of the mean $(\overline{x} \pm 3s)$. 
  - Hence, in using z-scores to identify outliers, we recommend treating any data value with a z-score less than −3 or greater than +3 as an outlier. 
  - Such data values can then be reviewed for accuracy and to determine whether they belong in the data set.
  - In the case of normally distributed data, the three sigma rule can be used to identify outliers. 
    - In a sample of 1000 observations, the presence of up to five observations deviating from the mean by more than three times the standard deviation is within the range of what can be expected. If the sample size is only 100, however, just three such outliers are already reason for concern. 
- Another approach to identifying outliers is based upon IQR 
  - $\text{Lower Limit} = Q_1 - 1.5 \space \text{IQR}$ and $\text{Upper Limit} = Q_3 + 1.5 \space \text{IQR}$ 
  - An observation is classified as an outlier if its value is less than the lower limit or greater than the upper limit. 

## Summary {#boxplot-c03 .tabset .tabset-fade}

Five-Number Summary is used to quickly summarise a dataset. i.e. Min, Q1, Median, Q3, Max

- A boxplot is a graphical display of data based on a five-number summary. 
  - By using the interquartile range, IQR = Q3 − Q1, limits are located at 1.5(IQR) below Q1 and 1.5(IQR) above Q3
  - The \textcolor{pink}{whiskers} are drawn from the ends of the box to the smallest and largest values inside the limits 
  - Boxplots can also be used to provide a graphical summary of two or more groups and facilitate visual comparisons among the groups. 

### BoxPlot {.unlisted .unnumbered}

```{r 'C03-BoxPlot', include=FALSE}
# #nycflights13::weather
bb <- weather
# #NA are present in the data
summary(bb$temp)
#
# #BoxPlot
C03P01 <- bb %>% drop_na(temp) %>% mutate(month = factor(month, ordered = TRUE)) %>% {
    ggplot(data = ., mapping = aes(x = month, y = temp)) +
    #geom_violin() +
    geom_boxplot(aes(fill = month), outlier.colour = 'red', notch = TRUE) +
    stat_summary(fun = mean, geom = "point", size = 2, color = "steelblue") + 
    scale_y_continuous(breaks = seq(0, 110, 10), limits = c(0, 110)) +
    #geom_point() +
    #geom_jitter(position=position_jitter(0.2)) +
    k_gglayer_box +
    theme(legend.position = 'none') +
    labs(x = "Months", y = "Temperature", subtitle = "With Mean & Notch", 
         caption = "C03P01", title = "BoxPlot")
}
```

```{r 'C03P01-Save', include=FALSE}
ggsave(paste0(.z$PX,  "C03P01", "_BoxPlot", ".png"), plot = C03P01)
```

```{r 'C03P01', echo=FALSE, fig.cap="geom_boxplot()"}
knitr::include_graphics(paste0(.z$PX, "C03P01", "_BoxPlot", ".png"))
```

### Code {.unlisted .unnumbered}
```{r 'C03-BoxPlot-A', ref.label=c('C03-BoxPlot'), eval=FALSE}
#
```

## Relationship between Two Variables

### Covariance {#covariance-c03 .tabset .tabset-fade}

```{definition 'Covariance'}
\textcolor{pink}{Covariance} is a measure of linear association between two variables. Positive values indicate a positive relationship; negative values indicate a negative relationship.
```

- Refer equation \@ref(eq:covariance)
  - For a sample of size ${n}$ with the observations $(x_1, y_1), (x_2, y_2)$, and so on, the covariance is given by equation \@ref(eq:covariance)
  - A positive value for $s_{xy}$ indicates a positive linear association between x and y; that is, as the value of x increases, the value of y increases. Similarly a negative value shows a negative linear association.
    - In the example, $s_{xy} = 11$
  - If the points are evenly distributed in the scatterplot, the value of $s_{xy}$ will be close to zero, indicating no linear association between x and y.
  - \textcolor{orange}{Caution:} Problem with using covariance as a measure of the strength of the linear relationship is that the value of the covariance depends on the units of measurement for x and y.

\begin{equation} 
  \begin{align} 
    \sigma_{xy} &= \frac{\sum (x_i - \mu_x)(y_i - \mu_y)}{n} \\
    s_{xy} &= \frac{\sum (x_i - \overline{x})(y_i - \overline{y})}{n-1} 
  \end{align}
  (\#eq:covariance)
\end{equation} 

```{r 'C03-Trendline', include=FALSE}
bb <- f_getRDS(xxCommercials) 

# #Define the formula for Trendline calculation
k_gg_formula <- y ~ x
#
# #Scatterplot, Trendline Equation, R2, mean x & y
C03P02 <- bb %>% {
  ggplot(data = ., aes(x = Commercials, y = Sales)) + 
  geom_smooth(method = 'lm', formula = k_gg_formula, se = FALSE) +
  stat_poly_eq(aes(label = paste0("atop(", ..eq.label.., ", \n", ..rr.label.., ")")), 
               formula = k_gg_formula, eq.with.lhs = "italic(hat(y))~`=`~",
               eq.x.rhs = "~italic(x)", parse = TRUE) +
  geom_vline(aes(xintercept = round(mean(Commercials), 3)), color = 'red', linetype = "dashed") +
  geom_hline(aes(yintercept = round(mean(Sales), 3)), color = 'red', linetype = "dashed") +
  geom_text(aes(label = TeX(r"($\bar{x} = 3$)", output = "character"), 
                x = round(mean(Commercials), 3), y = -Inf), 
            color = 'red', , hjust = -0.2, vjust = -0.5, parse = TRUE, check_overlap = TRUE) + 
  geom_text(aes(label = TeX(r"($\bar{y} = 51$)", output = "character"), 
                x = Inf, y = round(mean(Sales), 3)), 
            color = 'red', , hjust = 1.5, vjust = -0.5, parse = TRUE, check_overlap = TRUE) + 
  geom_point() +
  k_gglayer_scatter +
  labs(x = "Commercials", y = "Sales ($100s)",
       subtitle = TeX(r"(Trendline Equation, $R^{2}$, $\bar{x}$ and $\bar{y}$)"), 
       caption = "C03P02", title = "Scatter Plot")
}
```

```{r 'C03P02-Save', include=FALSE}
ggsave(paste0(.z$PX, "C03P02", "_Scatter_Mean", ".png"), plot = C03P02)
```

```{r 'C03P02', echo=FALSE, fig.cap="Added Mean", include=FALSE}
knitr::include_graphics(paste0(.z$PX, "C03P02", "_Scatter_Mean", ".png"))
```

```{r 'C03P03-A', echo=FALSE, fig.cap="Original", include=FALSE}
# #Reading image of another file
knitr::include_graphics(paste0(.z$PX, "C02P05", "_ScatterPlot", ".png"))
```

```{r 'C03P02-A', ref.label=c('C03P03-A', 'C03P02'), echo=FALSE, fig.cap="Scatter Plot Quadrants for Covariance"}
#
```

#### Covariance {.unlisted .unnumbered}

```{r 'C03-Covariance'}
# #Get 'Deviation about the mean' i.e. devX and devY and their Product devXY
ii <- bb %>% 
  mutate(devX = Commercials - mean(Commercials), devY = Sales - mean(Sales), devXY = devX * devY) 
#
# #Sample Covariance
sxy <- sum(ii$devXY) / {length(ii$devXY) -1}
print(sxy)
```

#### Code {.unlisted .unnumbered}

```{r 'C03-Trendline-A', ref.label=c('C03-Trendline'), eval=FALSE}
#
```

#### More Text  {.unlisted .unnumbered}

- [This](https://stats.stackexchange.com/questions/229667 "https://stats.stackexchange.com")
  - Unlike Pearson correlation, covariance itself is not a measure of the magnitude of linear relationship. It is a measure of co-variation (which could be just monotonic). This is because covariance depends not only on the strength of linear association but also on the magnitude of the variances. 
- More details are in following links
  -  [This](https://stats.stackexchange.com/questions/17537   "https://stats.stackexchange.com/questions/17537/understanding-variance-intuitively")
  -  [This](https://stats.stackexchange.com/questions/26/what-is-a-standard-deviation   "https://stats.stackexchange.com/questions/26/what-is-a-standard-deviation")
  -  [This](https://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who  -understands-only-the-mean "https://stats.stackexchange.com/questions/18058/how-would-you-explain-covaria  nce-to-someone-who-understands-only-the-mean")
  -  [This](https://stats.stackexchange.com/questions/12842/covariance-and-independence   "https://stats.stackexchange.com/questions/12842/covariance-and-independence")
  -  [This](https://stats.stackexchange.com/questions/18082/how-would-you-explain-the-difference-between-co  rrelation-and-covariance "https://stats.stackexchange.com/questions/18082/how-would-you-explain-the-diffe  rence-between-correlation-and-covariance")
  -  [This](https://stats.stackexchange.com/questions/29713/what-is-covariance-in-plain-language   "https://stats.stackexchange.com/questions/29713/what-is-covariance-in-plain-language")
  -  [This](https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance   "https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance")
  -  [This](https://stats.stackexchange.com/questions/229667/difference-between-correlation-and-covariance-is-covariance-only-useful-if-the "https://stats.stackexchange.com/questions/229667/difference-between-correlation-and-covariance-is-covariance-only-useful-if-the")
  -  [This](https://stats.stackexchange.com/questions/483964/how-come-covariance-can-pick-up-non-linear-relationships-but-correlation-cant "https://stats.stackexchange.com/questions/483964/how-come-covariance-can-pick-up-non-linear-relationships-but-correlation-cant")
  -  [This](https://stats.stackexchange.com/questions/9415/measuring-non-linear-dependence "https://stats.stackexchange.com/questions/9415/measuring-non-linear-dependence")

### Correlation Coefficient {#correlation-c03 .tabset .tabset-fade}

```{definition 'Correlation-Coefficient'}
\textcolor{pink}{Correlation coefficient} is a measure of linear association between two variables that takes on values between −1 and +1. Values near +1 indicate a strong positive linear relationship; values near −1 indicate a strong negative linear relationship; and values near zero indicate the lack of a linear relationship.
```

- Refer equation \@ref(eq:correlation) & Table \@ref(tab:C03T01)
  - The 'Pearson Product Moment Correlation Coefficient' or \textcolor{pink}{sample correlation coefficient} is computed by dividing the sample covariance $s_{xy}$ by the product of the sample standard deviation of x ($s_{x}$) and the sample standard deviation of y ($s_{y}$).
    - Values close to −1 (negative) or +1 (positive) indicate a strong linear relationship. The closer the correlation is to zero, the weaker the relationship.
  - In the example, $s_{xy} = 11$ (Equation \@ref(eq:covariance)) and $s_{x} = 1.49$, $s_{y} = 7.93$ (Equation \@ref(eq:sd))
  - Thus, $r_{xy} = 0.93$
  - \textcolor{orange}{Caution:} Correlation provides a measure of linear association and not necessarily causation. A high correlation between two variables does not mean that changes in one variable will cause changes in the other variable.
  - \textcolor{orange}{Caution:} Because the correlation coefficient measures only the strength of the linear relationship between two quantitative variables, it is possible for the correlation coefficient to be near zero, suggesting no linear relationship, when the relationship between the two variables is nonlinear. 

\begin{equation} 
  \begin{align} 
    \rho_{xy} &= \frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}} \\
    r_{xy} &= \frac{s_{xy}}{s_{x}s_{y}}
  \end{align}
  (\#eq:correlation)
\end{equation} 

#### Correlation  {.unlisted .unnumbered}

```{r 'C03-Correlation', include=FALSE}
jj <- ii %>% mutate(devXsq = devX * devX, devYsq = devY * devY)
# #Sample Covariance Sx, Sample Standard Deviations Sx Sy
sxy <- sum(ii$devXY) / {nrow(ii) -1}
sx <- round(sqrt(sum(jj$devXsq) / {nrow(jj) -1}), 2)
sy <- round(sqrt(sum(jj$devYsq) / {nrow(jj) -1}), 2)
cat(paste0("Sxy =", sxy, ", Sx =", sx, ", Sy =", sy, "\n"))
#
# #Correlation Coefficient Rxy
rxy <- round(sxy / {sx * sy}, 2)
cat(paste0("Correlation Coefficient Rxy =", rxy, "\n"))
```

```{r 'C03-Correlation-A', ref.label=c('C03-Covariance', 'C03-Correlation'), eval=TRUE}
#
```

#### Data  {.unlisted .unnumbered}

```{r 'C03T01', echo=FALSE}
bb <- jj
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kbl(bb,
  caption = "(C03T01) Correlation Calculation",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#C03030)
	row_spec(0, color = "white", background = "#C03030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C03-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, ii, jj, kk, tbl, xxPI, bb, rxy, sx, sxy, sy, C03P01, C03P02, C03P04, C03P05, C03P06,
          C03P07, C03P08, caption_hh, dis_lst, ee, even_skew, kurt_e_t1, kurt_e_t2, kurt_e_t3, 
          kurt_hh, kurt_man, kurt_mmt, neg_skew, pos_skew, q_colors, sig0l_hh, sig0r_hh, 
          skew_e_t1, skew_e_t2, skew_e_t3, skew_hh, skew_lst, skew_man, skew_mmt, title_hh, 
          v_colors, xxalpha, xxBeta, xxExp, xxflights, density_hh, density_hh_tbl, hh, mean_hh, 
          q05_hh, q95_hh, sd_hh, sig1l_hh, sig1r_hh, sig2l_hh, sig2r_hh, sig3l_hh, sig3r_hh, 
          xbreaks_hh, xlabels_hh, xpoints_hh, xxNormal, loc_png)
```

```{r 'C03-Validation', include=FALSE, cache=FALSE}
# #Summarised Packages and Objects
f_()
#
difftime(Sys.time(), k_start)
```

****
