# Numerical Measures (BE03) {#be03}

```{r '303A', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
invisible(lapply(f_getPathR(A09isPrime, A10getUtil), knitr::read_chunk))
```

## Overview

- This chapter covers 
  - Side Topic: [Number Theory](#numbers-be03 "be03"), [Primes](#primes-be03 "be03"), [Benchmark](#benchmark-be03 "be03"), Vectorising & Compiling & Profiling Functions,
  - Measures of Location: [Mean](mean-be03 "be03"), Weighted Mean, [Median](#median-be03 "be03"), Geometric Mean, [Mode](#mode-be03 "be03"), [Percentiles](#percentiles-be03 "be03"), Quantiles
  - Measures of Variability: Range, IQR, [Variance](#variance-be03 "be03"), [Standard Deviation](#sd-be03 "be03")
  - Relative Location: [z-scores](#z-scores-be03 "be03"), Chebyshev Theorem, Empirical Rule
  - Two Variables: [Covariance](#covariance-be03 "be03"), [Correlation Coefficient](#correlation-be03 "be03")
  - Graphs: [BoxPlot](#boxplot-be03 "be03"), [Scatterplot](#covariance-be03 "be03")

## Definitions

```{r '303D01', comment="", echo=FALSE, results='asis'}
f_getDef("Parameter-vs-Statistic")
```

## Number Theory {#numbers-be03 .tabset .tabset-fade}

```{definition 'Number'}
A \textcolor{pink}{number} is a mathematical object used to count, measure, and label. Their study or usage is called \textcolor{pink}{arithmetic}, a term which may also refer to \textcolor{pink}{number theory}, the study of the properties of numbers. 
```

Individual numbers can be represented by symbols, called \textcolor{pink}{numerals}; for example, "5" is a numeral that represents the 'number five'. 

As only a relatively small number of symbols can be memorized, basic numerals are commonly organized in a \textcolor{pink}{numeral system}, which is an organized way to represent any number. The most common numeral system is the \textcolor{pink}{Hindu–Arabic numeral system}, which allows for the representation of any number using a combination of ten fundamental numeric symbols, called \textcolor{pink}{digits}.

\textcolor{pink}{Counting} is the process of determining the number of elements of a finite set of objects, i.e., determining the size of a set. \textcolor{pink}{Enumeration} refers to uniquely identifying the elements of a set by assigning a number to each element. 

\textcolor{pink}{Measurement} is the quantification of attributes of an object or event, which can be used to compare with other objects or events.

```{r '303-Sets-A', include=FALSE}
# #NOT Working in Bookdown
# # For Infinity '\u221e'
cat(paste0(" \u2115 (0,1,2,...) \u2282 \u2124 (-1,0,1) \u2282 \u211a (-0.5,+0.5) \u2282 \u211d (\u221a2, e(\u2091), \u03c0) \u2282 \u2102 (a +i b) "))
```

### Sets {.unlisted .unnumbered}

> Formally, $\mathbb{N} \to \mathbb{Z} \to \mathbb{Q} \to \mathbb{R} \to \mathbb{C}$  
> Practically, $\mathbb{N} \subset \mathbb{Z} \subset \mathbb{Q} \subset \mathbb{R} \subset \mathbb{C}$

The \textcolor{pink}{natural numbers $\mathbb{N}$} are those numbers used for counting and ordering. ISO standard begin the natural numbers with 0, corresponding to the \textcolor{pink}{non-negative integers $\mathbb{N} = \{0, 1, 2, 3, \ldots \}$}, whereas others start with 1, corresponding to the \textcolor{pink}{positive integers $\mathbb{N^*} = \{1, 2, 3, \ldots \}$}

The set of \textcolor{pink}{integers $\mathbb{Z}$} consists of zero ($0$), the positive natural numbers $\{1, 2, 3, \ldots \}$ and their additive inverses (the negative integers). Thus i.e., \textcolor{pink}{$\mathbb{Z} = \{\ldots, -3, -2, -1, 0, 1, 2, 3, \ldots \}$}. An integer is colloquially defined as a number that can be written without a fractional component. 

\textcolor{pink}{Rational numbers $\mathbb{Q}$} are those which can be expressed as the quotient or fraction p/q of two integers, a numerator p and a non-zero denominator q. Thus, \textcolor{pink}{Rational Numbers $\mathbb{Q} = \{1, 2, 3, \ldots \}$}

A \textcolor{pink}{real number} is a value of a continuous quantity that can represent a distance along a line. The real numbers include all the rational numbers $\mathbb{Q}$, and all the irrational numbers. Thus, \textcolor{pink}{Real Numbers $\mathbb{R} = \mathbb{Q} \cup \{\sqrt{2}, \sqrt{3}, \ldots\} \cup \{ \pi, e, \phi, \ldots \}$}

The \textcolor{pink}{complex numbers $\mathbb{C}$} contain numbers which are expressed in the form $a + ib$, where $a$ and $b$ are real numbers. These have two components the real numbers and a specific element denoted by $i$ (imaginary unit) which satisfies the equation \textcolor{pink}{$i^2 = −1$}.

### Pi {.unlisted .unnumbered}

The number \textcolor{pink}{Pi $\pi = 3.14159\ldots$} is defined as the ratio of a circle's circumference to its diameter.

\begin{equation} 
  \pi = \int _{-1}^{1} \frac{dx}{\sqrt{1- x^2}}
  (\#eq:pi)
\end{equation} 

\begin{equation} 
  e^{i\varphi}=\cos \varphi +i\sin \varphi 
  (\#eq:euler-formula)
\end{equation} 

\begin{equation} 
  e^{i\pi} + 1 =0
  (\#eq:euler-identity)
\end{equation} 

```{r '303-PI'}
# #Read OIS File for 20000 PI digits including integral (3) and fractional (14159...)
# #md5sum = "daf0b33a67fd842a905bb577957a9c7f"
tbl <- read_delim(file = paste0(.z$XL, "PI-OIS-b000796.txt"), 
  delim = " ", col_names = c("POS", "VAL"), col_types = list(POS = "i", VAL = "i"))
attr(tbl, "spec") <- NULL
attr(tbl, "problems") <- NULL
xxPI <- tbl
f_setRDS(xxPI)
```

### e {.unlisted .unnumbered}

Euler's Number \textcolor{pink}{$e = 2.71828\ldots$}, is the base of the natural logarithm. 

\begin{equation} 
  e = \lim_{n \to \infty} \left(1 + \frac{1}{n} \right)^{n} = \sum \limits_{n=0}^{\infty} \frac{1}{n!}
  (\#eq:euler-number-e)
\end{equation} 

### Phi {.unlisted .unnumbered}

Two quantities are in the \textcolor{pink}{golden ratio $\varphi = 1.618\ldots$} if their ratio is the same as the ratio of their sum to the larger of the two quantities.

\begin{equation} 
  \varphi^2 - \varphi -1 =0 \\
  \varphi = \frac{1+\sqrt{5}}{2}
  (\#eq:golden-ratio-phi)
\end{equation} 

### Groups {.unlisted .unnumbered}

```{definition 'Prime'}
A \textcolor{pink}{prime number} is a natural number greater than 1 that is not a product of two smaller natural numbers. A natural number greater than 1 that is not prime is called a 'composite number'. 1 is neither a Prime nor a composite, it is a 'Unit'. Thus, by definition, Negative Integers and Zero cannot be Prime.
```

```{definition 'Parity-Odd-Even'}
\textcolor{pink}{Parity} is the property of an integer $\mathbb{Z}$ of whether it is even or odd. It is \textcolor{pink}{even} if the integer is divisible by 2 with no remainders left and it is \textcolor{pink}{odd} otherwise. Thus, -2, 0, +2 are even but -1, 1 are odd. Numbers ending with 0, 2, 4, 6, 8 are even. Numbers ending with 1, 3, 5, 7, 9 are odd.
```

```{definition 'Positive-Negative'}
An integer $\mathbb{Z}$ is \textcolor{pink}{positive} if it is greater than zero, and \textcolor{pink}{negative} if it is less than zero. Zero is defined as neither negative nor positive. 
```

## Primes {#primes-be03 .tabset .tabset-fade}

### Empty Vector {.unlisted .unnumbered}

```{r '303-EmptyVector'}
# #Create empty Vector with NA
aa <- rep(NA_integer_, 10)
print(aa)
```

### f_isPrime() {.unlisted .unnumbered}

```{r '303F-isPrime', ref.label=c('A09A-isPrime', 'A09B-isPrimeV', 'A09C-isPrimeC'), eval=FALSE }
#
```

### Primes {.unlisted .unnumbered}

```{r '303-ListPrime', eval=FALSE}
# #There are 4 Primes in First 10, 25 in 100, 168 in 1000, 1229 in 10000.
# # Using Vectorise Version, get all the Primes
aa <- 1:10
bb <- aa[f_isPrimeV(aa)]
ii <- f_getPrimeUpto(10)
stopifnot(identical(bb, ii))
# #
xxPrime10 <- c(2, 3, 5, 7) |> as.integer()
# #
xxPrime100 <- c(2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 
               53, 59, 61, 67, 71, 73, 79, 83, 89, 97)  |> as.integer()
#
# #Generate List of ALL Primes till 524287 (i.e. Total 43,390 Primes)
xxPrimes <- f_getPrimeUpto(524287L)
# #Save as RDS
f_setRDS(xxPrimes)
```

### Mersenne Prime {.unlisted .unnumbered}

```{r '303-LargeNumbers', eval=FALSE}
# #Mersenne primes are those prime number that are of the form (2^n -1).
# #3, 7, 31, 127, 8191 (1028th), 131071 (12,251th), 524287 (43,390th), 2147483647 (105,097,565th), 
# #2305843009213693951, 618970019642690137449562111, 
# #162259276829213363391578010288127 170141183460469231731687303715884105727

# #Following cannot be stored as Integer, thus it is automatically converted to Double
tryCatch({
    aa <- 2305843009213693951L
  }, warning = function(w) {
    # #NOTE Unable to Print this Warning properly!
    print(paste0(w))
	  #suppressWarnings(print(aa))
  })  
# #Warning: non-integer value 2305843009213693951L qualified with L; using numeric value 
#
#aa <- 2305843009213693951
# #NOTE that the value changed. It is explicitly NOT a prime anymore.
#print(aa, digits = 20)
# #[1] 2305843009213693952
#
# #Largest Mersenne Prime that can be safely checked
# #The number 2147483647 is the eighth Mersenne prime, equal to (2^231 − 1). 
# #It is one of only four known double Mersenne primes.
# #In computing, this number represents the largest value that a signed 32-bit integer field can hold. 
aa <- 2147483647L
print(aa)
```

### f_getPrime() {.unlisted .unnumbered}

```{r '303F-getPrimeUpto', ref.label=c('A09D-getPrimeUpto'), eval=FALSE}
#
```

### Benchmark {#benchmark-be03 .unlisted .unnumbered}

```{r '303-Benchmark', eval=FALSE}
# #Compare any number of functions
result <- microbenchmark(
  sum(1:100)/length(1:100), 
  mean(1:100),
  #times = 1000,
  check = 'identical'
)
# #Print Table
print(result)
##Unit: microseconds
##                     expr min    lq    mean median     uq    max neval cld
## sum(1:100)/length(1:100) 1.2 1.301 1.54795 1.5005 1.6005  7.501   100  a 
##              mean(1:100) 5.9 6.001 6.56989 6.1010 6.2010 28.001   100   b
#
# #Boxplot of Benchmarking Result
#autoplot(result)
# #Above testcase showed a surprising result of sum()/length() being much faster than mean()
```

### Sum-Mean {.unlisted .unnumbered}

"ForLater" - Include `rowsum(), rowSums(), colSums(), rowMeans(), colMeans()` in this also.

```{r '303-SumMean'}
# #Conclusion: use mean() because precision is difficult to achieve compared to speed
#
# #sum()/length() is faster than mean()
# #However, mean() does double pass, so it would be more accurate
# #mean.default() and var() compute means with an additional pass and so are more accurate
# #e.g. the variance of a constant vector is (almost) always zero 
# #and the mean of such a vector will be equal to the constant value to machine precision.
aa <- 1:100
#
microbenchmark(
  sum(aa)/length(aa), 
  mean(aa),
  mean.default(aa),
  .Internal(mean(aa)),
  #times = 1000,
  check = 'identical'
)
# #rnorm() generates random deviates of given length
set.seed(3)
aa <- rnorm(1e7)
str(aa)
#
# #NOTE manual calculation and mean() is NOT matching
identical(sum(aa)/length(aa), mean(aa))
#
# #There is a slight difference
sum(aa)/length(aa) - mean(aa)
```

### Remove Objects  {.unlisted .unnumbered}

```{r '303-rmPattern', eval=FALSE}
if(FALSE) {
  # #Remove all objects matching a pattern
  rm(list = ls(pattern = "f_"))
}
```

### Options Memory {.unlisted .unnumbered}

```{r '303-Memory'}
# #Check the Current Options Value
getOption("expressions")
if(FALSE) {
  # #Change Value
  # #NOTE it didn't help when recursive function failed
  # #Error: node stack overflow
  # #Error during wrapup: node stack overflow
  # #Error: no more error handlers available ...
  options(expressions=10000)
}
```

### Vectorize() {.unlisted .unnumbered}

```{r '303-Vectorize', eval=FALSE}
# #To Vectorise a Function
f_isPrimeV <- Vectorize(f_isPrime)
```

### Compiling {.unlisted .unnumbered}

```{r '303-Compile', eval=FALSE}
# #To Pre-Compile a Function for faster performance
f_isPrimeC <- cmpfun(f_isPrime)
```

### Profiling {.unlisted .unnumbered}

```{r '303-Profile', eval=FALSE}
# #To Profile a Function Calls for improvements
Rprof("file.out")
f_isPrime(2147483647L)
#f_getPrimesUpto(131071L)
Rprof(NULL)
summaryRprof("file.out")
```

### Legacy A {.unlisted .unnumbered}

```{r '303-isPrime-NOT', eval=FALSE}
# #Functions to check for PRIME - All of them have various problems
# #"-3L -2L -1L 0L 1L 8L" FALSE "2L 3L ... 524287L 2147483647L" TRUE
isPrime_a <- function(x) {
  # #Fails for "2147483647L" Error: cannot allocate vector of size 8.0 Gb
  if (x == 2L) {
    return(TRUE)
  } else if (any(x %% 2:(x-1) == 0)) {
    return(FALSE)
  } else return(TRUE)
}

isPrime_b <- function(x){
  # #Comparison of Division and Integer Division by 1,2,...,x
  # #Fails for "2147483647L" Error: cannot allocate vector of size 16.0 Gb
  # #Fails for "-ve and zero" Error: missing value where TRUE/FALSE needed
  # vapply(x, function(y) sum(y / 1:y == y %/% 1:y), integer(1L)) == 2L
  if(sum(x / 1:x == x %/% 1:x) == 2) {
    return(TRUE) 
  } else return(FALSE)
}

isPrime_c <- function(x) {
  # #RegEx Slowest: Iit converts -ve values and coerce non-integers which may result in bugs
  x <- abs(as.integer(x))
  if(x > 8191L) {
    print("Don't run this with large values. RegEx is really slow.")
    stop()
  }
  !grepl('^1?$|^(11+?)\\1+$', strrep('1', x))
}

isPrime_d <- function(x) {
  # #Fails for "1" & returns TRUE
  # #Fails for "-ve and zero" Error: NA/NaN argument
  if(x == 2L || all(x %% 2L:max(2, floor(sqrt(x))) != 0)) {
    return(TRUE)
  } else return(FALSE)
}

isPrime_e <- function(x) {
  # #Fails for "-ve and zero" Error: NA/NaN argument
  # #This is the most robust which can be improved by conditional check for positive integers
  # #However, this checks the number against ALL Smaller values including non-primes
  if(x == 2L || all(x %% 2L:ceiling(sqrt(x)) != 0)) {
    # # "seq.int(3, ceiling(sqrt(x)), 2)" is slower
    return(TRUE)
  } else {
    ## (any(x %% 2L:ceiling(sqrt(x)) == 0))
    ## (any(x %% seq.int(3, ceiling(sqrt(x)), 2) == 0))
    ## NOTE Further, if sequence starts from 3, add 2 also as a Prime Number
    return(FALSE)
  }
}
```

### Legacy B {.unlisted .unnumbered}

```{r '303-getPrime-NOT', eval=FALSE}
# #131071 (12,251th), 524287 (43,390th), 2147483647 (105,097,565th)
aa <- 1:131071
# #Following works but only till 524287L, Memory Overflow ERROR for 2147483647L
bb <- aa[f_isPrimeV(aa)]

getPrimeUpto_a <- function(x){
  # #Extremely Slow, Can't go beyond 8191L in benchmark testing
  if(x < 2) return("ERROR")
  y <- 2:x
  primes <- rep(2L, x)
  j <- 1L
  for (i in y) {
    if (!any(i %% primes == 0)) {
      j <- j + 1L
      primes[j] <- i
	  #cat(paste0("i=", i, ", j=", j, ", Primes= ",paste0(head(primes, j), collapse = ", ")))
    }
	#cat("\n")
  }
  result <- head(primes, j)
  #str(result)
  #cat(paste0("Head: ", paste0(head(result), collapse = ", "), "\n"))
  #cat(paste0("Tail: ", paste0(tail(result), collapse = ", "), "\n"))
  return(result)
}

getPrimeUpto_b <- function(x){
# #https://stackoverflow.com/questions/3789968/
  # #This is much faster even from the "aa[f_isPrimeV(aa)]"
    if(x < 2) return("ERROR")
    y <- 2:x
    i <- 1
    while (y[i] <= sqrt(x)) {
        y <-  y[y %% y[i] != 0 | y == y[i]]
        i <- i+1
    }
	result <- y
    #str(result)
    #cat(paste0("Head: ", paste0(head(result), collapse = ", "), "\n"))
    #cat(paste0("Tail: ", paste0(tail(result), collapse = ", "), "\n"))
    return(result)
}

getPrimeUpto_c <- function(x) {
  # #Problems and Slow
  # #Returns a Vetor of Primes till the Number i.e. f_getPrimesUpto(7) = (2, 3, 5, 7)
  # #NOTE: f_getPrimesUpto(1) and f_getPrimesUpto(2) both return "2"
  if(!is.integer(x)) {
    cat("Error! Integer required. \n")
    stop()
  } else if(!identical(1L, length(x))) {
    cat("Error! Unit length vector required. \n")
    stop()
  } else if(x <= 0L) {
    cat("Error! Positive Integer required. \n")
    stop()
  } else if(x > 2147483647) {
    cat(paste0("Doubles are stored as approximation. Prime will not be calculated for value higher than '2147483647' \n"))
    stop()
  }
  
  # #Can't create vector of length 2147483647L and also not needed that many
  # #ceiling(sqrt(7L)) return 3, however we need length 4 (2, 3, 5, 7)
  # #So, added extra 10
  #primes <- rep(NA_integer_, 10L + sqrt(2L))
  primes <- rep(2L, 10L + sqrt(2L))
  j <- 1L
  primes[j] <- 2L
  #
  i <- 2L
  while(i <= x) {
    # #na.omit() was the slowest step, so changed all NA to 2L in the primes
    #k <- na.omit(primes[primes <= ceiling(sqrt(i))])
    k <- primes[primes <= ceiling(sqrt(i))]
    if(all(as.logical(i %% k))) {
      j <- j + 1
      primes[j] <- i
    }  
    # #Increment with INTEGER Addition
    i = i + 1L
  }
  result <- primes[complete.cases(primes)]
  str(result)
  cat(paste0("Head: ", paste0(head(result), collapse = ", "), "\n"))
  cat(paste0("Tail: ", paste0(tail(result), collapse = ", "), "\n"))
  return(result)
}

getPrimeUpto_d <- function(n = 10L, i = 2L, primes = c(2L), bypass = TRUE){
  # #Using Recursion is NOT a good solution
  # #Function to return N Primes upto 1000 Primes (7919) or Max Value reaching 10000.
  if(i > 10000){
    cat("Reached 10000 \n")
    return(primes)
  }
  if(bypass) {
    maxN <- 1000L
    if(!is.integer(n)) {
      cat("Error! Integer required. \n")
      stop()
    } else if(!identical(1L, length(n))) {
      cat("Error! Unit length vector required. \n")
      stop()
    } else if(n <= 0L) {
      cat("Error! Positive Integer required. \n")
      stop()
    } else if(n > maxN) {
      cat(paste0("Error! This will calculate only upto ", maxN, " prime Numebers. \n"))
      stop()
    }
  }
  if(length(primes) < n) {
    if(all(as.logical(i %% primes[primes <= ceiling(sqrt(i))]))) {
      # #Coercing 0 to FALSE, Non-zero Values to TRUE
      # # "i %% 2L:ceiling(sqrt(i))" checks i agains all integers till sqrt(i)
      # # "primes[primes <= ceiling(sqrt(i))]" checks i against only the primes till sqrt(i)
      # #However, the above needs hardcoded 2L as prime so the vector is never empty
      # #Current Number is Prime, so include it in the vector and check the successive one
      f_getPrime(n, i = i+1, primes = c(primes, i), bypass = FALSE)
    } else {
      # #Current Number is NOT Prime, so check the successive one
      f_getPrime(n, i = i+1, primes = primes, bypass = FALSE)
    }
  } else {
    # #Return the vector when it reaches the count
    return(primes)
  }
}
```

## Measures of Location

### Mean {#mean-be03 .tabset .tabset-fade}

```{definition 'Mean'}
Given a data set ${X=\{x_1,x_2,\ldots,x_n\}}$, the \textcolor{pink}{mean $\overline{x}$} is the sum of all of the values ${x_1,x_2,\ldots,x_n}$ divided by the count $n$.
```

- Refer equation \@ref(eq:mean)
  - Sample mean is denoted by \textcolor{pink}{$\overline{x}$} (x bar) and Population mean is denoted by \textcolor{pink}{$\mu$}. 
  - Mean is the most commonly used measure of central location, even though it is influenced by extreme values.

\begin{equation} 
  \bar{x} = \frac{1}{n}\left (\sum_{i=1}^n{x_i}\right ) = \frac{x_1+x_2+\cdots +x_n}{n}
  (\#eq:mean)
\end{equation} 

In the mean calculation, normally each $x_i$ is given equal importance or weightage of $1/n$. However, in some instances the mean is computed by giving each observation a weight that reflects its relative importance. A mean computed in this manner is referred to as the \textcolor{pink}{weighted mean}, as given in equation \@ref(eq:mean-weighted)

\begin{equation} 
  \bar{x} = \frac{\sum_{i=1}^n{w_ix_i}}{\sum_{i=1}^n{w_i}}
  (\#eq:mean-weighted)
\end{equation} 

\textcolor{orange}{Caution:} Unit of mean is same as unit of the variable e.g. cost_per_kg thus 'w' would be 'kg'.

#### Mean {.unlisted .unnumbered}

```{r '303-Mean'}
aa <- 1:10
# #Mean of First 10 Numbers
mean(aa)
```

#### More {.unlisted .unnumbered}

```{r '303-MeanMore'}
aa <- 1:10
# #Mean of First 10 Numbers
ii <- mean(aa)
print(ii)
jj <- sum(aa)/length(aa)
stopifnot(identical(ii, jj))
#
# #Mean of First 10 Prime Numbers (is neither Prime nor Integer)
mean(f_getRDS(xxPrimes)[1:10])
#
# #Mean of First 100 Digits of PI
f_getRDS(xxPI)[1:100, ] %>% pull(VAL) %>% mean()
```

#### Weighted Mean {.unlisted .unnumbered}

```{r '303-WeightedMean'}
aa <- tibble(Purchase = 1:5, cost_per_kg = c(3, 3.4, 2.8, 2.9, 3.25), 
             kg = c(1200, 500, 2750, 1000, 800))
# #NOTE that unit of mean is same as unit of the variable e.g. cost_per_kg thus 'w' would be 'kg'
(ii <- sum(aa$cost_per_kg * aa$kg)/sum(aa$kg))
jj <- with(aa, sum(cost_per_kg * kg)/sum(kg))
kk <- weighted.mean(x = aa$cost_per_kg, w = aa$kg)
stopifnot(all(identical(ii, jj), identical(ii, kk)))
```

### Median {#median-be03 .tabset .tabset-fade}

```{definition 'Median'}
\textcolor{pink}{Median} of a population is any value such that at most half of the population is less than the proposed median and at most half is greater than the proposed median.  
```

- Refer equation \@ref(eq:median)
  - The median is the value in the middle when the data is sorted
  - For an odd number of observations, the median is the middle value.
  - For an even number of observations, the median is the average of the two middle values.
  - Although the mean is the more commonly used measure of central location, whenever a data set contains extreme values, the median is preferred.
  - The median is well-defined for any ordered data, and is independent of any distance metric. 
    - The median can thus be applied to classes which are ranked but not numerical (ordinal), although the result might be halfway between classes if there is an even number of cases. 

\begin{equation} 
  \begin{align} 
    \text{if n is odd, } median(x) & = x_{(n+1)/2} \\
    \text{if n is even, } median(x) & = \frac{x_{(n/2)}+x_{(n/2)+1}}{2}
  \end{align}
  (\#eq:median)
\end{equation} 

#### Median {.unlisted .unnumbered}

```{r '303-Median'}
aa <- 1:10 
# #Median of First 10 Numbers
median(aa)
```

#### More {.unlisted .unnumbered}

```{r '303-MedianMore'}
aa <- 1:10 
# #Median of First 10 Numbers
median(aa)
#
# #Median of First 10 Prime Numbers (is NOT prime)
median(f_getRDS(xxPrimes)[1:10])
#
# #Median of First 100 Digits of PI
f_getRDS(xxPI)[1:100, ] %>% pull(VAL) %>% median()
```

### Geometric Mean {.tabset .tabset-fade}

```{definition 'Geometric-Mean'}
The \textcolor{pink}{geometric mean $\overline{x}_g$} is a measure of location that is calculated by finding the n^{th} root of the product of $n$ values. 
```

- Refer equation \@ref(eq:geometric-mean)
  - The geometric mean applies only to positive numbers
  - The geometric mean is often used for a set of numbers whose values are meant to be multiplied together or are exponential in nature
  - For all positive data sets containing at least one pair of unequal values, the harmonic mean is always the least of the three means, while the arithmetic mean is always the greatest of the three and the geometric mean is always in between.

\begin{equation} 
  \overline{x}_g = \left(\prod _{i=1}^{n} x_i\right)^{\frac{1}{n}} = \sqrt[{n}]{x_1 x_2 \ldots x_n}
  (\#eq:geometric-mean)
\end{equation} 

#### Geometric Mean {.unlisted .unnumbered }

```{r '303-GeometricMean'}
aa <- 1:10
# #Geometric Mean of of First 10 Numbers
exp(mean(log(aa)))
```

#### More {.unlisted .unnumbered }

```{r '303-GMmore'}
aa <- 1:10
# #Geometric Mean of of First 10 Numbers
ii <- exp(mean(log(aa)))
jj <- prod(aa)^(1/length(aa))
stopifnot(identical(ii, jj))
#
# #Geometric Mean of First 10 Prime Numbers 
exp(mean(log(f_getRDS(xxPrimes)[1:10])))
```

### Mode {#mode-be03 .tabset .tabset-fade}

```{definition 'Mode'}
The \textcolor{pink}{mode} is the value that occurs with greatest frequency.
```

- The median makes sense when there is a linear order on the possible values. Unlike median, the concept of mode makes sense for any random variable assuming values from a vector space.


#### Mode {.unlisted .unnumbered }

```{r '303-Mode'}
# #Mode of First 100 Digits of PI
bb <- f_getRDS(xxPI)[1:100, ] %>% pull(VAL)
f_getMode(bb)
```

#### More {.unlisted .unnumbered }

```{r '303-ModeMore'}
# #Mode of First 100 Digits of PI
bb <- f_getRDS(xxPI)[1:100, ]
#
# #Get Frequency
bb %>% count(VAL)
#
# #Get Mode
bb %>% pull(VAL) %>% f_getMode()
```

#### f_getMode() {.unlisted .unnumbered }

```{r '303F-getMode', ref.label=c('A10A-getMode')}
#
```

### Percentiles {#percentiles-be03 .tabset .tabset-fade}

```{definition 'Percentile'}
A \textcolor{pink}{percentile} provides information about how the data are spread over the interval from the smallest value to the largest value. For a data set containing $n$ observations, the $p^{th}$ percentile divides the data into two parts: approximately p% of the observations are less than the $p^{th}$ percentile, and approximately (100 – p)% of the observations are greater than the $p^{th}$ percentile. 
```

- Refer equation \@ref(eq:percentiles)
  - Percentile is the value which divides the data into two groups when it is sorted
  - \textcolor{pink}{Quartiles} are specific percentiles of 25%, 50% and 75%
  - \textcolor{pink}{Median} is 50% percentile
  - \textcolor{orange}{Caution:} Excel "PERCENTILE.EXC" calculations match with `type =6` option of `quantile()`, default is `type =7`
  
\begin{equation} 
  L_p = \frac{p}{100}(n+1)
  (\#eq:percentiles)
\end{equation}

#### Percentiles {.unlisted .unnumbered}

```{r '303-Percentile'}
# #First 100 Digits of PI
bb <- f_getRDS(xxPI)[1:100, ]
#
# #50% Percentile of Digits i.e. Median
quantile(bb$VAL, 0.5)
```

#### More {.unlisted .unnumbered}

```{r '303-PercentileMore'}
# #First 100 Digits of PI
bb <- f_getRDS(xxPI)[1:100, ]
#
# #50% Percentile of Digits i.e. Median
ii <- quantile(bb$VAL, 0.5)
print(ii)
jj <- median(bb$VAL)
stopifnot(identical(unname(ii), jj))
# 
# #All Quartiles
quantile(bb$VAL, seq(0, 1, 0.25))
# #summary()
summary(bb$VAL)
#
# #To Match with Excel "PERCENTILE.EXC" use type=6 in place of default type=7
quantile(bb$VAL, seq(0, 1, 0.25), type = 6)
```


## Measures of Variability
In addition to measures of location, it is often desirable to consider measures of variability, or dispersion.

- Range `range()`
  - (Largest value - Smallest value) i.e. `max() - min()`
  - Range is based on only two of the observations and thus is highly influenced by extreme values.
- Interquartile Range (IQR) `IQR()`
  - The difference between the third quartile, and the first quartile
  - It overcomes the dependency on extreme values
- Mean Absolute Error (MAE)
  - $MAE = \frac{\sum |x_i - \overline{x}|}{n}$

### Variance {#variance-be03 .tabset .tabset-fade}

```{definition 'Variance'}
The \textcolor{pink}{variance} is based on the difference between the value of each observation $x_i$ and the mean $\overline{x}$. The average of the squared deviations is called the variance. 
```

- Refer equation \@ref(eq:variance)
  - Sample Variance is denoted by \textcolor{pink}{$s^2$} and Population Variance is denoted by \textcolor{pink}{$\sigma^2$}
  - The variance is a measure of variability that utilizes all the data.
  - The difference between each $x_i$ and the mean ($\overline{x}, \mu$) is called a \textcolor{pink}{deviation about the mean} i.e. ($x_i - \overline{x}$). Sum of deviation about the mean is always zero i.e. $\sum (x_i - \overline{x}) =0$
  - In the computation of the variance, the deviations about the mean are squared.

\begin{equation} 
  \begin{align} 
    \sigma^2 &= \frac{1}{n} \sum _{i=1}^{n} \left(x_i - \mu \right)^2 \\
    s^2 &= \frac{1}{n-1} \sum _{i=1}^{n} \left(x_i - \overline{x} \right)^2
  \end{align}
  (\#eq:variance)
\end{equation} 

### Standard Deviation {#sd-be03 .tabset .tabset-fade}

```{definition 'Standard-Deviation'}
The \textcolor{pink}{standard deviation ($s, \sigma$)} is defined to be the positive square root of the variance.
```

- Refer equation \@ref(eq:sd)
  - Standard deviation for sample is denoted by \textcolor{pink}{${s}$} and for Population by \textcolor{pink}{$\sigma$}
  - The \textcolor{pink}{coefficient of variation} is a relative measure of variability. It measures the standard deviation relative to the mean. It is given in percentage as $100 \times \sigma / \mu$

\begin{equation} 
  \begin{align} 
     \sigma &= \sqrt{\frac{1}{N} \sum_{i=1}^N \left(x_i - \mu\right)^2} \\
    {s} &= \sqrt{\frac{1}{N-1} \sum_{i=1}^N \left(x_i - \bar{x}\right)^2}
  \end{align}
  (\#eq:sd)
\end{equation} 

## Measures of Distribution Shape

### Skewness

"ForLater" 

## Relative Location

### z-Scores {#z-scores-be03 .tabset .tabset-fade}

Measures of relative location help us determine how far a particular value is from the mean. By using both the mean and standard deviation, we can determine the relative location of any observation. 

```{definition 'TheSample'}
A sample of \textcolor{pink}{${n}$} observations given by ${X=\{x_1,x_2,\ldots,x_n\}}$ have a sample mean \textcolor{pink}{$\overline{x}$} and the sample standard deviation, \textcolor{pink}{${s}$}.
```

```{definition 'z-Scores'}
The \textcolor{pink}{z-score, $z_i$}, can be interpreted as the number of standard deviations $x_i$ is from the mean $\overline{x}$. It is associated with each $x_i$. The z-score is often called the \textcolor{pink}{standardized value}. 
```

- Refer equation \@ref(eq:z-scores)
  -  For example, $z_1 = 1.2$  would indicate that $x_1$ is 1.2 standard deviations greater than the sample mean. Similarly, $z_2 = -0.5$ would indicate that $x_2$ is 0.5 standard deviation less than the sample mean. 
  - A z-score greater than zero occurs for observations with a value greater than the mean, and a z-scoreless than zero occurs for observations with a value less than the mean. 
  - A z-score of zero indicates that the value of the observation is equal to the mean.
  - The z-score for any observation can be interpreted as a measure of the relative location of the observation in a data set.
  - The process of converting a value for a variable to a z-score is often referred to as a \textcolor{pink}{z transformation}.

\begin{equation} 
  z_i = \frac{x_i - \overline{x}}{s}
  (\#eq:z-scores)
\end{equation} 

### Chebyshev Theorem {.tabset .tabset-fade}

```{definition 'Chebyshev-Theorem'}
\textcolor{pink}{Chebyshev Theorem} can be used to make statements about the proportion of data values that must be within a specified number of standard deviations $\sigma$, of the mean $\mu$.
```

- Refer to \@ref(def:Chebyshev-Theorem)
  - \textcolor{pink}{Chebyshev Theorem:} At least $(1-1/z^2)$ of the data values must be within z standard deviations of the mean, where z is any value greater than 1. 
    - Thus, at least 75% of the data values must be within $\overline{x} \pm 2s$, 89% within $\overline{x} \pm 3s$, and 94% $\overline{x} \pm 4s$.
  - Chebyshev's theorem can be applied to any data set regardless of the shape of the distribution of the data. 
  - Ex: Test scores of 100 students have $(\mu = 70, \sigma = 5)$
  - How many students had test scores between 60 and 80
    - From equation \@ref(eq:z-scores), $z_{60} = \frac{60 - 70}{5} = -2$
    - Similarly, $z_{80} = \frac{80 - 70}{5} = +2$
    - According to theorem \@ref(def:Chebyshev-Theorem), values that must be within $z$ standard deviation are
      - ${(1-1/z^2) = (1 - 1/2^2) = 0.75 = 75\%}$
      - i.e. 75 students must have test scores between 60 and 80
  - How many students had test scores between 58 and 82
    - $z_{58} = -2.4, z_{82} = +2.4$
    - ${(1 - 1/2.4^2) \approx 0.826 \approx 83\%}$
      - i.e. 83 students must have test scores between 58 and 82

### Empirical Rule {.tabset .tabset-fade}

```{definition 'Empirical-Rule'}
\textcolor{pink}{Empirical rule} is used to compute the percentage of data values that must be within one, two, and three standard deviations $\sigma$ of the mean $\mu$ for a normal distribution.
```

- According to the \textcolor{pink}{empirical rule}, for a Normal distribution
  - $\approx 68.26\%$ of the data values will be within $\overline{x} \pm s$
  - $\approx 95.44\%$ within $\overline{x} \pm 2s$
  - and almost all $\approx 99.74\%$ of the data values will be within $\overline{x} \pm 3s$. 

## Outliers

- Sometimes unusually large or unusually small values are called \textcolor{pink}{outliers}. 
  - data value that has been incorrectly recorded /included - should be removed
  - unusual data value that has been recorded correctly and belongs in the data set - should be kept
- Standardized values (z-scores) can be used to identify outliers. 
  - Empirical rule (\@ref(def:Empirical-Rule)) allows us to conclude that for normal distribution, almost all the data values will be within three standard deviations of the mean $(\overline{x} \pm 3s)$. 
  - Hence, in using z-scores to identify outliers, we recommend treating any data value with a z-score less than −3 or greater than +3 as an outlier. 
  - Such data values can then be reviewed for accuracy and to determine whether they belong in the data set.
- Another approach to identifying outliers is based upon IQR 
  - $\text{Lower Limit} = Q_1 - 1.5 \space \text{IQR}$ and $\text{Upper Limit} = Q_3 + 1.5 \space \text{IQR}$ 
  - An observation is classified as an outlier if its value is less than the lower limit or greater than the upper limit. 

## Summary {#BoxPlot .tabset .tabset-fade}

Five-Number Summary is used to quickly summarise a dataset. i.e. Min, Q1, Median, Q3, Max

- A boxplot is a graphical display of data based on a five-number summary. 
  - By using the interquartile range, IQR = Q3 − Q1, limits are located at 1.5(IQR) below Q1 and 1.5(IQR) above Q3
  - The \textcolor{pink}{whiskers} are drawn from the ends of the box to the smallest and largest values inside the limits 
  - Boxplots can also be used to provide a graphical summary of two or more groups and facilitate visual comparisons among the groups. 


### BoxPlot {#boxplot-be03 .unlisted .unnumbered}

```{r '303-BoxPlot', include=FALSE}
# #nycflights13::weather
bb <- weather
# #NA are present in the data
summary(bb$temp)
#
# #BoxPlot
g303_01 <- bb %>% drop_na(temp) %>% mutate(month = factor(month, ordered = TRUE)) %>% {
    ggplot(data = ., mapping = aes(x = month, y = temp)) +
    #geom_violin() +
    geom_boxplot(aes(fill = month), outlier.colour = 'red', notch = TRUE) +
    stat_summary(fun = mean, geom = "point", size = 2, color = "steelblue") + 
    scale_y_continuous(breaks = seq(0, 110, 10), limits = c(0, 110)) +
    #geom_point() +
    #geom_jitter(position=position_jitter(0.2)) +
    k_gglayer_box +
    theme(legend.position = 'none') +
    labs(x = "Months", y = "Temperature", title = "BoxPlot", subtitle = "With Mean & Notch")
}
```

```{r '303-SaveBox', include=FALSE}
ggsave(paste0(.z$PX, "g303_boxplot_01", ".png"), plot = g303_01)
```

```{r '303G-Box01', echo=FALSE, fig.cap="geom_boxplot()", include=TRUE}
knitr::include_graphics(paste0(.z$PX, "g303_boxplot_01", ".png"))
```

### Code {.unlisted .unnumbered}
```{r '303-BoxPlot-A', ref.label=c('303-BoxPlot'), eval=FALSE}
#
```

## Relationship between Two Variables

### Covariance {#covariance-be03 .tabset .tabset-fade}

```{definition 'Covariance'}
\textcolor{pink}{Covariance} is a measure of linear association between two variables. Positive values indicate a positive relationship; negative values indicate a negative relationship.
```

- Refer equation \@ref(eq:covariance)
  - For a sample of size $n$ with the observations $(x_1, y_1), (x_2, y_2)$, and so on, the covariance is given by equation \@ref(eq:covariance)
  - A positive value for $s_{xy}$ indicates a positive linear association between x and y; that is, as the value of x increases, the value of y increases. Similarly a negative value shows a negative linear association.
    - In the example, $s_{xy} = 11$
  - If the points are evenly distributed in the scatterplot, the value of $s_{xy}$ will be close to zero, indicating no linear association between x and y.
  - \textcolor{orange}{Caution:} Problem with using covariance as a measure of the strength of the linear relationship is that the value of the covariance depends on the units of measurement for x and y.

\begin{equation} 
  \begin{align} 
    \sigma_{xy} &= \frac{\sum (x_i - \mu_x)(y_i - \mu_y)}{n} \\
    s_{xy} &= \frac{\sum (x_i - \overline{x})(y_i - \overline{y})}{n-1} 
  \end{align}
  (\#eq:covariance)
\end{equation} 

```{r '303-Trendline', include=FALSE}
bb <- f_getRDS(be0214) 

# #Define the formula for Trendline calculation
k_gg_formula <- y ~ x
#
# #Scatterplot, Trendline Equation, R2, mean x & y
g303_02 <- bb %>% {
  ggplot(data = ., aes(x = Commercials, y = Sales)) + 
  geom_smooth(method = 'lm', formula = k_gg_formula, se = FALSE) +
  stat_poly_eq(aes(label = paste0("atop(", ..eq.label.., ", \n", ..rr.label.., ")")), 
               formula = k_gg_formula, eq.with.lhs = "italic(hat(y))~`=`~",
               eq.x.rhs = "~italic(x)", parse = TRUE) +
  geom_vline(aes(xintercept = round(mean(Commercials), 3)), color = 'red', linetype = "dashed") +
  geom_hline(aes(yintercept = round(mean(Sales), 3)), color = 'red', linetype = "dashed") +
  geom_text(aes(label = TeX(r"($\bar{x} = 3$)", output = "character"), 
                x = round(mean(Commercials), 3), y = -Inf), 
            color = 'red', , hjust = -0.2, vjust = -0.5, parse = TRUE, check_overlap = TRUE) + 
  geom_text(aes(label = TeX(r"($\bar{y} = 51$)", output = "character"), 
                x = Inf, y = round(mean(Sales), 3)), 
            color = 'red', , hjust = 1.5, vjust = -0.5, parse = TRUE, check_overlap = TRUE) + 
  geom_point() +
  k_gglayer_scatter +
  labs(x = "Commercials", y = "Sales ($100s)", title = "Scatter Plot",
       subtitle = TeX(r"(Trendline Equation, $R^{2}$, $\bar{x}$ and $\bar{y}$)"))
}
```

```{r '303-SaveTrend', include=FALSE}
ggsave(paste0(.z$PX, "g303_trendline_01", ".png"), plot = g303_02)
```

```{r '303G-Trend01', echo=FALSE, fig.cap="Added Mean", include=FALSE}
knitr::include_graphics(paste0(.z$PX, "g303_trendline_01", ".png"))
```

```{r '303G-Trend02', echo=FALSE, fig.cap="Original", include=FALSE}
knitr::include_graphics(paste0(.z$PX, "g302_trendline_01", ".png"))
```

```{r '303G-Trend03', ref.label=c('303G-Trend02', '303G-Trend01'), echo=FALSE, fig.cap="Scatter Plot Quadrants for Covariance"}
#
```

#### Covariance {.unlisted .unnumbered}

```{r '303-Covariance'}
# #Get 'Deviation about the mean' i.e. devX and devY and their Product devXY
ii <- bb %>% 
  mutate(devX = Commercials - mean(Commercials), devY = Sales - mean(Sales), devXY = devX * devY) 
#
# #Sample Covariance
sxy <- sum(ii$devXY) / {length(ii$devXY) -1}
print(sxy)
```

#### Code {.unlisted .unnumbered}

```{r '303-Trendline-A', ref.label='303-Trendline', eval=FALSE}
#
```

#### More Text  {.unlisted .unnumbered}

- [This](https://stats.stackexchange.com/questions/229667 "https://stats.stackexchange.com")
  - Unlike Pearson correlation, covariance itself is not a measure of the magnitude of linear relationship. It is a measure of co-variation (which could be just monotonic). This is because covariance depends not only on the strength of linear association but also on the magnitude of the variances. 
- More details are in following links
  -  [This](https://stats.stackexchange.com/questions/17537   "https://stats.stackexchange.com/questions/17537/understanding-variance-intuitively")
  -  [This](https://stats.stackexchange.com/questions/26/what-is-a-standard-deviation   "https://stats.stackexchange.com/questions/26/what-is-a-standard-deviation")
  -  [This](https://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who  -understands-only-the-mean "https://stats.stackexchange.com/questions/18058/how-would-you-explain-covaria  nce-to-someone-who-understands-only-the-mean")
  -  [This](https://stats.stackexchange.com/questions/12842/covariance-and-independence   "https://stats.stackexchange.com/questions/12842/covariance-and-independence")
  -  [This](https://stats.stackexchange.com/questions/18082/how-would-you-explain-the-difference-between-co  rrelation-and-covariance "https://stats.stackexchange.com/questions/18082/how-would-you-explain-the-diffe  rence-between-correlation-and-covariance")
  -  [This](https://stats.stackexchange.com/questions/29713/what-is-covariance-in-plain-language   "https://stats.stackexchange.com/questions/29713/what-is-covariance-in-plain-language")
  -  [This](https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance   "https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance")
  -  [This](https://stats.stackexchange.com/questions/229667/difference-between-correlation-and-covariance-is-covariance-only-useful-if-the "https://stats.stackexchange.com/questions/229667/difference-between-correlation-and-covariance-is-covariance-only-useful-if-the")
  -  [This](https://stats.stackexchange.com/questions/483964/how-come-covariance-can-pick-up-non-linear-relationships-but-correlation-cant "https://stats.stackexchange.com/questions/483964/how-come-covariance-can-pick-up-non-linear-relationships-but-correlation-cant")
  -  [This](https://stats.stackexchange.com/questions/9415/measuring-non-linear-dependence "https://stats.stackexchange.com/questions/9415/measuring-non-linear-dependence")

### Correlation Coefficient {#correlation-be03 .tabset .tabset-fade}

```{definition 'Correlation-Coefficient'}
\textcolor{pink}{Correlation coefficient} is a measure of linear association between two variables that takes on values between −1 and +1. Values near +1 indicate a strong positive linear relationship; values near −1 indicate a strong negative linear relationship; and values near zero indicate the lack of a linear relationship.
```

- Refer equation \@ref(eq:correlation) & Table \@ref(tab:303-CorrelationKbl)
  - The 'Pearson Product Moment Correlation Coefficient' or \textcolor{pink}{sample correlation coefficient} is computed by dividing the sample covariance $s_{xy}$ by the product of the sample standard deviation of x ($s_{x}$) and the sample standard deviation of y ($s_{y}$).
    - Values close to −1 (negative) or +1 (positive) indicate a strong linear relationship. The closer the correlation is to zero, the weaker the relationship.
  - In the example, $s_{xy} = 11$ (Equation \@ref(eq:covariance)) and $s_{x} = 1.49$, $s_{y} = 7.93$ (Equation \@ref(eq:sd))
  - Thus, $r_{xy} = 0.93$
  - \textcolor{orange}{Caution:} Correlation provides a measure of linear association and not necessarily causation. A high correlation between two variables does not mean that changes in one variable will cause changes in the other variable.
  - \textcolor{orange}{Caution:} Because the correlation coefficient measures only the strength of the linear relationship between two quantitative variables, it is possible for the correlation coefficient to be near zero, suggesting no linear relationship, when the relationship between the two variables is nonlinear. 

\begin{equation} 
  \begin{align} 
    \rho_{xy} &= \frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}} \\
    r_{xy} &= \frac{s_{xy}}{s_{x}s_{y}}
  \end{align}
  (\#eq:correlation)
\end{equation} 

#### Correlation  {.unlisted .unnumbered}

```{r '303-Correlation', include=FALSE}
jj <- ii %>% mutate(devXsq = devX * devX, devYsq = devY * devY)
# #Sample Covariance Sx, Sample Standard Deviations Sx Sy
sxy <- sum(ii$devXY) / {nrow(ii) -1}
sx <- round(sqrt(sum(jj$devXsq) / {nrow(jj) -1}), 2)
sy <- round(sqrt(sum(jj$devYsq) / {nrow(jj) -1}), 2)
cat(paste0("Sxy =", sxy, ", Sx =", sx, ", Sy =", sy, "\n"))
#
# #Correlation Coefficient Rxy
rxy <- round(sxy / {sx * sy}, 2)
cat(paste0("Correlation Coefficient Rxy =", rxy, "\n"))
```

```{r '303-Correlation-A', ref.label=c('303-Covariance', '303-Correlation'), eval=TRUE}
#
```

#### Data  {.unlisted .unnumbered}

```{r '303-CorrelationKbl', echo=FALSE}
bb <- jj
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kbl(bb,
  caption = 'Correlation Calculation',
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r '303-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, ii, jj, kk, tbl, xxPI, bb, g303_01, g303_02, rxy, sx, sxy, sy)
```

```{r '303-Validation', include=FALSE, cache=FALSE}
# #Summarised Packages and Objects
f_()
#
difftime(Sys.time(), k_start)
```

****
