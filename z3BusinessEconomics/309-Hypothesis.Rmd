# Hypothesis Tests {#c09}

```{r 'C09', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- This chapter covers 
  - [Hypothesis Testing](#hypothesis-c09 "c09"), [Type I and Type II Errors](#errors-ab-c09, "c09"), [Test Statistic ](##test-stat-c09 "c09"), [One Tail Test](#one-tail-c09 "c09"), [Two Tail Test](#two-tail-c09 "c09"), [Tests with unknown sigma](#get-pt-c09 "c09")
  - [Calculate P(t) by pt()](#get-pt-c09 "c09")
  - "ForLater" - Population Proportion, Calculating the Probability of Type II Errors, Determining the Sample Size

## Hypothesis Testing {#hypothesis-c09}

```{definition 'Hypothesis-Testing'}
\textcolor{pink}{Hypothesis testing} is a process in which, using data from a sample, an inference is made about a population parameter or a population probability distribution. 
```

Note:

- Hypothesis testing is used to determine whether a statement about the value of a population parameter should or should not be rejected.
- It is the process to check whether the sample information is matching with population information.
- The hypothesis testing procedure uses data from a sample to test the two competing statements indicated by ${H_0}$ and ${H_a}$

```{definition 'Hypothesis-Null'}
\textcolor{pink}{Null Hypothesis $(H_0)$} is a tentative assumption about a population parameter. It is assumed True, by default, in the hypothesis testing procedure.
```

```{definition 'Hypothesis-Alternative'}
\textcolor{pink}{Alternative Hypothesis $(H_a)$} is the complement of the Null Hypothesis. It is concluded to be True, if the Null Hypothesis is rejected.
```

Note:

- The conclusion that the alternative hypothesis $(H_a)$ is true is made if the sample data provide sufficient evidence to show that the null hypothesis $(H_0)$ can be rejected.
- The null and alternative hypotheses are competing statements about the population. Either the null hypothesis ${H_0}$ is true or the alternative hypothesis ${H_a}$ is true, but not both. 


## Developing Null and Alternative Hypotheses 

All hypothesis testing applications involve collecting a sample and using the sample results to provide evidence for drawing a conclusion.

In some situations it is easier to identify the alternative hypothesis first and then develop the null hypothesis.

- The Alternative Hypothesis as a Research Hypothesis 
  - A new fuel injection system designed to increase the miles-per-gallon rating from the current value 24 miles per gallon.
    - $H_a : \mu > 24 \iff H_0: \mu \leq 24$
  - A new teaching method is developed that is believed to be better than the current method. 
    - $H_a : \text{\{New method is better}\} \iff H_0: \text{\{New method is NOT better}\}$
  - A new sales force bonus plan is developed in an attempt to increase sales. 
    - $H_a : \text{\{New plan increases sales}\} \iff H_0: \text{\{New plan does not increase sales}\}$
  - A new drug is developed with the goal of lowering blood pressure more than an existing drug. 
    - ${H_a}$ : New drug lowers blood pressure more than the existing drug
    - ${H_0}$ : New drug does not provide lower blood pressure than the existing drug
  - In each case, rejection of the null hypothesis ${H_0}$ provides statistical support for the research hypothesis ${H_a}$.

- The Null Hypothesis as an Assumption to Be Challenged
  - The null hypothesis ${H_0}$ expresses the belief or assumption about the value of the population parameter. The alternative hypothesis ${H_a}$ is that the belief or assumption is incorrect.
  - Ex: The label on a soft drink bottle states that it contains 67.6 fluid ounces. 
    - We consider the label correct provided the population mean filling weight for the bottles is \textcolor{pink}{at least} 67.6 fluid ounces. 
    - Without any reason to believe otherwise, we would give the manufacturer the benefit of the doubt and assume that the statement provided on the label is correct. 
    - $H_0 : \mu \geq 67.6 \iff H_a: \mu < 67.6$
    - If the sample results lead to the conclusion to reject ${H_0}$, the inference that $H_a: \mu < 67.6$ is true can be made. With this statistical support, the agency is justified in concluding that the label is incorrect and underfilling of the bottles is occurring. Appropriate action to force the manufacturer to comply with labeling standards would be considered. 
    - However, if the sample results indicate ${H_0}$ cannot be rejected, the assumption that the labeling is correct cannot be rejected. With this conclusion, no action would be taken.
    - A product information is usually assumed to be true and stated as the null hypothesis. The conclusion that the information is incorrect can be made if the null hypothesis is rejected.
  - Same situation, from the point of view of the manufacturer
    - The company does not want to underfill the containers (legal requirement). However, the company does not want to overfill containers either because it would be an unnecessary cost. 
    - $H_0 : \mu = 67.6 \iff H_a: \mu \neq 67.6$
    - If the sample results lead to the conclusion to reject ${H_0}$, the inference is made that $H_a: \mu \neq 67.6$ is true. We conclude that the bottles are not being filled properly and the production process should be adjusted. 
    - However, if the sample results indicate ${H_0}$ cannot be rejected, the assumption that the process is functioning properly cannot be rejected. In this case, no further action would be taken.


## Three forms of hypotheses  {.tabset .tabset-fade}

For hypothesis tests involving a population mean, we let ${\mu}_0$ denote the hypothesized value and we must choose one of the following three forms for the hypothesis test.

Alternative is One-Sided, if it states that a parameter is larger or smaller than the null value. Alternative is Two-sided, if it states that the parameter is different from the null value. 

```{definition 'Hypothesis-1T-Lower-Tail'}
\textcolor{pink}{$\text{\{Left or Lower \} }\space\thinspace {H_0} : {\mu} \geq {\mu}_0 \iff {H_a}: {\mu} < {\mu}_0$}
```

```{definition 'Hypothesis-1T-Upper-Tail'}
\textcolor{pink}{$\text{\{Right or Upper\} } {H_0} : {\mu} \leq {\mu}_0 \iff {H_a}: {\mu} > {\mu}_0$}
```

```{definition 'Hypothesis-2T-Two-Tail'}
\textcolor{pink}{$\text{\{Two Tail Test \} } \thinspace {H_0} :{\mu} = {\mu}_0 \iff {H_a}: {\mu} \neq {\mu}_0$}
```

Refer [Equality in Hypothesis](#equality-b14 "b14")

### Exercises  {.unlisted .unnumbered}

- The manager of an automobile dealership is considering a new bonus plan designed to increase sales volume. Currently, the mean sales volume is 14 automobiles per month. The manager wants to conduct a research study to see whether the new bonus plan increases sales volume. 
  - Solution: \textcolor{black}{$H_0 : \mu \leq 14 \iff H_a: \mu > 14$}
- A director of manufacturing must convince management that a proposed manufacturing method reduces costs before the new method can be implemented. The current production method operates with a mean cost of 220 dollars per hour. 
  - Solution: \textcolor{black}{$H_0 : \mu \geq 220 \iff H_a: \mu < 220$}


## Type I and Type II Errors  {#errors-ab-c09}

Refer [Type I and Type II Errors (B12)](#errors-ab-b12, "b12")

Ideally the hypothesis testing procedure should lead to the acceptance of ${H_0}$ when ${H_0}$ is true and the rejection of ${H_0}$ when ${H_a}$ is true. Unfortunately, the correct conclusions are not always possible. Because hypothesis tests are based on sample information, we must allow for the possibility of errors. 

```{r 'C09P01', echo=FALSE, fig.cap="Type-I $(\\alpha)$ and Type-II $(\\beta)$ Errors"}
knitr::include_graphics(paste0(.z$PX, "C09P01", "_Hypothesis_Errors", ".jpg")) #iiii
```

```{definition 'Error-Type-I'}
The error of rejecting ${H_0}$ when it is true, is \textcolor{pink}{Type I error $({\alpha})$}.
```

```{definition 'Error-Type-II'}
The error of accepting ${H_0}$ when it is false, is \textcolor{pink}{Type II error $({\beta})$}.
```

```{definition 'Level-of-Significance'}
The \textcolor{pink}{level of significance $(\alpha)$} is the probability of making a Type I error when the null hypothesis is true as an equality.
```

```{r 'C09D05', comment="", echo=FALSE, results='asis'}
f_getDef("Confidence-Coefficient")
```


In practice, the person responsible for the hypothesis test specifies the level of significance. By selecting ${\alpha}$, that person is controlling the probability of making a Type I error.

- Most common value are ${\alpha} = 0.05, 0.01$. 
  - For example, a significance level of ${\alpha} = 0.05$ indicates a 5% risk of concluding that a difference exists when there is no actual difference. 
  - Lower significance levels indicate that you require stronger evidence before you will reject the null hypothesis.
  - If the cost of making a Type I error is high, small values of ${\alpha}$ are preferred. Ex: ${\alpha} = 0.01$ 
  - If the cost of making a Type I error is not too high, larger values of ${\alpha}$ are typically used. Ex: $\alpha = 0.05$ 


```{definition 'Significance-Tests'}
Applications of hypothesis testing that only control for the Type I error $(\alpha)$ are called \textcolor{pink}{significance tests}. 
```

Although most applications of hypothesis testing control for the probability of making a Type I error, they do not always control for the probability of making a Type II error. Hence, if we decide to accept ${H_0}$, we cannot determine how confident we can be with that decision. Because of the uncertainty associated with making a Type II error when conducting significance tests, statisticians usually recommend that we use the statement \textcolor{pink}{"do not reject ${H_0}$"} instead of "accept ${H_0}$." Using the statement "do not reject ${H_0}$" carries the recommendation to withhold both judgment and action. In effect, by not directly accepting ${H_0}$, the statistician avoids the risk of making a Type II error.

### Additional

Refer figure \@ref(fig:C09P01)

1. Type I (\textcolor{pink}{${\alpha}$}): 
    - False Positive: Rejecting a True ${H_0}$ thus claiming False ${H_a}$
    - An alpha error is when you mistakenly reject the Null and believe that something significant happened
      - i.e. you believe that the means of the two populations are different when they are not
      - i.e. you report that your findings are significant when in fact they have occurred by chance
    - The probability of making a type I error is represented by alpha level ${\alpha}$, which is the p-value below which you reject the null hypothesis
      - The \textcolor{pink}{p-value} is the actual risk you have in being wrong if you reject the null
        - You would like that to be low
        - This p-value is compared with and should be lower than the alpha
        - A p-value of 0.05 indicates that you are willing to accept a 5% chance that you are wrong when you reject the null hypothesis. You can reduce your risk of committing a type I error by using a lower value for p. For example, a p-value of 0.01 would mean there is a 1% chance of committing a Type I error.
        - However, using a lower value for alpha means that you will be less likely to detect a true difference if one really exists (thus risking a type II error).
    - ${\alpha}$ is \textcolor{pink}{Significance Level} (for $(1-{\alpha})$ confidence of not committing Type 1 error)
      - It is the boundary for specifying a statistically significant finding when interpreting the p-value
    - NOTE: Fail to reject True ${H_0}$ ($\approx$ accept) is the correct decision shown in Top Left Quadrant
1. Type II (\textcolor{pink}{${\beta}$}): 
    - False Negative: Failing to reject ($\approx$ accept) a False ${H_0}$ 
    - A beta error is when you fail to reject the null when you should have
      - i.e. you missed something significant and failed to take action
      - i.e. you conclude that there is not a significant effect, when actually there really is
      - You can decrease your risk of committing a type II error by ensuring your test has enough \textcolor{pink}{power.}
      - You can do this by ensuring your sample size is large enough to detect a practical difference when one truly exists.

```{r 'C09D06', comment="", echo=FALSE, results='asis'}
f_getDef("Power")
```


- The consequences of making a type I error mean that changes or interventions are made which are unnecessary, and thus waste time, resources, etc.
- Type II errors typically lead to the preservation of the status quo (i.e. interventions remain the same) when change is needed.
- Generally max 5% ${\alpha}$ and max 20% ${\beta}$ errors are recommended
    

## Known SD


### Test Statistic {#test-stat-c09}

```{definition 'Test-Statistic'}
\textcolor{pink}{Test statistic} is a number calculated from a statistical test of a hypothesis. It shows how closely the observed data match the distribution expected under the null hypothesis of that statistical test. It helps determine whether a null hypothesis should be rejected.
```

```{r 'C09D04', comment="", echo=FALSE, results='asis'}
f_getDef("Probability-Distribution")
```

The test statistic summarizes the observed data into a single number using the central tendency, variation, sample size, and number of predictor variables in the statistical model. Refer Table \@ref(tab:C09V01)

Table: (\#tab:C09V01) (C09V01) Test Statistic

| Test statistic | ${H_0}$ and ${H_a}$ | Statistical tests that use it |
| :--- | :--- | :--- | 
| t-value | Null: The means of two groups are equal | T-test, Regression tests | 
| | Alternative: The means of two groups are not equal | |
| z-value | Null: The means of two groups are equal | Z-test |
| | Alternative:The means of two groups are not equal | |
| F-value | Null: The variation among two or more groups is greater than or equal to the variation between the groups | ANOVA, ANCOVA, MANOVA |
| | Alternative: The variation among two or more groups is smaller than the variation between the groups  | |
| ${\chi}^2\text{-value}$ | Null: Two samples are independent | Chi-squared test, Non-parametric correlation tests |
| | Alternative: Two samples are not independent (i.e. they are correlated) | |

### Tails

```{r 'C09D08', comment="", echo=FALSE, results='asis'}
f_getDef("Tails")
```

```{definition 'Tailed-Test'}
A \textcolor{pink}{one-tailed test} and a \textcolor{pink}{two-tailed test} are alternative ways of computing the statistical significance of a parameter inferred from a data set, in terms of a test statistic.
```

One tailed-tests are concerned with one side of a statistic. Thus, one-tailed tests deal with only one tail of the distribution, and the z-score is on only one side of the statistic. Whereas, Two-tailed tests deal with both tails of the distribution, and the z-score is on both sides of the statistic. 

In a one-tailed test, the area under the rejection region is equal to the level of significance, ${\alpha}$. When the rejection region is below the acceptance region, we say that it is a \textcolor{pink}{left-tail test}. Similarly, when the rejection region is above the acceptance region, we say that it is a \textcolor{pink}{right-tail test}.

In the two-tailed test, there are two critical regions, and the area under each region is $\frac{\alpha}{2}$. 


One-Tail vs. Two-Tail

- One-tailed tests have more statistical power to detect an effect in one direction than a two-tailed test with the same design and significance level. 
  - One-tailed tests occur most frequently for studies where one of the following is true:
    - Effects can exist in only one direction.
    - Effects can exist in both directions but the researchers only care about an effect in one direction.
- The disadvantage of one-tailed tests is that they have no statistical power to detect an effect in the other direction.
  - Whereas, A two-tailed hypothesis test is designed to show whether the sample mean is significantly greater than OR significantly less than the mean of a population.
    - A two-tailed test is designed to examine both sides of a specified data range as designated by the probability distribution involved. 
- Thumb rule
  - Consider both directions when deciding if you should run a one tailed test or two. If you can skip one tail and it is not irresponsible or unethical to do so, then you can run a one-tailed test.
  - Two-tail test is done when you do not know about direction, so you test for both sides.


### One-tailed Test {#one-tail-c09}

One-tailed tests about a population mean take one of the following two forms:

```{r 'C09D01', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-1T-Lower-Tail")
```

```{r 'C09D02', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-1T-Upper-Tail")
```

```{definition 'One-Tailed-Test'}
\textcolor{pink}{One-tailed test} is a hypothesis test in which rejection of the null hypothesis occurs for values of the test statistic in one tail of its sampling distribution.
```

Example: The label on a can of Hilltop Coffee states that the can contains 3 pounds of coffee. As long as the population mean filling weight is at least 3 pounds per can, the rights of consumers will be protected. Thus, the government (FTC) interprets the label information on a large can of coffee as a claim by Hilltop that the population mean filling weight is at least 3 pounds per can. 
  
- Develop the null and alternative hypotheses for the test
  - $H_0 : \mu \geq 3 \iff H_a: \mu < 3$
- Take a Sample
  - Suppose a sample of 36 cans of coffee is selected and the sample mean ${\overline{x}}$ is computed as an estimate of the population mean ${\mu}$. If the value of the sample mean ${\overline{x}}$ is less than 3 pounds, the sample results will cast doubt on the null hypothesis. 
  - What we want to know is how much less than 3 pounds must ${\overline{x}}$ be before we would be willing to declare the difference significant and risk making a Type I error by falsely accusing Hilltop of a label violation. A key factor in addressing this issue is the value the decision maker selects for the level of significance. 
- Specify the level of significance ${\alpha}$ 
  - FTC is willing to risk a 1% chance of making such an error i.e. $\alpha 0.01$
- Compute the value of test statistic 
  - Assume, known ${\sigma} = 0.18$ and Normal distribution
  - Refer equation \@ref(eq:sigma-x-bar), standard error of ${\overline{x}}$ is ${\sigma}_{\overline{x}} = \frac{{\sigma}}{\sqrt{n}} = \frac{0.18}{\sqrt{36}} = 0.03$
  - Because the sampling distribution of ${\overline{x}}$ is normally distributed, $z = \frac{\overline{x} - {\mu}_0}{{\sigma}_{\overline{x}}} = \frac{\overline{x} - 3}{0.03}$
  - Because the sampling distribution of x is normally distributed, the sampling distribution of ${z}$ is a standard normal distribution. 
  - A value of $z = −1$ means that the value of ${\overline{x}}$ is one standard error below the hypothesized value of the mean. For a value of $z = −2$, it would be two standard errors below the mean, and so on. 
  - We can use the standard normal probability table to find the lower tail probability ${P_{\left(z\right)}}$ corresponding to any ${z}$ value. Refer [Get P(z) by pnorm() or z by qnorm()](#get-pz-c06 "c06")
    - Ex: $P_{\left(z = -3\right)} = 0.0013$
    - As a result, the probability of obtaining a value of ${\overline{x}}$ that is 3 or more standard errors below the hypothesized population mean ${\mu}_0 = 3$ is also 0.0013. i.e. Such a result is unlikely if the null hypothesis is true. 


```{definition '1s-known-sd'}
If ${\sigma}$ is known, the standard normal random variable \textcolor{pink}{${z}$ is used as test statistic} to determine whether ${\overline{x}}$ deviates from the hypothesized value of ${\mu}$ enough to justify rejecting the null hypothesis. Refer equation \@ref(eq:z-sd) $\to z = \frac{\overline{x} - {\mu}_0}{{\sigma}_{\overline{x}}} =  \frac{\overline{x} - {\mu}_0}{{\sigma}/\sqrt{n}}$
```

\begin{equation}
  z = \frac{\overline{x} - {\mu}_0}{{\sigma}_{\overline{x}}} = \frac{\overline{x} - {\mu}_0}{{\sigma}/\sqrt{n}}
  (\#eq:z-sd)
\end{equation}

The key question for a lower tail test is, \textcolor{pink}{How small must the test statistic ${z}$ be before we choose to reject the null hypothesis} 

Two approaches can be used to answer this: the p-value approach and the critical value approach.

#### p-value approach

```{definition 'Approach-p-value'}
The \textcolor{pink}{p-value approach} uses the value of the test statistic ${z}$ to compute a probability called a \textcolor{pink}{p-value}.
```

```{definition 'p-value'}
A \textcolor{pink}{p-value} is a probability that provides a measure of the evidence against the null hypothesis provided by the sample. The p-value is used to determine whether the null hypothesis should be rejected. Smaller p-values indicate more evidence against ${H_0}$.
```

\textcolor{pink}{p-value (p)} is the probability of obtaining a result equal to or more extreme than was observed in the data. It is the probability of observing the result given that the null hypothesis is true. A small p-value indicates the value of the test statistic is unusual given the assumption that ${H_0}$ is true. 

For a \textcolor{pink}{lower tail test}, the p-value is the probability of obtaining a value for the test statistic as small as or smaller than that provided by the sample. 
- we use the standard normal distribution to find the probability that ${z}$ is less than or equal to the value of the test statistic. 
- After computing the p-value, we must then decide whether it is small enough to reject the null hypothesis; this decision involves comparing the p-value to the level of significance.

For the Hilltop Coffee Example

- Suppose the sample of 36 Hilltop coffee cans provides a sample mean of ${\overline{x}}$ = 2.92 pounds. 
  - Is $\overline{x} = 2.92$ small enough to cause us to reject ${H_0}$
- Because this is a lower tail test, the p-value is the area under the standard normal curve for values of ${z}$ less than or equal to the value of the test statistic. 
  - Refer equation \@ref(eq:z-sd), $z = \frac{\overline{x} - {\mu}_0}{{\sigma}/\sqrt{n}} = \frac{2.92 - 3}{0.18/\sqrt{36}} = -2.67$
  - Thus, the p-value is the probability that ${z}$ is less than or equal to −2.67 (the lower tail area corresponding to the value of the test statistic).
- Refer [Get P(z) by pnorm() or z by qnorm()](#get-pz-c06 "c06"), to get the p-value
  - $P_{\left(\overline{x} = 2.92\right)} = P_{\left(z = -2.67\right)} = 0.0038$
  - This p-value does not provide much support for the null hypothesis, but is it small enough to cause us to reject ${H_0}$
- Compare p-value with Level of significance $\alpha = 0.01$
  - Because .0038 is less than or equal to $\alpha = 0.01$, we reject ${H_0}$. Therefore, we find sufficient statistical evidence to reject the null hypothesis at the .01 level of significance.
  - We can conclude that Hilltop is underfilling the cans.
  
\textcolor{pink}{Rejection Rule: Reject ${H_0}$ if p-value $\leq {\alpha}$}

Further, in this case, we would reject ${H_0}$ for any value of ${\alpha} \geq (p = 0.0038)$. For this reason, the p-value is also called the \textcolor{pink}{observed level of significance}.

- (Aside)
  - For the p-value approach, the likelihood (p-value) of the numerical value of the test statistic is compared to the specified significance level (${\alpha}$) of the hypothesis test.
  - The p-value corresponds to the probability of observing sample data at least as extreme as the actually obtained test statistic. Small p-values provide evidence against the null hypothesis. The smaller (closer to 0) the p-value, the stronger is the evidence against the null hypothesis.
  - "If the null hypothesis is true, what is the probability that we would observe a more extreme test statistic in the direction of the alternative hypothesis than we did" 
  - Ex: (criminal trials) "If the defendant is innocent, what is the chance that we would observe such extreme criminal evidence"
  - pnorm() returns the cumulative probability up to q (i.e. ${\overline{x}}$) for a normal distribution with a given mean ${\mu}$ and standard deviation ${\sigma}$.

#### Critical value approach

```{definition 'Approach-Critical-Value'}
The \textcolor{pink}{critical value approach} requires that we first determine a value for the test statistic called the \textcolor{pink}{critical value}. 
```


```{definition 'Critical-Value'}
\textcolor{pink}{Critical value} is the value that is compared with the test statistic to determine whether ${H_0}$ should be rejected. Significance level ${\alpha}$, or confidence level ($1 - {\alpha}$), dictates the \textcolor{pink}{critical value ($Z$)}, or critical limit. Ex: For Upper Tail Test, $Z_{{\alpha} = 0.05} = 1.645$. 
```

For a \textcolor{pink}{lower tail test}, the critical value serves as a benchmark for determining whether the value of the test statistic is small enough to reject the null hypothesis. 
  - \textcolor{pink}{Critical value} is the value of the test statistic that corresponds to an area of ${\alpha}$ (the level of significance) in the lower tail of the sampling distribution of the test statistic.
  - In other words, the critical value is the largest value of the test statistic that will result in the rejection of the null hypothesis. 

Hilltop Coffee Example

- The sampling distribution for the test statistic ${z}$ is a standard normal distribution. 
  - Therefore, the critical value is the value of the test statistic that corresponds to an area of $\alpha = 0.01$ in the lower tail of a standard normal distribution. 
  - Using the standard normal probability table, we find that $P_{\left(z\right)} = 0.01$ for $z_{\alpha = 0.01} = −2.33$ 
  - Refer [Get P(z) by pnorm() or z by qnorm()](#get-pz-c06 "c06")
  - Thus, if the sample results in a value of the test statistic that is less than or equal to −2.33, the corresponding p-value will be less than or equal to .01; in this case, we should reject the null hypothesis. 
- Compare test statistic with z-value
  - Because $(z = -2.67) < (z_{\alpha = 0.01} = −2.33)$, we can reject ${H_0}$
  - We can conclude that Hilltop is underfilling the cans.

\textcolor{pink}{Rejection Rule: Reject ${H_0}$ if $z \leq z_{\alpha}$}


#### Summary

The p-value approach to hypothesis testing and the critical value approach will always lead to the same rejection decision; that is, whenever the p-value is less than or equal to ${\alpha}$, the value of the test statistic will be less than or equal to the critical value. 

- The advantage of the p-value approach is that the p-value tells us how significant the results are (the observed level of significance). 
  - If we use the critical value approach, we only know that the results are significant at the stated level of significance.

For \textcolor{pink}{upper tail test} The test statistic ${z}$ is still computed as earlier. But, for an upper tail test, the p-value is the probability of obtaining a value for the test statistic as large as or larger than that provided by the sample. 
Thus, to compute the p-value for the upper tail test in the ${\sigma}$ known case, we must use the standard normal distribution to find the probability that ${z}$ is greater than or equal to the value of the test statistic. Using the critical value approach causes us to reject the null hypothesis if the value of the test statistic is greater than or equal to the critical value $z_{\alpha}$; in other words, we reject ${H_0}$ if $z \geq z_{\alpha}$.


#### Acceptance and Rejection Region

```{r 'C09D07', comment="", echo=FALSE, results='asis'}
f_getDef("Interval-Estimate") 
```


```{definition 'Acceptance-Region'}
A \textcolor{pink}{acceptance region} (confidence interval), is a set of values for the test statistic for which the null hypothesis is accepted. i.e. if the observed test statistic is in the confidence interval then we accept the null hypothesis and reject the alternative hypothesis.
```

\begin{equation} 
  Z = \frac {{\overline{x}} - {\mu}}{{\sigma}/{\sqrt{n}}} \quad \iff {\mu} = {\overline{x}} - Z \frac{{\sigma}}{\sqrt{n}} \quad \to {\mu} = {\overline{x}} \pm Z \frac{{\sigma}}{\sqrt{n}} \quad \to {\mu} \approx {\overline{x}} \pm Z \frac{{s}}{\sqrt{n}}
  (\#eq:z-mu)
\end{equation} 

```{definition 'Margin-Error'}
The \textcolor{pink}{margin of error} tells how far the original population means might be from the sample mean. It is given by $Z\frac{{\sigma}}{\sqrt{n}}$ 
```

```{definition 'Rejection-Region'}
A \textcolor{pink}{rejection region} (critical region), is a set of values for the test statistic for which the null hypothesis is rejected. i.e. if the observed test statistic is in the critical region then we reject the null hypothesis and accept the alternative hypothesis.
```


### Two-tailed Test {#two-tail-c09}

```{r 'C09D03', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-2T-Two-Tail")
```


```{definition 'Two-Tailed-Test'}
\textcolor{pink}{Two-tailed test} is a hypothesis test in which rejection of the null hypothesis occurs for values of the test statistic in either tail of its sampling distribution.
```


Ex: Golf Company, mean driving distance is 295 yards i.e. $({\mu}_0 = 295)$

- $H_0 : \mu = 295 \iff H_a: \mu \neq 295$
- The quality control team selected $\alpha = 0.05$ as the level of significance for the test.
- From previous tests, assume known ${\sigma} = 12$
- For a sample size $n = 50$
  - Standard Error of ${\overline{x}}$ is ${\sigma}_{\overline{x}} = \frac{{\sigma}}{\sqrt{n}} = \frac{12}{\sqrt{50}} = 1.7$
  - [Central Limit Theorem](#clt-c07 "c07"), allows us to conclude that the sampling distribution of ${\overline{x}}$ can be approximated by a normal distribution.
- Suppose for the sample, $\overline{x} = 297.6$


"ForLater" - This part needs to be moved to the Next Chapter.

- p-value approach
  - For a two-tailed test, the p-value is the probability of obtaining a value for the test statistic as unlikely as or more unlikely than that provided by the sample. 
  - Refer equation \@ref(eq:z-sd), $z = \frac{\overline{x} - {\mu}_0}{{\sigma}/\sqrt{n}} = \frac{297.6 - 295}{12/\sqrt{50}} = 1.53$
  - Now to compute the p-value we must find the probability of obtaining a value for the test statistic at least as unlikely as $z = 1.53$. 
    - Clearly values of $z \geq 1.53$ are at least as unlikely. 
    - But, because this is a two-tailed test, values of $z \leq −1.53$ are also at least as unlikely as the value of the test statistic provided by the sample. 
  - Refer [Get P(z) by pnorm() or z by qnorm()](#get-pz-c06 "c06"), to get the p-value
    - $P_{\left(z\right)} = P_{\left(z \leq -1.53\right)} + P_{\left(z \geq 1.53\right)}$
    - $P_{\left(z\right)} = 2 \times P_{\left(z \geq 1.53\right)}$, Because the normal curve is symmetric
    - $P_{\left(z\right)} = 2 \times 0.0630 = 0.1260$
  - Compare p-value with Level of significance $\alpha = 0.05$
    - We do not reject ${H_0}$ because the $(\text{p-value}= 0.1260) > (\alpha = 0.05)$
    - Because the null hypothesis is not rejected, no action will be taken.
- critical value approach
  - The critical values for the test will occur in both the lower and upper tails of the standard normal distribution. 
  - With a level of significance of $\alpha = 0.05$, the area in each tail corresponding to the critical values is $\alpha/2 = 0.025$. 
  - Refer [Get P(z) by pnorm() or z by qnorm()](#get-pz-c06 "c06")
    - Using the standard normal probability table, we find that $P_{\left(z\right)} = 0.025$ for $-z_{\alpha/2 = 0.025} = −1.96$ and $z_{\alpha/2 = 0.025} = 1.96$ 
  - Compare test statistic with z-value
    - Because $(z = 1.53)$ is NOT greater than  $(z_{\alpha/2 = 0.025} = 1.96)$, we cannot reject ${H_0}$

\textcolor{pink}{Rejection Rule: Reject ${H_0}$ if $z \leq -z_{\alpha/2}$ or $z \geq z_{\alpha/2}$}


(Online, might be wrong) Ex: Assume that for a Population with mean ${\mu}$ unknown and standard deviation ${\sigma} = 15$, if we take a sample ${n = 100}$ its sample mean is ${\overline{x}} = 42$.

Assume ${\alpha} = 0.05$ and if we are conducting a Two Tail Test, $Z_{\alpha/2 = 0.05/2} = 1.960$

- If we take a different sample of same size or a sample of different size, the sample mean calculated for those would be different.
- So, our sample mean ${\overline{x}}$ might not be the true population mean ${\mu}$
- Thus, a range is inferred using the sample size, the sample mean, and the population standard deviation, and it is assumed that the true population means falls under this interval. This interval is called a \textcolor{pink}{confidence interval}.
- Confidence interval is calculated using critical limit ${z}$, and thus are calculated for specific significance level ${\alpha}$
- Margin of Error $= Z\frac{{\sigma}}{\sqrt{n}} = 1.96 \times 15 /\sqrt{100} = 2.94$

As shown in the equation \@ref(eq:z-mu), our interval range is $\mu = \overline{X} \pm 2.94 = 42 \pm 2.94 \rightarrow \mu  \in (39.06, 44.94)$

> We are 95% confident that the population mean will be between 39.04 and 44.94

Note that a 95% confidence interval does not mean there is a 95% chance that the true value being estimated is in the calculated interval. Rather, given a population, there is a 95% chance that choosing a random sample from this population results in a confidence interval which contains the true value being estimated.

## Steps of Hypothesis Testing 

Common Steps

1. Develop the null and alternative hypotheses.
1. Specify the level of significance. 
1. Collect the sample data and compute the value of the test statistic. 

p-Value Approach Step 

4. Use the value of the test statistic to compute the p-value. 
4. Reject ${H_0}$ if the p-value $\leq {\alpha}$. 
4. Interpret the statistical conclusion in the context of the application. 


```{definition 'Approach-p-value-Steps'}
\textcolor{pink}{p-value Approach:} Form Hypothesis | Specify ${\alpha}$ | Calculate test statistic | Calculate p-value | Compare p-value with ${\alpha}$ | Interpret
```

Critical Value Approach 

4. Use the level of significance to determine the critical value and the rejection rule. 
4. Use the value of the test statistic and the rejection rule to determine whether to reject ${H_0}$.
4. Interpret the statistical conclusion in the context of the application.


## Relationship Between Interval Estimation and Hypothesis Testing 

Refer equation \@ref(eq:interval-with-sigma), For the ${\sigma}$ known case, the ${(1 - \alpha)}\%$ confidence interval estimate of a population mean is given by 

\begin{equation*}
  \overline{x} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}
\end{equation*}

We know that $100 {(1 - \alpha)}\%$ of the confidence intervals generated will contain the population mean and $100 {\alpha}\%$ of the confidence intervals generated will not contain the population mean. 

Thus, if we reject ${H_0}$ whenever the confidence interval does not contain ${\mu}_0$, we will be rejecting the null hypothesis when it is true $(\mu = {\mu}_0)$ with probability ${\alpha}$. 

The level of significance is the probability of rejecting the null hypothesis when it is true. So constructing a $100 {(1 - \alpha)}\%$ confidence interval and rejecting ${H_0}$ whenever the interval does not contain ${\mu}_0$ is equivalent to conducting a two-tailed hypothesis test with ${\alpha}$ as the level of significance.

Ex: Golf company

- For ${\alpha} = 0.05$, 95% confidence interval estimate of the population mean is 
  - ${\overline{x}} \pm z_{0.025} \frac{{\sigma}}{\sqrt{n}} = 297.6 \pm 1.96 \frac{12}{\sqrt{50}} = 297.6 \pm 3.3$
  - Interval: $[294.3, 300.9]$
  - We can conclude with 95% confidence that the mean distance for the population of golf balls is between 294.3 and 300.9 yards. 
  - Because the hypothesized value for the population mean, ${\mu}_0 = 295$, is in this interval, the hypothesis testing conclusion is that the null hypothesis, ${H_0: {\mu} = 295}$, cannot be rejected.

"ForLater" - Exercises


## Unknown SD  {#get-pt-c09}

```{definition '1s-unknown-sd'}
If ${\sigma}$ is unknown, the sampling distribution of the test statistic follows the \textcolor{pink}{t distribution} with $(n − 1)$ degrees of freedom. Refer equation \@ref(eq:t-nsd) $\to t = \frac{{\overline{x}} - {\mu}_0}{{s}/\sqrt{n}}$
```

\begin{equation}
  t = \frac{{\overline{x}} - {\mu}_0}{{s}/\sqrt{n}}
  (\#eq:t-nsd)
\end{equation}

One-Tailed Test

- Ex: Heathrow Airport, testing for mean rating 7 i.e. ${\mu}_0 = 7$
  - ${H_0}: {\mu} \leq 7 \iff {H_a} \geq 7$
  - Sample: ${\overline{x}} = 7.25, s = 1.052, n = 60$
  - ${\alpha} = 0.05$
  - Refer equation \@ref(eq:t-nsd), $t = \frac{\overline{x} - {\mu}_0}{s/\sqrt{n}} = \frac{7.25 - 7}{1.052/\sqrt{60}} = 1.84$
  - $\text{DOF} = n-1 = 60 -1 = 59$
  - Refer [For P(t), find t by qt()](#get-t-c08 "c08") and This is a Right Tail Test
    - ${P_{\left(t \geq 1.84\right)}} = 0.0354$ i.e. between 0.05 and 0.025
  - Comparison
    - ${(P_{\left(t \geq 1.84\right)}} = 0.035) < ({\alpha} = 0.05)$
    - Thus, we can reject the ${H_0}$ and can accept the ${H_a}$

Critical Value Approach
  - $(\text{DOF = 59}), \, t_{{\alpha} = 0.05} = 1.671$
  - Because $(t = 1.84) > (t_{{\alpha} = 0.05} = 1.671)$, Reject ${H_0}$

```{r 'C09-PtQt'}
# #Like pnorm() is for P(z) and qnorm() is for z, pt() is for P(t) and qt() is for t.
#
# #p-value approach: Find Commulative Probability P corresponding to the given t-value & DOF=59
pt(q = 1.84, df = 59, lower.tail = FALSE)
#
# #Critical Value: t-value for which Area under the curve towards Right is alpha=0.05 & DOF=59
qt(p = 0.05, df = 59, lower.tail = FALSE)
```

Two Tailed Test

- Ex: Holiday Toys, testing for sale of 40 units, i.e. ${\mu}_0 = 40$
  - ${H_0}: {\mu} = 40 \iff {H_a} \neq 40$
  - Sample: ${\overline{x}} = 37.4, s = 11.79, n = 25$
  - ${\alpha} = 0.05$
  - Refer equation \@ref(eq:t-nsd), $t = \frac{\overline{x} - {\mu}_0}{s/\sqrt{n}} = \frac{37.4 - 40}{11.79/\sqrt{25}} = -1.10$
  - $\text{DOF} = n-1 = 25 -1 = 24$
  - Because we have a two-tailed test, the p-value is two times the area under the curve of the t distribution for $t \leq -1.10$
    - $P_{\left(t\right)} = P_{\left(t \leq -1.10\right)} + P_{\left(z \geq 1.10\right)}$
    - $P_{\left(t\right)} = 2 \times P_{\left(t \leq -1.10\right)}$, Because the normal curve is symmetric
    - $P_{\left(t\right)} = 2 \times 0.1411 = 0.2822$ i.e. between 2 * (0.20 and 0.10) or (0.40, 0.20)
  - Comparison
    - $(P_{\left(t\right)}  = 0.282)2 > ({\alpha} = 0.05)$
    - Thus, we cannot reject the ${H_0}$
  
Critical Value Approach
  - $(\text{DOF = 24})$
  - We find that $P_{\left(t\right)} = 0.025$ for $-t_{\alpha/2 = 0.025} = -2.064$ and $t_{\alpha/2 = 0.025} = 2.064$ 
  - Compare test statistic with z-value
    - Because $(t = -1.10)$ is NOT lower than  $(-z_{\alpha/2 = 0.025} = -2.064)$, we cannot reject ${H_0}$


## Population Proportions

### Hypothesis

Using ${p}_0$ to denote the hypothesized value for the population proportion, the three forms for a hypothesis test about a population proportion ${p}$ are : 

```{definition 'H-1s-p-Lower'}
\textcolor{pink}{$\text{\{Left or Lower \} }\space\thinspace {H_0} : {p} \geq {p}_0 \iff {H_a}: {p} < {p}_0$}
```

```{definition 'H-1s-p-Upper'}
\textcolor{pink}{$\text{\{Right or Upper\} } {H_0} : {p} \leq {p}_0 \iff {H_a}: {p} > {p}_0$}
```

```{definition 'H-1s-p-Two'}
\textcolor{pink}{$\text{\{Two Tail Test \} } \thinspace {H_0} : {p} = {p}_0 \iff {H_a}: {p} \neq {p}_0$}
```

Hypothesis tests about a population proportion are based on the difference between the sample proportion ${\overline{p}}$ and the hypothesized population proportion ${p}_0$

The sampling distribution of ${\overline{p}}$, the point estimator of the population parameter ${p}$, is the basis for developing the test statistic.

When the null hypothesis is true as an equality, the expected value of ${\overline{p}}$ equals the hypothesized value ${p}_0$ i.e. $E_{(\overline{p})} = {p}_0$

The standard error of ${\overline{p}}$ is given in equation \@ref(eq:se-1s-p)

\begin{equation}
  {\sigma}_{\overline{p}} = \sqrt{\frac{{p}_0 (1 - {p}_0)}{n}}
  (\#eq:se-1s-p)
\end{equation}

If $np \geq 5$ and $n(1 − p) \geq 5$, the sampling distribution of ${p}$ can be approximated by a normal distribution. Under these conditions, which usually apply in practice, the quantity ${z}$ as given in equation \@ref(eq:z-1s-p) has a standard normal probability distribution.

\textcolor{pink}{Test Statistic for Hypothesis Tests about a Population Proportion :}

\begin{equation}
  z = \frac{{\overline{p}} - {p}_0}{{\sigma}_{\overline{p}}} = \frac{{\overline{p}} - {p}_0}{\sqrt{\frac{{p}_0 (1 - {p}_0)}{n}}}
  (\#eq:z-1s-p)
\end{equation}


Example: Pine Creek: Determine whether the proportion of women golfers increased from $p_0 = 0.20$

```{r 'C09D09', comment="", echo=FALSE, results='asis'}
f_getDef("H-1s-p-Upper") #dddd
```

- Count of Success $({x})$ is Number of Women
- $\{n = 400, x = 100\} \to {\overline{p}} = {n}/{x} = 0.25$ 
- \@ref(eq:z-1s-p) $z = \frac{{\overline{p}} - {p}_0}{{\sigma}_{\overline{p}}} = \frac{{\overline{p}} - {p}_0}{\sqrt{\frac{{p}_0 (1 - {p}_0)}{n}}}$
  - $z = \frac{0.25 - 0.20}{\sqrt{\frac{0.20 (1 - 0.20)}{400}}} = 2.50$
    - `{0.25 - 0.20}/{sqrt(0.20 * {1 - 0.20} / 400)}` \textcolor{pink}{$\#\mathcal{R}$} 
- ${}^U\!P_{(z = 2.50)} = 0.0062$
  - `pnorm(q = 2.50, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^U\!P_{(z)} < {\alpha} \to {H_0}$ is rejected i.e. the proportions are different
  - We can conclude that the proportion of women players has increased.

## Hypothesis Testing and Decision Making

If the purpose of a hypothesis test is to make a decision when ${H_0}$ is true and a different decision when ${H_a}$ is true, the decision maker may want to, and in some cases be forced to, take action with both the conclusion do not reject ${H_0}$ and the conclusion reject ${H_0}$. 

If this situation occurs, statisticians generally recommend controlling the probability of making a Type II error. With the probabilities of both the Type I and Type II error controlled, the conclusion from the hypothesis test is either to accept ${H_0}$ or reject ${H_0}$. In the first case, ${H_0}$ is concluded to be true, while in the second case, ${H_a}$ is concluded true. Thus, a decision and appropriate action can be taken when either conclusion is reached.

"ForLater" - Calculate ${\beta}$

When the true population mean ${\mu}$ is close to the null hypothesis value of ${\mu} = 120$, the probability is high that we will make a Type II error. However, when the true population mean ${\mu}$ is far below the null hypothesis value of ${\mu} = 120$, the probability is low that we will make a Type II error.


```{definition 'Power'}
The probability of correctly rejecting ${H_0}$ when it is false is called the \textcolor{pink}{power} of the test. For any particular value of ${\mu}$, the power is \textcolor{pink}{$1 − \beta$}.
```


```{definition 'Power-Curve'}
\textcolor{pink}{Power Curve} is a graph of the probability of rejecting ${H_0}$ for all possible values of the population parameter ${\mu}$ not satisfying the null hypothesis. It provides the probability of correctly rejecting the null hypothesis.
```

Note that the power curve extends over the values of ${\mu}$ for which the null hypothesis is false. The height of the power curve at any value of ${\mu}$ indicates the probability of correctly rejecting ${H_0}$ when ${H_0}$ is false.

## Summary

We can make 3 observations about the relationship among ${\alpha}, \beta, n (\text{sample size})$. 

1. Once two of the three values are known, the other can be computed. 
1. For a given level of significance ${\alpha}$, increasing the sample size will reduce ${\beta}$.
1. For a given sample size, decreasing ${\alpha}$ will increase ${\beta}$, whereas increasing ${\alpha}$ will decrease ${\beta}$.

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C09-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C09-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
