# Model Data {#c37}

```{r 'C37', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Preparing to Model the Data"

## Data Mining

- Data mining methods may be categorized as either supervised or unsupervised.
  - Most data mining methods are supervised methods.
  - Unsupervised : Clustering
  - Supervised : Regression, decision trees, neural networks, k-nearest neighbors
  - Another data mining method, which may be supervised or unsupervised, is association rule mining. 

```{definition 'Unsupervised-Methods'}
In \textcolor{pink}{unsupervised methods}, no target variable is identified as such. Instead, the data mining algorithm searches for patterns and structures among all the variables. The most common unsupervised data mining method is clustering. Ex: Voter Profile.
```

```{definition 'Supervised-Methods'}
\textcolor{pink}{Supervised methods} are those in which there is a particular prespecified target variable and the algorithm is given many examples where the value of the target variable is provided. This allows the algorithm to learn which values of the target variable are associated with which values of the predictor variables. 
```

## Statistical Inference vs. Data Mining

- Statistical methodology and data mining methodology differ in the following two ways: 
  - Applying statistical inference using the huge sample sizes encountered in data mining tends to result in statistical significance, even when the results are not of practical significance. 
  - In statistical methodology, the data analyst has an a priori hypothesis in mind. Data mining procedures usually do not have an a priori hypothesis.

```{definition 'A-Priori-Hypothesis'}
An \textcolor{pink}{a priori hypothesis} is one that is generated prior to a research study taking place.
```


## Cross-validation


```{definition 'Cross-Validation'}
\textcolor{pink}{Cross-validation} is a technique for insuring that the results uncovered in an analysis are generalizable to an independent, unseen, data set. 
```

- In data mining, the most common methods are \textcolor{pink}{twofold} cross-validation and \textcolor{pink}{k-fold} cross-validation. 
  - In twofold cross-validation, the data are partitioned, using random assignment, into a training data set and a test data set. The test data set should then have the target variable omitted. Thus, the only systematic difference between the \textcolor{pink}{training data set} and the \textcolor{pink}{test data set} is that the training data includes the target variable and the test data does not. 
  - A provisional data mining model is then constructed using the training samples provided in the training data set.
  - However, the algorithm needs to guard against \textcolor{pink}{"memorizing"} the training set and blindly applying all patterns found in the training set to the future data. Ex: Just because all people named 'David' in the training set are in the high income bracket, it may not be True for all people in general.
  - Therefore, the next step is to examine how the provisional model performs on a test set of data. In the test set the provisional model performs classification according to the patterns and structures it learned from the training set. 
  - The efficacy of the classifications is then evaluated by comparing them against the true values of the target variable. 
  - The provisional model is then adjusted to minimize the error rate on the test set.
- We must insure that the training and test data sets are independent, by \textcolor{pink}{validating the partition}. 
  - By performing graphical and statistical comparisons between the two sets. 
  - For example, we may find that, even though the assignment of records was made randomly, a significantly higher proportion of positive values of an important flag variable were assigned to the training set, compared to the test set. This would bias our results.
  - It is especially important that the characteristics of the target variable be as similar as possible between the training and test data sets. 
  - Hypothesis tests for validating the target variable, based on the type of target variable: t-test (for difference in means), z-test (for difference in proportions), test for homogeneity of proportions


```{definition 'Overfitting-c37'}
\textcolor{pink}{Overfitting} results when the provisional model tries to account for every possible trend or structure in the training set, even idiosyncratic ones such as the 'David' example above. 
```

- Increasing the complexity of the model in order to increase the accuracy on the training set eventually and inevitably leads to a degradation in the generalizability of the provisional model to the test set.
  - As the model complexity increases, the error rate on the training set continues to fall in a monotone manner. 
  - However, as the model complexity increases, the test set error rate soon begins to flatten out and increase because the provisional model has memorized the training set rather than leaving room for generalizing to unseen data.

- The low complexity model suffers from some classification errors. The classification errors can be reduced by a more complex model. 
  - We might be tempted to adopt the greater complexity in order to reduce the error rate. 
  - However, we should be careful not to depend on the idiosyncrasies of the training set. 
  - The low-complexity model need not change very much to accommodate new data points. i.e. low-complexity model has \textcolor{pink}{low variance}. 
  - However, the high-complexity model must alter considerably if it is to maintain its low error rate.  i.e. high-complexity model has a \textcolor{pink}{high variance}. 
  - 


```{definition 'Bias–variance-Trade-off'}
Even though the high-complexity model has low bias (error rate), it has a high variance; and even though the low-complexity model has a high bias, it has a low variance. This is known as the \textcolor{pink}{bias–variance trade-off}. The bias–variance trade-off is another way of describing the overfitting-underfitting dilemma.
```

- The goal is to construct a model in which neither the bias nor the variance is too high
  - A common method of evaluating how accurate model estimation is proceeding for a continuous target variable is to use the mean-squared error (MSE). (Target: Low MSE) 
    - MSE is a good evaluative measure because it combines both bias and variance. 
    - The MSE is a function of the estimation error (sum of squared errors, SSE) and the model complexity (e.g., degrees of freedom). 


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C37-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C37-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
