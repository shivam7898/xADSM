# Variance {#c11}

```{r 'C11', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Inferences About Population Variances"
  - "ForLater" - Hypothesis Testing, Inferences About Two Population Variances 


## Inferences About a Population Variance 

In many manufacturing applications, controlling the process variance is extremely important in maintaining quality.

The sample variance ${s^2}$, given by equation \@ref(eq:var-1s), is the point estimator of the population variance ${\sigma}^2$.

\begin{equation}
  {s^2} = \frac{\sum {({x}_i - {\overline{x}})}^2}{n-1}
  (\#eq:var-1s)
\end{equation}

```{definition 'Distribution-Chi-Square'}
Whenever a simple random sample of size ${n}$ is selected from a normal population, the sampling distribution of $\frac{(n-1)s^2}{{\sigma}^2}$ is a \textcolor{pink}{chi-square distribution} with ${n − 1}$ degrees of freedom.
```

Note:

- The chi-square distribution is based on sampling from a normal population.
- It can be used to develop interval estimates and conduct hypothesis tests about a population variance.
- The notation \textcolor{pink}{${\chi_{\alpha}^2}$} denotes the value for the chi-square distribution that provides an area or probability of ${\alpha}$ to the \textcolor{pink}{right} of the ${\chi_{\alpha}^2}$ value. 

### Interval Estimation

Example: A sample of 20 containers ${n = 20}$ has the sample variance ${s^2} = 0.0025$ 

- ${\chi_{\alpha = 0.025}^2} = 32.852$
  - `qchisq(p = 0.025, df = 19, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
  - It indicates that 2.5% of the chi-square values are to the right of 32.852
  - Also, ${\chi_{\alpha = 0.975}^2} = 8.907$ indicates that 97.5% of the chi-square values are to the right of 8.907.
    - `qchisq(p = 0.975, df = 19, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
  - Thus, 95% of the chi-square values are between ${\chi_{\alpha = 0.975}^2}$ and ${\chi_{\alpha = 0.025}^2}$ 
  - There is a .95 probability of obtaining a ${\chi^2}$ value such that ${\chi_{0.975}^2} \leq {\chi^2} \leq{\chi_{0.025}^2}$

```{r 'C11-ChiSq'}
# #pnorm() qnorm() | pt() qt() | pchisq() qchisq() | pf() qf() 
#
# #p-value approach: Find Commulative Probability P corresponding to the given ChiSq & DOF=59
pchisq(q = 32.852, df = 19, lower.tail = FALSE)
#
# #ChiSq value for which Area under the curve towards Right is alpha=0.025 & DOF=19 #32.852
qchisq(p = 0.025, df = 19, lower.tail = FALSE)
```

- Using equation \@ref(eq:var-1s), we can get \@ref(eq:chi-95), which provides a 95% confidence interval estimate for the population variance ${\sigma}^2$.

\begin{equation}
  \frac{(n-1)s^2}{{\chi_{0.025}^2}} \leq {\sigma}^2 \leq \frac{(n-1)s^2}{{\chi_{0.975}^2}}
  (\#eq:chi-95)
\end{equation}

- In the example, $(n-1)s^2 = 19 * 0.0025 = 0.0475$
- \@ref(eq:chi-95) $\frac{0.0475}{32.852} \leq {\sigma}^2 \leq \frac{0.0475}{8.907} \to 0.0014 \leq {\sigma}^2 \leq 0.0053 \to 0.0380 \leq {\sigma} \leq 0.0730$
  - which gives the 95% confidence interval for the population standard deviation

Generalising the equation \@ref(eq:chi-95), the equation \@ref(eq:interval-estm-1s-var) is the interval estimate of a population variance.

\begin{equation}
  \frac{(n-1)s^2}{{\chi_{{\alpha}/2}^2}} \leq {\sigma}^2 \leq \frac{(n-1)s^2}{{\chi_{{1-\alpha}/2}^2}}
  (\#eq:interval-estm-1s-var)
\end{equation}

where the ${\chi^2}$ values are based on a chi-square distribution with ${n-1}$ degrees of freedom and where $(1 − {\alpha})$ is the confidence coefficient.

### Hypothesis Tests {.tabset .tabset-fade}


Using ${{\sigma}_0^2}$ to denote the hypothesized value for the population variance, the three forms for a hypothesis test are as follows: 

```{definition 'H-1s-Var-Lower'}
\textcolor{pink}{$\text{\{Left or Lower \} }\space\thinspace {H_0} : {\sigma}^2 \geq {{\sigma}_0^2} \iff {H_a}: {\sigma}^2 < {{\sigma}_0^2}$}
```

```{definition 'H-1s-Var-Upper'}
\textcolor{pink}{$\text{\{Right or Upper\} } {H_0} : {\sigma}^2 \leq {{\sigma}_0^2} \iff {H_a}: {\sigma}^2 > {{\sigma}_0^2}$}
```

```{definition 'H-1s-Var-Two'}
\textcolor{pink}{$\text{\{Two Tail Test \} } \thinspace {H_0} : {\sigma}^2 = {{\sigma}_0^2} \iff {H_a}: {\sigma}^2 \neq {{\sigma}_0^2}$}
```

Note: In general, Upper Tail test is the most frequently observed test because low variances are generally desirable. With a statement about the maximum allowable population variance, we can test the null hypothesis that the population variance is less than or equal to the maximum allowable value against the alternative hypothesis that the population variance is greater than the maximum allowable value. With this test structure, corrective action will be taken whenever rejection of the null hypothesis indicates the presence of an excessive population variance. 

\textcolor{pink}{Test Statistic for Hypothesis Tests About a Population Variance: } Refer \@ref(eq:chi-1s), where ${\chi^2}$ has a chi-square distribution with ${n - 1}$ degrees of freedom.

\begin{equation}
  {\chi^2} = \frac{(n - 1){s}^2}{{\sigma}_0^2}
  (\#eq:chi-1s)
\end{equation}

Example: Louis: the company standard specifies an arrival time variance of 4 or less

```{r 'C11D01', comment="", echo=FALSE, results='asis'}
f_getDef("H-1s-Var-Upper")
```

- (Sample) ${n} = 24, {s}^2 = 4.9$
- \@ref(eq:chi-1s) ${\chi^2} = \frac{(n - 1){s}^2}{{\sigma}_0^2} = \frac{(24 - 1) * 4.9}{4} = 28.18$
- ${}^U\!P_{(\chi^2)} = 0.209$
  - `pchisq(q = 28.18, df = 23, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^U\!P_{(\chi^2)} > {\alpha} \to {H_0}$ cannot be rejected
  - The sample results do not provide sufficient evidence to conclude that the variance is high.

Example: bureau of motor vehicles:  Evaluate the variance in the new examination test scores with the historical value ${\sigma}_0^2 = 100$

```{r 'C11D02', comment="", echo=FALSE, results='asis'}
f_getDef("H-1s-Var-Two")
```

- (Sample) ${n} = 30, {s}^2 = 162$
- \@ref(eq:chi-1s) ${\chi^2} = \frac{(n - 1){s}^2}{{\sigma}_0^2} = \frac{(30 - 1) * 162}{100} = 46.98$
- ${}^2\!P_{(\chi^2)} = 2 * {}^U\!P_{(\chi^2)} = 2 * 0.0187 = 0.0374$
  - `2 * pchisq(q = 46.98, df = 29, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^2\!P_{(\chi^2)} < {\alpha} \to {H_0}$ is rejected i.e. the change is significant
  - The new examination test scores have a population variance different from the historical variance

## Inferences About Two Population Variances

The two sample variances ${s}_1^2$ and ${s}_2^2$ will be the basis for making inferences about the two population variances ${\sigma}_1^2$ and ${\sigma}_2^2$. 

```{definition 'Distribution-F'}
Whenever independent simple random samples of sizes ${n}_1$ and ${n}_2$ are selected from two normal populations with equal variances $({\sigma}_1^2 = {\sigma}_2^2)$, the sampling distribution of $\frac{{s}_1^2}{{s}_2^2}$ is an \textcolor{pink}{F distribution} with $({n}_1 - 1)$ degrees of freedom for the numerator and $({n}_2 - 1)$ degrees of freedom for the denominator. 
```

Note:

- The \textcolor{pink}{F distribution} is based on sampling from two normal populations. 
- The F distribution is not symmetric, and the F values can never be negative.
  - The shape of any particular F distribution depends on its numerator and denominator degrees of freedom.
- We refer to the population providing the larger sample variance as population 1.
  - Because the F test statistic is constructed with the larger sample variance ${s}_1^2$ in the numerator, the value of the test statistic will be in the upper tail of the F distribution. 

\textcolor{pink}{Test Statistic for Hypothesis Tests About Population Variances with $({\sigma}_1^2 = {\sigma}_2^2)$ :} Refer equation \@ref(eq:f-2s)


\begin{equation}
  F = \frac{{s}_1^2}{{s}_2^2}
  (\#eq:f-2s)
\end{equation}

\textcolor{pink}{Hypothesis Tests :}

```{definition 'H-2s-Var-Lower'}
\textcolor{pink}{$\text{\{Left or Lower \} }\space\thinspace \text{Do not do this.}$}
```

```{definition 'H-2s-Var-Upper'}
\textcolor{pink}{$\text{\{Right or Upper\} } {H_0} : {\sigma}_1^2 \leq {\sigma}_2^2 \iff {H_a}: {\sigma}_1^2 > {\sigma}_2^2$}
```

```{definition 'H-2s-Var-Two'}
\textcolor{pink}{$\text{\{Two Tail Test \} } \thinspace {H_0} : {\sigma}_1^2 = {\sigma}_2^2 \iff {H_a}: {\sigma}_1^2 \neq {\sigma}_2^2$}
```


Example: Dullus County Schools: 

```{r 'C11D03', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Var-Two")
```

- (1: Milbank) ${n} = 26, {s}_1^2 = 48$
- (2: Gulf ) ${n} = 16, {s}_1^2 = 20$
- \@ref(eq:f-2s) $F = \frac{{s}_1^2}{{s}_2^2} = \frac{48}{20} = 2.4$
- ${}^2\!P_{(F)} =  2 * {}^U\!P_{(F)} = 2 * 0.0406 = 0.0812$
  - `2 * pf(q = 2.4, df1 = 25, df2 = 15, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.10$
  - ${}^2\!P_{(F)} < {\alpha} \to {H_0}$ is rejected i.e. The two populations have difference variances
  - The sample results provide sufficient evidence to conclude that the variances are different.

Example:  public opinion survey: do women show a greater variation in attitude on political issues than men

```{r 'C11D04', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Var-Upper") #dddd
```

- (1: Women) ${n} = 41, {s}_1^2 = 120$
- (2: Men) ${n} = 31, {s}_1^2 = 80$
- \@ref(eq:f-2s) $F = \frac{{s}_1^2}{{s}_2^2} = \frac{120}{80} = 1.5$
- ${}^U\!P_{(F)} = 0.1256$
  - `pf(q = 1.5, df1 = 40, df2 = 30, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^U\!P_{(F)} > {\alpha} \to {H_0}$ cannot be rejected i.e. The two populations have same variances
  - The sample results does not provide sufficient evidence to conclude that women have higher variance in political opinion compared to men

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C11-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C11-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
