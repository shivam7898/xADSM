# Hierarchical and K-means Clustering {#c49}

```{r 'C49', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

```{definition 'Clustering'}
\textcolor{pink}{Clustering} refers to the grouping of records, observations, or cases into classes of similar objects. Clustering differs from classification in that there is no target variable for clustering.
```

```{definition 'Cluster'}
A \textcolor{pink}{cluster} is a collection of records that are similar to one another and dissimilar to records in other clusters.
```

- Cluster vs. Classification
  - Clustering differs from classification in that there is no target variable for clustering. 
  - The clustering task does not try to classify, estimate, or predict the value of a target variable.
  - Instead, clustering algorithms seek to segment the entire data set into relatively homogeneous subgroups or clusters, where the similarity of the records within the cluster is maximized, and the similarity to records outside this cluster is minimized.
    - Example: All zipcodes of a country can be described in terms of distinct lifestyle types (e.g. 66 types). One of them "Upper Crust" might be the wealthiest lifestyle of the country i.e. couples between ages of 45-64 without any dependents (children, parents) living at their home. This segment might be having median earnings of ~1-million dollars and might be possessing a postgraduate degree. No other lifestyle type would have a more opulent standard of living.
    - Methods: Hierarchical and k-means clustering, Kohonen networks, BIRCH clustering
  - For optimal performance, clustering algorithms, just like algorithms for classification, require the data to be normalized so that no particular variable or subset of variables dominates the analysis.
  - Clustering algorithms seek to construct clusters of records such that the \textcolor{pink}{between-cluster} variation is large compared to the \textcolor{pink}{within-cluster} variation. 
  - For continuous variables, we can use euclidean distance
  - For categorical variables, we may again define the "different from" function for comparing the $i^{\text{th}}$ attribute values of a pair as 0 when $x_i = y_i$ and 1 otherwise.

```{definition 'Euclidean-Distance'}
\textcolor{pink}{Euclidean distance} between records is given by equation, $d_{\text{Euclidean}}(x,y) = \sqrt{\sum_i{\left(x_i - y_i\right)^2}}$, where $x = \{x_1, x_2, \ldots, x_m\}$ and $y = \{y_1, y_2, \ldots, y_m\}$ represent the ${m}$ attribute values of two records.
```

## Hierarchical Clustering

```{definition 'Hierarchical-Clustering'}
In \textcolor{pink}{hierarchical clustering}, a treelike cluster structure (\textcolor{pink}{dendrogram}) is created through recursive partitioning (divisive methods) or combining (agglomerative) of existing clusters. 
```

```{definition 'Agglomerative-Clustering'}
\textcolor{pink}{Agglomerative clustering} methods initialize each observation to be a tiny cluster of its own. Then, in succeeding steps, the two closest clusters are aggregated into a new combined cluster. In this way, the number of clusters in the data set is reduced by one at each step. Eventually, all records are combined into a single huge cluster. mMost computer programs that apply hierarchical clustering use agglomerative methods.
```

```{definition 'Divisive-Clustering'}
\textcolor{pink}{Divisive clustering} methods begin with all the records in one big cluster, with the most dissimilar records being split off recursively, into a separate cluster, until each record represents its own cluster. 
```

## Distance between Clusters

```{definition 'Single-Linkage'}
\textcolor{pink}{Single linkage}, the nearest-neighbor approach, is based on the minimum distance between any record in cluster A and any record in cluster B. Cluster similarity is based on the similarity of the most similar members from each cluster. It tends to form long, slender clusters, which may sometimes lead to heterogeneous records being clustered together. 
```

```{definition 'Complete-Linkage'}
\textcolor{pink}{Complete linkage}, the farthest-neighbor approach, is based on the maximum distance between any record in cluster A and any record in cluster B. Cluster similarity is based on the similarity of the most dissimilar members from each cluster. It tends to form more compact, spherelike clusters. 
```

```{definition 'Average-Linkage'}
\textcolor{pink}{Average linkage} is designed to reduce the dependence of the cluster-linkage criterion on extreme values, such as the most similar or dissimilar records. The criterion is the average distance of all the records in cluster A from all the records in cluster B. The resulting clusters tend to have approximately equal within-cluster variability. In general, average linkage leads to clusters more similar in shape to complete linkage than does single linkage.
```

## k-means Clustering

- Refer [k-means Algorithm](#k-means-b20 "b20")
- Refer [Pseudo F-Statistic](#f-stat-c49 "c49")

- One potential problem for applying the k-means algorithm is: Who decides how many clusters to search for i.e. who decides k
  - Unless the analyst has a priori knowledge of the number of underlying clusters; therefore, an "outer loop" should be added to the algorithm, which cycles through various promising values of k. 
  - Clustering solutions for each value of k can therefore be compared, with the value of k resulting in the largest F statistic being selected. 
  - Alternatively, some clustering algorithms, such as the \textcolor{pink}{BIRCH} clustering algorithm, can select the optimal number of clusters.

- What if some attributes are more relevant than others to the problem formulation
  - As cluster membership is determined by distance, we may apply the axis-stretching methods for quantifying attribute relevance.

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C49-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C49-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
