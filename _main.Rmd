--- 
title: "Data Analytics"
author: "Shivam Upadhyay"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
description: |
  This is a minimal example.
link-citations: yes
url: 'https\://bookdown.org/shivam7898/xADSM'
github-repo: "shivam7898/xADSM"
subtitle: "ADSM: Statistics and R"
---

# About {#FIRST .unnumbered .tabset .tabset-fade} 
[Goto LAST](#LAST "End of the Document")

~~"Copy/Paste is the mother of learning."~~  
"Repetition! Repetition is the mother of learning."

Sources: [GitHub](https://github.com/shivam7898/xADSM/tree/main/docs "https://github.com") | [Google Drive](https://drive.google.com/drive/folders/1KS69a-U_64OkVbIr9VKhxu97HBPzbVsz?usp=sharing "https://drive.google.com") | [OneDrive](https://1drv.ms/u/s!AjRu26ZthlQCitEmPzybZMV0tISo5w?e=JFE5ju "https://onedrive.live.com")

\textcolor{pink}{Caution:}  
The HTML File access internet to render LaTex Equations via MathJax. Thus, equations will NOT be rendered properly if the file is viewed offline. Otherwise, the file is a standalone document containing all the relevant images, tables and codes to generate them.

\textcolor{pink}{Disclaimer:}  
This document is a collection of multiple lecture notes, books, and online material. The current status of the document is 'work-in-progress' as it is incomplete and it may contain inconsistencies and errors. 

\textcolor{pink}{Attribution:}  
Proper reference to the original source may sometimes be lacking. This is regrettable and these references – if ever known to the author – will be included in later versions.

<!-- Bx Load, Cx Assign, Dx Kable, Ix Jx Kx ... Datasets, xxxG image -->

```{r '000A', include=FALSE, cache=FALSE}
# #Knitr Setup will be run only once while building a Book or a Chapter
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
# #Packages for inclusion in Bibliography
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
# #Source ALL Functions
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
```

## Environment {.unnumbered .tabset .tabset-fade}

\textcolor{pink}{Assumption: Working directory has sub-folders named "data", "images", "code", "docs".}

### R Version {.unlisted .unnumbered}

```{r '000B-RVersion'}
# #R Version
R.version.string
```  

### Working Directory {.unlisted .unnumbered}

```{r '000C-Directory'}
# #Working Directory
getwd()
```

### Session Info {.unlisted .unnumbered}

```{r '000D-Session'}
# #Version information about R, the OS and attached or loaded packages
sessionInfo()
``` 
 
### Pandoc {.unlisted .unnumbered}

```{r '000E-Pandoc'}
# #Pandoc Version being used by RStudio
rmarkdown::pandoc_version()
```  

```{r '000F-Bib', include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

## Aside {.unlisted .unnumbered}

I wanted to have a single document containing Notes, Codes, & Output for a quick reference for the lectures. Combination of multiple file formats (docx, csv, xlsx, R, png etc.) was not working out for me. So, I found the Bookdown package to generate this HTML file.

All of us had to stumble through some of the most common problems individually and as we are approaching deeper topics, a more collaborative approach might be more beneficial.

Further, the lectures are highly focused and thus I had to explore some sidetopics in more details to get the most benefit from them. I have included those topics and I am also interested in knowing about your experiences too.

Towards that goal, I am sharing these notes and hoping that you would run the code in your own environment and would raise any queries, problems, or difference in outcomes. Any suggestion or criticism is welcome. I have tried to not produce any significant changes in your working environment. Please let me know if you observe otherwise. 

Currently, my priority is to get in sync with the ongoing lectures. The time constraint has led to issues given below. These will be corrected as and when possible.

- Tone of the document may be a little abrupt, please overlook that
- Source references are not added as much as I wanted to (from where I ~~copy/pasted~~ learned) and there is no easy solution for this.
- I have NOT explained some of the functions before their usage (lapply(), identical() etc.) Hyperlinks for that will be added as and when those topics are covered
- Code has been checked only on Windows 10. For Mac or Linux, ~~if~~ when you find something that has different output or behaviour, please let me know
- Although these notes are generated using R Markdown and Bookdown, I have not yet covered these. If you need any help in creating your own notes, please let me know. If I have the solution for your problem, I will share.

Last, but not the least, I am also learning while creating this, so if you think I am wrong somewhere, please point it out. I am always open for suggestions.

Thank You all for the encouragement.

Shivam

****

<!--chapter:end:index.Rmd-->

# (B01-B08) {#b01 .unlisted .unnumbered}

```{r 'B01', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview


## Validation {.unlisted .unnumbered .tabset .tabset-fade}
-->

```{r 'B01-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'B01-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/201-B01B08.Rmd-->

# R Introduction (B09, Aug-31) {#b09}

```{r 'B09', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## R Basics

> R is Case-sensitive i.e. c() not ~~C()~~ and View() not ~~view()~~

> Hash Sign "#" comments out anything after it, till the newline. There are no multiline comments.

> Backslash "\\" is reserved to escape the character that follows it.

> Escape key stops the parser i.e. "+" sign where R is waiting for more input before evaluation.

Overview 

- [Change Working Directory](#working-dir-b09 "b09")

### R Studio

- There are 4 Panes -
  1. Top Left - R Editor, Source
  1. Bottom Left - Console, Terminal, ...
  1. Top Right - Environment, History, ...
  1. Bottom Right - Plots, ...
- Sometimes there are only 3 panes i.e. Editor Pane is missing
  - To Open Editor Pane - Create a New R Script by "File | New File | R Script" or \textcolor{pink}{"Ctrl+ Shift+ N"}
- To Modify Pane Settings - Tools | Global Options | Pane Layout

### Shortcuts

- Execute the current expression in Source Pane (Top): "Ctrl+ Enter"
- Execute the current expression in Console Pane (Bottom): "Enter"
- Clear the Console Pane (Bottom): "Ctrl+ L"
- Restart the Current R Session: "Ctrl+ Shift+ F10"
- Create a New R Script: "Ctrl+ Shift+ N"
- Insert "  <- " i.e. Assignment Operator with Space: "Alt+ -"
- Insert " %>% " i.e. Pipe Operator with Space: "Ctrl+ Shift+ M"
- Comment or Uncomment Lines: "Ctrl+ Shift+ C"
- Set Working Directory: "Ctrl+ Shift+ H"
- Search Command History: "Ctrl+ Up Arrow"
- Search Files: "Ctrl+ ."

### Executing an Expression

Execute the current expression in Source Pane (Top) by 'Run' Button or \textcolor{pink}{"Ctrl+ Enter"}

Execute the current expression in Console Pane (Bottom) by "Enter"

### PATH and Working Directory {#working-dir-b09}

Windows 10 uses backslash "\\" for PATH. R, however, uses slash "/". Backslash "\\" is escape character in R.

- So, To provide "C:\\Users\\userName\\Documents" as PATH
  - Use: "C:\\\\Users\\\\userName\\\\Documents"
  - OR: "C:/Users/userName/Documents"
  - OR: "~" Tilde acts as a Reference to Home Directory

In R Studio, Set Working Directory by: 

  - Session | Set Working Directory | Choose Directory or \textcolor{pink}{"Ctrl+ Shift+ H"}

```{r 'B09-SetWorkingDir'}
# #Current Working Directory
getwd()
#
# #R Installation Directory (Old DOS Convention i.e. ~1 after 6 letters)
R.home()
Sys.getenv("R_HOME") 
#
# #This is Wrapped in IF Block to prevent accidental execution
if(FALSE){
# #WARNING: This will change your Working Directory
  setwd("~")
}
```

### Printing

If the R program is written over the console, line by line, then the output is printed automatically i.e. no function needed for printing. This is called \textcolor{pink}{implicit printing}. 

Inside an R Script File, implicit printing does not work and the expression needs to be printed explicitly.

In R, the most common method to print the output 'explicitly' is by the function \textcolor{pink}{print()}. 


```{r 'B09-Printing', eval=FALSE}
# #Implicit Printing: This will NOT be printed to Console, if it is inside an R Script.
"Hello World!"
#
# #Implicit Printing using '()': Same as above
("Hello World!")
#
# #Explicit Printing using print() : To print Objects to Console, even inside an R Script.
print("Hello World!")
## [1] "Hello World!"
```

## Objects

### List ALL Objects 

Everything that exists in R is an object in the sense that it is a kind of data structure that can be manipulated. Expressions for evaluation are themselves objects; Evaluation consists of taking the object representing an expression and returning the object that is the value of that expression.


```{r 'B09-ListObjetcs', eval=FALSE}
# #ls(): List ALL Objects in the Current NameSpace (Environment)
ls()
## character(0)
```

### Assign a Value to an Object 

\textcolor{orange}{Caution:} Always use "<-" for the assignment, NOT the "="

While the "=" can be used for assignment, its usage for assignment is highly discouraged because it may behave differently under certain subtle conditions which are difficult to debug. Convention is to use "=" only during function calls for arguments association (syntactic token).

There are 5 assignment operators (<-, =, <<-, ->, ->>), others are not going to be discussed for now.

All the created objects are listed in the Environment Tab of the Top Right Pane.

```{r 'B09-FirstObject'}
# #Assignment Operator "<-" is used to assign any value (ex: 10) to any object (ex: 'bb')
bb <- 10
#
# #Print Object
print(bb)
```

### Remove an Object

In the Environment Tab, any object can be selected and deleted using Brush.

```{r 'B09-RemoveObject'}
# #Trying to Print an Object 'bb' and Handling the Error, if thrown
tryCatch(print(bb), error = function(e) print(paste0(e)))
#
# #Remove an Object
rm(bb)
#
# #Equivalent
if(FALSE) {rm("bb")} #Same
if(FALSE) {rm(list = "bb")} #Faster, verbose, and would not work without quotes
#
# #Trying to Print an Object 'bb' and Handling the Error, if thrown
tryCatch(print(bb), error = function(e) print(paste0(e)))
```

## Data

```{r 'B09D01', comment="", echo=FALSE, results='asis'}
f_getDef("Data")
```

```{r 'B09D02', comment="", echo=FALSE, results='asis'}
f_getDef("Elements")
```

```{r 'B09D03', comment="", echo=FALSE, results='asis'}
f_getDef("Variable")
```


```{r 'B09D04', comment="", echo=FALSE, results='asis'}
f_getDef("Observation")
```


```{r 'B09D05', comment="", echo=FALSE, results='asis'}
f_getDef("Statistics") #dddd
```

## Vectors {#vectors-b09 .tabset .tabset-fade}

R has 6 basic data types (logical, integer, double, character, complex, and raw). These data types can be combined to form Data Structures (vector, list, matrix, dataframe, factor etc.). Refer [What is a Vector!](#vectors-b10 "b10")

```{definition 'Vectors'}
\textcolor{pink}{Vectors} are the simplest type of data structure in R. A vector is a sequence of data elements of the same basic type. 
```

```{definition 'Components'}
Members of a vector are called \textcolor{pink}{components}.
```

Atomic vectors are homogeneous i.e. each component has the same datatype. A vector type can be checked with the \textcolor{pink}{typeof()} or \textcolor{pink}{class()} function. Its length, i.e. the number of elements in the vector, can be checked with the function \textcolor{pink}{length()}.

If the output of an expression does not show numbers in brackets like '[1]' then it is a \textcolor{pink}{'NULL'} type return. [Numbers] show that it is a Vector. Ex: \textcolor{pink}{str()} and \textcolor{pink}{cat()} outputs are of NULL Type.

Use function \textcolor{pink}{c()} to create a vector (or a list) - 

- In R, a literal character or number is just a vector of length 1. So, c() 'combines' them together in a series of 1-length vectors. 
- c() neither creates nor concatenates the vectors, it combines them. Thus, it combines list into a list and vectors into a vector.
- In R, list is a 'Vector' but not an 'Atomic Vector'.
- All arguments are coerced to a common type which is the type of the returned value.
- All attributes (e.g. dim) except 'names' are removed.
- The output type is determined from the highest type of the components in the hierarchy `NULL` < `raw` < `logical` < `integer` < `double` < `complex` < `character` < `list` < `expression`.
- To "index a vector" means, to address specific elements by using square brackets, i.e. x[10] means the ${10^{th}}$ element of vector 'x'.

\textcolor{orange}{Caution:} Colon ":" might produce unexpected length of vectors (in case of 0-length vectors). Suggestion: Use colon only with hardcoded numbers i.e. "1:10" is ok, "1:n" is dangerous and should be avoided.

\textcolor{orange}{Caution:} seq() function might produce unexpected type of vectors (in case of 1-length vectors). Suggestion: Use seq_along(), seq_len().

### Atomic Vectors {.unnumbered}

```{r 'B09-Vectors'}
# #To know about an Object: str(), class(), length(), dim(), typeof(), is(), attributes(), names()
# #Integer: To declare as integer "L" (NOT "l") is needed
ii_int <- c(1L, 2L, 3L, 4L, 5L)
str(ii_int)
#
# #Double (& Default)
dd_dbl <- c(1, 2, 3, 4, 5)
str(dd_dbl)
#
# #Character
cc_chr <- c('a', 'b', 'c', 'd', 'e')
str(cc_chr)
#
# #Logical
ll_lgl <- c(TRUE, FALSE, FALSE, TRUE, TRUE)
str(ll_lgl)
```

### Integer {.unnumbered}

```{r 'B09-Integer'}
# #Integer Vector of Length 1
nn <- 5L
#
# #Colon ":" Operator - Avoid its usage
str(c(1:nn))
c(typeof(pi:6), typeof(6:pi))
#
# #seq() - Avoid its usage
str(seq(1, nn))
str(seq(1, nn, 1))
str(seq(1, nn, 1L))
str(seq(1L, nn, 1L))
#
# #seq_len()
str(seq_len(nn))
```

### Double {.unnumbered}

```{r 'B09-Double'}
str(seq(1, 5, 1))
```

### Character {.unnumbered}

```{r 'B09-Character'}
str(letters[1:5])
```

### Logical {.unnumbered}

```{r 'B09-Logical'}
str(1:5 %% 2 == 0)
```

## DataFrame {#dataframe-b09}

```{r 'B09-DataFrame'}
# #Create Two Vectors
income <- c(100, 200, 300, 400, 500)
gender <- c("male", "female", "female", "female", "male")
#
# #Create a DataFrame
bb <- data.frame(income, gender)
#
# #Print or View DataFrame
#View(bb)
print(bb)
#
# #Struture
str(bb)
#
# #Names
names(bb)
```

## Save and Load an R Script {#script-b09 .tabset .tabset-fade}

R Script file extension is ".R"

\textcolor{pink}{"Ctrl+ S"} will Open Save Window at Working Directory.

\textcolor{pink}{"Ctrl+ O"} will Open the Browse Window at Working Directory.

### Check File Exist {.unlisted .unnumbered}

```{r 'B09-CheckFileExists'}
# #Subdirectory "data" has data files like .csv .rds .txt .xlsx
# #Subdirectory "code" has scripts files like .R 
# #Subdirectory "images" has images like .png
#
# #Check if a File exists 
path_relative <- "data/aa.xlsx" #Relative Path
#
if(file.exists(path_relative)) {
    cat("File Exists\n") 
  } else {
    cat(paste0("File does not exist at ", getwd(), "/", path_relative, "\n"))
  }
#
if(exists("XL", envir = .z)) {
  cat(paste0("Absolute Path exists as: ", .z$XL, "\n"))
  path_absolute <- paste0(.z$XL, "aa", ".xlsx") #Absolute Path
  #
  if(file.exists(path_absolute)) {
    cat("File Exists\n") 
  } else {
    cat(paste0("File does not exist at ", path_absolute, "\n"))
  }
} else {
  cat(paste0("Object 'XL' inside Hidden Environment '.z' does not exist. \n", 
             "It is probably File Path of the Author, Replace the File Path from Your own Directory\n"))
}
```

### Aside {.unlisted .unnumbered}

- This section is NOT useful for general reader and can be safely ignored. It contains my notes related to building this book. These are useful only for someone who is building his own book. (Shivam)
- "Absolute Path" is NOT a problem in Building a Book, Knitting a Chapter, or on Direct Console.
- "Absolute Path" has a problem only when Running code chunk directly from the Rmd document and when the Rmd document is inside a sub-directory (like in this book), then only the Working Directory differs.

## CSV Import /Export {.tabset .tabset-fade}

\textcolor{pink}{write.csv()} and \textcolor{pink}{read.csv()} combination can be used to export data and import it back into R. But, it has some limitations - 

- Re-imported object "yy_data" will NOT match with the original object "xx_data" under default conditions
  1. write.csv(), by default, write row.names (or row numbers) in the first column. 
      - So, either use row.names = FALSE while writing 
      - OR use row.names = 1 while reading
  1. row.names attribute is always read as 'character' even though originally it might be 'integer'. 
      - So, that attribute needs to be coerced
  1. colClasses() needs to be defined to match with the original dataframe, otherwise 'income' is read as 'integer', even though originally it was 'numeric'.
  1. Conclusion: Avoid, if possible.
- Alternative: \textcolor{pink}{saveRDS()} and \textcolor{pink}{readRDS()}
  - Functions to write a single R object to a file, and to restore it.
  - Imported /Exported objects are always identical


```{conjecture 'cannot-open-connection'}
\textcolor{brown}{Error in file(file, ifelse(append, "a", "w")) : cannot open the connection}
```

- Check the path, file name, & file extension for typing mistakes
- Execute getwd(), just before the command, to confirm that the working directory is as expected

### write.csv() {.unnumbered}

```{r 'B09-WriteToCSV'}
str(bb)
#
xx_data <- bb
#
# #Write a dataframe to a CSV File
write.csv(xx_data, "data/B09_xx_data.csv")
#
# #Read from the CSV into a dataframe
yy_data <- read.csv("data/B09_xx_data.csv")
#
# #Check if the object being read is same as the obejct that was written 
identical(xx_data, yy_data)
```

```{r 'B09-WriteToCSV-Manual', include=FALSE, eval=FALSE}
# #Above code does not work When 
# #The Rmd File is inside a Sub-directory and Chunk is Run Manually from the File.
# #In all other conditions when Chunk is executed by Knitr, it works.
# #To Enable Debugging, Execute this Chunk, which is equivalent as above 
# #It is neither executed nor shown in the Finished Document.
xx_data <- bb
write.csv(xx_data, paste0(.z$XL, "B09_xx_data", ".csv"))
yy_data <- paste0(.z$XL, "B09_xx_data", ".csv")
identical(xx_data, yy_data)
```

### Match Objects {.unnumbered}

```{r 'B09-MatchDataFrame'}
# #Exercise to show how to match the objects being imported /exported from CSV
str(bb)
xx_data <- bb
# #Write to CSV
write.csv(xx_data, "data/B09_xx_data.csv")
#
# #Read from CSV by providing row.names Column and colClasses()
yy_data <- read.csv("data/B09_xx_data.csv", row.names = 1,
                    colClasses = c('character', 'numeric', 'character'))
#
# #Coerce row.names attribute to integer
attr(yy_data, "row.names") <- as.integer(attr(yy_data, "row.names"))
#
# #Check if the objects are identical
identical(xx_data, yy_data)
stopifnot(identical(xx_data, yy_data))
```

### RDS {.unnumbered}

```{r 'B09-RDS'}
str(bb)
xx_data <- bb
#
# #Save the Object as RDS File
saveRDS(xx_data, "data/B09_xx_data.rds")
#
# #Read from the RDS File
yy_data <- readRDS("data/B09_xx_data.rds")
#
# #Objects are identical (No additional transformations are needed)
identical(xx_data, yy_data)
```

## Modify Dataframe

```{r 'B09-ModifyDataFrame'}
str(xx_data)
# #Adding a Column to a dataframe
xx_data <- data.frame(xx_data, age = 22:26)
#
# #Adding a Column to a dataframe by adding a Vector
x_age <- 22:26
xx_data <- data.frame(xx_data, x_age)
str(xx_data)
#
# #Adding a Column to a dataframe by using dollar "$"
xx_data$age1 <- x_age
#
# #Adding a Blank Column using NA
xx_data$blank <- NA
#
# #Editing of a dataframe can also be done
# edit(xx_data)
str(xx_data)
#
# #Removing a Column by subsetting
xx_data <- xx_data[ , -c(3)]
#
# #Removing a Column using NULL
xx_data$age1 <- NULL
str(xx_data)
```

## Packages {#packages-b09 .tabset .tabset-fade}

```{definition 'Packages'}
\textcolor{pink}{Packages} are the fundamental units of reproducible R code. 
```

Packages include reusable functions, the documentation that describes how to use them, and sample data.

In R Studio: Packages Tab | Install | Package Name = "psych" | Install

- Packages are installed from \textcolor{pink}{CRAN} Servers
  - To Change Server: Tools | Global Options | Packages | Primary CRAN Repository | Change | CRAN Mirrors (Select Your Preference) | OK
  - All Installed Packages are listed under Packages Tab
  - All Loaded Packages are listed under Packages Tab with a Tick Mark
  - Some packages are dependent on other packages and those are also installed when 'dependencies = TRUE'
  - If a package is NOT installed properly, it will show error when loaded by \textcolor{pink}{library()}

### Install Packages {.unnumbered}

```{r 'B09-InstallPackage', eval=FALSE}
if(FALSE){
  # #WARNING: This will install packages and R Studio will NOT work for that duration
  # #Install Packages and their dependencies
  install.packages("psych", dependencies = TRUE)
}
```

### Load Packages {.unnumbered}

```{r 'B09-Library', eval=FALSE}
# #Load a Package with or without Quotes
library(readxl)
library("readr")
```

### Load Multiple Packages {.unnumbered}

```{r 'B09-LoadPackages', eval=FALSE}
# #Load Multiple Packages
pkg_chr <- c("ggplot2", "tibble", "tidyr", "readr", "dplyr")
#lapply(pkg_chr, FUN = function(x) {library(x, character.only = TRUE)})
#
# #Load Multiple Packages, Suppress Startup Messages, and No console output
invisible(lapply(pkg_chr, FUN = function(x) {
  suppressMessages(library(x, character.only = TRUE))}))
```

### Detach Package {.unnumbered}

```{r 'B09-Detach', eval=FALSE}
# #Detach a package
#detach("package:psych", unload = TRUE)
#
# #Search Package in the already loaded packages
pkg_chr <- "psych"
if (pkg_chr %in% .packages()) {
# #Detach a package that has been loaded previously
  detach(paste0("package:", pkg_chr), character.only = TRUE, unload = TRUE)
}
```

### Install Older Version of Package {.unnumbered}

```{r 'B09-OlderVersion', eval=FALSE}
# #When Update of a Package breaks your code and you want to postpone that debugging
# #Get the URL of older version of the Package from CRAN
packageurl <- "https://cran.r-project.org/src/contrib/Archive/latex2exp/latex2exp_0.5.0.tar.gz"
#
if(FALSE) {# #WARNING: Installation may take some time.
  install.packages(packageurl, repos = NULL, type = "source")  
}
```

### Package Version {.unnumbered}

```{r 'B09-PackageVersion'}
packageVersion("dplyr")
packageVersion("latex2exp")
```

### Rtools4 on Windows {.unnumbered}

\textcolor{orange}{Caution:} It is NOT recommeded. However the instructions are available at [Rtools4](https://cran.r-project.org/bin/windows/Rtools/rtools40.html "https://cran.r-project.org")

```{r 'B09-RTools', eval=FALSE}
# #Only for the debugging purposes, install from GitHub.
if(FALSE) {# #WARNING: Installation may take some time.
  Sys.which("make") #"D:\\Installations\\rtools40\\usr\\bin\\make.exe"
  devtools::install_github("stefano-meschiari/latex2exp", ref = "0.9.3") 
}
```

### Update Packages {.unnumbered}

- Update Packages from RStudio in general
- However there are Packages that come together with R. Those needs to be updated with each R update
  - Run R as an administrator.
  - Packages | Updaate Packages
    - If it asks for "Do you want to use Personal Directory" - Decline. It is another headache.
    - If Some Packages fail to update in Administrator Mode, Rerun the update one by one. It works.


## Import Flights Data {#flights-b09 .tabset .tabset-fade}

To Import Excel in R Studio : Environment | Dropdown | From Excel | Browse

Object imported by \textcolor{pink}{read.csv()} i.e. 'mydata' is NOT same as the one imported by \textcolor{pink}{read_excel()} i.e. 'mydata_xl'

- read_excel() imports as a Tibble which is a modern view of dataframe. It is more restrictive so that output would be more predictable.
- read.csv(), if possible, imports as integer (ex: 'year' column). But, read_excel() imports, if possible, as a numeric.
- Further, read_excel() has imported many columns as 'character' that should have been 'numeric' ex: dep_time
- NOTE: To complete the set \textcolor{pink}{readr::read_csv()} is also covered here which reads CSV and generates a Tibble.


All of these objects can be converted into any other form as needed i.e. dataframe to tibble or vice-versa.

### Flights {.unnumbered}

```{r 'B09-LoadCSV', eval=FALSE}
# #Data File Name has been modified to include lecture number "B09"
# #All Data Files are in the sub-directory named 'data'
mydata <- read.csv("data/B09-FLIGHTS.csv")
#
# #To Copy from Clipboard, assuming copied from xlsx i.e. tab separated data
mydata_clip <- read.csv("clipboard", sep = '\t', header = TRUE)
```

### RDS {.unnumbered}

```{r 'B09-Flights', eval=FALSE}
# #Following Setup allows us to read CSV only once and then create an RDS file
# #Its advantage is in terms of faster loading time and lower memory requirment
xx_csv <- paste0("data/", "B09-FLIGHTS", ".csv")
xx_rds <- paste0("data/", "b09_flights", ".rds")
b09_flights <- NULL
if(file.exists(xx_rds)) {
  b09_flights <- readRDS(xx_rds)
} else {
  # #Read CSV
  b09_flights <- read.csv(xx_csv)
  # #Write Object as RDS
  saveRDS(b09_flights, xx_rds)
}
rm(xx_csv, xx_rds)
mydata <- b09_flights
```

```{r 'B09-Flights-Manual', include=FALSE, eval=TRUE}
# #Only for Local Debugging Purpose. Should be : include=FALSE, eval=TRUE
mydata <- b09_flights <- f_getRDS(b09_flights)
```

### Structure {.unnumbered}

```{r 'B09-StructureFlight'}
str(mydata)
```

### Head {.unnumbered}

```{r 'B09-HeadFlight'}
head(mydata)
```

### Tail {.unnumbered}

```{r 'B09-TailFlight'}
tail(mydata)
```

### Excel {.unnumbered}

```{r 'B09-LoadXl', eval=FALSE}
# #library(readxl)
mydata_xl <- read_excel("data/B09-FLIGHTS.xlsx", sheet = "FLIGHTS")
```

### Excel RDS {.unnumbered}

```{r 'B09-LoadXlRDS'}
# #library(readxl)
xx_xl <- paste0("data/", "B09-FLIGHTS", ".xlsx")
xx_rds_xl <- paste0("data/", "b09_flights_xls", ".rds")
b09_flights_xls <- NULL
if(file.exists(xx_rds_xl)) {
  b09_flights_xls <- readRDS(xx_rds_xl)
} else {
  b09_flights_xls <- read_excel(xx_xl, sheet = "FLIGHTS")
  saveRDS(b09_flights_xls, xx_rds_xl)
}
rm(xx_xl, xx_rds_xl)
mydata_xl <- b09_flights_xls
#
```

### xlsx {.unnumbered}

```{r 'B09-XL'}
str(mydata_xl)
```

### readr {.unnumbered}

```{r 'B09-ReadR'}
# #Following Setup allows us to read CSV only once and then create an RDS file
# #Its advantage is in terms of faster loading time and lower memory requirment
# #library(readr)
xx_csv <- paste0("data/", "B09-FLIGHTS", ".csv")
xx_rds <- paste0("data/", "xxflights", ".rds")
xxflights <- NULL
if(file.exists(xx_rds)) {
  xxflights <- readRDS(xx_rds)
} else {
  xxflights <- read_csv(xx_csv, show_col_types = FALSE)
  attr(xxflights, "spec") <- NULL
  attr(xxflights, "problems") <- NULL
  saveRDS(xxflights, xx_rds)
}
rm(xx_csv, xx_rds)
mydata_rdr <- xxflights
```

## Subsetting

```{r 'B09-Subset'}
# #Subset All Rows and last 3 columns
data6 <- mydata[ , c(17:19)]
str(data6)
# #Subset by deleting the 1:16 columns
data7 <- mydata[ , -c(1:16)]
stopifnot(identical(data6, data7))
```

## Attach a Dataset

\textcolor{orange}{Caution:} Attaching a Dataset should be avoided to prevent unexpected behaviour due to 'masking'. Using full scope resolution i.e. 'data_frame$column_header' would result in fewer bugs. However, if a Dataset has been attached, please ensure that it is detached also.

\textcolor{orange}{Caution:} If a dataset is attached more than once e.g. 4 times, please note that there will be 4 copies attached to the environment. It can be checked with search(). Each needs to be detached.

```{r 'B09-AttachDataSet', eval=FALSE}
if(FALSE){
  # #WARNING: Attaching a Dataset is discouraged because of 'masking'
  # #'dep_time' is Column Header of a dataframe 'mydata'
  tryCatch(str(dep_time), error = function(e) print(paste0(e)))
## [1] "Error in str(dep_time): object 'dep_time' not found\n"
  # #Attach the Dataset
  attach(mydata)
  # #Now all the column headers are accessible without the $ sign
  str(dep_time)
## int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...
  # #But, there are other datasets also, attaching another one results in MESSAGE
  attach(mydata_xl)
## The following objects are masked from mydata:
##
##     air_time, arr_delay, arr_time, carrier, day, dep_delay, dep_time, dest,
##     distance, flight, hour, minute, month, origin, sched_arr_time,
##     sched_dep_time, tailnum, time_hour, year
  str(dep_time)
## chr [1:336776] "517" "533" "542" "544" "554" "554" "555" "557" "557" "558" "558" ...
#
# #'mydata_xl$dep_time' masked the already present 'mydata$dep_time'.
# #Thus now it is showing as 'chr' in place of original 'int'
# #Column Header Names can be highly varied and those will silently mask other variable
# #Hence, attaching a dataset would result in random bugs or unexpected behaviours
#
# #Detach a Dataset
  detach(mydata_xl)
  detach(mydata)
}
```

## Package "psych" {.tabset .tabset-fade}

- \textcolor{pink}{pairs.panels()} -
  - It shows a scatter plot of matrices, with bivariate scatter plots below the diagonal, histograms on the diagonal, and the Pearson correlation above the diagonal.
    - Calculation time is highly dependent on dataset size and type 
    - See Figure \@ref(fig:B09P01)
    - Conclusion: "air_time and distance are highly correlated"

```{conjecture 'need-finite-xlim'}
\textcolor{brown}{Error in plot.window(...) : need finite 'xlim' values}
```

- In this case, the error will be observed if the output of pairs.panels() is assigned to an object.
- Direct console output (i.e. no assignment) should not be a problem

```{conjecture 'par-old-par'}
\textcolor{brown}{Error in par(old.par) : invalid value specified for graphical parameter "pin"}
```

- The Error is generally observed because the Plot does not have enough space in R Studio (Lower Right Pane). In general, it is NOT a problem. It is an error of map() xlim[1] should be less than xlim[2].
- Use larger window size or control image size output

### Image {.unlisted .unnumbered}

```{r 'B09-Psych', include=FALSE}
# # Subset 3 Columns and 10,000 rows 
x_rows <- 10000L
data_pairs <- mydata[1:x_rows, c(7, 16, 9)]
#
# #Equivalent
data_pairs <- mydata  %>%
  select(air_time, distance, arr_delay) %>%
  slice_head(n = x_rows)
#
if( nrow(data_pairs) * ncol(data_pairs) > 1000000 ) {
  print("Please reduce the number of points to a sane number!")
  ggplot()
} else {
  #B09P01
  # #IN: hh, cap_hh, ttl_hh, loc_png
  hh <- data_pairs
  loc_png <- paste0(.z$PX, "B09P01", "-Psych", ".png")
  ttl_hh <- "Flight: Correlation using pairs.panels()"
  cap_hh <- "B09P01"
#
  if(!file.exists(loc_png)) {
    png(filename = loc_png) 
    #dev.control('enable') 
	  pairs.panels(hh, smooth = FALSE, jiggle = FALSE, rug = FALSE, ellipses = FALSE, 
                 cex.cor = 1, cex = 1, gap = 0, main = ttl_hh)
    #title(main = ttl_hh, line = 2, adj = 0)
    title(sub = cap_hh, line = 4, adj = 1)
    B09 <- recordPlot()
    dev.off()
    assign(cap_hh, B09)
    rm(B09)
  }
}
```

```{r 'B09P01', echo=FALSE, fig.cap="(B09P01) Correlation using psych::pairs.panels()"}
knitr::include_graphics(paste0(.z$PX, "B09P01", "-Psych", ".png")) #iiii
```

### Code {.unlisted .unnumbered}

```{r 'B09-Psych-Code', eval=FALSE}
# # Subset 3 Columns and 10,000 rows 
x_rows <- 10000L
data_pairs <- mydata[1:x_rows, c(7, 16, 9)]
#
# #Equivalent
ii <- mydata %>%
  select(air_time, distance, arr_delay) %>%
  slice_head(n = x_rows)
#
stopifnot(identical(ii, data_pairs))
#
if( nrow(data_pairs) * ncol(data_pairs) > 1000000 ) {
  print("Please reduce the number of points to a sane number!")
  ggplot()
} else {
  #B09P01
  pairs.panels(data_pairs)
  if(FALSE){# Cleaner Graph
    pairs.panels(data_pairs, smooth = FALSE, jiggle = FALSE, rug = FALSE, ellipses = FALSE, 
                 cex.cor = 1, cex = 1, gap = 0, main = "Title")
    title(sub = "Caption", line = 4, adj = 1)   
  }
}
```

## Operators in R

- There are multiple \textcolor{pink}{infix binary operators}
- \textcolor{pink}{a %in% b} : returns a logical vector indicating if there is a match or not for its left operand
- \textcolor{pink}{%/%} and \textcolor{pink}{%%} perform integer division and modular division respectively
- \textcolor{pink}{%o%} gives the outer product of arrays.
- \textcolor{pink}{%*%} performs matrix multiplication.
- \textcolor{pink}{%x%} performs the Kronecker product of arrays.
- Magritter Pipe is given by \textcolor{pink}{%>%}
- There are other pipes and other packages also but this is general summary.

## Printing Decimal Numbers in R {#scipen-b09 .tabset .tabset-fade}

- By default, R gives numbers in Scientic Notation. 'E'
  - Personally, it is irritating to read p-values, residuals etc. of model output and to convert them everywhere.
  - Defaults can be changed, so that Scientific Notation is disabled
  - Functions \textcolor{pink}{round()}, \textcolor{pink}{signif()} does not have 'E' option
  - Functions \textcolor{pink}{sprintf()}, \textcolor{pink}{prettyNum()}, \textcolor{pink}{format()} converts to 'character'
    - And there is a \textcolor{pink}{formatC()} which does not get affected by global options()


```{definition 'Rounding'}
\textcolor{pink}{Rounding} means replacing a number with an approximate value that has a shorter, simpler, or more explicit representation.
```

```{definition 'Significant-Digits'}
\textcolor{pink}{Significant digits}, (or significant figures, or precision or resolution), of a number in positional notation are digits in the number that are reliable and necessary to indicate the quantity of something. 
```

- Significant digits
  - Even when some of the digits are not certain, as long as they are reliable, they are considered significant because they indicate the actual volume within the acceptable degree of uncertainty. 
  - Not Significant digits
    - Leading Zeroes e.g. 013 kg or 0.056 m (= 56 mm) both have 2 significant digits
    - Trailing zeros when they are merely placeholders 
    
### Change Defaults {.unlisted .unnumbered}

```{r 'B09-SciFi', eval=FALSE}
if(FALSE) {# #Disable Scientific Notation 'E' in R. Run Once for current session. 
# #Put it in .Rprofile File to always execute this.  
  options(scipen = 999)
}
#
if(FALSE) {# #To revert back to original defaults
  options(scipen = 0, digits = 7)
}
```

### Numerical Printing {.unlisted .unnumbered}

```{r 'B09-Rounding', eval=FALSE}
# #These change in options have NOT been executed to prevent any uninteded consequences
if(FALSE) options(scipen = 0, digits = 7) #From the Disbled 'E' to Default for testing
# #Sequence of Powers of 10
ii <- 10^(2:-4) 
jj <- abs(ii - 1L)
#
round(ii, 2)  #1e+02 1e+01 1e+00 1e-02 0e+00 0e+00
signif(ii, 2) #1e+02 1e+01 1e+00 1e-02 1e-03 1e-04
#
# #Follwoing are better BUT print "character" not "numeric" i.s. suitable for final printing only
#"100"    "10"     "1"      "0.1"    "0.01"   "0.001"  "0.0001"
prettyNum(ii, scientific = FALSE)
format(ii, scientific = FALSE, drop0trailing = TRUE, trim = TRUE)
#
if(FALSE) options(scipen = 999) #From Default to the Disabled 'E'
#
# #round() does not distinguish between 0.001 & 0.0001 and always prints specified decimal places
round(ii, 2)  #100.00  10.00   1.00   0.10   0.01   0.00   0.00
# #signif() handles significant digits but prints trailing zeroes depending upon lowest value
signif(ii, 2) #100.0000  10.0000   1.0000   0.1000   0.0100   0.0010   0.0001
#
round(jj, 2)
signif(jj, 2) #Rounds 0.999 to 1
#
# #Printing "character" and digits option is for rounding
#"100"    "10"     "1"      "0.1"    "0.01"   "0.001"  "0.0001"
prettyNum(ii)
format(ii, drop0trailing = TRUE, trim = TRUE)
#
prettyNum(jj)
format(jj, drop0trailing = TRUE, trim = TRUE, digits = 2) #Rounds 0.999 to 1
#
# #formatC() does not get affected by change in options(). This is undesirable.
formatC(ii, digits = 2)
```

## Find Datasets {.tabset .tabset-fade}


```{r 'B09-Datasets'}
# #List All Datasets of a Loaded Package
data(package = "ggplot2")$results[ , "Item"]
data(package = "nycflights13")$results[ , "Item"]
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B09-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, b09_flights, b09_flights_xls, cc_chr, data6, data7, dd_dbl, 
          gender, ii_int, income, ll_lgl, mydata, mydata_rdr, mydata_xl, nn, path_absolute, 
          path_relative, x_age, xx_data, xxflights, yy_data, B09P01, data_pairs, x_rows, cap_hh)
```

```{r 'B09-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/209-IntroR.Rmd-->

# R Introduction (B10, Sep-05) {#b10}

```{r 'B10', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Introduction to R"
  - 

- Links (Ref)
  - [Install or Load Multiple Packages](#packages-b09 "b09")  
  - [Load Flights Dataset](#flights-b09 "b09")  
  - [Change Working Directory](#working-dir-b09 "b09")
  - [Print Dataframe](#dataframe-b09 "b09")

```{r 'B10-Flights', include=FALSE}
# #Load Data: Flights
xxflights <- f_getRDS(xxflights)
mydata <- xxflights
```

## Notebooks

These allow you to combine executable code and rich text in a single document, along with images, HTML, LaTeX and more.

```{definition 'R-Markdown'}
\textcolor{pink}{R Markdown} is a file format for making dynamic documents with R. 
```

An R Markdown document is written in markdown (an easy-to-write plain text format) and contains chunks of embedded R code.
To know more [Go to Rstudio](https://rmarkdown.rstudio.com/articles_intro.html "https://rmarkdown.rstudio.com/articles_intro.html") 

To know more about Google Colab [Go to Google Colab](https://colab.research.google.com "https://colab.research.google.com") 

NOTE: As I am not using Google Colab, the workflow explained between 00:00 to 35:10 is NOT covered here. If someone is using Google Colab, and is willing to share their notes, I would include those.

## Plot

Base R graphs /plots as shown in figure \@ref(fig:B10P01)

```{conjecture 'plot-finite-xlim'}
\textcolor{brown}{Error in plot.window(...) : need finite 'xlim' values}
```

- If this error is coming when base R plot() function is called
- Check if data has NA values or if character data is supplied where numerical is needed
- Also, do not use assignment to save base R plot and then print

```{r 'B10P01-Save', echo=c(5, 6), results='hide'}
loc_png <- paste0(.z$PX, "B10P01", "-Flights-Scatter-Arr-Dep", ".png")
if(!file.exists(loc_png)) {
# #Save Base Plot without using ggsave()
png(filename = loc_png)
#dev.control('enable') 
plot(mydata$dep_time, mydata$arr_time)
title("Flights", line = 2, adj = 0)
title(sub="B10P01", line=4, adj = 1)
B10P01 <- recordPlot()
dev.off()
}
```

```{r 'B10P01', echo=FALSE, fig.cap="(B10P01) Flights: Arrival Time (Y) vs. Departure (X) Time"}
knitr::include_graphics(paste0(.z$PX, "B10P01", "-Flights-Scatter-Arr-Dep", ".png")) #iiii
```


## Dataset {.tabset .tabset-fade}

- Use cbind() or rbind() to merge dataframes 

### Dimensions {.unnumbered}

```{r 'B10-Dim'}
# #Create a Subset of Dataframe of 1000 Rows for quick calculations
bb <- head(mydata, 1000)
#
# #Dimensions: dim() Row x Column; nrow(); ncol()
dim(bb)
#
stopifnot(identical(nrow(bb), dim(bb)[1]))
stopifnot(identical(ncol(bb), dim(bb)[2]))
```

### Split {.unnumbered}

```{r 'B10-Split'}
# #Split a Dataframe by subsetting
data_1 <- bb[ , 1:8]
data_2 <- bb[ , 9:19]
# str(data_1)
```

### Merge {.unnumbered}

```{r 'B10-MergeCbind'}
# #Merge a Dataframe by cbind()
data_3 <- cbind(data_1, data_2)
# #Equivalent
data_4 <- data.frame(data_1, data_2)
# str(bb_3)
stopifnot(identical(data_3, data_4))
```

### RowSplit {.unnumbered}

```{r 'B10-RowSplit'}
# #Row Split
data_5 <- bb[1:300, ]
data_6 <- bb[301:1000, ]
#
# #Equivalent
n_rows <- 300L
data_5 <- bb[1:n_rows, ]
data_6 <- bb[(n_rows + 1L):nrow(bb), ]
#
stopifnot(identical(data_5, head(bb, n_rows)))
stopifnot(identical(data_6, tail(bb, (nrow(bb) - n_rows))))
```

### RowMerge {.unnumbered}

```{r 'B10-RowMerge'}
# #Merge a Dataframe by rbind()
data_7 <- rbind(data_5, data_6)
stopifnot(identical(bb, data_7))
```

## Change Column Headers 

```{r 'B10-ModifyNames'}
# #Change A Specific Name based on Index Ex: First Header "year" -> "YEAR"
# #NOTE: Output of 'names(bb)' is a character vector, not a dataframe
# #So, [1] is being used to subset for 1st element and NOT the [ , 1] (as done for dataframe)
(names(bb)[1] <- "YEAR")
#
# #Change all Column Headers to Uppercase by toupper() or Lowercase by tolower()
names(bb) <- toupper(names(bb))
```


## NA {#na-b10 .tabset .tabset-fade}

```{definition 'NA'}
\textcolor{pink}{NA} is a logical constant of length 1 which contains a missing value indicator. 
```

NA can be coerced to any other vector type except raw. There are also constants like NA_integer_, NA_real_ etc.
For checking only the presence of NA, \textcolor{pink}{anyNA()} is faster than \textcolor{pink}{is.na()}

Overview of 'Not Available'

- If the imported data has blank cell, it would be imported as NA

To remove all NA

- \textcolor{pink}{na.omit()}
  - Output is a dataframe
  - It is slower but adds the omitted row numbers as an attribute i.e. na.action
- \textcolor{pink}{complete.cases()}
  - Output is a logical vector, thus it needs subsetting to get the dataframe
  - Faster and also allows partial selection of columns i.e. ignore NA in other columns
  - \textcolor{orange}{Caution:} It may throw Error if 'POSIXlt' Columns are present
- \textcolor{pink}{tidyr::drop_na()}
- \textcolor{pink}{rowSums(is.na())}
  - It can also be used for excluding rows with more than allowed numbers of NA. However, in general, this is not recommended because random columns retain NA. These may break the code later or change the number of observations. It is useful when all columns are similar in nature e.g. if each column represent response to a survey question.



### NA {.unnumbered}

```{r 'B10-NA'}
bb <- xxflights
# #anyNA() is faster than is.na()
if(anyNA(bb)) print("NA are Present!") else print("NA not found")
#
# #Columnwise NA Count
bb_na_col <- colSums(is.na(bb))
# #
bb %>% summarise(across(everything(), ~ sum(is.na(.)))) %>% 
  pivot_longer(everything()) %>% filter(value > 0)
#
colSums(is.na(bb)) %>% as_tibble(rownames = "Cols") %>% filter(value > 0)
#
# #Vector of Columns having NA
which(bb_na_col != 0)
stopifnot(identical(which(bb_na_col != 0), which(vapply(bb, anyNA, logical(1)))))
#
# #Indices of Rows with NA
head(which(!complete.cases(bb)))
#
# #How many rows contain NA
sum(!complete.cases(bb))
#
# #How many rows have NA in specific Columns
sum(!complete.cases(bb[ , c(6, 9, 4)]))
```


### RemoveNA {.unnumbered}

```{r 'B10-RemoveNA'}
# #Remove all rows which have any NA 
# #na.omit(), complete.cases(), tidyr::drop_na(), rowSums(is.na())
bb_1 <- na.omit(bb)
# #Print the Count of removed rows containg NA
print(paste0("Note: ", length(attributes(bb_1)$na.action), " rows removed."))
#
# #Remove additional Attribute added by na.omit()
attr(bb_1, "na.action") <- NULL
#
# #Equivalent 
bb_2 <- bb[complete.cases(bb), ]
bb_3 <- bb %>% drop_na()
bb_4 <- bb[rowSums(is.na(bb)) == 0, ]
#Validation
stopifnot(all(identical(bb_1, bb_2), identical(bb_1, bb_3), identical(bb_1, bb_4)))
#
# #complete.cases also allow partial selection of specific columns
# #Remove rows which have NA in some columns i.e. ignore NA in other columns
dim(bb[complete.cases(bb[ , c(6, 9, 4)]), ])
# #Equivalent 
dim(bb %>% drop_na(dep_delay, arr_delay, dep_time))
#
# #Remove rows which have more than allowed number of NA (ex:4) in any column
# #Caution: In general, this is not recommended because random columns retain NA
dim(bb[rowSums(is.na(bb)) <= 4L, ])
```

## Apply {#apply-b10}

Sources:  [(SO) Grouping Functions and the Apply Family](https://stackoverflow.com/questions/3505701 "https://stackoverflow.com"), [(SO) Why is vapply safer than sapply](https://stackoverflow.com/questions/12339650 "https://stackoverflow.com"), [Hadley - Advanced R - Functionals](https://adv-r.hadley.nz/functionals.html "https://adv-r.hadley.nz/functionals.html"), 
[This](https://www.r-bloggers.com/2016/03/apply-lapply-rapply-sapply-functions-in-r "https://www.r-bloggers.com/2016/03/apply-lapply-rapply-sapply-functions-in-r"), 
[This](https://towardsdatascience.com/dealing-with-apply-functions-in-r-ea99d3f49a71 "https://towardsdatascience.com/dealing-with-apply-functions-in-r-ea99d3f49a71"), 
& [This](https://www.datacamp.com/community/tutorials/r-tutorial-apply-family "https://www.datacamp.com/community/tutorials/r-tutorial-apply-family")

Apply Function in R are designed to avoid explicit use of loop constructs. 

- To manipulate slices of data in a repetitive way. 
- They act on an input list, matrix or array, and apply a named function with one or several optional arguments.

1. \textcolor{pink}{apply(X, MARGIN, FUN, ..., simplify = TRUE)}
    - Refer R Manual p72 - "Apply Functions Over Array Margins"
    - Returns a vector or array or list of values obtained by applying a function to margins of an array or matrix.
    - When you want to apply a function to the rows or columns of a matrix (and higher-dimensional analogues); not generally advisable for data frames as it will coerce to a matrix first
    - MARGIN = 1 indicates application over ROWS, 2 indicates COLUMNS 
    - Examples & Details: "ForLater"
1. \textcolor{pink}{lapply(X, FUN, ...)}
    - Refer R Manual p342 - "Apply a Function over a List or Vector"
    - 'list' apply i.e. lapply returns a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X.
    - Examples & Details: "ForLater"
    - When you want to apply a function to each element of a list in turn and get a list back.
    - \textcolor{pink}{lapply(x, mean)}
    - \textcolor{pink}{lapply(x, function(x) c(mean(x), sd(x)))}
1. \textcolor{pink}{sapply(X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)}
    - 'simplified' wrapper of lapply 
    - When you want to apply a function to each element of a list in turn, but you want a vector back, rather than a list.
    - \textcolor{orange}{Caution:} It sometimes fails silently or unexpectedly changes output type 
1. \textcolor{pink}{vapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE)}
    - 'verified' apply i.e. vapply is similar to sapply, but has a pre-specified type of return value, so it can be safer (and sometimes faster) to use.
    - vapply returns a vector or array of type matching the FUN.VALUE.
    - With FUN.VALUE you can specify the type and length of the output that should be returned each time your applied function is called. 
    - It improves consistency by providing limited return type checks.
    - Further, if the input length is zero, sapply will always return an empty list, regardless of the input type (Thus behaving differently from non-zero length input). Whereas, with vapply, you are guaranteed to have a particular type of output, so you do not need to write extra checks for zero length inputs.
1. Others - "ForLater"
    - tapply is a tagged apply where the tags identify the subsets
    - mapply for applying a function to multiple arguments
    - rapply for a 'recursive' version of lapply
    - eapply for applying a function to each entry in an 'environment'


```{r 'B10-Apply'}
# #Subset Dataframe 
bb <- xxflights
data_8 <- bb[ , c("dep_delay", "arr_delay", "dep_time")]
#data_8 <- bb %>% select(dep_delay, arr_delay, dep_time) 
#
# #Remove missing values
data_9 <- na.omit(data_8)
#
# #Calculate Columnwise Mean
(bb_1 <- apply(data_9, 2, mean))
bb_2 <- unlist(lapply(data_9, mean))
bb_3 <- sapply(data_9, mean)
bb_4 <- vapply(data_9, mean, numeric(1))
#
stopifnot(all(identical(bb_1, bb_2), identical(bb_1, bb_3), identical(bb_1, bb_4)))
```

## Vectors {#vectors-b10 .tabset .tabset-fade}

Refer [The 6 Datatypes of Atomic Vectors](#vectors-b09 "b09")

Create a Basic Tibble, Table\@ref(tab:B10T01), for evaluating 'is.x()' series of functions in Base R

- anyNA() is TRUE if there is an NA present, FALSE otherwise
- is.atomic() is TRUE for All Atomic Vectors, factor, matrix but NOT for list
- is.vector() is TRUE for All Atomic Vectors, list but NOT for factor, matrix, DATE & POSIXct
  - \textcolor{orange}{Caution:} With vapply() it returns TRUE for matrix (it checks individual elements)
  - \textcolor{orange}{Caution:} FALSE if the vector has attributes (except names) ex: DATE & POSIXct
- is.numeric() is TRUE for both integer and double
- is.integer(), is.double(), is.character(), is.logical() are TRUE for their respective datatypes only
- is.factor(), is.ordered() are membership functions for factors with or without ordering
  - For more: nlevels(), levels()
- lubridate
  - is.timepoint() is TRUE for POSIXct, POSIXlt, or Date
  - is.POSIXt(), is.Date() are TRUE for their respective datatypes only


```{r 'B10-BasicTibble', include=FALSE}
# #Basic Tibble
nn <- 6L
xxbasic10 <- tibble(ii = 1:nn, dd = seq(1, nn, 1), cc = head(letters, nn), 
             ll = (ii %% 2) == 0, ff = factor(rep(c("odd", "even"), length.out = nn)),
			 fo = factor(rep(c("odd", "even"), length.out = nn), ordered = TRUE),
			 dtm = Sys.time() + 1:nn, dat = Sys.Date() + 1:nn)
bb <- xxbasic10
str(bb)
```

```{r 'B10-SaveTibble', include=FALSE}
# #Save Tibble
#f_setRDS(xxbasic10)
```

```{r 'B10T01', echo=FALSE}
bb <- xxbasic10
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
# #Kable Table
kbl(head(bb),
  caption = "(B10T01) Vector Classes",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```


### Basic Tibble {.unnumbered}

```{r 'B10-BasicTibble-A', ref.label=c('B10-BasicTibble'), eval=FALSE}
#
```

### is {.unnumbered}

```{r 'B10-is'}
# #Validation
# #anyNA() is TRUE if there is an NA present, FALSE otherwise
vapply(bb, anyNA, logical(1))
#
# #is.atomic() is TRUE for All Atomic Vectors, factor, matrix but NOT for list
vapply(bb, is.atomic, logical(1))
#
# #is.vector() is TRUE for All Atomic Vectors, list but NOT for factor, matrix, DATE & POSIXct
# #CAUTION: With vapply() it returns TRUE for matrix (it checks individual elements)
# #CAUTION: FALSE if the vector has attributes (except names) ex: DATE & POSIXct
vapply(bb, is.vector, logical(1))
#
# #is.numeric() is TRUE for both integer and double
vapply(bb, is.numeric, logical(1))
#
# #is.integer() is TRUE only for integer
vapply(bb, is.integer, logical(1))
#
# #is.double() is TRUE only for double
vapply(bb, is.double, logical(1))
#
# #is.character() is TRUE only for character
vapply(bb, is.character, logical(1))
#
# #is.logical() is TRUE only for logical
vapply(bb, is.logical, logical(1))
```

### Factor {.unnumbered}

```{r 'B10-Factors'}
# #Factors
# #is.factor() is TRUE only for factor (unordered or ordered)
vapply(bb, is.factor, logical(1))
#
# #is.ordered() is TRUE only for ordered factor
vapply(bb, is.ordered, logical(1))
#
# #nlevels()
vapply(bb, nlevels, integer(1))
#
# #levels()
vapply(bb, function(x) !is.null(levels(x)), logical(1))
#
# #table()
table(bb$ff)
```

### lubridate::is {.unnumbered}

```{r 'B10-LubridateIS'}
# #Package lubridate covers the missing functions for POSIXct, POSIXlt, or Date 
# #is.timepoint() is TRUE for POSIXct, POSIXlt, or Date
vapply(bb, is.timepoint, logical(1))
#
# #is.POSIXt() is TRUE only for POSIXct 
vapply(bb, is.POSIXt, logical(1))
#
# #is.Date() is only TRUE for DATE 
vapply(bb, is.Date, logical(1))
```


### Duplicates {.unnumbered}

```{r 'B10-Duplicates'}
# #Which Columns have Duplicate Values
vapply(bb, function(x) anyDuplicated(x) != 0L, logical(1))
```

## Factors {#factors-b10 .tabset .tabset-fade}

```{definition 'Factors'}
\textcolor{pink}{Factors} are the data objects which are used to categorize the data and store it as levels. 
```

They can store both strings and integers. They are useful in the columns which have a limited number of unique values. Like "Male, Female" and "True, False" etc. They are useful in data analysis for statistical modelling.

Factor is nothing but the numeric representation of the character vector.

\textcolor{pink}{as.factor()} vs. \textcolor{pink}{factor()}

- as.factor() is faster than factor() when input is a factor or integer
- as.factor retains unused or NA levels whereas factor drops them
  - levels can also be dropped using \textcolor{pink}{droplevels()}

- [(SO) Levels vs. Labels](https://stackoverflow.com/questions/5869539 "https://stackoverflow.com")
  - Levels are Input, Labels are Output in factor(). 
  - There is only 'level' attribute, no 'label' attribute.
  - In R (unlike SPSS) there is NO difference between what is stored and what is displayed. As soon as the levels ("papaya", "banana") are given labels ("pink", "black"), there is NO way to get back the original levels.
  - (Aside) This misconception persists because generally we change between numerical to factor or binary character to factor. In these situations generally we know which level shows what and edit /sort them immediately.

- \textcolor{orange}{Caution:} It is highly advised to use 'levels' when creating factors to keep a link on their ordering and not to be dependent upon their occurance in the vector.
  - Further, this helps while relabeling them because the order is known.
  - By using 'forcats' or 'car' packages (probably) it can be done for default situation. By sorting factor levels by their integer equivalent before assigning new labels to them. However, this is low priority for now.
    - [(SO) This](https://stackoverflow.com/questions/37715937 "https://stackoverflow.com")
    - [(SO) This](https://stackoverflow.com/questions/11810605 "https://stackoverflow.com")


### Transformation {.unnumbered} 

```{r 'B10-AsFactor'}
str(bb$ll)
# #Coercion to Factor
bb$new <- as.factor(bb$ll)
str(bb$new)
#
# #table()
table(bb$ll)
table(bb$new)
#
# #Levels can be Labelled differently also
str(bb$ff)
# # 
str(factor(bb$ff, levels = c("even", "odd"), labels = c("day", "night")))
str(factor(bb$ff, levels = c("odd", "even"), labels = c("day", "night")))
#
# #Coercion from Factor to character, logical etc.
bb$xcc <- as.character(bb$new)
bb$xll <- as.logical(bb$new)
#
str(bb)
```

### Flights {.unnumbered} 

```{r 'B10-FactorMod'}
bb <- xxflights
aa <- c("month", "day")
str(bb[aa])
# #To factor
bb$day <- as.factor(bb$day)
bb$month <- as.factor(bb$month)
# #Equivalent
#bb[aa] <- lapply(bb[aa], as.factor)
str(bb[aa])
```

### Re-Label {.unnumbered} 

```{r 'B10-Relabel'}
# #Unordered Named Vector of Fruits with Names of Colours
# #NOTE: First letters of each colour and fruit match
ii <- c("pink" = "papaya", "black" = "banana", "orchid" = "orange", "amber" = "apple")
ii
#
# #Factor Vectors (default is Alphabetical Sorting) using unname() to remove names
fruit_base <- factor(unname(ii))
# #sort()
fruit_sort <- factor(unname(sort(ii)))
# #unique() provides the values in the sequence of their appearance 
fruit_uniq <- factor(unname(ii), levels = unique(ii))
#
# #By Default Levels Match even though the actual Vectors do not Match
stopifnot(identical(levels(fruit_base), levels(fruit_sort)))
fruit_base
fruit_sort
fruit_uniq
#
# #Relabelling: First letters should always match between Fruits and Colours
color_base <- fruit_base
color_sort <- fruit_sort
color_uniq <- fruit_uniq
#
levels(color_base) <- names(ii)[match(color_base, ii)] #WRONG
levels(color_sort) <- names(ii)[match(color_sort, ii)]
levels(color_uniq) <- names(ii)[match(color_uniq, ii)]
#
# #CAUTION: This is WRONG. 
color_base #WRONG
#
color_sort 
color_uniq 
```


## Lists {#lists-b10 .tabset .tabset-fade}

```{definition 'Lists'}
\textcolor{pink}{Lists} are by far the most flexible data structure in R. They can be seen as a collection of elements without any restriction on the class, length or structure of each element.
```

\textcolor{orange}{Caution:} The only thing you need to take care of, is that you do not give two elements the same name. R will NOT throw ERROR.

```{definition 'DataFrame'}
\textcolor{pink}{Data Frames} are lists with restriction that all elements of a data frame are of equal length.
```

Due to the resulting two-dimensional structure, data frames can mimic some of the behaviour of matrices. You can select rows and do operations on rows. You cannot do that with lists, as a row is undefined there.

A Dataframe is intended to be used as a relational table. This means that elements in the same column are related to each other in the sense that they are all measures of the same metric. And, elements in the same row are related to each other in the sense that they are all measures from the same observation or measures of the same item. This is why when you look at the structure of a Dataframe, it will state the the number of observations and the number of variables instead of the number of rows and columns. 

Dataframes are distinct from Matrices because they can include heterogenous data types among columns/variables. Dataframes do not permit multiple data types within a column/variable, for reasons that also follow from the relational table idea.

All this implies that you should use a data frame for any dataset that fits in that two-dimensional structure. Essentially, you use data frames for any dataset where a column coincides with a variable and a row coincides with a single observation in the broad sense of the word. For all other structures, lists are the way to go.

- Does everything in R have (exactly one) class 
  - Everything has (at least one) class. Objects can have multiple classes
  - It is mostly just the class attribute of an object. But when the class attribute is not set, the \textcolor{pink}{class()} function makes up a class from the object 'type' and the 'dim' attribute.
  - lists and dataframes have same typeof 'list' but different class
- Then what does \textcolor{pink}{typeof()} tell us
  - It tells us the internal 'storage mode' of an object. How the R perceives the object and interacts with it.
  - An object has one and only one mode [(SO) Difference between mode and class](https://stackoverflow.com/questions/35445112 "https://stackoverflow.com")
  - class is an attribute and thus can be defined/overridden by a user, however, mode (i.e. typeof ) cannot be
- To define an object, what should be known about it
  - class(), typeof(), is(), attributes(), str(), inherits(), ...


### list {.unnumbered}

```{r 'B10-List'}
# #CAUTION: Do not Create a list with duplicate names (R will NOT throw ERROR)
bb <- list(a=1, b=2, a=3)
# # 3rd index cannot be accessed using $
bb$a
identical(bb$a, bb[[1]])
identical(bb$a, bb[[3]])
bb[[3]]
```


### class vs. typeof {.unnumbered}

```{r 'B10-LstVsDft'}
# #Create a list
bb_lst <- list( a = c(1, 2), b = c('a', 'b', 'c'))
tryCatch(
# #Trying to create varying length of variables in dataframe like in list
  bb_dft <- data.frame(a = c(1, 2), b = c('a', 'b', 'c')), 
  error = function(e) {
# #Print ERROR
    cat(paste0(e))
# #Double Arrow Assignment '<<-' to assign in parent environment
	bb_dft <<- data.frame(a = c(1, 2), b = c('a', 'b'))
	}
  )
#
# #Both list and dataframe have same type() 
typeof(bb_lst)
typeof(bb_dft)
#
# #But, class() is different for list and dataframe
class(bb_lst)
class(bb_dft)
#
str(bb_lst)
str(bb_dft)
#
# #Although 'bb_lst_c' is a list but inside coercion takes place i.e. '9' is character
bb_lst_c <- list( a = c(8, 'x'), b = c('y', 9))
str(bb_lst_c[[2]][2])
#
# #Here, '9' is numeric, it is stored as list element so note the extra [[]]
bb_lst_l <- list( a = list(8, 'x'), b = list('y', 9))
str(bb_lst_l[[2]][[2]])
```


## Matrix

```{r 'B10-Matrix'}
# #Create a Matrix
bb_mat <- matrix(1:6, nrow = 2, ncol = 3)
print(bb_mat)
str(bb_mat)
class(bb_mat)
typeof(bb_mat)
```


## Merge

```{r 'B10-Merge'}
# #Basic Tibble
bb <- xxbasic10
str(bb)
# #Split with 'cc' as common ID column
bb_a <- bb[1:3]
bb_b <- bb[3:ncol(bb)]
#
# #merge() using the common ID column 'cc'
bb_new <- merge(bb_a, bb_b, by = "cc")
bb_new
```


## Sort {#sorting-b10 .tabset .tabset-fade}

- \textcolor{pink}{sort()}
  - It sorts the vector in an ascending order
- \textcolor{pink}{rank()}
  - rank returns the order of each element in an ascending list
  - The smallest number receives the rank 1
  - If there are ties, it returns numeric not integer with ranks being 2.5 etc
- \textcolor{pink}{order()}
  - order returns the index each element would have in an ascending list
- \textcolor{pink}{dplyr::arrange()}
  - arrange() orders the rows of a data frame by the values of selected columns.
  - NA are always sorted to the end, even when wrapped with desc().

```{conjecture 'Function-Not-Found'}
\textcolor{brown}{Error in arrange(bb, day) : could not find function "arrange"}
```

- Load the Package (dplyr etc.) having the function.

### order {.unnumbered}

```{r 'B10-Order'}
bb <- xxflights
# #Sort ascending (default)
bb_1 <- bb[order(bb$dep_delay), ]
# #Sort descending
bb_2 <- bb[order(-bb$dep_delay), ]
#
bb[1:5, c("dep_time", "dep_delay", "tailnum", "carrier")]
bb_1[1:5, c("dep_time", "dep_delay", "tailnum", "carrier")]
bb_2[1:5, c("dep_time", "dep_delay", "tailnum", "carrier")]
```

### Multi Column {.unnumbered}

```{r 'B10-OrderMulti'}
bb <- xxbasic10
bb
# #Sort ascending (default)
(bb_1 <- bb[order(bb$ll), ])
# #Sort on Multiple Columns with ascending and descending
(bb_2 <- bb[order(bb$ll, -bb$dd), ])
#
stopifnot(identical(bb_2, arrange(bb, ll, -dd)))
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B10-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, B10P01, bb_1, bb_2, bb_3, bb_4, bb_a, bb_b, bb_dft, bb_lst, 
          bb_lst_c, bb_lst_l, bb_mat, bb_na_col, bb_new, data_1, data_2, data_3, data_4, data_5, 
          data_6, data_7, data_8, data_9, mydata, n_rows, nn, xxbasic10, xxflights, loc_png, 
          color_base, color_sort, color_uniq, fruit_base, fruit_sort, fruit_uniq)
```

```{r 'B10-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/210-IntroR.Rmd-->

# Data Manipulation (B11, Sep-12) {#b11}

```{r 'B11', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Understanding basic data manipulation using R"
  - [Filter Rows](#rowfilter-b11 "b11")
  
- Links (Ref)
  - [Transform columns as Factors](#factors-b10 "b10")
  - [Sorting Basics](#sorting-b10 "b10")
  - [Do not miss the missing values i.e. NA](#na-b10 "b10")

```{r 'B11-Flights', include=FALSE}
# #Load Data: Flights
xxflights <- f_getRDS(xxflights)
bb <- xxflights
```

## Get Help

```{r 'B11-Help', eval=FALSE}
# #To get the Help files on any Topic including 'loaded' Packages
?dplyr
?mutate
# #Help files on any Topic including functions from 'installed' but 'not loaded' Packages
?dplyr::mutate()
# #Operators need Backticks i.e. ` . In keyboards it is located below 'Esc' Key i.e. Tilde "~" Key
?`:`
# #To Get the list of All Options used by Base R (including user defined)
?options
```


## Logical Operators and Functions {#logicals-b11 .tabset .tabset-fade}

- "\textcolor{pink}{|}"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Or, binary, vectorized)
- "\textcolor{pink}{||}"&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;(Or, binary, not vectorized)
- "\textcolor{pink}{&}"&nbsp;&nbsp;&nbsp;&nbsp;&thinsp;(And, binary, vectorized)
- "\textcolor{pink}{&&}"&nbsp;(And, binary, not vectorized)
- Functions - \textcolor{pink}{any()}, \textcolor{pink}{all()}

Overview

- Vectorised forms are "&" "|" 
  - Thus, these compare vectors elementwise and operate over complete vector length.
  - \textcolor{pink}{NA is a valid logical object}. Where a component of x or y is NA, the result will be NA if the outcome is ambiguous.
  - All components of x or y are evaluated
  - (\textcolor{pink}{recycling}) of elements occur if vector lengths are different
  - These are NOT recommended for use inside if() clauses
  - These are generally used for filtering
  - &, | do the pairwise operation in R (vs bitwise in Python, C etc.) 
- Non-vectorised forms are "&&" "||" 
  - These examine only the first element of each vector
    - \textcolor{orange}{Caution:} For these, vector length should always be 1
    - Use all() and any() to reduce the length to one
  - (\textcolor{pink}{short-circuit}) These stop execution as soon as these find at least one definite condition i.e. TRUE for ||, FALSE for &&.
    - They will not evaluate the second operand if the first operand is enough to determine the value of the expression. 
  - These are preferred in if() clauses
  - &&, || do the bitwise operation in R (vs pairwise in Python, C etc.) 
- all() and any()
  - all() : Are All Values TRUE
    - TRUE for 0-length vector
  - any() : Is at least one of the values TRUE
    - FALSE for 0-length vector
  - The value is a logical vector of length one being `TRUE`, `FALSE`, or `NA`.


### Operators {.unnumbered}

```{r 'B11-PairwiseSingle'}
# #At lease one TRUE is present
NA | TRUE
# #Depending upon what the unknown is, the outcome will change
NA | FALSE
# #Depending upon what the unknown is, the outcome will change
NA & TRUE
# #At lease one FALSE is present
NA & FALSE 
#
# #For length 1 vectors, output of vectorised and non-vectorised forms is same
stopifnot(all(identical(NA || TRUE, NA | TRUE), identical(NA || FALSE, NA | FALSE),
              identical(NA && TRUE, NA & TRUE), identical(NA && FALSE, NA & FALSE)))
#
# #But for vectors of >1 length, output is different
x <- 1:5
y <- 5:1
(x > 2) & (y < 3)
(x > 2) && (y < 3)
#
# # '&&' evaluates only the first element of Vector, thus caution is advised
TRUE & c(TRUE, FALSE)
TRUE & c(FALSE, FALSE)
TRUE && c(TRUE, FALSE)
TRUE && c(FALSE, FALSE)
TRUE && all(c(TRUE, FALSE))
TRUE && any(c(TRUE, FALSE))
```

### Evaluation {.unnumbered}

```{r 'B11-ShortCircuit'}
if(exists("x")) rm(x)
exists("x")
#
# # No short-circuit for "|" or "&", Evaluates Right and throws Error
tryCatch( TRUE | x, error = function(e) cat(paste0(e)))
tryCatch( FALSE & x, error = function(e) cat(paste0(e)))
#
# #Does not evaluate Right input because outcome already determined
tryCatch( TRUE || x, error = function(e) cat(paste0(e)))
tryCatch( FALSE && x, error = function(e) cat(paste0(e)))
# #evaluates Right input because outcome cannot be determined and throws error
tryCatch( TRUE && x, error = function(e) cat(paste0(e)))
```

### AnyAll {.unnumbered}

```{r 'B11-AnyAll'}
# #any()
any(NA, TRUE)
any(NA, FALSE)
any(NA, TRUE, na.rm = TRUE)
any(NA, FALSE, na.rm = TRUE)
any(character(0))
#
# #all()
all(NA, TRUE)
all(NA, FALSE)
all(NA, TRUE, na.rm = TRUE)
all(NA, FALSE, na.rm = TRUE)
all(character(0))
```

## Relational Operators {#relationals-b11}

\textcolor{pink}{$>$} , \textcolor{pink}{$<$} , \textcolor{pink}{$==$} , \textcolor{pink}{$>=$} , \textcolor{pink}{$<=$} , \textcolor{pink}{$!=$} 


## Filter  {.tabset .tabset-fade}

- \textcolor{pink}{dplyr::filter()}
  - (Masks) base::filter()
  - [Do not miss the missing values i.e. NA](#na-b10 "b10")
  - It treats comma as '&'
- \textcolor{pink}{subset()} vs. \textcolor{pink}{filter()} - 
  - \textcolor{orange}{Caution:} R Manual itself warns against usage of subset(). It is better to use \textcolor{pink}{[]} for subsetting
  - \textcolor{orange}{Caution:} NOT Verified Yet
    - subset works on matrices, however, filter does not
    - subset does not work on databases, filter does
    - subset does not drop the rownames, however, filter removes them
    - filter preserves the class of the column, subset does not
    - filter works with grouped data, subset ignores them
  - filter is stricter and thus would lead to fewer causes of unexpected outcome
- \textcolor{pink}{which()}
  - Takes a Boolean vector and returns a shorter vector containing the indices of the elements which were true.
  - If you want to know 'which' elements of a logical vector are TRUE i.e. their indices.
    - Ex: Get the position of the maximum or minimum values
  - If NA are present and you do not want them in the output
- \textcolor{pink}{with()}
  - with() is a wrapper for functions with no 'data' argument. It allows usage of function as if it had a data argument.
  
  
```{conjecture 'Object-Not-Found-01'}
\textcolor{brown}{Error in match.arg(method) : object 'day' not found}
```

- when 'dplyr' package is not loaded, base::filter() throws this error. 
- Either Load the Package (dplyr etc.) or use scope resolution '::'

### Basics {.unnumbered}

```{r 'B11-dplyrFilter'}
# #dplyr::filter() - Filter Rows based on Multiple Columns
bb_1 <- filter(bb, month == 1, day == 1)
dim(bb_1)
# #Filtering by multiple criteria within a single logical expression
stopifnot(identical(bb_1, filter(bb, month == 1 & day == 1)))
#
if(anyNA(bb_1)) {
  bb_na <- na.omit(bb_1)
  print(paste0("Note: ", length(attributes(bb_na)$na.action), " rows removed."))
} else {
  print("NA not found")
}
dim(bb_na)
```

### Conditional {.unnumbered}

```{r 'B11-ConditionalFilter'}
dim(bb)
#
# #Flights in either months of November or Decemeber
dim(bb_2 <- filter(bb, month == 11 | month == 12))
#
# #Flights with arrival delay '<= 120' or departure delay '<= 120' 
# #It excludes flights where arrival & departure BOTH are delayed by >2 hours
# #If either delay is less than 2 hours, the flight is included
dim(bb_3 <- filter(bb, arr_delay <= 120 | dep_delay <= 120))
dim(bb_4 <- filter(bb, !(arr_delay > 120 & dep_delay > 120)))
dim(bb_5 <- filter(bb, (!arr_delay > 120 | !dep_delay > 120)))
#
# #Destination to IAH or HOU
dim(bb_6 <- filter(bb, dest == "IAH" | dest == "HOU"))
dim(bb_7 <- filter(bb, dest %in% c("IAH", "HOU")))
#
# #Carrier being "UA", "US", "DL"
dim(bb_8 <- filter(bb, carrier == "UA" | carrier == "US" | carrier == "DL"))
dim(bb_9 <- filter(bb, carrier %in% c("UA", "US", "DL")))
#
# #Did not leave late (before /on time departure) but Arrived late by >2 hours
dim(bb_10 <- filter(bb, (arr_delay > 120) & !(dep_delay > 0)))
# 
# #Departed between midnight and 6 AM (inclusive)
dim(bb_11 <- filter(bb, (sched_dep_time >= 00 & sched_dep_time <= 600)))
```


### subset() {.unnumbered}

```{r 'B11-Subset'}
# #subset() - Recommendation is against its usage. Use either '[]' or filter()
dim(bb_12 <- subset(bb, month == 1 | !(dep_delay >= 120), 
                    select = c("flight", "arr_delay")))
dim(bb_13 <- subset(bb, month == 1 | !(dep_delay >= 120) | carrier == "DL", 
                select = c("flight", "arr_delay")))
```


### Filter Rows {#rowfilter-b11 .unnumbered}

```{r 'B11-SubsetRow'}
# #Data: mtcars, 32x11, "mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb"
bb <- aa <- mtcars
#str(bb)
#summary(bb)
# #
# #Avoid subset()
ii <- subset(bb, wt > 2 & wt < 3)
# #which() 
jj <- bb[which(bb$wt > 2 & bb$wt <= 3), ]
#
# #which() select only TRUE and NOT the NA
(1:2)[which(c(TRUE, NA))]
(1:2)[c(TRUE, NA)]
#
# #which() is faster than head() 
ee <- bb[which(bb$wt > 2 & bb$wt <= 3)[1:6], ] 
ff <- head(bb[bb$wt > 2 & bb$wt <= 3, ], 6)
stopifnot(identical(ee, ff))
#
# #Normal Filter using [] operator
kk <- bb[bb$wt > 2 & bb$wt <= 3, ]
#
# #with()
ll <- with(bb, bb[wt > 2 & wt <= 3, ])
#
# #filter()
mm <- bb %>% filter(wt > 2 & wt <= 3)
#
stopifnot(all(identical(ii, jj), identical(ii, kk), identical(ii, ll), identical(ii, mm)))
#
# #Another set of equivalent operations for OR 
ii <- subset(bb, cyl == 4 | cyl == 6)
jj <- bb[bb$cyl %in% c(4, 6), ]
kk <- bb[which(bb$cyl %in% c(4, 6)), ]
ll <- bb %>% filter(cyl == 4 | cyl == 6)
mm <- bb %>% filter(cyl %in% c(4, 6))
#
stopifnot(all(identical(ii, jj), identical(ii, kk), identical(ii, ll), identical(ii, mm)))
#
# #General Conditional Subsetting on Flights data
bb <- xxflights
dim(bb)
#
dim(bb[which(bb$day == 1 & !(bb$month ==1)), ])
dim(bb[which(bb$day == 1 | bb$month ==1), ])
dim(bb[which(bb$day == 1 & bb$month ==1), ])
dim(bb[which(bb$day == 1, bb$month ==1), ])
dim(bb[which(bb$day == 1 & !(bb$carrier == "DL")), ])
dim(bb[which(bb$day == 1 | bb$carrier == "DL"), ])
dim(bb[which(bb$day == 1 & bb$carrier == "DL"), ])
dim(bb[which(bb$day == 1, bb$carrier == "DL"), ])
```



## Subsetting {.tabset .tabset-fade}

\textcolor{pink}{$[ \ \ ]$} , \textcolor{pink}{$[[ \ \ ]]$} , \textcolor{pink}{$\$$}

- Extract or Replace Parts of an Object
  - Operators acting on vectors, matrices, arrays and lists to extract or replace parts. 
  - The most important distinction between "[ ]", "[[ ]]" and "$" is that the "[ ]" can select more than one element whereas the other two select a single element. 
  - "$" does not allow computed indices, whereas "[[ ]]" does.
  - Subsetting (except by an empty index) will drop all attributes except names, dim and dimnames. Indexing will keep them.
  
```{conjecture 'Comparison-possible'}
\textcolor{brown}{Error in day == 1 : comparison (1) is possible only for atomic and list types}
```

- It occurs when the data is not available i.e. column name is NOT found
- It might happen when the original code assumed that the dataframe is attached
- Either attach the dataframe (NOT Recommended) or use "$" to access column names

\textcolor{pink}{dplyr::select()}

- It can use Range "\textcolor{pink}{:}", Not "\textcolor{pink}{!}", And "\textcolor{pink}{&}", Or "\textcolor{pink}{|}" 
- Selection Helpers
  - everything(): Matches all variables.
  - last_col(): Select last variable, possibly with an offset.
- These helpers select variables by matching patterns in their names:
  - starts_with(): Starts with a prefix.
  - ends_with(): Ends with a suffix.
  - contains(): Contains a literal string.
  - matches(): Matches a regular expression.
  - num_range(): Matches a numerical range like x01, x02, x03.
- These helpers select variables from a character vector:
  - all_of(): Matches variable names in a character vector. All names must be present, otherwise an out-of-bounds error is thrown.
  - any_of(): Same as all_of(), except that no error is thrown for names that do not exist.
- This helper selects variables with a function:
  - where(): Applies a function to all variables and selects those for which the function returns TRUE.


### Cols {.unnumbered}

```{r 'B11-SubsetCol'}
dim(bb)
#
# #Subset Consecutive Columns using Colon
stopifnot(identical(bb[ , 2:5], bb[ , -c(1, 6:ncol(bb))]))
#
# #dplyr::select()
bb_14 <- select(bb, year:day, arr_delay, dep_delay, distance, air_time)
bb_15 <- bb %>% select(year:day, arr_delay, dep_delay, distance, air_time)
stopifnot(identical(bb_14, bb_15))
```


## Grouped Summary {.tabset .tabset-fade}

- \textcolor{pink}{dplyr::summarise()} or dplyr::summarize()
- \textcolor{pink}{dplyr::group_by()}
  - It converts an existing Tibble into a grouped Tibble where operations are performed "by group". 
  - \textcolor{pink}{ungroup()} removes grouping.
  - \textcolor{pink}{n()} gives the number of observations in the current group.

### Summarise {.unnumbered}

```{r 'B11-Summarise'}
bb <- xxflights
# #dplyr::summarise() & dplyr::summarize() are same
# #Get the mean of a column with NA excluded
#
summarize(bb, delay_mean = mean(dep_delay, na.rm = TRUE))
#
# #base::summary()
summary(bb$dep_delay)
#
# #Grouped Summary
by_ymd <- group_by(bb, year, month, day)
mysum <- summarize(by_ymd, 
                   dep_delay_mean = mean(dep_delay, na.rm = TRUE), 
                   arr_delay_mean = mean(arr_delay, na.rm = TRUE),
                   .groups = "keep")
# #Equivalent 
bb %>% 
  group_by(year, month, day) %>% 
  summarize(dep_delay_mean = mean(dep_delay, na.rm = TRUE), 
			arr_delay_mean = mean(arr_delay, na.rm = TRUE),
			.groups= "keep")
```


### group_by() {.unnumbered}

```{r 'B11-SummaryDistance'}
# #Get delay grouped by distance 'Distance between airports, in miles.'
summary(bb$distance)
#
# #How many unique values are present in this numeric data i.e. factors
str(as.factor(bb$distance))
str(sort(unique(bb$distance)))
bb %>% 
  group_by(distance) %>% 
  summarize(count = n(),
            dep_delay_mean = mean(dep_delay, na.rm = TRUE), 
            arr_delay_mean = mean(arr_delay, na.rm = TRUE),
            .groups= "keep")
#
# #For distance =17, there is only 1 flight and that too has NA, so the mean is NaN
bb[bb$distance == 17, ]
#
# #In general, Flight to any destination (ex: ABQ) has travelled same distance (1826)
unique(bb %>% filter(dest == "ABQ") %>% select(distance))
#
# #Mean Delays for Destinations with more than 1000 miles distance
bb %>% 
  group_by(dest) %>% 
  filter(distance > 1000) %>% 
  summarize(count = n(), 
            distance_mean = mean(distance, na.rm = TRUE),
            dep_delay_mean = mean(dep_delay, na.rm = TRUE), 
            arr_delay_mean = mean(arr_delay, na.rm = TRUE))
```


## Mutate {.tabset .tabset-fade}

- \textcolor{pink}{dplyr::mutate()}
  - Newly created variables are available immediately
  - New variables overwrite existing variables of the same name. 
  - Variables can be removed by setting their value to NULL.
  - mutate() adds new variables and preserves existing ones
    - mutate() can also keep or drop column according to the .keep argument.
  - transmute() adds new variables and drops existing ones. 

```{conjecture 'UseMethod-No-applicable-method'}
\textcolor{brown}{Error in UseMethod("select") : no applicable method for 'select' applied to an object of class "function"}
```

- Run 'str(MyObject)' to check if 'MyObject' exists, looks as expected and R is not finding something else. 
- Most probably R reserved keyword 'data' was called in place of the actual 'data'.
- To minimise this type of Error, do not use the keywords which match with Base R Functions e.g. 'data' (Function in utils) or 'df' (function in stats)

```{conjecture 'Object-Not-Found-02'}
\textcolor{brown}{Error: Problem with mutate() column ... column object 'arr\_delay' not found}
```

- Run 'str(MyObject)' to check if the column exists in the dataset 
- \textcolor{orange}{Caution:} if the dataset was attached earlier, then R will NOT throw this error. However, later when the code is being executed in a clean environment, it will fail. To avoid this, it is recommended to use proper scope resolution and to avoid attaching the dataset (if possible)

```{r 'B11-Mutate'}
dim(bb)
#
bb_16 <- select(bb, year:day, arr_delay, dep_delay, distance, air_time)
bb_17 <- mutate(bb_16,
       gain = arr_delay - dep_delay,
       speed = distance / air_time * 60,
       hours = air_time / 60,
       gain_per_hour = gain / hours)
# #Equivalent
bb %>% 
  select(year:day, arr_delay, dep_delay, distance, air_time) %>% 
  mutate(gain = arr_delay - dep_delay,
         speed = distance / air_time * 60,
         hours = air_time / 60,
         gain_per_hour = gain / hours)
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B11-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, bb_1, bb_10, bb_11, bb_12, bb_13, bb_14, bb_15, bb_16, bb_17, bb_2, bb_3, bb_4, bb_5, bb_6, bb_7, bb_8, bb_9, bb_na, by_ymd, mysum, x, xxflights, y, ff)
```

```{r 'B11-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/211-Manipulate.Rmd-->

# Statistics (B12, Sep-26) {#b12}

```{r 'B12', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Introduction to Statistics"
  - [Effect of Sample Size and Repeat Sampling](#sample-sampling-b12 "b12")
  - [Normal Distribution](#normal-d-b12 "b12")
  - [Type I and Type II Errors (B12)](#errors-ab-b12, "b12")

- Links (Ref)
  - [Population vs Sample](#population-sample-c01 "c01")

```{r 'B12-Flights', include=FALSE}
# #Load Data: Flights
xxflights <- f_getRDS(xxflights)
bb <- xxflights
```

## Definitions

```{r 'B12D01', comment="", echo=FALSE, results='asis'}
f_getDef("Population")
```

```{r 'B12D02', comment="", echo=FALSE, results='asis'}
f_getDef("Census")
```

```{r 'B12D03', comment="", echo=FALSE, results='asis'}
f_getDef("Sample")
```

```{r 'B12D04', comment="", echo=FALSE, results='asis'}
f_getDef("Random-Sample")
```

## Inferential Statistics

```{r 'B12D05', comment="", echo=FALSE, results='asis'}
f_getDef("Statistical-Inference")
```

Inferential statistics are used for Hypothesis Testing. Refer [Statistical Inference](#stat-inference-c01 "c01")


## Hypothesis Testing

Refer [Hypothesis Testing](#hypothesis-c09 "c09")

```{r 'B12D06', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-Testing")
```

```{r 'B12D07', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-Null")
```

```{r 'B12D17', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-Alternative")
```

Refer [Steps of Hypothesis Testing](#one-tail-c09 "c09")

1. State the NULL Hypothesis ${H_0}$
    - The null will always be in the form of decisions regarding the population, not the sample. 
      - If we have population data, we can do the census and then there is no requirement of any hypothesis or estimation.
    - The Null Hypothesis will always be written as the absence of some parameter or process characteristic
      - The test is designed to assess the strength of the evidence against the null hypothesis. 
      - Often the null hypothesis is a statement of "no difference."
    - Equality part of expression always appears in ${H_0}$ i.e. it can be \textcolor{pink}{$>=$} , \textcolor{pink}{$<=$} , \textcolor{pink}{$==$} 
    - The term 'null' is used because this hypothesis assumes that there is no difference between the two means or that the recorded difference is not significant.
1. An Alternative Hypothesis ${H_a}$, is then stated which will be the complement of the Null Hypothesis.
    - ${H_a}$ cannot have equality part of expression i.e. it can be \textcolor{pink}{$<$} , \textcolor{pink}{$>$} , \textcolor{pink}{$!=$} 
    - The claim about the population that evidence is being sought for is the alternative hypothesis
      - However, to prove it is true, its complement (null hypothesis) is tried to be proven false. Because it is easier to prove something false.
1. For Hypothesis tests involving a population mean, let ${\mu}_0$ denote the hypothesized value 

```{r 'B12D08', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-1T-Lower-Tail")
```

```{r 'B12D09', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-1T-Upper-Tail")
```

```{r 'B12D10', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-2T-Two-Tail")
```


- Sample data is used to determine whether or not you can be statistically confident that you can reject or fail to reject the ${H_0}$. 
    - If the ${H_0}$ is \textcolor{pink}{rejected}, the statistical conclusion is that the ${H_a}$ is \textcolor{pink}{TRUE}.
- Notes:
    - Sometimes it is easier to formulate the alternative hypothesis (the conclusion that you hope to support) and create NULL hypothesis based on that. 
    - Ex: If we are testing for validity of the claim that number of defects are less than 2%
      - ${H_a} : {\mu} < 2\% \iff {H_0} : {\mu} \geq 2\%$
      - If the ${H_0}$ is rejected, then the statistical conclusion is that the ${H_a}$ is TRUE i.e. defects are less than 2% in the population
      - If the ${H_0}$ is not rejected, then no conclusion can be formed about the ${H_a}$.

\textcolor{pink}{Question:} Is there an ideal sample size

  - NO
  - ("ForLater") However, there exists a relationship between (I guess) alpha, beta and sample size n. (I could not find the link on later search.) 
  - (Paraphrasing and only memory based so can be worng!) Basically, for a given analysis, if we want to keep both types of errors to a managable level, we can calculate minimum number of samples that would help us in determining the outcome at a certain minimum confidence level etc.

## Point Estimation

```{r 'B12D11', comment="", echo=FALSE, results='asis'}
f_getDef("Point-Estimation")
```

```{r 'B12D12', comment="", echo=FALSE, results='asis'}
f_getDef("Point-Estimator")
```

```{r 'B12D13', comment="", echo=FALSE, results='asis'}
f_getDef("Point-Estimate")
```

Example: ${\overline{x}}$ is an estimator (of populataion parameter 'mean' ${\mu}$). Its estimate is 3 and this calculation process is an estimation.

## Standard Deviation

```{r 'B12D44', comment="", echo=FALSE, results='asis'}
f_getDef("Mean")
```

Refer [Standard Deviation](#sd-c03 "c03") and equation \@ref(eq:sd)

```{r 'B12D14', comment="", echo=FALSE, results='asis'}
f_getDef("Standard-Deviation")
```

\begin{equation} 
  \begin{align} 
     \sigma &= \sqrt{\frac{1}{N} \sum_{i=1}^N \left(x_i - \mu\right)^2} \\
    {s} &= \sqrt{\frac{1}{N-1} \sum_{i=1}^N \left(x_i - \overline{x}\right)^2}
  \end{align}
\end{equation} 

A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. 

## Variance

Refer [Variance](#variance-c03 "c03") and equation \@ref(eq:variance)

```{r 'B12D15', comment="", echo=FALSE, results='asis'}
f_getDef("Variance")
```

\begin{equation} 
  \begin{align} 
    \sigma^2 &= \frac{1}{n} \sum _{i=1}^{n} \left(x_i - \mu \right)^2 \\
    s^2 &= \frac{1}{n-1} \sum _{i=1}^{n} \left(x_i - \overline{x} \right)^2
  \end{align}
\end{equation} 

Variability is most commonly measured with the Range, IQR, SD, and Variance.

## Standard Error or Sampling Fluctuation

The sample we draw from the population is only one from a large number of potential samples. 

- If ten researchers were all studying the same population, drawing their own samples then they may obtain different answers i.e. each of the ten researchers may come up with a different mean 
- Thus, the statistic in question (mean) varies for sample to sample. It has a distribution called a sampling distribution. 
- We can use this distribution to understand the uncertainty in our estimate of the population parameter.

Refer [Standard Error](#standard-error-c07 "c07")

```{r 'B12D16', comment="", echo=FALSE, results='asis'}
f_getDef("Standard-Error")
```

```{r 'B12D20', comment="", echo=FALSE, results='asis'}
f_getDef("Sampling-Error")
```

Sampling fluctuation (Standard Error) refers to the extent to which a statistic (mean, median, mode, sd etc.) takes on different values with different samples i.e. it refers to how much the value of the statistic fluctuates from sample to sample. 

```{r 'B12D43', comment="", echo=FALSE, results='asis'}
f_getDef("Sampling-Distribution")
```

Standard Deviation of ${\overline{x}}$, \textcolor{pink}{$\sigma_{\overline{x}}$} is given by equation \@ref(eq:sigma-x-bar) i.e. $\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}}$

- Generally, the standard error is unknown.
- Higher the standard error, higher the deviation from sample to sample i.e. lower the reliability.

## Test Statistic

Refer [Test Statistic ](##test-stat-c09 "c09")

```{r 'B12D22', comment="", echo=FALSE, results='asis'}
f_getDef("Test-Statistic")
```

```{r 'B12D45', comment="", echo=FALSE, results='asis'}
f_getDef("1s-known-sd") #dddd
```

## Calculate SD & SE {#stat-height-b12 .tabset .tabset-fade}

Standard Error (SE) is same as 'the standard deviation of the sampling distribution'.
The 'variance of the sampling distribution' is the Variance of the data divided by N.

### Calculate Statistics {.unnumbered}

```{r 'B12-ExHeight'}
# #DataSet: Height of 5 people in 'cm'
hh <- c(170.5, 161, 160, 170, 150.5)
#
# #N by length()
print(hh_len <- length(hh))
#
# #Mean by mean()
hh_mean <- mean(hh)
cat("Mean = ", hh_mean)
#
# #Variance by var()
hh_var <- round(var(hh), 3)
cat("Variance = ", hh_var)
#
# #Standard Deviation (SD) by sd()
hh_sd <- round(sd(hh), 3)
cat("Standard Deviation (SD) = ", hh_sd)
#
# #Standard Error (SE) 
hh_se_sd <- round(hh_sd / sqrt(hh_len), 3)
cat("Standard Error (SE) = ", hh_se_sd)
```

### R Functions {.unnumbered}

```{r 'B12-CalHeight'}
# #DataSet: Height of 5 people in 'cm'
print(hh)
#
# #N by length()
print(hh_len <- length(hh))
#
# #sum by sum()
print(hh_sum <- sum(hh))
#
# #Mean by mean()
hh_mean <- mean(hh)
hh_mean_cal <- hh_sum / hh_len
stopifnot(identical(hh_mean, hh_mean_cal))
cat("Mean = ", hh_mean)
#
# #Calculate the deviation from the mean by subtracting each value from the mean
print(hh_dev <- hh - hh_mean)
#
# #Square the deviation
print(hh_sqdev <- hh_dev^2)
#
# #Get Sum of the squared deviations
print(hh_sqdev_sum <- sum(hh_sqdev))
#
# #Divide it by the 'sample size (N) - 1' for the Variance or use var()
hh_var <- round(var(hh), 3)
hh_var_cal <- hh_sqdev_sum / (hh_len -1)
stopifnot(identical(hh_var, hh_var_cal))
cat("Variance = ", hh_var)
#
# #Variance of the sampling distribution 
hh_var_sample <- hh_var / hh_len
cat("Variance of the Sampling Distribution = ", hh_var)
#
# #Take square root of the Variance for the Standard Deviation (SD) or use sd()
hh_sd_cal <- round(sqrt(hh_var), 3)
hh_sd <- sd(hh)
stopifnot(identical(round(hh_sd, 3), hh_sd_cal))
cat("Standard Deviation (SD) = ", hh_sd)
#
# #Standard Error (SE)
# #SE
# #Divide the SD by the square root of the sample size for the Standard Error (SE)
# #
hh_se_sd <- round(hh_sd / sqrt(hh_len), 3)
#
# #Calculate SE from Variance 
hh_se_var <- round(sqrt(hh_var_sample), 3)
stopifnot(identical(hh_se_sd, hh_se_var))
cat("Standard Error (SE) = ", hh_se_sd)
```

## Histogram and Density  {.tabset .tabset-fade}

Using Dataset Flights : "air_time" -Amount of time spent in the air, in minutes. Refer figure \@ref(fig:B12P0102)

### Graphs {.unnumbered}

```{r 'B12-Histogram-Eval', echo=FALSE, eval=TRUE, ref.label=c('B12-Histogram')}
#
```

```{r 'B12P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B12P01", "-Flights-Hist-Air", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12P01, device = "png", dpi = 144) 
}
```

```{r 'B12P01', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B12P01", "-Flights-Hist-Air", ".png"))
```

```{r 'B12-Density-Eval', echo=FALSE, ref.label=c('B12-Density')}
#
```

```{r 'B12P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B12P02", "-Flights-Dens-Air", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12P02, device = "png", dpi = 144) 
}
```

```{r 'B12P02', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B12P02", "-Flights-Dens-Air", ".png"))
```

```{r 'B12P0102', echo=FALSE, ref.label=c('B12P01', 'B12P02'), fig.cap="(B12P01 B12P02) Flights: Air Time (min) excluding NA (Histogram and Density)"}
#
```

### NA {.unnumbered}

```{r 'B12-FlightsRemoveNA'}
# #Remove All NA
aa <- na.omit(xxflights$air_time)
attr(aa, "na.action") <- NULL
str(aa)
summary(aa)
```

### Stats {.unnumbered}

```{r 'B12-FlightStats'}
# #Overview of Data after removal of NA
bb <- aa
stopifnot(is.null(dim(bb)))
summary(bb)
# #min(), max(), range(), summary()
min_bb <- summary(bb)[1]
max_bb <- summary(bb)[6]
range_bb <- max_bb - min_bb
cat(paste0("Range = ", range_bb, " (", min_bb, ", ", max_bb, ")\n"))
# #IQR(), summary()
iqr_bb <- IQR(bb)
cat(paste0("IQR = ", iqr_bb, " (", summary(bb)[2], ", ", summary(bb)[5], ")\n"))
# #median(), mean(), summary()[3], summary()[4] 
median_bb <- median(bb)
cat("Median =", median_bb, "\n")
mu_mean_bb <- mean(bb)
cat("Mean \u03bc =", mu_mean_bb, "\n")
#
sigma_sd_bb <- sd(bb)
cat("SD (sigma) \u03c3 =", sigma_sd_bb, "\n")
#
variance_bb <- var(bb)
cat(sprintf('Variance (sigma)%s %s%s =', '\u00b2', '\u03c3', '\u00b2'), variance_bb, "\n")
```

### Historgram {.unnumbered}

```{r 'B12-Histogram', eval=FALSE}
# #Histogram
bb <- na.omit(xxflights$air_time)
hh <- tibble(ee = bb)
# #Basics
median_hh <- round(median(hh[[1]]), 1)
mean_hh <- round(mean(hh[[1]]), 1)
sd_hh <- round(sd(hh[[1]]), 1)
len_hh <- nrow(hh)
#
B12P01 <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) + 
  geom_histogram(bins = 50, alpha = 0.4, fill = '#FDE725FF') + 
  geom_vline(aes(xintercept = mean_hh), color = '#440154FF') +
  geom_text(data = tibble(x = mean_hh, y = -Inf, 
                          label = paste0("Mean= ", mean_hh)), 
            aes(x = x, y = y, label = label), 
            color = '#440154FF', hjust = -0.5, vjust = 1.3, angle = 90) +
  geom_vline(aes(xintercept = median_hh), color = '#3B528BFF') +
  geom_text(data = tibble(x = median_hh, y = -Inf, 
                          label = paste0("Median= ", median_hh)), 
            aes(x = x, y = y, label = label), 
            color = '#3B528BFF', hjust = -0.5, vjust = -0.7, angle = 90) +
  theme(plot.title.position = "panel") + 
  labs(x = "x", y = "Frequency", 
       subtitle = paste0("(N=", len_hh, "; ", "Mean= ", mean_hh, 
                         "; Median= ", median_hh, "; SD= ", sd_hh,
                         ")"), 
        caption = "B12P01", title = "Flights: Air Time")
}
```

### Density {.unnumbered}

```{r 'B12-Density', eval=FALSE}
# #Density Curve
# #Get Quantiles and Ranges of mean +/- sigma 
q05_hh <- quantile(hh[[1]], .05)
q95_hh <- quantile(hh[[1]], .95)
density_hh <- density(hh[[1]])
density_hh_tbl <- tibble(x = density_hh$x, y = density_hh$y)
sig3r_hh <- density_hh_tbl %>% filter(x >= {mean_hh + 3 * sd_hh})
sig3l_hh <- density_hh_tbl %>% filter(x <= {mean_hh - 3 * sd_hh})
sig2r_hh <- density_hh_tbl %>% filter(x >= {mean_hh + 2 * sd_hh}, {x < mean_hh + 3 * sd_hh})
sig2l_hh <- density_hh_tbl %>% filter(x <= {mean_hh - 2 * sd_hh}, {x > mean_hh - 3 * sd_hh})
sig1r_hh <- density_hh_tbl %>% filter(x >= {mean_hh + sd_hh}, {x < mean_hh + 2 * sd_hh})
sig1l_hh <- density_hh_tbl %>% filter(x <= {mean_hh - sd_hh}, {x > mean_hh - 2 * sd_hh})
sig0r_hh <- density_hh_tbl %>% filter(x > mean_hh, {x < mean_hh + 1 * sd_hh})
sig0l_hh <- density_hh_tbl %>% filter(x < mean_hh, {x > mean_hh - 1 * sd_hh})
#
# #Change x-Axis Ticks interval
xbreaks_hh <- seq(-3, 3)
xpoints_hh <- mean_hh + xbreaks_hh * sd_hh
#
# # Latex Labels 
xlabels_hh <- c(TeX(r'($\,\,\mu - 3 \sigma$)'), TeX(r'($\,\,\mu - 2 \sigma$)'), 
                TeX(r'($\,\,\mu - 1 \sigma$)'), TeX(r'($\mu$)'), TeX(r'($\,\,\mu + 1 \sigma$)'), 
                TeX(r'($\,\,\mu + 2 \sigma$)'), TeX(r'($\,\,\mu + 3\sigma$)'))
#
B12P02 <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) + 
  geom_density(alpha = 0.2, colour = "#21908CFF") + 
  geom_area(data = sig3l_hh, aes(x = x, y = y), fill = '#440154FF') + 
  geom_area(data = sig3r_hh, aes(x = x, y = y), fill = '#440154FF') + 
  geom_area(data = sig2l_hh, aes(x = x, y = y), fill = '#3B528BFF') + 
  geom_area(data = sig2r_hh, aes(x = x, y = y), fill = '#3B528BFF') + 
  geom_area(data = sig1l_hh, aes(x = x, y = y), fill = '#21908CFF') + 
  geom_area(data = sig1r_hh, aes(x = x, y = y), fill = '#21908CFF') + 
  geom_area(data = sig0l_hh, aes(x = x, y = y), fill = '#5DC863FF') + 
  geom_area(data = sig0r_hh, aes(x = x, y = y), fill = '#5DC863FF') + 
  #scale_y_continuous(limits = c(0, 0.009), breaks = seq(0, 0.009, 0.003)) +
  scale_x_continuous(breaks = xpoints_hh, labels = xlabels_hh) + 
  ggplot2::annotate("segment", x = xpoints_hh[4] - 0.5 * sd_hh, xend = xpoints_hh[2], y = 0.007, 
                    yend = 0.007, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) + 
  ggplot2::annotate("segment", x = xpoints_hh[4] + 0.5 * sd_hh, xend = xpoints_hh[6], y = 0.007, 
                    yend = 0.007, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) + 
  ggplot2::annotate(geom = "text", x = xpoints_hh[4], y = 0.007, label = "95.4%") + 
  theme(plot.title.position = "panel") + 
  labs(x = "x", y = "Density", 
       subtitle = paste0("(N=", nrow(.), "; ", "Mean= ", round(mean(.[[1]]), 1), 
                         "; Median= ", round(median(.[[1]]), 1), "; SD= ", round(sd(.[[1]]), 1),
                         ")"), 
        caption = "B12P02", title = "Flights: Air Time")
}
```

### Aside {.unlisted .unnumbered}

- This section is NOT useful for general reader and can be safely ignored. It contains my notes related to building this book. These are useful only for someone who is building his own book. (Shivam)
- Side by Side Images need a Caption in Final Chunk
- LaTex Inside Tex() will not be able to execute braces as usual, avoid them or escape them

## Effect of Sample Size and Repeat Sampling {#sample-sampling-b12}

Using Dataset Flights : "air_time" -Amount of time spent in the air, in minutes. 

A.  Effect of increasing sample size (N =100, 1000, 10000), Refer figure \@ref(fig:B12P030405G)
    - the precision and confidence in the estimate increases and uncertainty decreases
    - the distribution of sample means become thinner. i.e. the sample standard deviation decreases
A.  Effect of increasing the Sampling, Refer figure \@ref(fig:B12P060708G)
    - The mean of the distribution of sample means equals the mean of the parent distribution. 
    - Refer [Standard Error](#standard-error-c07 "c07")

\textcolor{orange}{Caution:} Trend here does not match with the theory. However, the exercise shows the 'How to do it' part. It can be repeated with better data, larger sample size, or repeat sampling.

### Sample Size {.tabset .tabset-fade}

```{r 'B12-Sample', include=FALSE}
bb <- na.omit(xxflights$air_time)
# #Fix Seed
set.seed(3)
# #Set Sample Size
#nn <- 100L
# #Take a sample from dataset
xb100 <- sample(bb, size = 100L)
xb1000 <- sample(bb, size = 1000L)
xb10000 <- sample(bb, size = 10000L)
# #Population Mean
mu_hh <- round(mean(bb), 1)
```

```{r 'B12-Sample100', include=FALSE, eval=FALSE}
# #Histogram: N = 100
hh <- tibble(ee = xb100)
ylim_hh <- 12.5
cap_hh <- "B12P03"
```

```{r 'B12-Sample-100-A', ref.label=c('B12-Sample100', 'B12-SampleHist'), include=FALSE}
#
```

```{r 'B12P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B12P03", "-Flights-Hist-Air-100", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12P03, device = "png", dpi = 144) 
}
```

```{r 'B12P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B12P03", "-Flights-Hist-Air-100", ".png"))
```

```{r 'B12-Sample1000', include=FALSE, eval=FALSE}
# #Histogram: N = 1000
hh <- tibble(ee = xb1000)
ylim_hh <- 125
cap_hh <- "B12P04"
```

```{r 'B12-Sample1000-A', ref.label=c('B12-Sample1000', 'B12-SampleHist'), include=FALSE}
#
```

```{r 'B12P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B12P04", "-Flights-Hist-Air-1000", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12P04, device = "png", dpi = 144) 
}
```

```{r 'B12P04', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B12P04", "-Flights-Hist-Air-1000", ".png"))
```

```{r 'B12-Sample10000', include=FALSE, eval=FALSE}
# #Histogram: N = 10000
hh <- tibble(ee = xb10000)
ylim_hh <- 1250
cap_hh <- "B12P05"
```

```{r 'B12-Sample10000-A', ref.label=c('B12-Sample10000', 'B12-SampleHist'), include=FALSE}
#
```

```{r 'B12P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B12P05", "-Flights-Hist-Air-10000", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12P05, device = "png", dpi = 144) 
}
```

```{r 'B12P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B12P05", "-Flights-Hist-Air-10000", ".png"))
```

#### GIF {.unnumbered}

```{r 'B12-GrobSample', include=FALSE}
# #Get Grob
B12A_P03 <- ggplot_gtable(ggplot_build(B12P03))
B12A_P04 <- ggplot_gtable(ggplot_build(B12P04))
B12A_P05 <- ggplot_gtable(ggplot_build(B12P05))
#
# #Get Max Width of All the Plots
maxWidth = grid::unit.pmax(B12A_P03$widths[2:5], B12A_P04$widths[2:5], B12A_P05$widths[2:5])
# #Update Width
B12A_P03$widths[2:5] <- as.list(maxWidth)
B12A_P04$widths[2:5] <- as.list(maxWidth)
B12A_P05$widths[2:5] <- as.list(maxWidth)
```

```{r 'B12-GrobA-Save', include=FALSE}
# Save Grobs
loc_png <- paste0(.z$PX, "B12A_P03", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12A_P03, device = "png", dpi = 144) 
}
loc_png <- paste0(.z$PX, "B12A_P04", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12A_P04, device = "png", dpi = 144) 
}
loc_png <- paste0(.z$PX, "B12A_P05", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12A_P05, device = "png", dpi = 144) 
}
```

```{r 'B12-GifSampleSize', include=FALSE}
loc_gif <- paste0(.z$PX, "B12A", ".gif")
if(!file.exists(loc_gif)){
png_files <- list.files(.z$PX, pattern = "B12A.*.*png$", full.names = TRUE)
gifski(png_files, gif_file = loc_gif, delay = 2)
}
```

```{r 'B12P030405G', echo=FALSE, fig.cap="(B12P03 B12P04 B12P05) Effect of Increasing Sample Size"}
knitr::include_graphics(paste0(.z$PX, "B12A", ".gif"))
```

#### Images {.unnumbered}

```{r 'B12P030405', echo=FALSE, out.width='33%', ref.label=c('B12P03', 'B12P04', 'B12P05'), fig.cap="(B12P03 B12P04 B12P05) Effect of Increasing Sample Size"}
#
```

#### Code {.unnumbered}

```{r 'B12-Sample-A', ref.label=c('B12-Sample', 'B12-Sample100'), eval=FALSE}
#
```

```{r 'B12-SampleHist', eval=FALSE}
# #Assumes 'hh' has data in 'ee'. In: mu_hh, cap_hh, ylim_hh
#
B12 <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) + 
  geom_histogram(bins = 50, alpha = 0.4, fill = '#FDE725FF') + 
  geom_vline(aes(xintercept = mean(.data[["ee"]])), color = '#440154FF') +
  geom_text(aes(label = TeX(r'($\bar{x}$)', output = "character"), 
                x = mean(.data[["ee"]]), y = -Inf), 
            color = '#440154FF', hjust = 2, vjust = -2.5, parse = TRUE, check_overlap = TRUE) + 
  geom_vline(aes(xintercept = mu_hh), color = '#3B528BFF') +
  geom_text(aes(label = TeX(r'($\mu$)', output = "character"), x = mu_hh, y = -Inf),
            color = '#3B528BFF', hjust = -1, vjust = -2, parse = TRUE, check_overlap = TRUE) + 
  coord_cartesian(xlim = c(0, 800), ylim = c(0, ylim_hh)) + 
  theme(plot.title.position = "panel") + 
  labs(x = "x", y = "Frequency", 
       subtitle = paste0("(Mean= ", round(mean(.[[1]]), 1), 
                         "; SD= ", round(sd(.[[1]]), 1),
                         #"; Var= ", round(var(.[[1]]), 1),
                         "; SE= ", round(sd(.[[1]]) / sqrt(nrow(.)), 1),
                         ")"), 
      caption = cap_hh, title = paste0("Sample Size = ", nrow(.)))
}
assign(cap_hh, B12)
rm(B12)
```

#### Warnings {.unlisted .unnumbered}

- "In mean.default(gg) : argument is not numeric or logical: returning NA"
  - For ggplot() - This comes up if the object 'gg' is NULL. Check if the ggplot is looking into global scope in place of local dataframe that was passed.

#### Deprecated {.unlisted .unnumbered}

```{r 'B12-ToGGpassOneVal', include=FALSE, eval=FALSE}
# To Pass Average to the ggplot in Row 1 Column 2 
dd <- data.frame(gg = xb100, xxstore = c(mu_mean_bb, rep(NA, length(xb100) -1)))
# #Which can be later accessed 
geom_text(aes(label = paste0("mu=", round(xxstore[1], 1)), x = xxstore[1], y = -Inf), 
	  color = 'blue', hjust = -0.2, vjust = -0.5)
```

### Repeat Sampling  {.tabset .tabset-fade}

```{r 'B12-RepeatSampling', include=FALSE}
bb <- na.omit(xxflights$air_time)
# #Fix Seed
set.seed(3)
# #Set Sample Size
nn <- 10L
# #Set Repeat Sampling Rate
rr <- 20L
# #Take Sample of N = 10, get mean, repeat i.e. get distribution of mean
xr20 <- replicate(rr, mean(sample(bb, size = nn)))
rr <- 200L
xr200 <- replicate(rr, mean(sample(bb, size = nn)))
rr <- 2000L
xr2000 <- replicate(rr, mean(sample(bb, size = nn)))
#
# #Population Mean
mu_hh <- round(mean(bb), 1)
```

```{r 'B12-Sampling20', include=FALSE, eval=FALSE}
# #Histogram: N = 10, Repeat = 20
hh <- tibble(ee = xr20)
ylim_hh <- 2
cap_hh <- "B12P06"
```

```{r 'B12-Sampling20-A', ref.label=c('B12-Sampling20', 'B12-RepeatSampligHist'), include=FALSE}
#
```

```{r 'B12P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B12P06", "-N10-Rep20", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12P06, device = "png", dpi = 144) 
}
```

```{r 'B12P06', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B12P06", "-N10-Rep20", ".png"))
```

```{r 'B12-Sampling200', include=FALSE, eval=FALSE}
# #Histogram: N = 10, Repeat = 200
hh <- tibble(ee = xr200)
ylim_hh <- 20
cap_hh <- "B12P07"
```

```{r 'B12-Sampling200-A', ref.label=c('B12-Sampling200', 'B12-RepeatSampligHist'), include=FALSE}
#
```

```{r 'B12P07-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B12P07", "-N10-Rep200", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12P07, device = "png", dpi = 144) 
}
```

```{r 'B12P07', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B12P07", "-N10-Rep200", ".png"))
```

```{r 'B12-Sampling2000', include=FALSE, eval=FALSE}
# #Histogram: N = 10, Repeat = 2000
hh <- tibble(ee = xr2000)
ylim_hh <- 200
cap_hh <- "B12P08"
```

```{r 'B12-Sampling2000-A', ref.label=c('B12-Sampling2000', 'B12-RepeatSampligHist'), include=FALSE}
#
```

```{r 'B12P08-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B12P08", "-N10-Rep2000", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12P08, device = "png", dpi = 144) 
}
```

```{r 'B12P08', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B12P08", "-N10-Rep2000", ".png"))
```

#### GIF {.unnumbered}

```{r 'B12-GrobRepeatSample', include=FALSE}
# #Get Grob
B12B_P06 <- ggplot_gtable(ggplot_build(B12P06))
B12B_P07 <- ggplot_gtable(ggplot_build(B12P07))
B12B_P08 <- ggplot_gtable(ggplot_build(B12P08))
#
# #Get Max Width of All the Plots
maxWidth = grid::unit.pmax(B12B_P06$widths[2:5], B12B_P07$widths[2:5], B12B_P08$widths[2:5])
# #Update Width
B12B_P06$widths[2:5] <- as.list(maxWidth)
B12B_P07$widths[2:5] <- as.list(maxWidth)
B12B_P08$widths[2:5] <- as.list(maxWidth)
```

```{r 'B12-GrobB-Save', include=FALSE}
# Save Grobs
loc_png <- paste0(.z$PX, "B12B_P06", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12B_P06, device = "png", dpi = 144) 
}
loc_png <- paste0(.z$PX, "B12B_P07", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12B_P07, device = "png", dpi = 144) 
}
loc_png <- paste0(.z$PX, "B12B_P08", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B12B_P08, device = "png", dpi = 144) 
}
```

```{r 'B12-GifSampleRepeat', include=FALSE}
loc_gif <- paste0(.z$PX, "B12B", ".gif")
if(!file.exists(loc_gif)){
png_files <- list.files(.z$PX, pattern = "B12B.*.*png$", full.names = TRUE)
gifski(png_files, gif_file = loc_gif, delay = 2)
}
```

```{r 'B12P060708G', echo=FALSE, fig.cap="(B12P06 B12P07 B12P08) Effect of Increasing Sample Size"}
knitr::include_graphics(paste0(.z$PX, "B12B", ".gif"))
```

#### Images {.unnumbered}

```{r 'B12P060708', echo=FALSE, out.width='33%', ref.label=c('B12P06', 'B12P07', 'B12P08'), fig.cap="(B12P06 B12P07 B12P08) Effect of Increasing Sampling"}
#
```

#### Code {.unnumbered}

```{r 'B12-RepeatSampling-A', ref.label=c('B12-RepeatSampling', 'B12-Sampling20'), eval=FALSE}
#
```

```{r 'B12-RepeatSampligHist', eval=FALSE}
# #Assumes 'hh' has data in 'ee'. In: mu_hh, cap_hh, ylim_hh, nn
#
B12 <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) + 
  geom_histogram(bins = 50, alpha = 0.4, fill = '#FDE725FF') + 
  geom_vline(aes(xintercept = mean(.data[["ee"]])), color = '#440154FF') +
  geom_text(aes(label = TeX(r'($E(\bar{x})$)', output = "character"), 
                x = mean(.data[["ee"]]), y = -Inf), 
            color = '#440154FF', hjust = 1.5, vjust = -1.5, parse = TRUE, check_overlap = TRUE) + 
  geom_vline(aes(xintercept = mu_hh), color = '#3B528BFF') +
  geom_text(aes(label = TeX(r'($\mu$)', output = "character"), x = mu_hh, y = -Inf),
            color = '#3B528BFF', hjust = -1, vjust = -2, parse = TRUE, check_overlap = TRUE) + 
  coord_cartesian(xlim = c(0, 800), ylim = c(0, ylim_hh)) + 
  theme(plot.title.position = "panel") + 
  labs(x = TeX(r'($\bar{x} \, (\neq x)$)'), y = TeX(r'(Frequency of $\, \bar{x}$)'), 
       subtitle = TeX(sprintf(
         "($\\mu$=%.0f) $E(\\bar{x}) \\, =$%.0f $\\sigma_{\\bar{x}} \\, =$%.0f",
                             mu_hh, round(mean(.[[1]]), 1), round(sd(.[[1]])))),
       caption = cap_hh, 
       title = paste0("Sampling Distribution (N = ", nn, ") & Repeat Sampling = ", nrow(.)))
}
assign(cap_hh, B12)
rm(B12)
```

## Normal Distribution {#normal-d-b12}

```{r 'B12P09', echo=FALSE, fig.cap="(B12P09) Normal Distribution"}
knitr::include_graphics(paste0(.z$PX, "B12P09-Distribution-Normal.jpg"))
```

Refer [Normal Distribution](#normal-d-c06 "c06") and equation \@ref(eq:distribution-normal)

```{r 'B12D18', comment="", echo=FALSE, results='asis'}
f_getDef("Normal-Distribution")
```

Their importance is partly due to the [Central Limit Theorem](#clt-c07 "c07"). Assumption of normal distribution allow us application of [Parametric Methods](#parametric-c18 "c18")

```{r 'B12D23', comment="", echo=FALSE, results='asis'}
f_getDef("Parametric-Methods")
```

```{r 'B12D19', comment="", echo=FALSE, results='asis'}
f_getDef("Central-Limit-Theorem")
```

It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable—whose distribution converges to a normal distribution as the number of samples increases. 

Parametric statistical tests typically assume that samples come from normally distributed populations, but the central limit theorem means that this assumption is not necessary to meet when you have a large enough sample. A sample size of 30 or more is generally considered large.

This is the basis of [Empirical Rule](#empirical-be03 "be03"). 

```{r 'B12D21', comment="", echo=FALSE, results='asis'}
f_getDef("Empirical-Rule")
```

\textcolor{orange}{Caution:} If data from small samples do not closely follow this pattern, then other distributions like the t-distribution may be more appropriate.

## Standard Normal Distribution

Refer [Standard Normal](#standard-normal-c06 "c06") and equation \@ref(eq:normal-z)

```{r 'B12D24', comment="", echo=FALSE, results='asis'}
f_getDef("Standard-Normal")
```

The simplest case of a normal distribution is known as the standard normal distribution. 
Given the Population with normal distribution ${\mathcal{N}}_{(\mu, \, \sigma)}$ 

If $\overline{X}$ is the mean of a sample of size ${n}$ from this population, then the standard error is $\sigma/{\sqrt{n}}$ and thus the z-score is \textcolor{pink}{$Z=\frac {\overline{X} - \mu}{\sigma/{\sqrt{n}}}$}

The z-score is the test statistic used in a z-test. The z-test is used to compare the means of two groups, or to compare the mean of a group to a set value. Its null hypothesis typically assumes no difference between groups.

The area under the curve to the right of a z-score is the p-value, and it is the likelihood of your observation occurring if the null hypothesis is true.

Usually, a p-value of 0.05 or less means that your results are unlikely to have arisen by chance; it indicates a statistically significant effect.

## Outliers {#outliers-b12}

Refer [Outliers: C03](#outliers-c03 "c03")

```{r 'B12D25', comment="", echo=FALSE, results='asis'}
f_getDef("Outliers")
```

- \textcolor{pink}{Question:} If we include a datapoint which is 4 standard deviations away, would we be able to get the Normal Distribution
  - Shape of the curve will be tilted, thus it will be difficult to keep the datapoint and satify the condition for normality
  - Generally, only ${{\mu} - 3{\sigma} \leq {x} \leq {\mu} + 3{\sigma}}$ values are kept and the remaining are treated as outliers
- \textcolor{pink}{Question:} Is is a bad data if it is 4 standard deviations away
  - It means that if we keep the data point, there is a high possibility that we will violate the normality assumption. If we violate the assuption, parametric methods cannot be applied to the dataset
  - In general, convert to z-value, remove those which have z-value higher than +3 or lower than -3
- \textcolor{pink}{Question:} But, how many removals are too many removals
  - There are techniques for this consideration, will be covered later. "ForLater"
  - (Aside) In a sample of 1000 observations, the presence of up to five observations deviating from the mean by more than three times the standard deviation is within the range of what can be expected. If the sample size is only 100, however, just three such outliers are already reason for concern. 
- \textcolor{pink}{Concern:} Frequency or Proportion of outliers should not be very high 
  - It is true that we cannot have normal distribution in that case.
    - However, can we afford to remove all the data points with $z > 3$, this needs to be answered in the context of analysis.
      - Here the 'assignable cause' is applied. i.e. each datapoint that is proposed to be an outlier is individually analysed and either kept or removed
- \textcolor{pink}{Concern:} Sometimes the outliers are present because the dataset is a mixture of two distributions
  - In that case, those should be treated separately
- \textcolor{pink}{Question:} Are there tools for all of this ~~jugglery~~
  - Yes, there are, specially nonparametric methods does not take any assumption about distribution.
  - However, these are not as robust as parametric tests, so if possible, stay with parametric tests

## Type I and Type II Errors {#errors-ab-b12}

```{r 'B12P10', echo=FALSE, ref.label=c('C09P01'), fig.cap="(C09P01) Type-I $(\\alpha)$ and Type-II $(\\beta)$ Errors"}
# #Ref another file chunk
```

Example

- Type-I "An innocent person is convicted"
- Type-II "A guilty person is not convicted"

Since we are using sample data to make inferences about the population, it is possible that we will make an error. In the case of the Null Hypothesis, we can make one of two errors.

Refer [Type I and Type II Errors](#errors-ab-c09, "c09")

```{r 'B12D26', comment="", echo=FALSE, results='asis'}
f_getDef("Error-Type-I")
```

```{r 'B12D27', comment="", echo=FALSE, results='asis'}
f_getDef("Error-Type-II") 
```

```{r 'B12D28', comment="", echo=FALSE, results='asis'}
f_getDef("Level-of-Significance")
```

```{r 'B12D29', comment="", echo=FALSE, results='asis'}
f_getDef("Confidence-Coefficient") 
```

```{r 'B12D30', comment="", echo=FALSE, results='asis'}
f_getDef("Power")
```

There is always a tradeoff between Type-I and Type-II errors.

- Generally max 5% ${\alpha}$ and max 20% ${\beta}$ errors are recommended

In practice, the person responsible for the hypothesis test specifies the level of significance. By selecting ${\alpha}$, that person is controlling the probability of making a Type I error. 

- If the cost of making a Type I error is high, small values of ${\alpha}$ are preferred. Ex: $\alpha =0.01$ 
- If the cost of making a Type I error is not too high, larger values of ${\alpha}$ are typically used. Ex: $\alpha = 0.05$ 

```{r 'B12D31', comment="", echo=FALSE, results='asis'}
f_getDef("Significance-Tests")
```

Although most applications of hypothesis testing control for the probability of making a Type I error, they do not always control for the probability of making a Type II error. Because of the uncertainty associated with making a Type II error when conducting significance tests, statisticians usually recommend that we use the statement \textcolor{pink}{"do not reject ${H_0}$"} instead of "accept ${H_0}$."


## Critical Value

```{r 'B12P11', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B12P11-Tail-Left.jpg"))
```

```{r 'B12P12', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B12P12-Tail-Right.jpg"))
```

```{r 'B12P1112', echo=FALSE, ref.label=c('B12P11', 'B12P12'), fig.cap="(B12P11 B12P12) Left Tail vs. Right Tail"}
#
```

```{r 'B12P13', echo=FALSE, fig.cap="(B12P13) Two Tail"}
knitr::include_graphics(paste0(.z$PX, "B12P13-Tail-Two.jpg")) #iiii
```

```{r 'B12D32', comment="", echo=FALSE, results='asis'}
f_getDef("Critical-Value")
```

```{r 'B12-GetZ'}
# #Critical Value (z) for Common Significance level Alpha (α) or Confidence level (1-α)
xxalpha <- c("10%" = 0.1, "5%" = 0.05, "5/2%" = 0.025, "1%" = 0.01, "1/2%" = 0.005)
#
# #Left Tail Test
round(qnorm(p = xxalpha, lower.tail = TRUE), 4)
#
# #Right Tail Test
round(qnorm(p = xxalpha, lower.tail = FALSE), 4)
```

```{r 'B12D33', comment="", echo=FALSE, results='asis'}
f_getDef("p-value")
```

```{r 'B12D34', comment="", echo=FALSE, results='asis'}
f_getDef("Acceptance-Region")
```

```{r 'B12D35', comment="", echo=FALSE, results='asis'}
f_getDef("Rejection-Region")
```

## Tailed Tests

```{r 'B12D39', comment="", echo=FALSE, results='asis'}
f_getDef("Tailed-Test")
```

One tailed-tests are concerned with one side of a statistic. Whereas, Two-tailed tests deal with both tails of the distribution.

Two-tail test is done when you do not know about direction, so you test for both sides.


```{r 'B12D36', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-1T-Lower-Tail")
```

```{r 'B12D37', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-1T-Upper-Tail")
```

```{r 'B12D38', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-2T-Two-Tail") 
```

## Approaches 

```{r 'B12D40', comment="", echo=FALSE, results='asis'}
f_getDef("Approach-p-value")
```

Steps for the p-value approach or test statistic approach

- Calculate $z$ for given $x$: $z = \frac{\overline{x} - \mu_0}{s}$
- Refer [Get P(z) by pnorm() or z by qnorm()](#get-pz-c06 "c06"), to get the p-value from z-table
  - $P_{\left(\overline{x}\right)} = P_{\left(z\right)}$
- Compare p-value with Level of significance ${\alpha}$


```{r 'B12D41', comment="", echo=FALSE, results='asis'}
f_getDef("Approach-Critical-Value")
```

Steps for the critical value approach

- Calculate $z$ for given $x$: $z = \frac{\overline{x} - \mu_0}{s}$
- Using the z-table, find the z for given Level of significance ${\alpha} = 0.01$ 
  - $P_{\left(z\right)} = 0.01$ for $z_{\alpha = 0.01} = −2.33$ 
  - Refer [Get P(z) by pnorm() or z by qnorm()](#get-pz-c06 "c06")
- Compare test statistic with z-value i.e. $(z)$ vs. $(z_{\alpha = 0.01})$


## z-test vs. t-test

If the population standard error (SE) is known, apply z-test. If it is unknown, apply t-test. t-test will converge to z-test with increasing sample size.

\textcolor{pink}{Question:} Does the probability from t-table differ from the probability value from z-table

- No, practically for sample size greater than 30, there is no difference

It is assumed that $(\overline{x} - \mu)$ follows Normality. However the Standard Error (SE) does not follow normality, generally it follows chi-sq distribution. Thus, $(\overline{x} - \mu)/SE$ becomes 'Normal/ChiSq' and this ratio follows the t-distribution. Thus, the test we apply is called t-test.

```{r 'B12-GetT'}
# #For Degrees of Freedom = 10 (N=11)
# #Critical Value (z) for Common Significance level Alpha (α) or Confidence level (1-α)
xxalpha <- c("10%" = 0.1, "5%" = 0.05, "5/2%" = 0.025, "1%" = 0.01, "1/2%" = 0.005)
dof <- 10L
#
# #Left Tail Test
round(qt(p = xxalpha, df = dof, lower.tail = TRUE), 4)
#
# #Right Tail Test
round(qt(p = xxalpha, df = dof, lower.tail = FALSE), 4)
```

## t-test

### Degrees of Freedom

```{r 'B12D42', comment="", echo=FALSE, results='asis'}
f_getDef("Degrees-of-Freedom") 
```

Why $(n-1)$ are the degrees of freedom

- Degrees of freedom refer to the number of independent pieces of information that go into the computation. i.e. $\{(x_{1}-\overline{x}), (x_{2}-\overline{x}), \ldots, (x_{n}-\overline{x})\}$
- However, $\sum (x_{i}-\overline{x}) = 0$ for any data set. 
- Thus, only $(n − 1)$ of the $(x_{i}-\overline{x})$ values are independent.
  - if we know $(n − 1)$ of the values, the remaining value can be determined exactly by using the condition.

\textcolor{pink}{Question:} Is there any minimum sample size we must consider before calculating degrees of freedom

- Larger sample sizes are needed if the distribution of the population is highly skewed or includes outliers.

\textcolor{pink}{Guess:} Degrees of freedom is also calculated to remove the possible bias

- No


### How to use t-table

- Rows have degrees of freedom, Columns have ${\alpha}$ values, get the t-statistic at their intersection
  - For DOF = 10, and ${\alpha} = 0.05$, t-table has value 1.812 (Critical Limit)
  - In right tail test, if the test-statistic is greater than critical limit, we can reject the null


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B12-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ee, hh, ii, jj, kk, ll, mm, nn, oo, rr, vv, xx, yy, zz, 
          B12P01, B12P02, density_hh, density_hh_tbl, hh_dev, hh_len, hh_mean, hh_mean_cal, hh_sd,
          hh_sd_cal, hh_se_sd, hh_se_var, hh_sqdev, hh_sqdev_sum, hh_sum, hh_var, hh_var_cal,
          hh_var_sample, iqr_bb, len_hh, max_bb, mean_hh, median_bb, median_hh, min_bb, 
          mu_mean_bb, q05_hh, q95_hh, range_bb, sd_hh, sig0l_hh, sig0r_hh, sig1l_hh, sig1r_hh, 
          sig2l_hh, sig2r_hh, sig3l_hh, sig3r_hh, sigma_sd_bb, variance_bb, xbreaks_hh, 
          xlabels_hh, xpoints_hh, xxflights, B12A_P03, B12A_P04, B12A_P05, B12B_P06, B12B_P07, 
          B12B_P08, B12P03, B12P04, B12P05, B12P06, B12P07, B12P08, cap_hh, dof, maxWidth, 
          mu_hh, png_files, xb100, xb1000, xb10000, xr20, xr200, xr2000, xxalpha, ylim_hh, loc_gif,
          loc_png)
```

```{r 'B12-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/212-Statistics.Rmd-->

# Statistics (B13, Oct-03) {#b13}

```{r 'B13', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Introduction to Statistics"


```{r 'B13-Flights', include=FALSE}
# #Load Data: Flights
xxflights <- f_getRDS(xxflights)
bb <- xxflights
```

## Definitions

```{r 'B13P01', echo=FALSE, ref.label=c('C09P01'), fig.cap="(C09P01) Type-I $(\\alpha)$ and Type-II $(\\beta)$ Errors"}
# #Ref another file chunk #iiii
```

Refer [Type I and Type II Errors (B12)](#errors-ab-b12, "b12") & [Type I and Type II Errors](#errors-ab-c09, "c09")

```{r 'B13D01', comment="", echo=FALSE, results='asis'}
f_getDef("Error-Type-I")
```

```{r 'B13D02', comment="", echo=FALSE, results='asis'}
f_getDef("Error-Type-II") 
```

```{r 'B13D03', comment="", echo=FALSE, results='asis'}
f_getDef("Level-of-Significance")
```

```{r 'B13D04', comment="", echo=FALSE, results='asis'}
f_getDef("Confidence-Coefficient") 
```

```{r 'B13D05', comment="", echo=FALSE, results='asis'}
f_getDef("Power")
```

```{r 'B13D06', comment="", echo=FALSE, results='asis'}
f_getDef("Significance-Tests")
```

```{r 'B13D07', comment="", echo=FALSE, results='asis'}
f_getDef("Approach-p-value-Steps") 
```

```{r 'B13D08', comment="", echo=FALSE, results='asis'}
f_getDef("Standard-Error")
```


## Approaches {.tabset .tabset-fade}

Population Size = 100, ${\alpha} = 0.05$

Hypothesis: $\text{\{Right Tail or Upper Tail\} } {H_0} : {\mu} \leq 22 \iff {H_a}: {\mu} > 22$

Sample: n=4, dof = 3, ${\overline{x}} = 23$

Sample: n=10, dof = 9, ${\overline{x}} = 23$


We know if we take another sample, we will have a different sample mean. So, we need to confirm whether the above calculated sample mean ${\overline{x}} = 23$ represent the population mean ${\mu}$ i.e. Can we reject or fail to reject ${H_0}$ based on this sample!

3 Approaches for Hypothesis Testing - 

1.  Test Statistic Approach
    - Fictitious values: Standard Error (SE) = 0.22, so $t = \frac{23 - 22}{0.22} = 4.545$
    - For (DOF = 3): $P_{(t)} = {\alpha} = 0.05$, at ${}^{3}t_{\alpha} = 2.353$
    - For (DOF = 9): $P_{(t)} = {\alpha} = 0.05$, at ${}^{9}t_{\alpha} = 1.833$
    - For both the cases, ${t}$ is greater than ${}^{dof}t_{\alpha}$
    - Hence null is rejected, the 'test is statistically significant'
1.  p-value approach 
    - Fictitious values: Standard Error (SE) = 0.22, so $t = \frac{23 - 22}{0.22} = 4.545$
    - Get ${}^3\!P_{(t = 4.545)} = 0.00997$
    - Get ${}^9\!P_{(t = 4.545)} = 0.000697$
    - For both the cases, $P_{(t)}$ is lower than ${\alpha}$
    - Hence null is rejected, the 'test is statistically significant'

1.  Confidence Interval Approach


If the population standard error (SE) is known, apply z-test. If it is unknown, apply t-test. t-test will converge to z-test with increasing sample size.

2-T Rule of Thumb - Skipped "09:55"


```{r 'B13-GetPz'}
# #Get P(z)
z01 <- round(pnorm(3.44), digits = 6)
z02 <- 1 - round(pnorm(3.44), digits = 6)
z03 <- round(pnorm(3.44, lower.tail = FALSE), digits = 6)
z04 <- format(pnorm(4.55, lower.tail = FALSE), digits = 3, scientific = FALSE)
z05 <- format(pnorm(1.22, lower.tail = FALSE), digits = 5)
z06 <- format(pnorm(1.99, lower.tail = FALSE), digits = 5)
z07 <- format(pnorm(1.99, lower.tail = TRUE), digits = 5)
```

### Examples {.unlisted .unnumbered}

Example: 

1. \textcolor{pink}{Question:} If we get a z-value of 3.44 (Right Tail), What is the Probability $P_{(z)}$
    - For z = 3.44 & Left Tail, p-value = `r z01` (by \textcolor{pink}{`pnorm(z)`})
    - For z = 3.44 & Right Tail, p-value = `r z02` (by \textcolor{pink}{`1 - pnorm(z)`})
    - For z = 3.44 & Right Tail, p-value = `r z03` (by \textcolor{pink}{`pnorm(z, lower.tail = FALSE)`})
1. \textcolor{pink}{Question:} If we get a z-value of 4.55 (Right Tail), What is the Probability $P_{(z)}$
    - For z = 4.55 & Right Tail, p-value = `r z04` 
1. \textcolor{pink}{Question:} If we get a z-value of 1.22 (Right Tail), would we reject the null at ${\alpha} = 0.05$
    - For z = 1.22 & Right Tail, p-value = `r z05` 
    - Because $P_{(z)}$ is greater than the ${\alpha}$, we fail to reject the null, the 'test is statistically NOT significant'
1.  \textcolor{pink}{Question:} If we get a z-value of 1.99 (Right Tail), would we reject the null at ${\alpha} = 0.05$
    - For z = 1.99 & Right Tail, p-value = `r z06` ( = 1 - `r z07`)
    - Because $P_{(z)}$ is lower than the ${\alpha}$, null is rejected, the 'test is statistically significant'    

### Code {.unlisted .unnumbered}

```{r 'B13-GetPz-A', ref.label=c('B13-GetPz'), eval=FALSE}
#
```

## Flowchart

- Tests |
  - Test of Means |
    - One Sample |
      - z-test (Population Standard Deviation ${\sigma}$, is known)
      - t-test (Population Standard Deviation ${\sigma}$, is unknown)
    - Two Sample | 
      - Independent Sample | [Mean - Two Sample - Independent](#mean-2s-sd "c10")
      - Dependent Sample (or Repeated) | [Mean - Two Sample - Paired](#mean-paired-c10 "c10")
    - More than Two Samples
-

## Two Sample t-Test

```{r 'B13D11', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Lower")
```

```{r 'B13D12', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Upper")
```

```{r 'B13D13', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Two") #dddd
```

Example: 

```{r 'B13D09', comment="", echo=FALSE, results='asis'}
f_getDef("Independent-Sample-Design-Example")
```

```{r 'B13D10', comment="", echo=FALSE, results='asis'}
f_getDef("Matched-Sample-Design-Example")
```

Test Statistic for Independent Sample t-Test Statistic is given by \@ref(eq:t-2s-nsd) as shown below

\begin{equation}
  t = \frac{({\overline{x}}_1 - {\overline{x}}_2) - {D}_0}{{\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)}} = \frac{({\overline{x}}_1 - {\overline{x}}_2) - {D}_0}{\sqrt{\frac{{s}_1^2}{{n}_1} + \frac{{s}_2^2}{{n}_2}}}
\end{equation}

The t-test is any statistical hypothesis test in which the test statistic follows a Student t-distribution under the null hypothesis.

A t-test is the most commonly applied when the test statistic would follow a normal distribution if the value of a scaling term in the test statistic were known. 


Example: If we want to evaluate effect of a Training Program.

We can take two samples of 50 people each. First Set "Untrained" would be from the set of people who did not receive training. Second Set "Trained" would be from the set of people who have undergone the training. Comparison of these two sample mean performances would be done by "independent sample" t-test.

Or

We can take a sample of 50 "Untrained" people. Get their mean performance. Provide the training of these 50 people. Then again get their mean performance. Now, we have "paired" samples of performances of same people. One set has their performance before the training and another is after the training. Comparison of these two sample mean performances would be done by "paired sample" t-test.

Paired samples t-tests typically consist of a sample of matched pairs of similar units, or one group of units that has been tested twice (a "repeated measures" t-test). 

The matched sample design is generally preferred to the independent sample design because \textcolor{pink}{the matched-sample procedure often improves the precision of the estimate.}

- \textcolor{pink}{Question:} How do you conclude that the training was effective. Or even that the training didnot make the situation worse somehow. Assume 25 people performed better and 25 people performed worse (somehow!)
  - (Prof) In this situation we cannot claim that the training is effective.
    - We start with the null hypothesis that the two sample means are same (Two Tail Test). If we are able to reject this. Then we can perform Upper or Lower Tail Test and again try to find a significant result.
  - If we cannot reject the null hypothesis, then we conclude that "we cannot claim that the training is effective".


- Example: If we want to comment on performance of new employees compared to ideal value of 95. Take a sample of 20 people and get their perfomance. 
  - We would need to perform One Sample t-test
- Example: If we want to comment on performance of Engineers and Non-engineers
  - Two Sample Independent t-test
- Example: We want to check whether we have recruited more number of females compared to males. 
  - Two Sample Proportion Test
- Example: If we want to comment on their induction training program by conducting a test before and after the program
  - Two Sample Paired t-test

## More than Two Samples

Assume there are 3 samples A, B, C. We can do $C_2^3 = 3$ number of tests i.e. $\{(A, B), (B, C), (C, A)\}$. However, assuming ${\alpha} = 0.05 \iff {\gamma} = 0.95$ for each test, the confidence for 3 consecutive tests become ${\gamma}^3 = 0.857 \iff {\alpha} = 0.143$, which is a very high and unacceptable value. To avoid this, we use ANOVA as a single test.

High value of F-test would indicate that the populations are different.


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B13-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ee, hh, ii, jj, kk, ll, mm, nn, oo, rr, vv, xx, yy, zz, z01, z02, z03, z04, z05, 
          z06, z07, xxflights)
```

```{r 'B13-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/213-Statistics.Rmd-->

# Statistics (B14, Oct-10) {#b14}

```{r 'B14', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Inferential Statistics: Hypothesis Testing"
  - [Equality in Hypothesis](#equality-b14 "b14")

## Equality in Hypothesis  {#equality-b14 .unlisted .unnumbered}

- The equality part of the expression $\{\mu \geq \mu_0 \, | \, \mu \leq \mu_0 \, | \, \mu = \mu_0\}$ \textcolor{pink}{always} appears in the null hypothesis ${H_0}$. 
  - We try to reject null, so that we can confidently accept the alternate. If the alternate is ambiguous e.g. "is greater than or equal to" then we will not be able to conclude with confidence.
- Alternative hypothesis is often what the test is attempting to establish.
  - Hence, asking whether the user is looking for evidence to support $\{\mu < \mu_0 \, | \, \mu > \mu_0 \, | \, \mu \neq \mu_0\}$ will help determine ${H_a}$


```{r 'B14D02', comment="", echo=FALSE, results='asis'}
f_getDef("1s-known-sd")
```

```{r 'B14D03', comment="", echo=FALSE, results='asis'}
f_getDef("1s-unknown-sd")
```


## Example: WSES: Preprocessing

\textcolor{pink}{Please import the WSES data in xxWSES object. Due to copyright, it has not been shared.}

```{r 'B14-xxWSES', include=FALSE, eval=FALSE}
# #Load Data: WSES
xxWSES <- f_getObject("xxWSES", "B13-WSES.csv", "772c81252c54a3162f1f8dbd0ceb9aeb")
if(FALSE) {
  loc_src <- paste0(.z$XL, "B13-WSES.xlsx")
  aa <- read_excel(path = loc_src, sheet = excel_sheets(path = loc_src)[2])
  str(aa)
}
```

- Assuming: Average Sales 8-million $({\mu}_0 = 8)$, Standard Deviation 2-million $({\sigma} = 2)$
- Hypothesis test to check whether the average sales value in the population is at least 8-million

```{r 'B14D07', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-1T-Upper-Tail")
```

\textcolor{orange}{Caution:} While importing data, for Mac users, probably it will be easier to import CSV file. However, I am not a Mac user so cannot comment on this.

### Data {.unlisted .unnumbered}

```{r 'B14-Data'}
# #Import the Data and assign to a temporary variable for ease of use
xxWSES <- f_getRDS(xxWSES)
bb <- xxWSES
str(bb)
```

### Rename Headers {.unlisted .unnumbered}

```{r 'B14-Rename'}
# #List Column Headers
names(bb)
#
# #Rename Headers
bb_headers <- c("SN" , "RS" , "SO" , "PDT" , "INT" , "RG" , "RS1" , "PM" , "SVM" , "PP" , "JB" , "LCC")
names(bb) <- bb_headers
#
# #Verification
names(bb)
#
```

### Conversion to Factor {.unlisted .unnumbered .tabset .tabset-fade}

- From the case study, it can be seen that multiple columns are categorical (factor) or ordinal (ordered factor)

- \textcolor{pink}{Question:} What is the importance of having this kind of order factor over simple factor
  - Here order is also important. Also in future, ordered factors will be needed for some analysis
  - (Aside) Refer [Scales of Measurement](#scales-c01 "c01")
    - Simple factor (nominal) can provide only Mode whereas Ordered factor (ordinal) can provide Median also. Rank based statistical models can be applied on the ordinal data.
- \textcolor{pink}{Question:} If 'RS' is already integer 0 & 1, then why convert it to factor
  - While the data shows them as 0 & 1 but actually they are NOT integers
  - If we show male and female as 0 & 1, these are still categorical
- \textcolor{pink}{Question:} Why LCC is NOT ordinal (Aside)
  - Unknown "ForLater"

#### Data {.unlisted .unnumbered}

```{r 'B14-Factors'}
# #"Reporting Status i.e. RS" Converting "character" to "factor" and Label them
bb$RS <- factor(bb$RS, levels = c("Lost", "Won"), labels = c("0", "1"))
#
# #"Sales Outcome i.e. SO" Converting "numeric" to "factor" 
bb$SO <- factor(bb$SO)
#
# #"Product Vertical i.e. PDT" Ordinal
# #What are the unique values in this column
unique(bb$PDT)
#
# #Converting "character" to "Ordered factor"
# #Note: If level order is not provided, by default, alphabatical ordering will be assigned.
levels(factor(bb$PDT, ordered = TRUE))
#
# #Provide ordering of factor levels in Ascending Order.
bb$PDT <- factor(bb$PDT, ordered = TRUE, 
      levels = c("GTMSys", "Procsys", "LearnSys", "Finsys", "Lifesys", "Logissys", "ContactSys"))
#
# #"Industry i.e. INT" Ordinal
bb$INT <- factor(bb$INT, ordered = TRUE, 
      levels = c("Capital Markets", "Banks", "Defense", "Consumer goods", "Others", "Security", 
        "Energy", "Insurance", "Airline", "Finance", "Infrastructure", "Mobility", "Other Govt.", 
        "Govt.", "Telecom equipments", "Health", "Clinical research", "Agriculture"))
#
# #"Region i.e. RG" Ordinal
bb$RG <- factor(bb$RG, ordered = TRUE, levels = c("UK", "Other Europe", "Americas", "Africa",
                                                "India", "Japan", "Singapore", "Spain", "Canada"))
#
# #"Leads Conversion Class i.e. LCC" Ordinal, However we are going with Nominal here.
bb$LCC <- factor(bb$LCC, levels = c("E", "V", "F", "L"), labels = c(1, 2, 3, 4))
```

#### Structure after Conversion {.unlisted .unnumbered}

```{r 'B14-SaveData', include=FALSE, eval=FALSE}
xyWSES <- bb
f_setRDS(xyWSES)
```

```{r 'B14-WorkingData'}
str(bb)
```

#### factor() {.unlisted .unnumbered}

- \textcolor{pink}{factor()}
  - If level order is not provided, by default, alphabatical ordering will be assigned.
  - levels are the input, labels are the output in the factor() function. 
  - A factor has only a level attribute, which is set by the labels argument in the factor() function.
  - different levels are coded as ("E", "V", "F", "L")
    - for which you want the levels to be labeled as c(1, 2, 3, 4).
    - The factor function will look for the values ("E", "V", "F", "L") convert them to numerical factor classes and add the label values to the 'level' attribute of the factor. 
    - This attribute is used to convert the internal numerical values to the correct labels. 
    - Hoever, there is no 'label' attribute. 

### Conversion to Numeric 

```{r 'B14-ChangeType', eval=FALSE}
# #If there are "character" columns which should be "numeric"
bb$RS1 <- as.numeric(bb$RS1)
bb$PP <- as.numeric(bb$PP)
bb$JB <- as.numeric(bb$JB)
# #Equivalent
bb <- bb %>% mutate(across(c(RS1, PP, JB), as.numeric))
```

## WSES: Analysis

### Q1

Assume average sales of 8-million dollars and population standard deviation to be 2-million dollars. Check whether the average sales value in the population is at least 8 million dollars.

```{r 'B14D01', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-1T-Upper-Tail")
```

- Mean of "Sales Value in Million i.e. SVM"
  - ${\overline{x}} = 8.0442$
    - `mean(bb$SVM)` \textcolor{pink}{$\#\mathcal{R}$}
- $n = 1000, {\mu} = 8, {\sigma} = 2$
- $z = \frac{\overline{x} - {\mu}_0}{{\sigma}/\sqrt{n}} = 0.6988$
  - `{8.0442 - 8} / {2 / sqrt(1000)}` \textcolor{pink}{$\#\mathcal{R}$}
  - `{mean(bb$SVM) - 8} / {2 / sqrt(1000)}` \textcolor{pink}{$\#\mathcal{R}$}
- ${}^U\!P_{z = 0.6988} = 0.2423$
  - `pnorm(q = 0.6988, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
  - `1 - pnorm(q = 0.6988, lower.tail = TRUE)` \textcolor{pink}{$\#\mathcal{R}$} 
  - `1 - pnorm(q = 0.6988)` \textcolor{pink}{$\#\mathcal{R}$} 
  - `pnorm(q = -0.6988)` \textcolor{pink}{$\#\mathcal{R}$} 
  - \textcolor{orange}{Caution:} By default, `pnorm()` provides the probability to the left of z-value
- Compare with ${\alpha} = 0.05$
  - ${}^U\!P_{(z)} > {\alpha} \to {H_0}$ cannot be rejected 
  - The sample results do not provide sufficient evidence to conclude that the average sales is 'at least' 8-million 
  
### Q2

If the population standard deviation is unknown, evaluate the same hypothesis again.

- $t = \frac{\overline{x} - {\mu}_0}{{s}/\sqrt{n}} = 0.7049$
  - `{mean(bb$SVM) - 8} / {sd(bb$SVM) / sqrt(1000)}` \textcolor{pink}{$\#\mathcal{R}$}
- ${}^U\!P_{t = 0.7049} = 0.2405$
  - `pt(q = 0.7049, df = nrow(bb) - 1, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- \textcolor{pink}{t.test()}
  - `t.test(bb$SVM, mu = 8, alternative = "greater")` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^U\!P_{(t)} > {\alpha} \to {H_0}$ cannot be rejected 
  - The sample results do not provide sufficient evidence to conclude that the average sales is 'at least' 8-million (same as earlier)
- \textcolor{pink}{Question:} "alternative hypothesis: true mean is greater than 8" What is the meaning of this line in the output of t.test
  - We provided this information.
  - (Aside) This line is providing the Alternate Hypothesis for reference. ${\mu}_0 = 8$ and we are performing an upper tail test thus the Alternate Hypothesis is "true mean is greater than 8"


### Q3 {.tabset .tabset-fade}

Check whether the proportion of leads won by WSES is more than 50%.

```{r 'B14D04', comment="", echo=FALSE, results='asis'}
f_getDef("H-1s-p-Upper")
```

- Count of Success $({x})$ is Winning leads in the "Sales Outcome i.e. SO"
- ${p}_0 = 0.50$
  - \@ref(eq:se-1s-p) ${\sigma}_{\overline{p}} = \sqrt{\frac{{p}_0 (1 - {p}_0)}{n}} = 0.0158$
    - `sqrt(0.50 * {1 - 0.50} / 1000)` \textcolor{pink}{$\#\mathcal{R}$}
- $\{n = 1000, x = 481\} \to {\overline{p}} = {n}/{x} = 0.481$ 
- \@ref(eq:z-1s-p) $z = \frac{{\overline{p}} - {p}_0}{{\sigma}_{\overline{p}}} = -1.2016$
  - `{0.481 - 0.50}/{sqrt(0.50 * {1 - 0.50} / 1000)}` \textcolor{pink}{$\#\mathcal{R}$} 
- ${}^U\!P_{z = -1.2016} = 0.8852$
  - `pnorm(q = -1.2016, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
  - `1 - pnorm(q = -1.2016)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^U\!P_{(z)} > {\alpha} \to {H_0}$ cannot be rejected 
  - The sample results do not provide sufficient evidence to conclude that the company wins 'at least' 50% leads

#### Code {.unlisted .unnumbered}

```{r 'B14-Proportions'}
# #Proportions
bb %>% group_by(SO) %>% summarise(PCT = n() / nrow(.))
#
pnorm(q = -1.2016, lower.tail = FALSE)
```

#### Percentage {.unlisted .unnumbered}

```{r 'B14-GroupPercentage'}
# #Grouped Percentages
# #table() gives a count
table(bb$SO)
#
# #prop.table() can work only with numbers so it needs table()
prop.table(table(bb$SO))
#
# #Similar
bb %>% group_by(SO) %>% summarise(N = n()) %>% mutate(PCT = N / sum(N))
bb %>% group_by(SO) %>% summarise(PCT = n() / nrow(.))
```

### Q4 {.tabset .tabset-fade}

Check whether the probability of winning a sales lead for the product "learnsys" is more than that of "Finsys". 

```{r 'B14D05', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-p-Upper") 
```

- Count of Success $({x})$ is Winning leads in the "Sales Outcome i.e. SO"
- (1: learnsys) $\{{n}_1 = 55 + 71 = 126, {x}_1 = 71\} \to {\overline{p}}_1 = {n}_1/{x}_1 = 0.563$
- (2: Finsys) $\{{n}_2 = 83 + 34 = 117, {x}_2 = 34\} \to {\overline{p}}_2 = {n}_2/{x}_2 = 0.29$
- ${}^U\!P_{z} < {\alpha} \to {H_0}$ is rejected i.e. the proportions are different
  - We can conclude that the "learnsys" performs better than "Finsys"


- \textcolor{pink}{Question:} When we have filtered out the data, why the table shows 0 values
  - We can filter them out, however these are in memory
  - (Aside) Factor levels are not dropped, by default, when you filter them. Use factor() again to drop the unused levels. 

#### Test {.unlisted .unnumbered}

```{r 'B14-TestProportion'}
# #Data | Subset | Filter | Update Factor levels
ii <- bb %>% select(PDT, SO, SVM) %>% 
  filter(PDT %in% c("LearnSys", "Finsys")) %>% mutate(across(PDT, factor))
str(ii)
#
# #Count
table(ii$PDT, ii$SO)
#
# #Proportion Table
round(prop.table(table(ii$PDT, ii$SO), margin = 1), 3)
#
prop.test(x = c(71, 34), n = c(126, 117), alternative = "greater")
```

#### prop.table() {.unlisted .unnumbered}

```{r 'B14-MoreProportion'}
ii <- bb %>% select(PDT, SO, SVM) %>% 
  filter(PDT %in% c("LearnSys", "Finsys")) %>% mutate(across(PDT, factor))
str(ii)
#
# #Proportion Table: margin gives the margin to split by i.e. 
# #1 means rowwise sum, 2 means columnwise
round(prop.table(table(ii$PDT, ii$SO), margin = 1), 3)
round(prop.table(table(ii$PDT, ii$SO), margin = 2), 3)
#
# #Similar
ii %>% select(PDT, SO) %>% 
  count(PDT, SO) %>% pivot_wider(names_from = SO, values_from = n) %>% 
  mutate(SUM = rowSums(across(where(is.numeric)))) %>% 
  mutate(across(where(is.numeric), ~ round(. * 100 /SUM, 1))) 
```


### Q5

Check whether the average sales value of "learnsys" projects is higher than that of "Finsys" projects. (${\alpha} = 0.05$)

```{r 'B14D06', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Upper")
```

- (Pooled) t-test for difference of means can be applied only if the variances are same.
  - Location of two distributions can be compared only when their spread is similar.
  - (Aside) if the variances are not same, then Welch Test is applied in place of Pooled Test.
- ${}^U\!P_{t} > {\alpha} \to {H_0}$ cannot be rejected 
  - The sample results do not provide sufficient evidence to conclude that the "learnsys" has higher average sales than "Finsys"

- \textcolor{pink}{Question:} What is 'SVM ~ PDT'
  - It is the formula notation for t-test (in R) in the form of 'Continuous ~ Categorical'

"ForLater"

- \textcolor{pink}{Question:} How to decide whether to keep the Lost bids or exclude them
  - Including lost bids: $\text{(DOF)} = 241, t = 0.93503, P_{(t)} = 0.1754 \to {H_0}$ cannot be rejected
  - Excluding lost bids: $\text{(DOF)} = 103, t = 0.62152, P_{(t)} = 0.2678 \to {H_0}$ cannot be rejected
  - Thus, no change in Hypothesis results but which which process should be utilised 
  - Similar, question can be raised for Q6 i.e. should be consider profit of those bids which we lost

```{r 'B14-Q5t2s'}
str(ii)
ii_var <- var.test(SVM ~ PDT, data = ii)
if(ii_var$p.value > 0.05) {
  cat(paste0("Variances are same. Pooled Test can be applied. \n"))
  t.test(formula = SVM ~ PDT, data = ii, alternative = "greater", var.equal = TRUE)
} else {
  cat(paste0("Variances are NOT same. Welch Test should be applied. \n"))
  t.test(formula = SVM ~ PDT, data = ii, alternative = "greater", var.equal = FALSE)
}
```

```{r 'B14-NoLost'}
jj <- ii %>% filter(SO == "1")
jj_var <- var.test(SVM ~ PDT, data = jj)
if(jj_var$p.value > 0.05) {
  t.test(formula = SVM ~ PDT, data = jj, alternative = "greater", var.equal = TRUE)
} else {
  cat(paste0("Problem: Difference of means can be tested only if the variances are same.\n"))
}
```

### Q6

Check whether there is a difference in the average profit across the geographical locations: United Kingdom, India and the Americas. 

```{r 'B14D08', comment="", echo=FALSE, results='asis'}
f_getDef("H-ANOVA") #dddd
```

- We need to conduct ANOVA because there are more than 2 levels we are checking
- \textcolor{pink}{aov()}
  - Total Variance = Between or MSTR + Within or MSE
    - Sum Sq provides SSTR & SSE
    - Mean Sq provides MSTR (Between) & MSE (Within)
  - First Line (Column): $\text{DOF}_{(k-1)} = 2, \text{SSTR} = 297, \text{MSTR} = 148.7$
  - Residuals (Within) : $\text{DOF}_{(n-k)} = 689, \text{SSE} = 68075, \text{MSE} = 98.8$
  - \@ref(eq:f-anv) $F = \frac{\text{MSTR}}{\text{MSE}} = 1.5$
  - ${}^U\!P_{F = 1.5} = 0.2238$
    - `pf(q = 1.5, df1 = 2, df2 = 689, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
  - Compare with ${\alpha} = 0.05$
    - ${}^U\!P_{(F)} > {\alpha} \to {H_0}$ cannot be rejected 
    - The sample results do not provide sufficient evidence to conclude that the average profit differs across the 3 geographical locations. 

- \textcolor{pink}{Question:} Sample size of these 3 groups are different (UK 553, Americas 104, India 35). Would this impact our analysis
  - If the sample size are similar, then the power will be more but otherwise ANOVA is not very sensitive to unequal sample sizes. We can safely use it.

- \textcolor{orange}{Warning:} 
  - "Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...)"
  - remove 'type' option from aov()
    
```{r 'B14-Q6anova'}
ii <- bb %>% filter(RG %in% c("UK", "India", "Americas")) %>% select(RG, PP, SO)
str(ii)
#
# #ANOVA
ii_aov <- aov(formula = PP ~ RG, data = ii)
#
# #
model.tables(ii_aov, type = "means")
#
# #Summary
summary(ii_aov)
```

```{r 'B14-q6noLost'}
jj <- ii %>% filter(SO == "1")
str(jj)
#
# #ANOVA
jj_aov <- aov(formula = PP ~ RG, data = jj)
#
# #
model.tables(jj_aov, type = "means")
#
# #Summary
summary(jj_aov)
```

### Q7

Check whether the sales conversions are different for different geographical locations. 

- Both (Location and Sales Conversions) are categorical variables, So ChiSq Test is required
- $P_{\chi^2} < {\alpha} \to {H_0}$ is rejected i.e. proportions are different
  - It can be claimed that there is an association between these variables.
  - We can conclude that the sales conversions are different for different geographical locations
  


- \textcolor{orange}{Warning:} 
  - "Warning in chisq.test() Chi-squared approximation may be incorrect: 
  - Some of observed frequencies in the Table are too few e.g. Canada

```{r 'B14-Q7chisq'}
ii <- bb %>% select(RG, SO)
str(ii)
#
round(prop.table(table(ii$RG, ii$SO), margin = 1), 3)
#
# #Chi-Sq Test
tryCatch(chisq.test(table(ii$RG, ii$SO)), 
         warning = function(w) {
           print(paste0(w))
           suppressWarnings(chisq.test(table(ii$RG, ii$SO)))
           })
```

### Q8

- Check whether the sales conversions depend on the sales value. Check this claim by making 3 groups of Sales Value: <6-million, [6-8], >8-million dollar 

- Both are categorical variables, So ChiSq Test is required
- $P_{\chi^2} > {\alpha} \to {H_0}$ cannot be rejected. 
  - It cannot be claimed that there is an association between these variables.
  - The sample results do not provide sufficient evidence to conclude that the sales conversions depend on the sales value. 
  

  
```{r 'B14-Q8chisq'}
ii <- bb %>% select(SO, SVM)
#
# #Create 3 Groups with middle group inclusive of both 6 & 8 
ii$RSVM <- cut(ii$SVM, breaks = c(0, 5.9999, 8, 15), labels = 1:3)
#
summary(ii)
#
table(ii$RSVM, ii$SO)
#
# #Chi-Sq Test
chisq.test(table(ii$RSVM, ii$SO))
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B14-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ee, hh, ii, jj, kk, ll, mm, nn, oo, rr, vv, xx, yy, zz, bb_headers, ii_aov, 
          ii_var, jj_aov, jj_var, xxWSES)
```

```{r 'B14-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/214-Statistics.Rmd-->

# Quiz (B15, Oct-17) {#b15}

```{r 'B15', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- This covers a short quiz and a case study [Case Study: JAT](#jat-b15 "b15")
  - Case analysis done on Nov-07 has been merged here.


## Short Quiz

1. In hypothesis testing,
    a. the smaller the Type I error, the smaller the Type II error will be
    a. the smaller the Type I error, the larger the Type II error will be
    a. Type II error will not be affected by Type I error
    a. the sum of Type I and Type II errors must equal to 1
	  - Answer: \textcolor{black}{b}
1. What type of error occurs if you accept ${H_0}$ when, in fact, it is not true
    a. Type II
    a. Type I
    a. either Type I or Type II, depending on the level of significance
    a. either Type I or Type II, depending on whether the test is one tail or two tail
	  - Answer: \textcolor{black}{a}
1. If the level of significance of a hypothesis test is raised from .01 to .05, the probability of a Type II error
    a. will also increase from .01 to .05
    a. will not change
    a. will decrease
    a. will increase
	  - Answer: \textcolor{black}{c}
1. The sum of the values of ${\alpha}$ and ${\beta}$
    a. always add up to 1.0
    a. always add up to 0.5
    a. is the probability of Type II error
    a. None of these alternatives is correct
	  - Answer: \textcolor{black}{d}
1. Following the p-value approach, the null hypothesis is rejected if 
    a. p-value less than or equal to ${\alpha}$
    a. ${\alpha}$ < p-value
    a. p-value > ${\alpha}$
    a. p-value = 1 - ${\alpha}$
	  - Answer: \textcolor{black}{a}
1. The average manufacturing work week in metropolitan Chattanooga was 40.1 hours last year. It is believed that the recession has led to a reduction in the average work week. To test the validity of this belief, which of the following stated hypothesis formulation is true.
    a. ${H_0} : {\mu} < 40.1 \quad {H_a} : {\mu} \geq 40.1$
    a. ${H_0} : {\mu} \geq 40.1 \quad {H_a} : {\mu} < 40.1$
    a. ${H_0} : {\mu} > 40.1 \quad {H_a} : {\mu} \leq 40.1$
    a. ${H_0} : {\mu} = 40.1 \quad {H_a} : {\mu} \neq 40.1$
	  - Answer: \textcolor{black}{b}
1. Statement about a population developed for the purpose of testing is called:
    a. Hypothesis
    a. Hypothesis testing
    a. Level of significance
    a. Test-statistic
	  - Answer: \textcolor{black}{a}
1. The probability of rejecting the null hypothesis when it is true is called: 
    a. Level of confidence
    a. Level of significance
    a. Power of the test
    a. Difficult to tell
	  - Answer: \textcolor{black}{b}
1. The dividing point between the region where the null hypothesis is rejected and the region where it is not rejected is said to be:
    a. Critical region
    a. Critical value
    a. Acceptance region
    a. Significant region
	  - Answer: \textcolor{black}{b}
1. If the critical region is located equally in both sides of the sampling distribution of test-statistic, the test is called:
    a. One tailed
    a. Two tailed
    a. Right tailed
    a. Left tailed
	  - Answer: \textcolor{black}{b}
1. Test of hypothesis ${H_0}$: ${\mu} \leq 50$ against ${H_a}$: ${\mu} > 50$ leads to:
    a. Left-tailed test
    a. Right-tailed test
    a. Two-tailed test
    a. Difficult to tell
	  - Answer: \textcolor{black}{b}
1. Test of hypothesis ${H_0}$: ${\mu} = 20$ against ${H_1}$: ${\mu} < 20$ leads to:
    a. Right one-sided test
    a. Left one-sided test
    a. Two-sided test
    a. All of the above
	  - Answer: \textcolor{black}{b}
1. A failed student has been promoted by an examiner; it is an example of: 
    a. Type-I error
    a. Type-II error
    a. Unbiased decision
    a. Difficult to tell
	  - Answer: \textcolor{black}{b}
1. The probability of accepting ${H_0}$ when it is True is called: 
    a. Power of the test
    a. Size of the test
    a. Level of confidence
    a. Confidence coefficient
	  - Answer: \textcolor{black}{d}
1. Power of a test is directly related to:
    a. Type-I error
    a. Type-II error
    a. Both (a) and (b)
    a. Neither (a) nor (b)
	  - Answer: \textcolor{black}{b}
  
## Case: JAT {#jat-b15}

\textcolor{pink}{Please import the Jayalaxmi data}


```{r 'B15-Jnames', include=FALSE}
# #Object Names for each sheet
namesJ <- c("xxJdata", "xxJbela", "xxJdhar", "xxJdiseases")
```

```{r 'B15-Jaya', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum
xxJaya <- f_getObject("xxJaya", "B15-Jayalaxmi.xlsx", "8be20492c0181d2269754b3232b992aa")
```

```{r 'B15-Jobjects', include=FALSE, eval=FALSE}
# #Create Separate Tibbles for Each Sheet
# #Separate Objects
for(ii in seq_along(namesJ)){
  assign(namesJ[ii], xxJaya[[ii + 1]])
}
#
# #Modifications
# #Rename Headers
names(xxJdiseases) <- c("Type", "Disease", "RH", "Temperature")
# #Delete First Row
xxJdiseases <- xxJdiseases[-1, ]
#
# #Save Binary Files
for(ii in seq_along(namesJ)){
  saveRDS(eval(parse(text = namesJ[ii])), paste0(.z$XL, namesJ[ii], ".rds"))
}
```

```{r 'B15-ImportData', include=FALSE}
for(ii in seq_along(namesJ)){
  assign(namesJ[ii], readRDS(paste0(.z$XL, namesJ[ii], ".rds")))
}
```

```{r 'B15-List', include=FALSE}
# #Dimensions of these datasets
str(lapply(namesJ, function(x) {dim(eval(parse(text = x)))}))
```

```{r 'B15-List-A', ref.label=c('B15-Jnames', 'B15-List')}
#
```

### Trends {.tabset .tabset-fade}

#### Images {.unlisted .unnumbered}

```{r 'B15-ImageData', include=FALSE}
bb <- xxJdata %>% rename(Dates = "Month-Year", Users = "No of users") %>% 
  mutate(across(Dates, as_date)) %>% select(Dates, Week, Usage, Users)
#
ii <- bb %>% 
  mutate(NewDate = case_when(Week == "Week1" ~ Dates, Week == "Week2" ~ Dates + 7, 
                             Week == "Week3" ~ Dates + 14, Week == "Week4" ~ Dates + 21)) %>% 
  mutate(DIFF = c(1, diff(NewDate)), 
         TF = ifelse(DIFF > 10, FALSE, TRUE), 
         grp = cumsum(DIFF > 10) + 1)
#
str(ii)
```

```{r 'B15-UsageTrend', include=FALSE}
# #Plot
hh <- ii
ttl_hh <- "Usage Pattern"
cap_hh <- "B15P01"
#
B15 <- hh %>% {
  ggplot(data = ., aes(x = NewDate, y = Usage)) +
  geom_point() +
  #geom_smooth(aes(group = grp)) +
  geom_line(aes(group = grp)) +
  scale_x_date(date_breaks = "3 months", 
                labels = function(x) if_else(is.na(lag(x)) | !year(lag(x)) == year(x), 
                                             paste(month(x, label = TRUE), "\n", year(x)), 
                                             paste(month(x, label = TRUE)))) +
  labs(x = "Time", y = "Usage", 
       caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B15)
rm(B15)
```

```{r 'B15P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B15P01", "-JAT-Usage", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B15P01, device = "png", dpi = 144) 
}
```

```{r 'B15P01', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B15P01", "-JAT-Usage", ".png"))
```

```{r 'B15-UserTrend', include=FALSE}
# #Plot
hh <- ii
ttl_hh <- "User Pattern"
cap_hh <- "B15P02"
#
B15 <- hh %>% {
  ggplot(data = ., aes(x = NewDate, y = Users)) +
  geom_point() +
  #geom_smooth(aes(group = grp)) +
  geom_line(aes(group = grp)) +
  scale_x_date(date_breaks = "3 months", 
                labels = function(x) if_else(is.na(lag(x)) | !year(lag(x)) == year(x), 
                                             paste(month(x, label = TRUE), "\n", year(x)), 
                                             paste(month(x, label = TRUE)))) +
  labs(x = "Time", y = "Users", 
       caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B15)
rm(B15)
```

```{r 'B15P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B15P02", "-JAT-Users", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B15P02, device = "png", dpi = 144) 
}
```

```{r 'B15P02', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B15P02", "-JAT-Users", ".png"))
```

```{r 'B15P0102', echo=FALSE, ref.label=c('B15P01', 'B15P02'), fig.cap="(B15P01 B15P02) JAT: Usage and Users over the TimePeriod"}
#
```

#### Data {.unlisted .unnumbered}

```{r 'B15-ImageData-A', ref.label=c('B15-ImageData')}
#
```

#### Trendline {.unlisted .unnumbered}

```{r 'B15-UsageTrend-A', ref.label=c('B15-UsageTrend'), eval=FALSE}
#
```

#### Skip Missing Data {.unlisted .unnumbered}

```{r 'B15-MissingDataTrendLine'}
bb <- xxJdata %>% rename(Dates = "Month-Year", Users = "No of users") %>% 
  mutate(across(Dates, as_date)) %>% select(Dates, Week, Usage, Users)
#
ii <- bb %>% 
  mutate(NewDate = case_when(Week == "Week1" ~ Dates, Week == "Week2" ~ Dates + 7, 
                             Week == "Week3" ~ Dates + 14, Week == "Week4" ~ Dates + 21)) %>% 
  mutate(DIFF = c(1, diff(NewDate)), 
         TF = ifelse(DIFF > 10, FALSE, TRUE), 
         grp = cumsum(DIFF > 10) + 1) %>% 
  mutate(grp_step = rle(TF)$lengths %>% {rep(seq(length(.)), .)})
#
jj <- c(1, which(ii$DIFF > 10), nrow(ii) + 1)
kk <- rep(1:length(diff(jj)), diff(jj))
stopifnot(identical(kk, as.integer(ii$grp)))
#
# #Where are the missing values
ii %>% filter( !TF | lead(!TF) | lag(!TF) )
```

### Q1 {.tabset .tabset-fade}

Test the claim that disease 6 (leaf curl) information was accessed at least 60 times every month on average since October 2017 due to this disease outbreak. $({\alpha} = 0.05)$

NOTE: Actually the claim is "at least 60 times every week". ~~Month~~ is a printing error.

```{r 'B15D01', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-1T-Upper-Tail")
```


- ${}^U\!P_{t = 2.341} = 0.01329 \to {}^U\!P_{(z)} < {\alpha} \to {H_0}$ is rejected
  - We can conclude that the information on disease 6 was accessed at least 60 times every Week on average since October 2017. The claim is correct.
  - Originally tested for Monthly Data, which will obviously be True if Weekly rates are at least 60 times
    - ${}^U\!P_{t = 5.3771} = 0.0005 \to {}^U\!P_{(z)} < {\alpha} \to {H_0}$ is rejected

- R Notes: For ease during subsetting, convert the Date Column to Date Type 
  - \textcolor{pink}{as.Date(), as_date()} 

#### Code (per Week Claim) {.unlisted .unnumbered}


```{r 'B15-Q1week'}
# #Data | Rename | Change from Date Time to Date
bb <- xxJdata %>% 
  rename(Dates = "Month-Year") %>% 
  #group_by(Dates) %>% 
  #summarise(D6 = sum(D6)) %>% 
  mutate(across(Dates, as_date)) 
# 
# #Get relevant rows using filter() #xxJdata[95:123, ]
ii <- bb %>% filter(Dates >= "2017-10-01")
#
t_ii <- {mean(ii$D6) - 60} / {sd(ii$D6) / sqrt(nrow(ii))}
print(t_ii)
pt(t_ii, df = nrow(ii) - 1, lower.tail = FALSE)
#
# #One Sample t-Test
t.test(ii$D6, mu = 60, alternative = "greater", conf.level = 0.05)
```


#### Code (per Month Claim) {.unlisted .unnumbered}

```{r 'B15-Q1'}
# #Data | Rename | Sum Months D6 | Change from Date Time to Date
bb <- xxJdata %>% 
  rename(Dates = "Month-Year") %>% 
  group_by(Dates) %>% 
  summarise(D6 = sum(D6)) %>% 
  mutate(across(Dates, as_date)) 
# 
# #There are missing months, but those months are not applicable in this question
# #Get relevant rows using filter()
ii <- bb %>% filter(Dates >= "2017-10-01")
#
t_ii <- {mean(ii$D6) - 60} / {sd(ii$D6) / sqrt(nrow(ii))}
print(t_ii)
pt(t_ii, df = nrow(ii) - 1, lower.tail = FALSE)
#
# #One Sample t-Test
t.test(ii$D6, mu = 60, alternative = "greater", conf.level = 0.05)
```

#### Missing Months {.unlisted .unnumbered}

```{r 'B15-MissingMonths'}
str(bb)
summary(bb)
#
# #Assuming each row is one month with no duplicates
stopifnot(identical(anyDuplicated(bb$Dates), 0L))
#
# #Create Sequence of Months
#ii <- seq(ymd("2015-6-1"), ymd("2018-5-1"), by = "months")
ii <- tibble(Dates = seq(min(bb$Dates), max(bb$Dates), by = "months"))
#
diff_len <- nrow(ii) - nrow(bb)
#
if(!identical(diff_len, 0L)) {
  cat(paste0("Number of missing months = ", diff_len, "\n"))
  #
  # #Find Values that should be in Complete Sequence but are missing in the data
  as_date(setdiff(ii$Dates, bb$Dates))
  # #OR
  ii %>% anti_join(bb)
  #
  # #This does not need a separate Vector of all Months
  # #Get Months Difference using Integer Division and 
  # #Filter Rows which are not consecutive and rows above them
  bb %>% 
    mutate(diff_months = (interval(lag(Dates), Dates)) %/% months(1)) %>% 
    filter( (diff_months != 1) | lead(diff_months != 1) )
}
```


```{r 'B15-FillMissingMonths'}
# #Fill Missing Months 
jj <- as_tibble(merge(bb, ii, by = "Dates", all = TRUE)) 
kk <- right_join(bb, ii, by = "Dates") %>% arrange(Dates)
stopifnot(identical(jj, kk))
# #Replace NA 
ll <- kk %>% mutate(across(D6, coalesce, 0)) 
```


### Q2 {.tabset .tabset-fade}

Test the claim that Among the app users for disease information, at least 15% of them access disease information related to disease 6. $({\alpha} = 0.05)$

```{r 'B15D02', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-1T-Upper-Tail")
```

- ~~One Tail t-Test~~ \textcolor{orange}{Caution: Proportion Test is needed.} 
  - Wrong Analysis: 
    - ${}^U\!P_{t} = 0.094 \to {}^U\!P_{(t)} > {\alpha} \to {H_0}$ cannot be rejected.
    - ~~We cannot conclude that at least 15% of users access D6 related information.~~
  - (Aside) Why using t-test is wrong here 
    - 15% is NOT the ${\mu}$, it is proportion. Using that as Mean was wrong.
    - We are NOT looking for percentage of D6 lookup in each row or observation. It does not matter if one day farmers only looked for other diseases and another day farmers searched only D6. We are interested in overall share of D6 searches out of searches for Diseases

- Proportion Test

Determine whether the proportion of farmers searching for D6 is more than $p_0 = 0.15$

```{r 'B15D03', comment="", echo=FALSE, results='asis'}
f_getDef("H-1s-p-Upper")
```

- Count of Success $({x})$ is Searches of D6 
- $\{n = 26830, x = 4295\} \to {\overline{p}} = {n}/{x} = 0.160082$ 
- \@ref(eq:z-1s-p) 
  - ${}^U\!P_{({\chi}^2)} < {\alpha} \to {H_0}$ is rejected i.e. the proportions are different
  - We can conclude that the proportion of farmers searching for D6 is more than $p_0 = 0.15$ for all searches for Diseases.

#### Proportion Test {.unlisted .unnumbered}

```{r 'B15-PropTest'}
# #Data | Sum Disease, Variety, Micronutrients 
aa <- xxJdata %>% 
  mutate(sumD = rowSums(across(starts_with("D"))), 
         sumV = rowSums(across(starts_with("V")))) %>% 
  rename(Dates = "Month-Year", Users = "No of users", Micro = "Micronutrient") %>% 
  mutate(SUM = rowSums(across(c(sumD, sumV, Micro))),
         DIFF = Usage - SUM) %>% 
  select(Dates, Users, Usage, SUM, DIFF, sumD, sumV, Micro, D6) %>% 
  mutate(across(Dates, as_date)) %>% 
  mutate(Fraction = D6/sumD)
#
# #Confirmed that Usage is Sum Total of Disease, Variety, Micronutrients
unique(aa$DIFF)
#
# #Working Set | Exclude 1 NA i.e. where sumD is zero
bb <- aa %>% drop_na(Fraction) %>% select(Usage, sumD, D6, Fraction)
#
# #Check n (Sample Count) and x (Count of Success)
bb %>% summarise(across(c(sumD, D6), sum))
#
# #One Sample Proportion Test with continuity correction
bb_prop <- prop.test(x = sum(bb$D6), n = sum(bb$sumD), p = 0.15, 
                     alternative = "greater", conf.level = 0.95)
bb_prop
```

#### Proportion Test (Usage) {.unlisted .unnumbered}

```{r 'B15-PropTestALL'}
# #Impact if we try to evaluate propotion of D6 searches out of ALL Usage (Disease, Variety, Micro)
# #Check n (Sample Count) and x (Count of Success)
bb %>% summarise(across(c(Usage, D6), sum))
#
# #One Sample Proportion Test with continuity correction
# #With p-value = 1, we cannot claim that 15% searches are for D6 only out of ALL Usage
prop.test(x = sum(bb$D6), n = sum(bb$Usage), p = 0.15, 
                     alternative = "greater", conf.level = 0.95)
```

#### One Sample t-Test {.unlisted .unnumbered}

```{r 'B15-Q2-Wrong', eval=FALSE}
# #One Sample t-Test (Wrong)
if(FALSE) {
  t.test(bb$Fraction, mu = 0.15, alternative = "greater", conf.level = 0.05)
}
```

### Q3 {.tabset .tabset-fade}

Test the claim that the average number of users in year 2017-2018 is more than average number of users in year 2015-2016. $({\alpha} = 0.05)$

```{r 'B15D04', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Upper")
```

- If 1 is 2017-2018, 2 is 2015-2016 : Upper Tail Test is required
- If 1 is 2015-2016, 2 is 2017-2018 : Lower Tail Test is required
- Because the Variances are NOT same, Welch t-test is required, we cannot use Pooled Test
- $({}^L\!P_{t = -7.255} =  {}^U\!P_{t = 7.255}) < {\alpha} \to {H_0}$ is rejected 
  - The sample results provide sufficient evidence to conclude that the average number of users has increased.
- Also, Test the claim that app usage picked up after January 2016. (Moved from Q4)
  - ${}^U\!P_{t} < {\alpha} \to {H_0}$ is rejected 
  - The sample results provide sufficient evidence to conclude that the app usage has increased after January 2016.

- NOTE: Ideally, each missing month should have been added 4 times for the 4 weeks. "ForLater"

#### Code {.unlisted .unnumbered}

```{r 'B15-Q3'}
# #Data
bb <- xxJdata %>% 
  rename(Dates = "Month-Year", Users = "No of users") %>% 
  mutate(across(Dates, as_date)) %>% 
  select(Dates, Users, Usage)
#
# #Missing Months
ii <- tibble(Dates = seq(min(bb$Dates), max(bb$Dates), by = "months"))
jj <- right_join(bb, ii, by = "Dates") %>% arrange(Dates) %>% mutate(across(Users, coalesce, 0)) 
#
# #Create 2 Groups
jj$Year <- cut(jj$Dates, breaks = c(min(ii$Dates), as_date("2017-01-01"), Inf), 
               labels = c("2015-2016", "2017-2018"))
#
# #Verify Changes
jj[!duplicated(jj$Year), ]
jj %>% filter(Dates %in% ymd(c("2016-12-01", "2017-01-01")))
#
# #For Two Sample t-test, check if Variances are equal
jj_var <- var.test(Users ~ Year, data = jj)
jj_var
#
# #If Variances are Equal, Pooled Test otherwise Welch Test
isVarEqual <- ifelse(jj_var$p.value > 0.05, TRUE, FALSE)
#
# #Because 1 is "2015-2016", 2 is "2017-2018", we need to perform Lower Tail Test
jj_t <- t.test(formula = Users ~ Year, data = jj, alternative = "less", var.equal = isVarEqual)
jj_t
#
# #Alternatively, we can reverse Factor levels to perform Upper Tail Test
kk <- jj
kk$Year <- factor(kk$Year, levels = rev(levels(jj$Year)))
#
t.test(formula = Users ~ Year, data = kk, alternative = "greater", var.equal = isVarEqual)
```

#### 4b {.unlisted .unnumbered}

```{r 'B15-Q34b'}
# #Data
str(jj)
# #For Two Sample t-test, check if Variances are equal
jj_var <- var.test(Usage ~ Year, data = jj)
jj_var
#
# #If Variances are Equal, Pooled Test otherwise Welch Test
isVarEqual <- ifelse(jj_var$p.value > 0.05, TRUE, FALSE)
#
# #Because 1 is "2015-2016", 2 is "2017-2018", we need to perform Lower Tail Test
t.test(formula = Usage ~ Year, data = jj, alternative = "less", var.equal = isVarEqual)
```



### Q4 {.tabset .tabset-fade}

Check whether app usage is same or different across the four weeks of a month. 
Test the claim that app usage picked up after January 2016. (Answered with Q3)

NOTE: Question is 'check whether app usage is same or different across the four weeks of a month, using Jan-2016 - May-2018 data'. However, as seen in the figure \@ref(fig:B15P0102), this time period has 3 months missing data and completely different usage pattern after that, I believe that testing only this data would give biased results. So, this was not done.

```{r 'B15D05', comment="", echo=FALSE, results='asis'}
f_getDef("H-ANOVA")
```

- ANOVA is needed because more than two means are to be compared
  - Data groups are not normal and neither the variances are equal
  - Log transformation failed, Residual Test failed
    - NOTE: Data shown in class is subset "Jan-2016 - May-2018" i.e. log transformation of this dataset becomes normal.
  - However, Sqrt transformation passed Normality Test as well as Variances are found to be same
- Using Sqrt of Data ${}^U\!P_{(F)} > {\alpha} \to {H_0}$ cannot be rejected 
  - The sample results do not provide sufficient evidence to conclude that the app usage is different across the four weeks of a month. 

\textcolor{pink}{Question:} When ANOVA is done on transformed data and a conclusion is reached. Does this imply that the original data would also follow same conclusion
  - Look at the p-value of Transformed data for accepting or rejecting the Hypothesis. But, look at the mean of original data to apply those conclusions.
  - ANOVA p-value is NOT trustworthy if the data is NOT Normal.

\textcolor{pink}{Question:} When we are running any test, should we check whether the data is normal
  - Yes.

#### Images {.unlisted .unnumbered}


```{r 'B15-QQPlot', include=FALSE}
bb <- xxJdata %>% 
  rename(Dates = "Month-Year", Users = "No of users") %>% 
  mutate(Week = factor(Week)) %>% 
  mutate(Sqrt = sqrt(Usage)) %>% 
  select(Week, Usage, Sqrt)
#
hh <- bb
ttl_hh <- "QQ Plot of Usage"
cap_hh <- "B15P04"
#
B15 <- hh %>% { ggplot(., aes(sample = Usage, colour = Week)) +
    stat_qq() +
    stat_qq_line() +
    labs(caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B15)
rm(B15)
#
ttl_hh <- "QQ Plot of Sqrt(Usage)"
cap_hh <- "B15P05"
B15 <- hh %>% { ggplot(., aes(sample = Sqrt, colour = Week)) +
    stat_qq() +
    stat_qq_line() +
    labs(caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B15)
rm(B15)
```


```{r 'B15P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B15P04", "-JAT-QQ-Usage", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B15P04, device = "png", dpi = 144) 
}
```

```{r 'B15P04', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B15P04", "-JAT-QQ-Usage", ".png"))
```

```{r 'B15P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B15P05", "-JAT-QQ-Usage-Sqrt", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B15P05, device = "png", dpi = 144) 
}
```

```{r 'B15P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B15P05", "-JAT-QQ-Usage-Sqrt", ".png"))  #iiii
```

```{r 'B15P04P05', echo=FALSE, ref.label=c('B15P04', 'B15P05'), fig.cap="(B15P04 B15P05) JAT: QQ Plot of Usage and Sqrt(Usage)"}
#
```

#### Anova & Kruskal {.unlisted .unnumbered}

```{r 'B15-Q4'}
# #Data | Missing Months can be ignored because those are missing across all weeks
bb <- xxJdata %>% 
  rename(Dates = "Month-Year", Users = "No of users") %>% 
  select(Week, Usage)
#
str(bb)
summary(bb)
#
# #ANOVA (on original data : neither normal, nor of equal variance)
bb_aov <- aov(formula = Usage ~ Week, data = bb)
#
# #
model.tables(bb_aov, type = "means")
#
# #Summary
summary(bb_aov)
#
bb_aov
#
kruskal.test(Usage ~ Week, data = bb)
#
# #Poisson Test (ForLater)
#anova(glm(Usage ~ Week, data = ii, family = poisson), test = "LRT")
#
# #Transformation: Square Root Data
ii <- bb %>% mutate(Week = factor(Week)) %>% mutate(Sqrt = sqrt(Usage))
str(ii)
summary(ii)
#
# #ANOVA
ii_aov <- aov(formula = Sqrt ~ Week, data = ii)
# #
model.tables(ii_aov, type = "means")
#
# #Summary
summary(ii_aov)
#
kruskal.test(Sqrt ~ Week, data = ii)
```

#### Variance {.unlisted .unnumbered}

[(External) Variances in R](http://www.sthda.com/english/wiki/compare-multiple-sample-variances-in-r "http://www.sthda.com")

Statistical tests for comparing the variances of two or more samples. Equal variances across samples is called homogeneity of variances.

- Reason
  - Two independent samples T-test and ANOVA test, assume that variances are equal across groups. 
- Statistical tests for comparing variances
  - F-test: Compare the variances of two samples. The data must be normally distributed.
  - Bartlett test: Compare the variances of k samples, where k can be more than two samples. 
    - The data must be normally distributed. 
    - The Levene test is an alternative to the Bartlett test that is less sensitive to departures from normality.
  - Levene test: Compare the variances of k samples, where k can be more than two samples. 
    - It is an alternative to the Bartlett test that is less sensitive to departures from normality.
  - Fligner-Killeen test: a non-parametric test which is very robust against departures from normality.
- Hypothesis
  - For all these tests (Bartlett test, Levene test or Fligner-Killeen test): 

```{definition 'H-Variances'}
\textcolor{pink}{$\text{\{Variances\}} {H_0} : {\sigma}_1 = {\sigma}_2 = \dots = {\sigma}_k \iff {H_a}: \text{At least two variances differ.}$}
```

- On this data, all 3 tests have p-value less than 0.05, i.e. Variances are NOT same
- On the Transformed Data (Sqrt), Levene Test and Fligner Test fail to detect difference in Variances


```{r 'B15-Variance'}
# #Data
str(bb)
summary(bb)
#
# #Bartlett Test
bartlett.test(Usage ~ Week, data = bb)
#
# #Levene Test
ii <- bb %>% mutate(Week = factor(Week))
leveneTest(Usage ~ Week, data = ii)
#
# #Fligner-Killeen test
fligner.test(Usage ~ Week, data = bb)
#
# #Transformation: Square Root Data
ii <- bb %>% mutate(Week = factor(Week)) %>% mutate(Sqrt = sqrt(Usage))
bartlett.test(Sqrt ~ Week, data = ii)
leveneTest(Sqrt ~ Week, data = ii)
fligner.test(Sqrt ~ Week, data = ii)
```


#### Normality {.unlisted .unnumbered}

```{r 'B15-Normality'}
# #Are the data from each of the 4 groups follow a normal distribution
# #Shapiro-Wilk normality test
bb %>% mutate(Week = factor(Week)) %>% 
  group_by(Week) %>% 
  summarise(N = n(), Mean = mean(Usage), SD = sd(Usage),
            p_Shapiro = shapiro.test(Usage)$p.value, 
            isNormal = ifelse(p_Shapiro > 0.05, TRUE, FALSE))
# #Check Q-Q plot
#qqnorm(bb[bb$Week == "Week1", ]$Usage)
#
# #Transformation: Log (Did not pass Normality)
bb %>% mutate(Week = factor(Week)) %>% 
  mutate(Log = log(Usage)) %>% 
  group_by(Week) %>% 
  summarise(p_Shapiro = shapiro.test(Log)$p.value, 
            isNormal = ifelse(p_Shapiro > 0.05, TRUE, FALSE))
#
# #Transformation: Square Root (Success: Passed Normality) - Selected
bb %>% mutate(Week = factor(Week)) %>% 
  mutate(Sqrt = sqrt(Usage)) %>% 
  group_by(Week) %>% 
  summarise(p_Shapiro = shapiro.test(Sqrt)$p.value, 
            isNormal = ifelse(p_Shapiro > 0.05, TRUE, FALSE))
#
# #Transformation: Cube Root (Success: Passed Normality) Just to check
bb %>% mutate(Week = factor(Week)) %>% 
  mutate(CubeRoot = Usage^(1/3)) %>% 
  group_by(Week) %>% 
  summarise(p_Shapiro = shapiro.test(CubeRoot)$p.value, 
            isNormal = ifelse(p_Shapiro > 0.05, TRUE, FALSE))
#
# #Testing Residuals i.e. Data - Group Mean (Did not pass Normality)
bb %>% mutate(Week = factor(Week)) %>% 
  group_by(Week) %>% 
  mutate(Residuals = Usage - mean(Usage)) %>% 
  summarise(p_Shapiro = shapiro.test(Residuals)$p.value, 
            isNormal = ifelse(p_Shapiro > 0.05, TRUE, FALSE))

```

#### QQ Plot {.unlisted .unnumbered}

```{r 'B15-QQPlot-A', eval=FALSE, ref.label=c('B15-QQPlot')}
#
```

### Q5 {.tabset .tabset-fade}

```{r 'B15D06', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Lower") 
```

A new version of the app was released in August-2016. Which month in the given time frame after the launch of the new version, the mean usage pattern would start to show a statistically significant shift

- (1: OldApp, 2: NewApp) Lower Tail Test is required
- Because the Variances are NOT same, Welch t-test is required, we cannot use Pooled Test
- ${}^L\!P_{t} < {\alpha} \to {H_0}$ is rejected 
  - The sample results provide sufficient evidence to conclude that the mean usage has increased after August-2016.
  
#### Basic {.unlisted .unnumbered}

```{r 'B15-Q5'}
# #Data
bb <- xxJdata %>% rename(Dates = "Month-Year") %>% mutate(across(Dates, as_date)) %>% 
  select(Dates, Week, Usage)
#
# #Create 2 Groups
bb$Year <- cut(bb$Dates, breaks = c(min(bb$Dates), as_date("2016-08-01"), Inf), 
               labels = c("OldApp", "NewApp"))
#
# #For Two Sample t-test, check if Variances are equal
bb_var <- var.test(Usage ~ Year, data = bb)
bb_var
#
# #If Variances are Equal, Pooled Test otherwise Welch Test
isVarEqual <- ifelse(bb_var$p.value > 0.05, TRUE, FALSE)
#
# #Because 1 is "OldApp", 2 is "NewApp", we need to perform Lower Tail Test
bb_t <- t.test(formula = Usage ~ Year, data = bb, alternative = "less", var.equal = isVarEqual)
bb_t
#
```

#### rollapply() {.unlisted .unnumbered}

```{r 'B15-Q5b'}
# #Rolling Sums
old_sd <- sd(bb[bb$Year == "OldApp", ]$Usage)
old_n <- summary(bb$Year)[1]
#
ii <- bb %>% filter(Year == "NewApp", Dates <= '2017-05-01') %>% select(-Year)
#
jj <- ii %>% mutate(ID = row_number(), cSUM = cumsum(Usage), cMean = cSUM/ID) %>% 
  mutate(SD = across(Usage, ~ rollapply(., ID, sd, fill = NA, align = "right"))) %>% 
  mutate(DOF = floor({SD^2 / ID + old_sd^2 / old_n }^2 / 
                       {{SD^2 / ID}^2/{ID-1} + {old_sd^2 / old_n}^2/{old_n-1}})) %>% 
  mutate(Sigma = sqrt({SD^2 /ID + old_sd^2 /old_n}))
str(jj)
```


### Q6 {.tabset .tabset-fade}

If a disease is likely to spread in particular weather condition (data given in the disease index sheet), then the access of that disease should be more in the months having suitable weather conditions. Help the analyst in coming up with a statistical test to support the claim for two districts for which the sample of weather and disease access data is provided in the data sheet. Identify the diseases for which you can support this claim. Test this claim both for temperature and relative humidity at 95% confidence.

```{r 'B15D07', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Upper") #dddd
```


```{r 'B15-Q6', include=FALSE}
# #Merge both dataframes of Two districts
aa <- bind_rows(Belagavi = xxJbela, Dharwad = xxJdhar, .id = 'source') %>% 
  rename(Dates = Months, RH = "Relative Humidity", TMP = "Temperature") %>% 
  mutate(across(Dates, as_date)) %>% mutate(source = factor(source)) %>% 
  select(-c(10:13)) %>% select(-D6)
#
# #Based on Conditional T & RH, get each disease favourable condition = TRUE
q6_bb <- aa %>% mutate(iD1 = ifelse(TMP <= 24 & TMP >= 20 & RH > 80, TRUE, FALSE), 
                    iD2 = ifelse(TMP <= 24.5 & TMP >= 21.5 & RH > 83, TRUE, FALSE), 
                    iD3 = ifelse(TMP <= 24 & TMP >= 22, TRUE, FALSE), 
                    iD4 = ifelse(TMP <= 26 & TMP >= 22 & RH > 85, TRUE, FALSE), 
                    iD5 = ifelse(TMP <= 24.5 & TMP >= 22 & RH <= 85 & RH >= 77, TRUE, FALSE), 
                    iD7 = ifelse(TMP > 25 & RH > 80, TRUE, FALSE)) %>% 
  mutate(across(starts_with("i"), factor, levels = c(TRUE, FALSE)))
bb <- q6_bb
#
# #Create all Formulae for variance and t-test
formulas <- paste0(names(bb)[3:8], " ~ ", names(bb)[11:16])
#
# #Appply formulae
output <- t(sapply(formulas, function(f) {
    test_var <- var.test(as.formula(f), data = bb)
    isVarEqual <- ifelse(test_var$p.value > 0.05, TRUE, FALSE) 
    test_t <- t.test(as.formula(f), data = bb, alternative = "greater", var.equal = isVarEqual)
    c("p- Var" = format(round(test_var$p.value, 5), scientific = FALSE), 
      "Equal Var" = ifelse(test_var$p.value > 0.05, TRUE, FALSE), 
      "p- t-test" = format(round(test_t$p.value, 5), scientific = FALSE), 
      "Upper H0" = ifelse(test_t$p.value > 0.05, "Not Rejected", "Rejected"))
}))
```

```{r 'B15T01', echo=FALSE}
bb <- output
kbl(bb,
  caption = "(B15T01) Q6: Diseases p-value (Upper)",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
                #position = "float_left",
                fixed_thead = TRUE
  ) %>%
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```

- (1: Conditions Favourable for Disease i.e. TRUE, 2: FALSE) Upper Tail Test is required
- Testing variance and applying Welch t-test (variances not same) or Pooled Test (variances are same)
- Assuming both districts to be part of same sample
- Values are given in the \@ref(tab:B15T01)
  - ${}^U\!P_{t} > {\alpha} \to {H_0}$ cannot be rejected for disease 4
  - ${}^U\!P_{t} < {\alpha} \to {H_0}$ is rejected for diseases 1, 2, 3, 5, and 7
  - The sample results provide sufficient evidence to conclude that the claim is True for diseases 1, 2, 3, 5, and 7
  - Upper Tail Test was performed in place of Two Tail because it is assumed that there is no benefit in thinking that searches for a disease may decrease during the favourable conditions for it.
  - Both districts were combined with the thinking that political boundaries may not change the behaviour of people based on geographical factors (T & RH).
    - However, a case can be made when we are interested in observing difference in pattern of behaviour by the people of different districts. In that case, we would be performing a difference of means test between the two districts.

#### BoxPlot {.unlisted .unnumbered}

```{r 'B15-BoxPlot', include=FALSE}
bb <- q6_bb
hh <- q6_bb %>% 
  rename_with(~gsub("iD", "i", .x)) %>% 
  select(starts_with(c("D", "i"))) %>% 
  select(-Dates) %>% 
  pivot_longer(everything(), names_to = c(".value", "Disease"), names_pattern = "(.)(.)") %>%
  rename(Values = "D", Favourable = "i")
#
ttl_hh <- "BoxPlot of Searches for Diseases in both districts"
cap_hh <- "B15P03"
#
B15 <- hh %>% { ggplot(data = ., mapping = aes(x = Disease, y = Values, fill = Favourable)) +
        geom_boxplot(outlier.shape = NA) +
        #stat_summary(fun = mean, geom = "point", size = 2, color = "steelblue") + 
        #scale_y_continuous(breaks = seq(0, 110, 10), limits = c(0, 110)) +
        geom_point(position = position_jitterdodge(jitter.width = 0.1), 
                   size = 1, alpha = 0.7, colour = "#21908CFF") + 
        k_gglayer_box +
        theme(
            #legend.justification = c("right", "top"),
            #legend.box.just = "right",
            #legend.margin = margin(6, 6, 6, 6),
            legend.position = c(.90, .95)
        ) +
        labs(x = "Diseases", y = "Searches per month", fill = "Favourable",
             caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B15)
rm(B15)
```

```{r 'B15P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B15P03", "-JAT-Diseases", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B15P03, device = "png", dpi = 144) 
}
```

```{r 'B15P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B15P03", "-JAT-Diseases", ".png"))
```

```{r 'B15P03-A', echo=FALSE, ref.label=c('B15P03'), fig.cap="(B15P03) JAT: Disease Searches grouped with Faourable and unfavourable Condisions (T and RH)"}
#
```

#### Code {.unlisted .unnumbered}

```{r 'B15-Q6-A', ref.label=c('B15-Q6'), eval=FALSE}
#
```

#### BoxPlot More {.unlisted .unnumbered}

```{r 'B15-BoxPlot-A', ref.label=c('B15-BoxPlot'), eval=FALSE}
#
```

#### Pattern Match {.unlisted .unnumbered}

```{r 'B15-Pattern'}
# #rename_with() uses formula
# #Selection Helpers like starts_with() can take multiple conditions
# #pivot_longer() can return multiple groups
# #Pattern Match: Both First and Second Pattern can contain only 1 character
ii <- q6_bb %>% 
  rename_with(~gsub("iD", "i", .x)) %>% 
  select(starts_with(c("D", "i"))) %>% select(-Dates) %>% 
  pivot_longer(everything(), names_to = c(".value", "Disease"), names_pattern = "(.)(.)") %>%
  rename(Values = "D", Favourable = "i")
#
# #First Pattern can contain 1 or more characters but the Second can have only 1 character
jj <- q6_bb %>% 
  select(starts_with(c("D", "i"))) %>% select(-Dates) %>% 
  pivot_longer(everything(), names_to = c(".value", "Disease"), names_pattern = "(.*)(.)") %>% 
  rename(Values = "D", Favourable = iD)
stopifnot(identical(ii, jj))
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B15-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ee, hh, ii, jj, kk, ll, mm, nn, oo, rr, vv, xx, yy, zz, bb_aov, bb_prop, 
          diff_len, ii_aov, isVarEqual, jj_t, jj_var, namesJ, t_ii, xxJbela, xxJdata, xxJdhar,
          xxJdiseases, B15P01, B15P02, bb_t, bb_var, cap_hh, loc_png, ttl_hh, B15P03, 
          formulas, old_n, old_sd, output, q6_bb, B15P04, B15P05)
```

```{r 'B15-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/215-Quiz.Rmd-->

# Data Preprocessing (B16, Oct-24) {#b16}

```{r 'B16', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Data Pre-processing"
  - Refer [Data Processing](#c32 "c32")

## Packages

```{r 'B16-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("mice", dependencies = TRUE)
  install.packages("car", dependencies = TRUE)
}
```

## Data {.tabset .tabset-fade}

\textcolor{pink}{Please import the "B16-Cars2.csv"}

```{r 'B16-Cars', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum
xxB16Cars <- f_getObject("xxB16Cars", "B16-Cars2.csv", "30051fb47f65810f33cb992015b849cc")
```

```{r 'B16-ImportData', include=FALSE}
xxB16Cars <- f_getRDS(xxB16Cars)
bb <- aa <- xxB16Cars
```

### Cars {.unlisted .unnumbered}

```{r 'B16T01', echo=FALSE}
bb <- xxB16Cars
kbl(head(bb),
  caption = "(B16T01) Cars Data (Head)",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
                #position = "float_left",
                fixed_thead = TRUE
  ) %>%
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```

### Structure {.unlisted .unnumbered}

```{r 'B16-Structure'}
# #Structure
str(xxB16Cars)
```

### Summary {.unlisted .unnumbered}

```{r 'B16-Summary'}
# #Summary
summary(xxB16Cars)
```

## Missing Data

- [Do not miss the missing values i.e. NA](#na-b10 "b10")
- Relevant functions: \textcolor{pink}{`summary(), anyNA(), is.na(), na.omit(), complete.cases(), drop_na()`}

## Imputation {#immputation-b16}

```{definition 'Imputation'}
\textcolor{pink}{Imputation} is the process of replacing missing data with substituted values. Imputation preserves all cases by replacing missing data with an estimated value based on other available information.
```

- Imputation 
  - What would be the most likely value for this missing value, given all the other attributes for a particular record
  - "Multivariate Imputation by Chained Equations" (MICE) is used for Imputation.
  

- A common method of "handling" missing values is simply to omit the records or fields with missing values from the analysis. However, there are issues with this approach.
  - Sometimes it is not feasible or desirable to delete all the rows containing missing values
  - Sometimes the data is deliberately missing
    - Example: Customers with high value transactions have higher missing income data
      - This is also a pattern and we cannot simply remove these customers from our analysis
  - \textcolor{pink}{Question:} What if there are only a small number of missing values in a large dataset
    - Even then also, we need to look at whether it is a meaningful data or not. Some data points are so critical that they cannot be removed. If you delete them, you might be loosing highly relevant information.
  - \textcolor{pink}{Question:} Only 15 rows in a 1-lakh dataset
    - Do the rows belong to a new product which is relevant to our analysis
    - While these rows may not have mileage information of a car, other columns would have pricing and other critical information. Deleting these rows may result in loss of this important data.
    - It would be a waste to omit the information in all the other fields, just because one field value is missing.


- Mean value replacement 
  - Refer [Mean](#mean-c03 "c03")
  - Replace the missing value by 'mean' of the data
  - However, the 'mean' is affected by presence of extreme values (outliers)
- Median replacement
  - Refer [Median](#median-c03 "c03")
  - Although the mean is the more commonly used measure of central location, whenever a data set contains extreme values, the median is preferred.
  - Median is always a better measure for replacement compared to mean
- Mode replacement
  - Refer [Mode](#mode-c03 "c03")
  - For Categorical variable, mode is preferred.
  - However, using mode for NA replacement results in increasing the frequency of most frequent item. 
- Random value replacement
  - Replace NA with a random value from the observed distribution of the variable. 
  - However, the resulting observation (row) might not make sense in terms of grouping of variables (columns). 


- Problems:
  - These simple approaches usually introduce bias into the data
    - Ex: Applying mean substitution leaves the mean unchanged (desirable) but decreases variance (undesirable). The resulting confidence levels for statistical inference will be overoptimistic, as measures of spread will be artificially reduced.

### Introduce NA 

```{r 'B16-NA'}
aa <- xxB16Cars #No missing value
bb <- aa     #Will have missing value later
#
# #Identify the Number of Missing Values
if(anyNA(bb)) {
  cat(paste0("NA are Present! Total NA = ", sum(is.na(bb)), "\n")) 
  } else cat(paste0("NA not found.\n"))
#
# #Record Some Values, before deleteting them
bb_22 <- bb$mpg[2]        #bb[2, 2] #31.9
bb_39 <- bb$brand[3]      #bb[3, 9] #"US"
bb_43 <- bb$cylinders[4]  #bb[4, 3] #8
#
# #Delete 
bb$mpg[2] <- bb$brand[3] <- bb$cylinders[4] <- NA
#
# #Identify the Number of Missing Values
cat(paste0("NA are Present! Total NA = ", sum(is.na(bb)), "\n")) 
#
# #Which Columns have NA
#summary(bb)
bb_na_col <- colSums(is.na(bb))
#
# #Column Names with their Column Index
which(bb_na_col != 0)
#
# #Number of NA in each Column
bb_na_col[which(bb_na_col != 0)]
#
# #How many rows contain NA
sum(!complete.cases(bb))
#
# #Indices of Rows with NA
head(which(!complete.cases(bb)))
#
```

### summary() NA 

- \textcolor{orange}{Caution:} 
  - summary() does not identify NA in character column but shows them in factor
  - table() does not identify NA by default but shows them by 'useNA'

```{r 'B16-SummaryNA'}
ii <- bb
summary(ii$brand)
summary(factor(ii$brand))
#
# #table() by default does not show NA even in factor. However it has 'useNA' option
table(ii$brand)
table(factor(ii$brand))
table(ii$brand, useNA = "always")
```

### Mean and Median Replacement

- \textcolor{pink}{`na.rm = TRUE`}: All NA values will be ignored for the calculation.

```{r 'B16-ReplaceMean'}
ii <- bb
# #Mean Replacement
#ii %>% mutate(across(mpg, ~ replace(., is.na(.), round(mean(mpg, na.rm = TRUE), 2))))
ii$mpg[which(is.na(ii$mpg))] <- round(mean(ii$mpg, na.rm = TRUE), digits = 2)
jj <- ii
#
# #Median Replacement
ii <- bb
#ii %>% mutate(across(mpg, replace_na, round(median(mpg, na.rm = TRUE), 2)))
ii$mpg[which(is.na(ii$mpg))] <- round(median(ii$mpg, na.rm = TRUE), digits = 2)
```

### Mode Replacement 

```{r 'B16-ReplaceMode'}
table(bb$brand, useNA = "always")
bb %>% group_by(brand) %>% summarise(n()) 
#
# #Mode Replacement
#ii$brand[which(is.na(ii$brand))] <- f_getMode(ii$brand)
ii$brand[which(is.na(ii$brand))] <- "US"
#
# #Caution: Do not use max() on "character" for mode replacement
# #It will only look for ASCII value of letters
ii <- c("a", "z", "c", "b", "b", "b", "USA", NA, "a")
max(ii, na.rm = TRUE) #Wrong Value
f_getMode(ii) 
```

### md.pattern() 

- \textcolor{pink}{mice::md.pattern()}
  - It gives Number of NA in each column and those Rows which have missing NA
  - This is useful when there is correlation in the missing values of two or more columns 

```{r 'B16-MICE'}
# #Convert to Factor before using MICE
bb$brand <- factor(bb$brand)
#
# #mice::md.pattern() 
na_bb <- md.pattern(bb, plot = FALSE)
na_bb
```

```{r 'B16P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B16P01", "-Cars-NA-Pattern", ".png")
if(!file.exists(loc_png)) {
  png(filename = loc_png)
  #dev.control('enable') 
  md.pattern(bb, plot = TRUE, rotate.names = TRUE)
  title("Cars: Inserted Missing Value Pattern by md.pattern()", line = 2, adj = 0)
  title(sub="B16P01", line=4, adj = 1)
  B16P01 <- recordPlot()
  dev.off()
}
```

```{r 'B16P01', echo=FALSE, fig.cap="(B16P01) Cars: Inserted Missing Value Pattern by md.pattern()"}
knitr::include_graphics(paste0(.z$PX, "B16P01", "-Cars-NA-Pattern", ".png"))
```

### Seed {#seed-b16}

- \textcolor{pink}{set.seed()}
  - Random Number Generation can be fixed by a Seed for Reproducibility or Replication
  - The number given as seed '3' is not meaningful. It can be anything. 
  - However, it is recommended to use same number as seed throughout the calculations to avoid perception of fixing the values.
  
- \textcolor{pink}{Question:} Is it for learning purpose and not for real world data
  - No, it is required for reproducibility 


```{r 'B16-Seed'}
# #Choose Two Numbers from 1:10, Randomly
sample(1:10, 2)
sample(1:10, 2)
sample(1:10, 2)
# #All above calls to generate Two random numbers produce different outcomes
# #Using set.seed() we can regenerate same random numbers everytime
set.seed(3)
sample(1:10, 2) 
sample(1:10, 2)
#
# #If we re-fix the seed, the counter works along same pathway and re-generate numbers
set.seed(3)
sample(1:10, 2)
sample(1:10, 2)
```

### mice() {.tabset .tabset-fade}

#### MICE {.unlisted .unnumbered}

- \textcolor{pink}{package:mice} is used for imputation
  - "Multivariate Imputation by Chained Equations"
    - It does imputation of a factor vector based on a numeric vector
  - \textcolor{pink}{Question:} What happens if any categorical variable is associated with multiple numeric values. For example, if Car Honda has multiple mileage values
    - It will look at all other columns of data and based on these multiple columns identify a pattern which will be used for imputation.
  - Advantage:
    - During the mean replacement, we are using only one column for imputation. MICE is more robust because it looks for pattern in multiple columns
  - m = 2 is number of imputed sets. It shows up as "imp" column 1 2 in the output. It does not mean that only 2 columns have NA.
  - iter from 1 to 5 is number of iterations
  - \textcolor{orange}{Caution:} Check if the column is "factor". A "character" column will not be imputed and it will retain its NA.
  - \textcolor{pink}{Question:} For a categorical variable does it always give 'mode'
    - NO, it looks at the pattern based on other variables

```{r 'B16-Mice'}
# #Convert to Factor before using MICE
bb$brand <- factor(bb$brand)

# #mice() for imputation
# #Including all relevant data i.e. skipping Serial Number only
impute <- mice(bb[ , 2:9], m = 2, seed = 3)
#
print(impute)
#
# #For each iteration we have a different set of imputed data 
# #e.g. for 'mpg' in two sets values are
impute$imp$mpg
#
# #NOTE: Original Values that were removed earlier 
tibble(mpg = bb_22, brand = bb_39, cylinders = bb_43)
#
# #Complete First Set
set1_bb <- complete(impute, 1)
tibble(mpg = set1_bb$mpg[2], brand = set1_bb$brand[3], cylinders = set1_bb$cylinders[4])
#
# #Complete Second Set
set2_bb <- complete(impute, 2)
tibble(mpg = set2_bb$mpg[2], brand = set2_bb$brand[3], cylinders = set2_bb$cylinders[4])
```


#### MICE More {.unlisted .unnumbered}

```{r 'B16-SeedMice'}
set.seed(3)
ii <- mice(bb[ , 2:9], m = 3)
set.seed(3)
jj <- mice(bb[ , 2:9], m = 3)
#
# #identical() is FALSE but all.equal() is TRUE
identical(ii, jj)
all.equal(ii, jj)
#
# #Similarly
ii <- mice(bb[ , 2:9], m = 3, seed = 3)
jj <- mice(bb[ , 2:9], m = 3, seed = 3)
identical(ii, jj)
all.equal(ii, jj)
```

#### Warning Logged Event {.unlisted .unnumbered}

[(External) MICE Package Author](https://stefvanbuuren.name/fimd/sec-toomany.html "https://stefvanbuuren.name")

- The loggedEvents component of the 'mids' object is a data frame with five columns. 
  - 'it' 'im' stand for iteration and imputation number
  - 'dep' contains the name of the target variable, and is left blank at initialization
  - 'meth' signals the type of problem
    - 'constant' : oversized representation of a single value
      - This also comes up if the column is "character" and not converted into "factor"
    - 'collinear': The column is duplicate of another column
    - 'pmm'      : "ForLater" Unknown for now
  - 'out' contains the names of the removed variables. 
  - In general, strive for zero entries, in which case the loggedEvent component is equal to NULL.
- Guidance
  - Inspect all complete variables for forgotten missing data marks. Repair or remove these variables. Even one forgotten mark may ruin the imputation model. Remove outliers with improbable values.
  - Obtain insight into the strong and weak parts of the data by studying the influx-outflux pattern. Unless they are scientifically important, remove variables with low outflux, or with high fractions of missing data.
  - Perform a dry run with maxit=0 and inspect the logged events produced by mice(). Remove any constant and collinear variables before imputation.
  - Find out what will happen after the data have been imputed. Determine a set of variables that are important in subsequent analyses, and include these as predictors in all models. Transform variables to improve predictability and coherence in the complete-data model.
  - Run quickpred(), and determine values of mincor and minpuc such that the average number of predictors is around 25.
  - After imputation, determine whether the generated imputations are sensible by comparing them to the observed information, and to knowledge external to the data. Revise the model where needed.
  - Document your actions and decisions, and obtain feedback from the owner of the data.


```{r 'B16-WarningMice'}
# #Using the "character" to generate the Warning
ii <- aa
ii$mpg[2] <- ii$brand[3] <- ii$cylinders[4] <- NA
#
tryCatch(expr = {
  jj <- mice(ii[ , 2:9], m = 1, seed = 3)
  }, warning = function(w) {
	print(paste0(w))
  })
#
# #Warning message: Number of logged events
# #It can occur because of variety of issues in the data
jj$loggedEvents
```


## Outliers

Refer [Outliers: C03](#outliers-c03 "c03") and [Outliers: B12](#outliers-b12 "b12")

- How do we detect and deal with outliers
  - Use Visualisations for detecting outliers

### Histogram {.tabset .tabset-fade}

```{r 'B16-Histogram', include=FALSE}
# #Histogram
#bb <- na.omit(xxflights$air_time)
hh <- tibble(ee = aa$weightlbs)
ttl_hh <- "Cars: Histogram of Weight"
cap_hh <- "B16P02"
# #Basics
median_hh <- round(median(hh[[1]]), 1)
mean_hh <- round(mean(hh[[1]]), 1)
sd_hh <- round(sd(hh[[1]]), 1)
len_hh <- nrow(hh)
#
B16 <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) + 
  geom_histogram(bins = 50, alpha = 0.4, fill = '#FDE725FF') + 
  geom_vline(aes(xintercept = mean_hh), color = '#440154FF') +
  geom_text(data = tibble(x = mean_hh, y = -Inf, 
                          label = paste0("Mean= ", mean_hh)), 
            aes(x = x, y = y, label = label), 
            color = '#440154FF', hjust = -0.5, vjust = 1.3, angle = 90) +
  geom_vline(aes(xintercept = median_hh), color = '#3B528BFF') +
  geom_text(data = tibble(x = median_hh, y = -Inf, 
                          label = paste0("Median= ", median_hh)), 
            aes(x = x, y = y, label = label), 
            color = '#3B528BFF', hjust = -0.5, vjust = -0.7, angle = 90) +
  theme(plot.title.position = "panel") + 
  labs(x = "x", y = "Frequency", 
       subtitle = paste0("(N=", len_hh, "; ", "Mean= ", mean_hh, 
                         "; Median= ", median_hh, "; SD= ", sd_hh,
                         ")"), 
        caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B16)
rm(B16)
```

```{r 'B16P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B16P02", "-Cars-Hist-Weight", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B16P02, device = "png", dpi = 144) 
}
```

```{r 'B16P02', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B16P02", "-Cars-Hist-Weight", ".png"))
```

```{r 'B16-Density', include=FALSE}
# #Density Curve
ttl_hh <- "Cars: Density Plot of Weight"
cap_hh <- "B16P03"
# #Get Quantiles and Ranges of mean +/- sigma 
q05_hh <- quantile(hh[[1]], .05)
q95_hh <- quantile(hh[[1]], .95)
density_hh <- density(hh[[1]])
density_hh_tbl <- tibble(x = density_hh$x, y = density_hh$y)
sig3r_hh <- density_hh_tbl %>% filter(x >= {mean_hh + 3 * sd_hh})
sig3l_hh <- density_hh_tbl %>% filter(x <= {mean_hh - 3 * sd_hh})
sig2r_hh <- density_hh_tbl %>% filter(x >= {mean_hh + 2 * sd_hh}, {x < mean_hh + 3 * sd_hh})
sig2l_hh <- density_hh_tbl %>% filter(x <= {mean_hh - 2 * sd_hh}, {x > mean_hh - 3 * sd_hh})
sig1r_hh <- density_hh_tbl %>% filter(x >= {mean_hh + sd_hh}, {x < mean_hh + 2 * sd_hh})
sig1l_hh <- density_hh_tbl %>% filter(x <= {mean_hh - sd_hh}, {x > mean_hh - 2 * sd_hh})
sig0r_hh <- density_hh_tbl %>% filter(x > mean_hh, {x < mean_hh + 1 * sd_hh})
sig0l_hh <- density_hh_tbl %>% filter(x < mean_hh, {x > mean_hh - 1 * sd_hh})
#
# #Change x-Axis Ticks interval
xbreaks_hh <- seq(-3, 3)
xpoints_hh <- mean_hh + xbreaks_hh * sd_hh
# #Arrow
arr_y <- 0.0005 #mean(density_hh_tbl$y) #
arr_lst <- list(list("99.7%", xpoints_hh[1], xpoints_hh[7], arr_y),
                list("95.4%", xpoints_hh[2], xpoints_hh[6], arr_y),
                list("68.3%", xpoints_hh[3], xpoints_hh[5], arr_y))
arr_hh <- arr_lst[[1]]
#
# # Latex Labels 
xlabels_hh <- c(TeX(r'($\,\,\mu - 3 \sigma$)'), TeX(r'($\,\,\mu - 2 \sigma$)'), 
                TeX(r'($\,\,\mu - 1 \sigma$)'), TeX(r'($\mu$)'), TeX(r'($\,\,\mu + 1 \sigma$)'), 
                TeX(r'($\,\,\mu + 2 \sigma$)'), TeX(r'($\,\,\mu + 3\sigma$)'))
#
B16 <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) + 
  geom_density(alpha = 0.2, colour = "#21908CFF") + 
  geom_area(data = sig3l_hh, aes(x = x, y = y), fill = '#440154FF') + 
  geom_area(data = sig3r_hh, aes(x = x, y = y), fill = '#440154FF') + 
  geom_area(data = sig2l_hh, aes(x = x, y = y), fill = '#3B528BFF') + 
  geom_area(data = sig2r_hh, aes(x = x, y = y), fill = '#3B528BFF') + 
  geom_area(data = sig1l_hh, aes(x = x, y = y), fill = '#21908CFF') + 
  geom_area(data = sig1r_hh, aes(x = x, y = y), fill = '#21908CFF') + 
  geom_area(data = sig0l_hh, aes(x = x, y = y), fill = '#5DC863FF') + 
  geom_area(data = sig0r_hh, aes(x = x, y = y), fill = '#5DC863FF') + 
  #scale_y_continuous(limits = c(0, 0.009), breaks = seq(0, 0.009, 0.003)) +
  scale_y_continuous(labels = function(n){format(n, scientific = FALSE)}) +
  scale_x_continuous(breaks = xpoints_hh, labels = xlabels_hh) + 
  annotate("segment", x = xpoints_hh[4] - 0.5 * sd_hh, xend = arr_hh[[2]], y = arr_hh[[4]], 
            yend = arr_hh[[4]], arrow = arrow(type = "closed", length = unit(0.02, "npc"))) + 
  annotate("segment", x = xpoints_hh[4] + 0.5 * sd_hh, xend = arr_hh[[3]], y = arr_hh[[4]], 
            yend = arr_hh[[4]], arrow = arrow(type = "closed", length = unit(0.02, "npc"))) + 
  annotate(geom = "text", x = xpoints_hh[4], y = arr_hh[[4]], label = arr_hh[[1]]) + 
  theme(plot.title.position = "panel") + 
  labs(x = "x", y = "Density", 
       subtitle = paste0("(N=", nrow(.), "; ", "Mean= ", round(mean(.[[1]]), 1), 
                         "; Median= ", round(median(.[[1]]), 1), "; SD= ", round(sd(.[[1]]), 1),
                         ")"), 
        caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B16)
rm(B16)
```

```{r 'B16P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B16P03", "-Cars-Dens-Weight", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B16P03, device = "png", dpi = 144) 
}
```

```{r 'B16P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B16P03", "-Cars-Dens-Weight", ".png"))
```

#### Image {.unlisted .unnumbered}

```{r 'B16P0203', echo=FALSE, ref.label=c('B16P02', 'B16P03'), fig.cap="(B16P02 B16P03) Cars: Histogram and Density of Weight (lbs)"}
#
```

#### hist() {.unlisted .unnumbered}

```{r 'B16-PlotHist', eval=FALSE}
# Set up the plot area to visualise multiple 3 plots simultaneously
par(mfrow = c(1, 3))
# Create the histogram bars
hist(aa$weightlbs,
     breaks = 30,
     xlim = c(0, 5000),
     col = "blue",
     border = "black",
     ylim = c(0, 40),
     xlab = "Weight",
     ylab = "Counts",
     main = "Histogram of Car Weights")
# Make a box around # the plot
box(which = "plot",
    lty = "solid",
    col = "black")
```

#### Code Histogram {.unlisted .unnumbered}

```{r 'B16-Histogram-A', eval=FALSE, ref.label=c('B16-Histogram')}
#
```

#### Code Density {.unlisted .unnumbered}

```{r 'B16-Density-A', eval=FALSE, ref.label=c('B16-Density')}
#
```

### ScatterPlot {.tabset .tabset-fade}

```{r 'B16-ScatterAll', include=FALSE}
# #Scatterplot, Trendline Equation, R2, mean x & y
hh <- tibble(x = aa$weightlbs, y = aa$mpg)
ttl_hh <- "Cars: Scatterplot of Weight (x) vs MPG (y)"
cap_hh <- "B16P04"
x_hh <- "Weight"
y_hh <- "MPG"
```

```{r 'B16-ScatterAll-A', include=FALSE, ref.label=c('B16-ScatterAll', 'B16-ScatterPlot')}
#
```

```{r 'B16P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B16P04", "-Cars-Scatter-All", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B16P04, device = "png", dpi = 144) 
}
```

```{r 'B16P04', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B16P04", "-Cars-Scatter-All", ".png"))
```

```{r 'B16-ScatterMod', include=FALSE}
# #Excluded Outliers
hh <- tibble(x = aa$weightlbs, y = aa$mpg) %>% filter(x > min(x) & y < max(y))
ttl_hh <- "Cars: Scatterplot of Weight (x) vs MPG (y) - Excluded 2 outliers"
cap_hh <- "B16P05"
x_hh <- "Weight"
y_hh <- "MPG"
```

```{r 'B16-ScatterMod-A', include=FALSE, ref.label=c('B16-ScatterMod', 'B16-ScatterPlot')}
#
```

```{r 'B16P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B16P05", "-Cars-Scatter-Mod", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B16P05, device = "png", dpi = 144) 
}
```

```{r 'B16P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B16P05", "-Cars-Scatter-Mod", ".png"))
```

#### Image {.unlisted .unnumbered}

```{r 'B16P0405', echo=FALSE, ref.label=c('B16P04', 'B16P05'), fig.cap="(B16P04 B16P05) Cars: Scatterplot of Weight (x) vs MPG (y) with and without the two outliers"}
#
```

#### plot() {.unlisted .unnumbered}


```{r 'B16-PlotScatter', eval=FALSE}
# Create a Scatterplot
plot(aa$weightlbs,
     aa$mpg,
     xlim = c(0, 5000),
     ylim = c(0, 600),
     xlab = "Weight",
     ylab = "MPG",
     main = "Scatterplot of MPG by Weight",
     type = "p", #Points
     pch = 16,
     col = "blue")
#Add open black
# circles
points(aa$weightlbs,
       aa$mpg,
       type = "p",
       col = "black")
```


#### Code ScatterPlot {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B16-ScatterPlot', eval=FALSE}
# #IN: hh$x, hh$y, ttl_hh, cap_hh, x_hh, y_hh
# #Formula for Trendline calculation
k_gg_formula <- y ~ x
#
B16 <- hh %>% { ggplot(data = ., aes(x = x, y = y)) + 
    geom_smooth(method = 'lm', formula = k_gg_formula, se = FALSE) +
    stat_poly_eq(aes(label = paste0("atop(", ..eq.label.., ", \n", ..rr.label.., ")")), 
                 formula = k_gg_formula, eq.with.lhs = "italic(hat(y))~`=`~",
                 eq.x.rhs = "~italic(x)", parse = TRUE) +
    geom_vline(aes(xintercept = round(mean(x), 3)), color = '#440154FF', linetype = "dashed") +
    geom_hline(aes(yintercept = round(mean(y), 3)), color = '#440154FF', linetype = "dashed") +
    geom_text(data = tibble(x = mean(.[["x"]]), y = -Inf, 
                            label = TeX(r'($\bar{x}$)', output = "character")), 
              aes(x = x, y = y, label = label), 
              size = 4, color = '#440154FF', hjust = 1.5, vjust = -1, parse = TRUE ) +
    geom_text(data = tibble(x = 0, y = mean(.[["y"]]), 
                            label = TeX(r'($\bar{y}$)', output = "character")), 
              aes(x = x, y = y, label = label), 
              size = 4, color = '#440154FF', hjust = 1.5, vjust = 1.5, parse = TRUE ) +
    geom_point() +
    k_gglayer_scatter +
    labs(x = x_hh, y = y_hh,
        #subtitle = TeX(r"(Trendline Equation, $R^{2}$, $\bar{x}$ and $\bar{y}$)"), 
        caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B16)
rm(B16)
```

### BoxPlot {.tabset .tabset-fade}

- Interquartile (IQR) based approach for identification of Outliers
  - Refer [Precentiles](#percentiles-c03 "c03") for Percentil, Quartile and IQR
  - IQR = Q3 - Q1
  - Any data point not in [Q1 - 1.5 * IQR, Q3 + 1.5 * IQR] is an outlier
- \textcolor{pink}{Question:} Why sometimes the y-axis shows range of ~500 and sometimes ~50
  - If the 1 extreme outlier of mpg column is kept then the axis will show upto 500
  - If the point is deleted from the data, then the axis values are in range of 50 

```{r 'B16-BoxPlot', include=FALSE}
# #BoxPlot
hh <- aa %>% select(mpg, cylinders) %>% filter(!cylinders %in% c(3, 5)) %>% 
  filter(mpg < max(mpg)) %>% mutate(across(cylinders, factor)) 
#
ttl_hh <- "BoxPlot of MPG (excluding 1 point) vs. Cylinders (4, 6, 8)"
cap_hh <- "B16P06"
x_hh <- "Cylinders"
y_hh <- "MPG"
#
B16 <- hh %>% { ggplot(data = ., mapping = aes(x = cylinders, y = mpg, fill = cylinders)) +
    geom_boxplot(outlier.shape = NA) +
    geom_point(position = position_jitterdodge(jitter.width = 0.1), size = 1, alpha = 0.7) + 
    k_gglayer_box +
    theme(legend.position = 'none') +
    labs(x = x_hh, y = y_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B16)
rm(B16)
```

```{r 'B16P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B16P06", "-Cars-BoxPlot", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B16P06, device = "png", dpi = 144) 
}
```

#### Image {.unlisted .unnumbered}

```{r 'B16P06', echo=FALSE, fig.cap="(B16P06) Cars: BoxPlot of MPG (excluding 1 point) vs. Cylinders (4, 6, 8)"}
knitr::include_graphics(paste0(.z$PX, "B16P06", "-Cars-BoxPlot", ".png")) #iiii
```

#### boxplot() {.unlisted .unnumbered}

```{r 'B16-PlotBox', eval=FALSE}
boxplot(mpg ~ cyl, data = aa, xlab = "Number of Cylinders",
        ylab = "Miles Per Gallon", main = "Mileage Data")
```

#### Code BoxPlot {.unlisted .unnumbered}


```{r 'B16-BoxPlot-A', eval=FALSE, ref.label=c('B16-BoxPlot')}
#
```

## Numerical Methods for detecting Outliers 

### IQR Based {.tabset .tabset-fade}

- NOTE:
  - In 'weightlbs' there is no point outside IQR, thus no data point was eliminated
  - So specifically Cylinder 6 data points were selected for outlier detection and removal

#### All Values (No Outlier) {.unlisted .unnumbered}

```{r 'B16-IQRAll'}
bb <- aa 
dim(bb)
#
# #summary() or quantile()
summary(bb$weightlbs)
q_bb <- quantile(bb$weightlbs, probs = c(.25, .75), na.rm = TRUE)
q_bb
#
iqr_bb <- IQR(bb$weightlbs)
iqr_bb
#
upp_bb <- q_bb[2] + 1.5 * iqr_bb 
low_bb <- q_bb[1] - 1.5 * iqr_bb 
#
kept_bb <- bb[bb$weightlbs >= low_bb & bb$weightlbs <= upp_bb, ]
if(nrow(bb) == nrow(kept_bb)) {
  cat(paste0("No Point was removed because none was outside the range.\n"))
  } else cat(paste0("Number of Points removed = ", nrow(bb) - nrow(kept_bb), "\n"))
```

#### Cylinder = 6 (1 Outlier) {.unlisted .unnumbered}

```{r 'B16-IQR6'}
bb <- aa %>% filter(cylinders == 6)
dim(bb)
#
# #summary() or quantile()
summary(bb$weightlbs)
q_bb <- quantile(bb$weightlbs, probs = c(.25, .75), na.rm = TRUE)
q_bb
#
iqr_bb <- IQR(bb$weightlbs)
iqr_bb
#
upp_bb <- q_bb[2] + 1.5 * iqr_bb 
low_bb <- q_bb[1] - 1.5 * iqr_bb 
#
kept_bb <- bb[bb$weightlbs >= low_bb & bb$weightlbs <= upp_bb, ]
if(nrow(bb) == nrow(kept_bb)) {
  cat(paste0("No Point was removed because none was outside the range.\n"))
  } else cat(paste0("Number of Points removed = ", nrow(bb) - nrow(kept_bb), "\n"))
```

### Z-score Standardisation {.unlisted .unnumbered}

To be continued ...


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B16-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ee, hh, ii, jj, kk, ll, mm, nn, oo, rr, vv, xx, yy, zz, B16P01, bb_22, bb_39, 
          bb_43, bb_na_col, loc_png, na_bb, xxB16Cars, B16P02, cap_hh, impute, len_hh, mean_hh,
          median_hh, sd_hh, set1_bb, set2_bb, ttl_hh, arr_hh, arr_lst, arr_y, B16P03, B16P04, 
          B16P05, density_hh, density_hh_tbl, q05_hh, q95_hh, sig0l_hh, sig0r_hh, sig1l_hh, 
          sig1r_hh, sig2l_hh, sig2r_hh, sig3l_hh, sig3r_hh, x_hh, xbreaks_hh, xlabels_hh, 
          xpoints_hh, y_hh, B16P06, iqr_bb, kept_bb, low_bb, q_bb, upp_bb)
```

```{r 'B16-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/216-Processing.Rmd-->

# Data Preprocessing (B17, Oct-31) {#b17}

```{r 'B17', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
invisible(lapply(f_getPathR(A10getUtil), knitr::read_chunk))
```

## Data

\textcolor{pink}{Please import the "B16-Cars2.csv"}

```{r 'B17-ImportData', include=FALSE}
xxB16Cars <- f_getRDS(xxB16Cars)
bb <- aa <- xxB16Cars
```

## Z-score Standardisation {.tabset .tabset-fade}

```{r 'B17D01', comment="", echo=FALSE, results='asis'}
f_getDef("z-Scores")
```

- \@ref(eq:z-scores) $z_i = \frac{{x}_i - {\overline{x}}}{{s}}$
  - $z_i \notin \pm 3 \to z_i \in \text{Outlier}$ 
  - After Standardisation, we can compare values of different orders because these would be scaled into a dimensionless quantity
  - While comparing the Eucledean distance between Two Variables like Age and Income, the standardisation allow these to be scaled to a similar range
    - Ex: If Age range is 20-30 but Income range is 10k-30k, directly using these values will ignore any impact of change in Age.
  - For some data mining algorithms, large differences in the ranges will lead to a tendency for the variable with greater range to have undue influence on the results. Therefore, these numeric variables should be normalised, in order to standardize the scale of effect each variable has on the results. 
  - Scaling also benefits Neural networks, and algorithms that make use of distance measures, such as the k-nearest neighbors algorithm. 
  - Note that Mean itself includes the impact of extreme values thus it is not very robust. IQR is better because it is based on position
  - Standardisation does not convert the non-normal data to normal. It does not change the shape of the data. Outliers remain outliers, Skewness remain in the data. It simply changes the Scale.
- \textcolor{pink}{Question:} Can we use different methods for outlier identification on different variables
  - Yes, you can remove outliers of one variable usig IQR and of another variable using standardisation
- \textcolor{pink}{Question:} From the Normalised values, can be convert back to original data
  - Tiring process
  - (Aside) The scales() function attaches mean and sd as attributes to the output matrix. That can be used to convert back the data.

- \textcolor{pink}{scale()}
  - It converts any vector to standard normal
- (Aside) 
  - Scaling is less effective if the outliers are present. Extremely low value (e.g. 192.5 Weight) or  extremely high value (527 Mileage) are obviously going to impact the scale applied. Probably, it is better to follow: Scaling | Outlier Treatment (Identification, Removal or Imputation) | Scaling of original data with Outlier Treatment


### Normalisation {.unlisted .unnumbered}

```{r 'B17-Normalisation'}
# #Normalising Weight
bb <- aa %>% select(weightlbs) %>% mutate(z = as.vector(scale(weightlbs)))
str(bb)
#
# #Excluding Outliers
kept_bb <- bb[bb$z >= -3 & bb$z <= 3, ]
str(kept_bb)
#
# #Similarly with mpg
kept_bb <- aa %>% select(mpg) %>% mutate(z = as.vector(scale(mpg))) %>% filter(z >= -3 & z <= 3)
str(kept_bb)
summary(kept_bb)
```

### scale() {.unlisted .unnumbered}

```{r 'B17-Scale'}
# #scale(x, center = TRUE, scale = TRUE) output is Nx1 Matrix
bb <- aa %>% select(weightlbs) 
ii <- bb %>% mutate(z = as.vector(scale(weightlbs)))
#bb %>% mutate(z = across(weightlbs, scale)) #matrix
#bb %>% mutate(z = across(weightlbs, ~ as.vector(scale(.)))) #tibble
jj <- bb %>% mutate(across(weightlbs, list(z = ~ as.vector(scale(.))), .names = "{.fn}"))
kk <- bb
kk$z <- as.vector(scale(kk$weightlbs))
stopifnot(all(identical(ii, jj), identical(ii, kk)))
```

## Min-Max Scaling 

- $x^* = \frac{{x}_i - \text{min}(x)}{\text{range}(x)} = \frac{{x}_i - \text{min}(x)}{\text{max}(x) - \text{min}(x)} \to x^* \in [0, 1]$
  - This is for scaling only, not for removal of outliers

```{r 'B17-ScalingMinMax'}
# #Min-Max Scaling
min_aa <- min(aa$weightlbs)
max_aa <- max(aa$weightlbs)
bb <- aa %>% select(weightlbs) %>% mutate(z = {weightlbs - min_aa}/{max_aa - min_aa})
str(bb)
```

## Decimal Scaling 

- $x^* = \frac{{x}_i}{10^d} \to x^* \in [-1, 1]$
  - Where 'd' represents the number of digits in the largest absolute value i.e. if max(abs(x)) is 6997, d will be 4


```{r 'B17-ScalingDecimal'}
# #Count Digits in Maximum (NOTE: Take care of NA, 0, [-1, 1] values)
d_bb <- 10^{floor(log10(max(abs(aa$weightlbs)))) + 1}
# #Decimal Scaling
bb <- aa %>% select(weightlbs) %>% mutate(z = weightlbs/d_bb)
str(bb)
```

## Comparison {.tabset .tabset-fade}

### Histogram {.unlisted .unnumbered}

```{r 'B17-Histogram', include=FALSE}
# #Histogram
bb <- aa %>% select(weightlbs) %>% mutate(z = as.vector(scale(weightlbs)))
hh <- tibble(ee = bb$z)
ttl_hh <- "Cars: Histogram of Weight (Scaled)"
cap_hh <- "B17P01"
# #Basics
median_hh <- round(median(hh[[1]]), 1)
mean_hh <- round(mean(hh[[1]]), 1)
sd_hh <- round(sd(hh[[1]]), 1)
len_hh <- nrow(hh)
#
B17 <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) + 
  geom_histogram(bins = 50, alpha = 0.4, fill = '#FDE725FF') + 
  geom_vline(aes(xintercept = mean_hh), color = '#440154FF') +
  geom_text(data = tibble(x = mean_hh, y = -Inf, 
                          label = paste0("Mean= ", mean_hh)), 
            aes(x = x, y = y, label = label), 
            color = '#440154FF', hjust = -0.5, vjust = 1.3, angle = 90) +
  geom_vline(aes(xintercept = median_hh), color = '#3B528BFF') +
  geom_text(data = tibble(x = median_hh, y = -Inf, 
                          label = paste0("Median= ", median_hh)), 
            aes(x = x, y = y, label = label), 
            color = '#3B528BFF', hjust = -0.5, vjust = -0.7, angle = 90) +
  theme(plot.title.position = "panel") + 
  labs(x = "x", y = "Frequency", 
       subtitle = paste0("(N=", len_hh, "; ", "Mean= ", mean_hh, 
                         "; Median= ", median_hh, "; SD= ", sd_hh,
                         ")"), 
        caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B17)
rm(B17)
```

```{r 'B17P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B17P01", "-Cars-Hist-Weight-Scaled", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B17P01, device = "png", dpi = 144) 
}
```

```{r 'B17P01', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B17P01", "-Cars-Hist-Weight-Scaled", ".png"))
```

```{r 'B17P0102', echo=FALSE, ref.label=c('B16P02', 'B17P01'), fig.cap="(B16P02 B17P01) Cars: Histogram of Weight (Original vs Scaled)"}
# #Ref another file chunk
```

### hist() {.unlisted .unnumbered}

```{r 'B17-PlotHist', eval=FALSE}
par(mfrow = c(1,2))
# Create two histograms
hist(bb$weightlbs, breaks = 20,
     xlim = c(1000, 5000),
     main = "Histogram of Weight",
     xlab = "Weight",
     ylab = "Counts")
box(which = "plot",
    lty = "solid",
    col = "black")
#
hist(bb$z,
     breaks = 20,
     xlim = c(-2, 3),
     main = "Histogram of Zscore
of Weight",
     xlab = "Z-score of Weight",
     ylab = "Counts")
box(which = "plot",
    lty = "solid",
    col = "black")
```

## Skewness

- Refer [Skewness](#skewness-c03 "c03")
- Scaling does not change the skewness

```{r 'B17-Skewness'}
# #Skewness
bb <- aa %>% select(weightlbs) %>% mutate(z = as.vector(scale(weightlbs)))
ii <- bb$weightlbs
#
3 * {mean(ii) - median(ii)} / sd(ii)
#
ii <- bb$z
3 * {mean(ii) - median(ii)} / sd(ii)
```

## Non-linear Transformations {.tabset .tabset-fade}

- It is done for conversion of non-normal data to normal
  - Note that scaling is linear transformation
- Transformations
  - Sqare Root - \textcolor{pink}{sqrt()}
  - Log - \textcolor{pink}{log(), log10()}
  - Inverse Sqare Root
- 

```{r 'B17-Transformations'}
bb <- aa %>% select(weightlbs) %>% 
  mutate(z = as.vector(scale(weightlbs)), Sqrt = sqrt(weightlbs),
         Log = log(weightlbs), InvSqr = 1/Sqrt)
#
# #Check Skewness
vapply(bb, function(x) round(3 * {mean(x) - median(x)} / sd(x), 3), numeric(1))
```

### Histogram {.unlisted .unnumbered}

```{r 'B17-FacetWrap', include=FALSE}
# #Histogram
bb <- aa %>% select(weightlbs) %>% 
  mutate(z = as.vector(scale(weightlbs)), Sqrt = sqrt(weightlbs), 
         Log = log(weightlbs), InvSqr = 1/Sqrt) %>% 
  pivot_longer(everything(), names_to = "Key", values_to = "Values") %>% 
  mutate(across(Key, factor, levels = c("Sqrt", "Log", "InvSqr", "weightlbs", "z"), 
    labels = c("Square Root", "Natural Log", "Inverse Square", 
               "Original Weight", "Scaled Weight")))
#
hh <- bb
mean_hh <- hh %>% group_by(Key) %>% summarize(Mean = mean(Values))
#
ttl_hh <- "Cars: Weight with Transformed values and Mean"
cap_hh <- "B17P03"
#
B17 <- hh %>% { ggplot(data = ., mapping = aes(Values)) + 
    geom_histogram(bins = 50, alpha = 0.4, fill = '#FDE725FF') + 
    geom_vline(data = mean_hh, aes(xintercept = Mean), color = '#440154FF') +
	  geom_text(data = mean_hh, aes(x = Mean, y = -Inf, label = paste0("Mean= ", f_pNum(Mean))), 
            color = '#440154FF', hjust = -0.5, vjust = 1.3, angle = 90) +
    facet_wrap(~Key, scales = 'free_x') +
    theme(plot.title.position = "panel") + 
    labs(x = "x", y = "Frequency", caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B17)
rm(B17)
```

```{r 'B17P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B17P03", "-Cars-Weight-Transform", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B17P03, device = "png", dpi = 144) 
}
```

```{r 'B17P03', echo=FALSE, out.width='100%', fig.cap="(B17P03) Cars: Weight Transformed with Original & Scaled"}
knitr::include_graphics(paste0(.z$PX, "B17P03", "-Cars-Weight-Transform", ".png"))
```

### facet_wrap() {.unlisted .unnumbered}

```{r 'B17-FacetWrap-A', ref.label=c('B17-FacetWrap'), eval=FALSE}
#
```

### f_pNum() {.unlisted .unnumbered}

```{r 'B17F-pNum', ref.label=c('A10B-pNum'), eval=FALSE}
#
```

### hist() {.unlisted .unnumbered}

```{r 'B17-PlotHistInvSqare', eval=FALSE}
# #Histogram with Normal Distribution Overlay
par(mfrow=c(1,1))
hist(bb$InvSqr,
     breaks = 30,
     xlim=c(0.0125, 0.0275),
     col = "lightblue",
     prob = TRUE,
     border = "black",
     xlab="Inverse Square Root of Weight",
     ylab = "Counts",
     main = "Histogram of Inverse Square Root of Weight")
box(which = "plot",
    lty = "solid",
    col="black")
# #Overlay Normal density
lines(density(bb$InvSqr), col="red")
```


## QQ Plot {.tabset .tabset-fade}

QQ (quantile-quantile) plot is a probability plot for comparing two probability distributions by plotting their quantiles against each other. A point $(x, y)$ on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). If the two distributions being compared are similar, the points in the QQ plot will approximately lie on the line $y = x$.

- Refer figure \@ref(fig:B12P09)
  - The QQ Plot shows whether the values are within the specified limits of Normal Curve

### Image {.unlisted .unnumbered}

```{r 'B17-WeightQQ', include=FALSE}
# #QQ Plot
bb <- aa %>% select(weightlbs) %>% 
  filter(weightlbs > min(weightlbs)) %>% 
  mutate(z = as.vector(scale(weightlbs)), Sqrt = sqrt(weightlbs), 
         Log = log(weightlbs), InvSqr = 1/Sqrt) %>% 
  pivot_longer(everything(), names_to = "Key", values_to = "Values") %>% 
  mutate(across(Key, factor, levels = c("Sqrt", "Log", "InvSqr", "weightlbs", "z"), 
    labels = c("Square Root", "Natural Log", "Inverse Square", 
               "Original Weight", "Scaled Weight")))
#
hh <- bb
#hh %>% group_by(Key) %>% summarize(Max = max(Values), Min = min(Values))
max_hh <- min_hh <- hh %>% group_by(Key) %>% summarise(Values = 0)
#
# #Modify Number of Y-Axis Major Gridlines for Horizontal Comparison
max_hh$Values  <- c(100, 8.55, 0.0300, 5000, 2.35) #c(72, 8.55, 0.0255, 5000, 2.35)
min_hh$Values  <- c(20, 7.35, 0.0135, 1500, -1.65) #c(40, 7.35, 0.0135, 1500, -1.65)
#
ttl_hh <- "QQ Plots of Transformed Weight"
sub_hh <- "Excluded 1 Outlier and Modified Y-axis for alignment"
cap_hh <- "B17P04"
#
B17 <- hh %>% { ggplot(., aes(sample = Values)) +
    stat_qq() +
    stat_qq_line() +
    geom_blank(data=max_hh, aes(y = Values)) +
    geom_blank(data=min_hh, aes(y = Values)) +
    facet_wrap(~Key, scales = 'free') +
    scale_x_continuous(limits = c(-3, 3)) + 
    #coord_flip() +
    labs(subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B17)
rm(B17)
```

```{r 'B17P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B17P04", "-Cars-Weight-QQ", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B17P04, device = "png", dpi = 144, width = k_width, height = k_height) 
}
```

```{r 'B17P04', echo=FALSE, out.width='100%', fig.cap="(B17P04) Cars: QQ Plots of Transformed Weight"}
knitr::include_graphics(paste0(.z$PX, "B17P04", "-Cars-Weight-QQ", ".png"))
```

### Flipped Axis {.unlisted .unnumbered}

```{r 'B17-WeightQQ-Flip', include=FALSE}
ttl_hh <- "QQ Plots of Transformed Weight (Flipped)"
cap_hh <- "B17P05"
B17P05 <- B17P04 + coord_flip() + labs(caption = cap_hh, title = ttl_hh)
```

```{r 'B17P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B17P05", "-Cars-Weight-QQ-Flip", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B17P05, device = "png", dpi = 144, width = k_width, height = k_height) 
}
```

```{r 'B17P05', echo=FALSE, out.width='100%', fig.cap="(B17P05) Cars: QQ Plots of Transformed Weight"}
knitr::include_graphics(paste0(.z$PX, "B17P05", "-Cars-Weight-QQ-Flip", ".png")) #iiii
```

### Code {.unlisted .unnumbered}

```{r 'B17-WeightQQ-A', eval=FALSE, ref.label=c('B17-WeightQQ')}
#
```

### qqnorm() {.unlisted .unnumbered}

```{r 'B17-PlotQQ', eval=FALSE}
# Normal Q-Q Plot
qqnorm(bb$InvSqr,
       datax = TRUE,
       col = "red",
       ylim = c(0.01, 0.03),
       main = "Normal
Q-Q Plot of Inverse Square Root of Weight")
qqline(bb$InvSqr,
       col = "blue",
       datax = TRUE)
```

## Shapiro

```{r 'B17D02', comment="", echo=FALSE, results='asis'}
f_getDef("Shapiro-Wilk-Test") #dddd
```


- Refer [Shapiro-Wilk Test for Normality](#shapiro-c10 "c10")


### rnorm()

- \textcolor{pink}{rnorm(n, mean = 0, sd = 1)}
  - Create Vector of Random Numbers with given mean and sd

```{r 'B17-rnorm'}
set.seed(3)
ii <- rnorm(n = 100, mean = 50, sd = 5.99)
#
# #Check Normality of randomly generated Normal dataset
shapiro.test(ii)
#
# #Check Normality of Weight
ii <- aa %>% select(weightlbs) %>% 
  #filter(weightlbs > min(weightlbs)) %>%
  mutate(z = as.vector(scale(weightlbs)), Sqrt = sqrt(weightlbs), 
         Log = log(weightlbs), InvSqr = 1/Sqrt) %>% 
  pivot_longer(everything(), names_to = "Key", values_to = "Values") %>% 
  mutate(across(Key, factor, levels = c("Sqrt", "Log", "InvSqr", "weightlbs", "z"), 
    labels = c("Square Root", "Natural Log", "Inverse Square", 
               "Original Weight", "Scaled Weight")))
#
# #No Transformation was able to convert the data to Normality 
# #Even after excluding 1 outlier (Not shown here)
ii %>% group_by(Key) %>% 
  summarise(p_Shapiro = shapiro.test(Values)$p.value, 
            isNormal = ifelse(p_Shapiro > 0.05, TRUE, FALSE))
```

### cut()

- Refer [cut() for creating bins](#cut-c02 "c02")
- \textcolor{pink}{cut()} - It slightly increases the range

```{r 'B17-Cut'}
# #Continuous to Categorical (Bins)
cut_ii <- cut(aa$weightlbs, breaks = 3, dig.lab = 4, include.lowest = TRUE, ordered_result = TRUE)
levels(cut_ii)
#
# #ggplot2::cut_interval()
cut_jj <- cut_interval(aa$weightlbs, n = 3, dig.lab = 4, ordered_result = TRUE)
levels(cut_jj)
#
# #With Labels: NOTE default ordering is ascending
levels(cut(aa$weightlbs, breaks = 3, dig.lab = 4, include.lowest = TRUE, ordered_result = TRUE, 
           labels = c("low", "medium", "high")))
levels(cut_interval(aa$weightlbs, n = 3, dig.lab = 4, ordered_result = TRUE, 
                    labels = c("low", "medium", "high")))
```

## Continuos to Categorical Groups

```{r 'B17-ToCategorical'}
bb <- aa %>% select(weightlbs) %>% rename(Weight = 1)
#
# #Subsetting
# #Create Column explicitly to prevent Warning message: Unknown or uninitialised column: `ii`. 
bb$ii <- NA
bb$ii[bb$Weight >= 3000] <- 1
bb$ii[bb$Weight < 3000] <- 2
#
# #Using ifelse() or case_when()
bb <- bb %>% mutate(jj = ifelse(Weight >= 3000, 1, 2), 
                    kk = case_when(Weight >= 3000 ~ 1, Weight < 3000 ~ 2))
stopifnot(all(identical(bb$ii, bb$jj), identical(bb$ii, bb$kk)))
```

## Index

```{r 'B17-Index'}
# #Create Data
set.seed(3)
bb <- tibble(x = rnorm(n = 10, mean = 5, sd = 0.55), 
             y = rnorm(n = 10, mean = 4.5, sd = 0.66))
#
# #Basic Indexing
bb$i <- 1:nrow(bb)
# #Indexcan be started from anywhere. However it is not recommended.
bb$j <- 5:{nrow(bb) + 5L - 1L}
# 
# #Other Methods
bb$k <- seq_along(bb[[1]])
bb$l <- seq_len(nrow(bb))
bb$m <- seq.int(nrow(bb))
# #Note the placement of column at the beginning i.e. column index modified
bb <- cbind(n = 1:nrow(bb), bb)
bb <- rowid_to_column(bb, "o")
#
bb <- bb %>% mutate(p = row_number())
#
# #Excluding 'j' all other columns are equal. However, 'n' & 'o' modify column index
stopifnot(all(identical(bb$i, bb$k), identical(bb$i, bb$k), identical(bb$i, bb$l), 
  identical(bb$i, bb$m), identical(bb$i, bb$n), identical(bb$i, bb$o), identical(bb$i, bb$p)))
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B17-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ee, hh, ii, jj, kk, ll, mm, nn, oo, rr, vv, xx, yy, zz, B17P01, B17P03, B17P04, 
          B17P05, cap_hh, cut_ii, cut_jj, d_bb, kept_bb, len_hh, loc_png, max_aa, max_hh, 
          mean_hh, median_hh, min_aa, min_hh, sd_hh, sub_hh, ttl_hh, xxB16Cars)
```

```{r 'B17-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/217-Processing.Rmd-->

# Unsupervised Learning (B18, Nov-07) {#b18}

```{r 'B18', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```


## Overview

- "Unsupervised Learning Algorithms"
  - "ForLater"
  - Case Analysis of JAT is Merged in notes of [Case Study: JAT](#jat-b15 "b15")
  - NOTE: Discussion about Jupyter Notebook & Anaconda Navigator "15:45 - 16:05" is NOT covered because I am not working with it.
  - NOTE: Package 'esquisse' was not used because interactive is difficult to show in document format. "16:15 - 16:30"

## Install

```{r 'B18-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("esquisse", dependencies = TRUE)
}
```

## Data: Churn

\textcolor{pink}{Please import the "B18-Churn.xlsx"}

- About: [3333, 21]
  - Source: https://www.kaggle.com/ronitf/churn-prediction-telecom/data
  - Note the file headers are named differently and "churn" column Type is character otherwise identical


```{r 'B18-Churn', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum
#tools::md5sum(paste0(.z$XL, "B18-Churn.xlsx"))
xxB18ChurnXL <- f_getObject("xxB18ChurnXL", "B18-Churn.xlsx", "d20730cb7050db29691eda7ca536c914")
xxB18Churn <- xxB18ChurnXL[[1]]
f_setRDS(xxB18Churn)
```

```{r 'B18-getChurn', include=FALSE}
# #Load Data: Churn
xxB18Churn <- f_getRDS(xxB18Churn)
bb <- aa <- xxB18Churn
```

## Q1 {.tabset .tabset-fade}

- Explore whether there are missing values for any of the variables.
  - There are NO missing values

### Transposed Churn {.unlisted .unnumbered}

```{r 'B18-Transposed', include=FALSE}
# #Transposed and Filtered Data
t_aa <- aa[1:6, ] %>% 
  mutate(Row_Col = paste0("Row_", row_number())) %>% 
  relocate(Row_Col) %>% 
  mutate(across(!where(is.character), as.character)) %>% 
  `attr<-`("ColsLost", unlist(strsplit(names(.)[1], "_"))[1]) %>% 
  `attr<-`("RowsKept", unlist(strsplit(names(.)[1], "_"))[2]) %>% 
  pivot_longer(c(-1), 
               names_to = paste0(attributes(.)$RowsKept, "_", attributes(.)$ColsLost), 
               values_to = "Values") %>% 
  pivot_wider(names_from = 1, values_from = Values) %>% 
  `attr<-`("ColsLost", NULL) %>% `attr<-`("RowsKept", NULL)
```

```{r 'B18T02', echo=FALSE}
bb <- t_aa
kbl(bb,
  caption = "(B18T02) Churn Transposed",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
                #position = "float_left",
                fixed_thead = TRUE
  ) %>%
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```

### Churn Data {.unlisted .unnumbered}

```{r 'B18T01', echo=FALSE}
bb <- head(aa)
kbl(bb,
  caption = "(B18T01) Churn",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
                #position = "float_left",
                fixed_thead = TRUE
  ) %>%
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```

### NA {.unlisted .unnumbered}

```{r 'B18-NA'}
anyNA(bb)
```

## Q2 {.tabset .tabset-fade}

- Compare the area code and state fields. Discuss any apparent abnormalities.
  - There are only 3 Area Codes and all of them belong to California State.

### States Bar Chart {.unlisted .unnumbered}

```{r 'B18-Q2', include=FALSE}
# #Select | Rename 
bb <- aa %>% select(`Area Code`, State) %>% rename(Area = "Area Code") 
# #Select | Group | Frequency | Descending   
ii <- bb %>% select(State) %>% group_by(State) %>% summarise(CNT = n()) %>% arrange(desc(CNT)) %>% 
     mutate(across(State, factor, levels = rev(unique(State)), ordered = TRUE))
```

```{r 'B18-StatesBar', include=FALSE}
# #Proper Sorting of Factors for Flipped Axes
hh <- ii %>% mutate(nState = as.integer(State))
# #Because the CNT have duplicated values ggplot would add them up if used on x-axis
anyDuplicated(ii$CNT)
# #So, place it on Y-axis and then flip the axis
#
# #Set Alternate Labels as blanks on both Primary and Secondary x-axis
#x_sec <- x_prim <- as.character(hh$State)
#x_prim[1:nrow(hh) %%2 != 1] <- ""
#x_sec[1:nrow(hh) %%2 == 1] <- ""
#
# #Get Median Location
#hh %>% filter(CNT == median(CNT)) %>% mutate(as.integer(State))
median_loc_hh <- ceiling(nrow(hh)/2) 
#
cap_hh <- "B18P01"
ttl_hh <- "Churn: Frequency of States"
sub_hh <- paste0(nrow(hh), " States with Median = ",  median(hh$CNT)) #NULL
#
B18 <- hh %>% { ggplot(data = ., aes(x = nState, y = CNT)) +
    geom_bar(stat = 'identity', aes(fill = (nState %% 2 == 0))) + 
    geom_vline(aes(xintercept = median_loc_hh), color = '#440154FF') +
    scale_x_continuous( #sec.axis = sec_axis(~., breaks = 1:nrow(.), labels = rev(.$State)), 
      breaks = 1:nrow(.), guide = guide_axis(n.dodge = 2), labels = rev(.$State)) + 
    k_gglayer_bar + 
    coord_flip() +
    labs(x = "State", y = "Frequency", subtitle = sub_hh, 
         caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B18)
rm(B18)
```

```{r 'B18P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P01", "-Churn-States-Bar", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P01, device = "png", dpi = 144) 
}
```

```{r 'B18P01', echo=FALSE, fig.cap="(B18P01) Churn: States Frequency"}
knitr::include_graphics(paste0(.z$PX, "B18P01", "-Churn-States-Bar", ".png"))
```

### Exploration {.unlisted .unnumbered}

```{r 'B18-Q2-A', ref.label=c('B18-Q2'), eval=FALSE}
#
```

### State & Area {.unlisted .unnumbered}

```{r 'B18-Area'}
ii <- bb %>% mutate(across(everything(), factor))
#
# #Unique Values
ii %>% summarise(across(everything(), ~ length(unique(.))))
#
summary(ii)
#
str(levels(ii$Area))
str(levels(ii$State))
```

### Code Bar Chart {.unlisted .unnumbered}

```{r 'B18-StatesBar-A', ref.label=c('B18-StatesBar'), eval=FALSE}
#
```

## Q3 

- Use useful graphs to visually determine whether there are any outliers in the datasets (Note: check the same for all the numeric variables).
  - Histograms 
  - QQ Plots
  - Box Plots

```{r 'B18-Q3', include=FALSE}
# #Rename to Proper Names | To Lower, Replace by Underscore | Coercion 
bb <- aa %>% rename_with(make.names) %>% 
  rename_with(~ tolower(gsub(".", "_", .x, fixed = TRUE))) %>% 
  mutate(across(c(int_l_plan, vmail_plan), ~case_when(. == "yes" ~ TRUE, . == "no" ~ FALSE))) %>% 
  mutate(across(churn, ~case_when(. == "True." ~ TRUE, . == "False." ~ FALSE))) %>% 
  mutate(across(ends_with("_calls"), as.integer))
#t(bb %>% summarise(across(everything(), ~length(unique(.)))))
#str(bb)
#summary(bb)
```

### All Histograms {.tabset .tabset-fade} 

#### Image {.unlisted .unnumbered}

```{r 'B18-AllNumeric', include=FALSE}
ii <- bb %>% 
  select(where(is.numeric)) %>% 
  select(!area_code) %>% 
  relocate(ends_with("_mins")) %>% 
  relocate(ends_with("_calls")) %>% 
  relocate(vmail_message, .after =  last_col()) %>% 
  pivot_longer(everything(), names_to = "Key", values_to = "Values") %>% 
  mutate(across(Key, ~ factor(., levels = unique(Key))))
#
str(ii)
```

```{r 'B18-AllHistograms', include=FALSE}
# #Histogram
hh <- ii
ttl_hh <- "Churn: Histograms"
cap_hh <- "B18P03"
#
B18 <- hh %>% { ggplot(data = ., mapping = aes(x = Values)) + 
    geom_histogram(bins = ifelse(length(unique(.[[1]])) > 50, 50, length(unique(.[[1]]))),
                   alpha = 0.4, fill = '#FDE725FF') + 
    theme(plot.title.position = "panel", 
          strip.text.x = element_text(size = 10, colour = "white")) +
    facet_wrap(~Key, nrow = 3, scales = 'free') +
    labs(x = "x", y = NULL, subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B18)
rm(B18)
```

```{r 'B18P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P03", "-Churn-All-Hist", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P03, device = "png", dpi = 144, width = k_width, height = k_height) 
}
```

```{r 'B18P03', echo=FALSE, out.width='100%', fig.cap="(B18P03) Churn: All Histograms"}
knitr::include_graphics(paste0(.z$PX, "B18P03", "-Churn-All-Hist", ".png"))
```

#### Code {.unlisted .unnumbered}

```{r 'B18-AllHistograms-A', ref.label=c('B18-AllNumeric', 'B18-AllHistograms'), eval=FALSE}
#
```


### All QQ Plots {.tabset .tabset-fade} 

#### Image {.unlisted .unnumbered}

```{r 'B18-AllQQ', include=FALSE}
# #QQ Plots
hh <- ii
ttl_hh <- "Churn: QQ Plots"
cap_hh <- "B18P04"
#
B18 <- hh %>% { ggplot(., aes(sample = Values)) +
    stat_qq() +
    stat_qq_line() +
    facet_wrap(~Key, nrow = 3, scales = 'free_y') +
    #scale_x_continuous(limits = c(-3, 3)) + 
    #coord_flip() +
    theme(plot.title.position = "panel", 
          strip.text.x = element_text(size = 10, colour = "white")) +
    labs(x = "x", y = NULL, subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B18)
rm(B18)
```

```{r 'B18P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P04", "-Churn-All-QQ", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P04, device = "png", dpi = 144, width = k_width, height = k_height) 
}
```

```{r 'B18P04', echo=FALSE, out.width='100%', fig.cap="(B18P04) Churn: All QQ Plots"}
knitr::include_graphics(paste0(.z$PX, "B18P04", "-Churn-All-QQ", ".png"))
```

#### Code {.unlisted .unnumbered}

```{r 'B18-AllQQ-A', ref.label=c('B18-AllQQ'), eval=FALSE}
#
```

### 9 Box Plots {.tabset .tabset-fade} 

#### Image {.unlisted .unnumbered}

```{r 'B18-AllBoxData', include=FALSE}
# #Data
jj_calls <- bb %>% select(day_calls, eve_calls, night_calls) %>% 
  pivot_longer(everything(), names_to = "Key", values_to = "Values") %>% 
  mutate(across(Key, ~ factor(., levels = unique(Key))))
#
jj_mins <- bb %>% select(day_mins, eve_mins, night_mins) %>% 
  pivot_longer(everything(), names_to = "Key", values_to = "Values") %>% 
  mutate(across(Key, ~ factor(., levels = unique(Key))))
#
jj_charge <- bb %>% select(day_charge, eve_charge, night_charge) %>% 
  pivot_longer(everything(), names_to = "Key", values_to = "Values") %>% 
  mutate(across(Key, ~ factor(., levels = unique(Key))))
```

```{r 'B18-BoxCalls', include=FALSE, eval=FALSE}
# #Box Plots
hh <- jj_calls
ttl_hh <- "Churn: Calls"
cap_hh <- "B18P05"
```

```{r 'B18-BoxCalls-A', include=FALSE, ref.label=c('B18-BoxCalls', 'B18-BoxPlot')}
#
```

```{r 'B18P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P05", "-Churn-Box-Calls", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P05, device = "png", dpi = 144) 
}
```

```{r 'B18P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B18P05", "-Churn-Box-Calls", ".png"))
```

```{r 'B18-BoxMins', include=FALSE, eval=FALSE}
# #Box Plots
hh <- jj_mins
ttl_hh <- "Churn: Minutes"
cap_hh <- "B18P06"
```

```{r 'B18-BoxMins-A', include=FALSE, ref.label=c('B18-BoxMins', 'B18-BoxPlot')}
#
```

```{r 'B18P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P06", "-Churn-Box-Mins", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P06, device = "png", dpi = 144) 
}
```

```{r 'B18P06', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B18P06", "-Churn-Box-Mins", ".png"))
```

```{r 'B18-BoxCharges', include=FALSE, eval=FALSE}
# #Box Plots
hh <- jj_charge
ttl_hh <- "Churn: Charges"
cap_hh <- "B18P07"
```

```{r 'B18-BoxCharges-A', include=FALSE, ref.label=c('B18-BoxCharges', 'B18-BoxPlot')}
#
```

```{r 'B18P07-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P07", "-Churn-Box-Charges", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P07, device = "png", dpi = 144) 
}
```

```{r 'B18P07', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B18P07", "-Churn-Box-Charges", ".png"))
```

```{r 'B18P050607', echo=FALSE, out.width='33%', ref.label=c('B18P05', 'B18P06', 'B18P07'), fig.cap="(B18P05 B18P06 B18P07) Churn: BoxPlots of Calls, Minutes, & Charges"}
#
```

#### Code {.unlisted .unnumbered}

```{r 'B18-BoxPlot', eval=FALSE}
# #BoxPlot
B18 <- hh %>% { ggplot(data = ., mapping = aes(x = Key, y = Values, fill = Key)) +
    geom_boxplot() +
    k_gglayer_box +
    theme(legend.position = 'none') +
    labs(x = NULL, y = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B18)
rm(B18)
```

### International Calls Box Plots {.tabset .tabset-fade} 

#### Image {.unlisted .unnumbered}

```{r 'B18-BoxiCalls', include=FALSE, eval=FALSE}
# #Box Plots
hh <- tibble(Values = bb$intl_calls)
ttl_hh <- "Churn: International Calls"
cap_hh <- "B18P08"
```

```{r 'B18-BoxiCalls-A', include=FALSE, ref.label=c('B18-BoxiCalls', 'B18-SingleBoxPlot')}
#
```

```{r 'B18P08-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P08", "-Churn-Box-iCalls", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P08, device = "png", dpi = 144) 
}
```

```{r 'B18P08', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B18P08", "-Churn-Box-iCalls", ".png"))
```

```{r 'B18-BoxiMins', include=FALSE, eval=FALSE}
# #Box Plots
hh <- tibble(Values = bb$intl_mins)
ttl_hh <- "Churn: International Minutes"
cap_hh <- "B18P09"
```

```{r 'B18-BoxiMins-A', include=FALSE, ref.label=c('B18-BoxiMins', 'B18-SingleBoxPlot')}
#
```

```{r 'B18P09-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P09", "-Churn-Box-iMins", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P09, device = "png", dpi = 144) 
}
```

```{r 'B18P09', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B18P09", "-Churn-Box-iMins", ".png"))
```

```{r 'B18-BoxiCharges', include=FALSE, eval=FALSE}
# #Box Plots
hh <- tibble(Values = bb$intl_charge)
ttl_hh <- "Churn: International Charges"
cap_hh <- "B18P10"
```

```{r 'B18-BoxiCharges-A', include=FALSE, ref.label=c('B18-BoxiCharges', 'B18-SingleBoxPlot')}
#
```

```{r 'B18P10-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P10", "-Churn-Box-iCharges", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P10, device = "png", dpi = 144) 
}
```

```{r 'B18P10', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B18P10", "-Churn-Box-iCharges", ".png"))
```

```{r 'B18P080910', echo=FALSE, out.width='33%', ref.label=c('B18P08', 'B18P09', 'B18P10'), fig.cap="(B18P08 B18P09 B18P10) Churn: BoxPlots of International Calls, Minutes, & Charges"}
#
```



#### Code {.unlisted .unnumbered}

```{r 'B18-SingleBoxPlot', eval=FALSE}
# #BoxPlot
B18 <- hh %>% { ggplot(data = ., mapping = aes(y = Values)) +
    geom_boxplot() +
    k_gglayer_box +
    theme(legend.position = 'none') +
    labs(x = NULL, y = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B18)
rm(B18)
```

### 3 Box Plots 

```{r 'B18-BoxAcc', include=FALSE, eval=FALSE}
# #Box Plots
hh <- tibble(Values = bb$account_length)
ttl_hh <- "Churn: Account Length"
cap_hh <- "B18P11"
```

```{r 'B18-BoxAcc-A', include=FALSE, ref.label=c('B18-BoxAcc', 'B18-SingleBoxPlot')}
#
```

```{r 'B18P11-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P11", "-Churn-Box-ACC", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P11, device = "png", dpi = 144) 
}
```

```{r 'B18P11', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B18P11", "-Churn-Box-ACC", ".png"))
```

```{r 'B18-BoxVmail', include=FALSE, eval=FALSE}
# #Box Plots
hh <- tibble(Values = bb$vmail_message)
ttl_hh <- "Churn: Voice Messages"
cap_hh <- "B18P12"
```

```{r 'B18-BoxVmail-A', include=FALSE, ref.label=c('B18-BoxVmail', 'B18-SingleBoxPlot')}
#
```

```{r 'B18P12-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P12", "-Churn-Box-Vmail", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P12, device = "png", dpi = 144) 
}
```

```{r 'B18P12', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B18P12", "-Churn-Box-Vmail", ".png"))
```

```{r 'B18-BoxCustomer', include=FALSE, eval=FALSE}
# #Box Plots
hh <- tibble(Values = bb$custserv_calls)
ttl_hh <- "Churn: Customer Service Calls"
cap_hh <- "B18P13"
```

```{r 'B18-BoxCustomer-A', include=FALSE, ref.label=c('B18-BoxCustomer', 'B18-SingleBoxPlot')}
#
```

```{r 'B18P13-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P13", "-Churn-Box-iCharges", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P13, device = "png", dpi = 144) 
}
```

```{r 'B18P13', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B18P13", "-Churn-Box-iCharges", ".png"))
```

```{r 'B18P111213', echo=FALSE, out.width='33%', ref.label=c('B18P11', 'B18P12', 'B18P13'), fig.cap="(B18P11 B18P12 B18P13) Churn: BoxPlots of Reamining Three"}
#
```

### International Calls {.tabset .tabset-fade}


#### Image {.unlisted .unnumbered}

```{r 'B18-Histogram', include=FALSE}
# #Histogram
hh <- tibble(ee = bb$intl_calls)
ttl_hh <- "Churn: Histogram of International Calls"
cap_hh <- "B18P02"
# #Bins
summary(hh[[1]])
bins_hh <- ifelse(length(unique(hh[[1]])) > 50, 50, length(unique(hh[[1]])))
# #Basics
median_hh <- round(median(hh[[1]]), 1)
mean_hh <- round(mean(hh[[1]]), 1)
sd_hh <- round(sd(hh[[1]]), 1)
len_hh <- nrow(hh)
#
B18 <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) + 
  geom_histogram(bins = bins_hh, alpha = 0.4, fill = '#FDE725FF') + 
  geom_vline(aes(xintercept = mean_hh), color = '#440154FF') +
  geom_text(data = tibble(x = mean_hh, y = -Inf, 
                          label = paste0("Mean= ", mean_hh)), 
            aes(x = x, y = y, label = label), 
            color = '#440154FF', hjust = -0.5, vjust = 1.3, angle = 90) +
  geom_vline(aes(xintercept = median_hh), color = '#3B528BFF') +
  geom_text(data = tibble(x = median_hh, y = -Inf, 
                          label = paste0("Median= ", median_hh)), 
            aes(x = x, y = y, label = label), 
            color = '#3B528BFF', hjust = -0.5, vjust = -0.7, angle = 90) +
  theme(plot.title.position = "panel") + 
  labs(x = "x", y = "Frequency", 
       subtitle = paste0("(N=", len_hh, "; ", "Mean= ", mean_hh, 
                         "; Median= ", median_hh, "; SD= ", sd_hh,
                         ")"), 
        caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B18)
rm(B18)
```

```{r 'B18P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P02", "-Churn-iCalls-Hist", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P02, device = "png", dpi = 144) 
}
```

```{r 'B18P02', echo=FALSE, fig.cap="(B18P02) Churn: International Calls"}
knitr::include_graphics(paste0(.z$PX, "B18P02", "-Churn-iCalls-Hist", ".png"))
```

#### Exploration {.unlisted .unnumbered}

```{r 'B18-Q3-A', ref.label=c('B18-Q3'), eval=FALSE}
#
```

#### Code {.unlisted .unnumbered}

```{r 'B18-Histogram-A', ref.label=c('B18-Histogram'), eval=FALSE}
#
```

### Scale 

```{r 'B18-DepCheck01', include=FALSE, eval=FALSE}
#cat(paste0(names(bb), collapse = ", "), "\n")
#str(bb)
#tibble [3,333 x 21] (S3: tbl_df/tbl/data.frame)
# $ state         : chr [1:3333] "KS" "OH" "NJ" "OH" ...
# $ account_length: num [1:3333] 128 107 137 84 75 118 121 147 117 141 ...
# $ area_code     : num [1:3333] 415 415 415 408 415 510 510 415 408 415 ...
# $ phone         : chr [1:3333] "382-4657" "371-7191" "358-1921" "375-9999" ...
# $ int_l_plan    : logi [1:3333] FALSE FALSE FALSE TRUE TRUE TRUE ...
# $ vmail_plan    : logi [1:3333] TRUE TRUE FALSE FALSE FALSE FALSE ...
# $ vmail_message : num [1:3333] 25 26 0 0 0 0 24 0 0 37 ...
# $ day_mins      : num [1:3333] 265 162 243 299 167 ...
# $ day_calls     : int [1:3333] 110 123 114 71 113 98 88 79 97 84 ...
# $ day_charge    : num [1:3333] 45.1 27.5 41.4 50.9 28.3 ...
# $ eve_mins      : num [1:3333] 197.4 195.5 121.2 61.9 148.3 ...
# $ eve_calls     : int [1:3333] 99 103 110 88 122 101 108 94 80 111 ...
# $ eve_charge    : num [1:3333] 16.78 16.62 10.3 5.26 12.61 ...
# $ night_mins    : num [1:3333] 245 254 163 197 187 ...
# $ night_calls   : int [1:3333] 91 103 104 89 121 118 118 96 90 97 ...
# $ night_charge  : num [1:3333] 11.01 11.45 7.32 8.86 8.41 ...
# $ intl_mins     : num [1:3333] 10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...
# $ intl_calls    : int [1:3333] 3 3 5 7 3 6 7 6 4 5 ...
# $ intl_charge   : num [1:3333] 2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...
# $ custserv_calls: int [1:3333] 1 1 0 2 3 0 3 0 1 0 ...
# $ churn         : logi [1:3333] FALSE FALSE FALSE FALSE FALSE FALSE ...
```

```{r 'B18-Scale', include=FALSE}
ii <- bb %>% 
  select(where(is.numeric)) %>% 
  select(!area_code) %>% 
  relocate(ends_with("_mins")) %>% 
  relocate(ends_with("_calls")) %>% 
  relocate(vmail_message, .after =  last_col()) 
#
jj <- ii %>% mutate(across(everything(), ~ as.vector(scale(.)))) %>% 
  pivot_longer(everything(), names_to = "Key", values_to = "Values") %>% 
  mutate(across(Key, ~ factor(., levels = unique(Key))))
```


#### Histograms {.unlisted .unnumbered}

```{r 'B18-HistScale', include=FALSE}
# #Histogram 
hh <- jj
ttl_hh <- "Churn: Histograms (Scaled)"
cap_hh <- "B18P14"
#
# #x-axis has been kept free to identify which plots have outliers
B18 <- hh %>% { ggplot(data = ., mapping = aes(x = Values)) + 
    geom_histogram(bins = ifelse(length(unique(.[[1]])) > 50, 50, length(unique(.[[1]]))),
                   alpha = 0.4, fill = '#FDE725FF') + 
    theme(plot.title.position = "panel", 
          strip.text.x = element_text(size = 10, colour = "white")) +
    facet_wrap(~Key, nrow = 3, scales = 'free') +
    labs(x = "z", y = NULL, subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B18)
rm(B18)
```

```{r 'B18P14-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P14", "-Churn-Scale-Hist", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P14, device = "png", dpi = 144, width = k_width, height = k_height) 
}
```

```{r 'B18P14', echo=FALSE, out.width='100%', fig.cap="(B18P14) Churn: All Histograms (Scaled)"}
knitr::include_graphics(paste0(.z$PX, "B18P14", "-Churn-Scale-Hist", ".png"))
```

#### QQ Plots {.unlisted .unnumbered}

```{r 'B18-QQScale', include=FALSE}
# #QQ Plots
hh <- jj
ttl_hh <- "Churn: QQ Plots (Scaled)"
cap_hh <- "B18P15"
#
B18 <- hh %>% { ggplot(., aes(sample = Values)) +
    stat_qq() +
    stat_qq_line() +
    facet_wrap(~Key, nrow = 3, scales = 'free_y') +
    #scale_x_continuous(limits = c(-3, 3)) + 
    #coord_flip() +
    theme(plot.title.position = "panel", 
          strip.text.x = element_text(size = 10, colour = "white")) +
    labs(x = "z", y = NULL, subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B18)
rm(B18)
```

```{r 'B18P15-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P15", "-Churn-Scale-QQ", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P15, device = "png", dpi = 144, width = k_width, height = k_height) 
}
```

```{r 'B18P15', echo=FALSE, out.width='100%', fig.cap="(B18P15) Churn: All QQ Plots (Scaled)"}
knitr::include_graphics(paste0(.z$PX, "B18P15", "-Churn-Scale-QQ", ".png"))
```

#### BoxPlots {.unlisted .unnumbered}

```{r 'B18-ScaleBox', include=FALSE}
# #Box Plots
kk <- jj %>% mutate(Group = case_when(str_detect(Key, "day_") ~ "Day", 
                                      str_detect(Key, "night_") ~ "Night", 
                                      str_detect(Key, "eve_") ~ "Eve", 
                                      str_detect(Key, "intl_") ~ "Intl", 
                                      str_detect(Key, "custserv_") ~ "Service", 
                                      str_detect(Key, "account_") ~ "Account", 
                                      str_detect(Key, "vmail_") ~ "Vmail"))
#
hh <- kk
ttl_hh <- "Churn: BoxPlots (Scaled)"
cap_hh <- "B18P16"
#
B18 <- hh %>% { ggplot(data = ., mapping = aes(x = Key, y = Values, fill = Group)) +
    geom_boxplot() +
    k_gglayer_box +
    coord_flip() +
    theme(legend.position = 'none') +
    labs(x = NULL, y = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B18)
rm(B18)
```

```{r 'B18P16-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P16", "-Churn-Scale-Box", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P16, device = "png", dpi = 144, width = k_width, height = k_height) 
}
```

```{r 'B18P16', echo=FALSE, out.width='100%', fig.cap="(B18P16) Churn: All Box Plots (Scaled)"}
knitr::include_graphics(paste0(.z$PX, "B18P16", "-Churn-Scale-Box", ".png"))
```

### Overlaid Histograms

```{r 'B18-Overlaid', include=FALSE}
# #Scaled Data was NOT useful. Original Data had distinguished features
ii <- bb %>% select(matches('day_|night_|eve_')) %>% 
  #mutate(across(everything(), ~ as.vector(scale(.)))) %>% 
  pivot_longer(everything(), names_to = "Key", values_to = "Values") %>% 
  mutate(across(Key, ~ factor(., levels = unique(Key)))) %>% 
  mutate(Group = case_when(str_detect(Key, "day_") ~ "Day", 
                           str_detect(Key, "night_") ~ "Night", 
                           str_detect(Key, "eve_") ~ "Eve")) %>% 
  mutate(Facets = case_when(str_detect(Key, "_mins") ~ "Minutes", 
                           str_detect(Key, "_calls") ~ "Calls", 
                           str_detect(Key, "_charge") ~ "Charges"))
```

```{r 'B18-HistOverlaid', include=FALSE}
# #Histogram
hh <- ii
ttl_hh <- "Churn: Multi Histograms of Day, Eve, Night"
cap_hh <- "B18P17"
#
B18 <- hh %>% { ggplot(data = ., mapping = aes(x = Values, fill = Group)) + 
    geom_histogram(position = "identity", bins = 80, alpha = 0.4) + 
    facet_wrap(~Facets, scales = 'free') +
    scale_fill_viridis_d() +
    theme(plot.title.position = "panel", 
          strip.text.x = element_text(size = 10, colour = "white"), 
          #legend.justification = c("right", "top"),
          #legend.box.just = "right",
          #legend.margin = margin(6, 6, 6, 6),
          legend.position = c(.5, .95),
          #legend.position = "bottom", 
          legend.box = "horizontal", 
          legend.direction = "horizontal") +
    labs(x = "x", y = NULL, fill = NULL, subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B18)
rm(B18)
```

```{r 'B18P17-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B18P17", "-Churn-mHist-Charges", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B18P17, device = "png", dpi = 144, width = k_width, height = k_height) 
}
```

```{r 'B18P17', echo=FALSE, out.width='100%', fig.cap="(B18P17) Churn: All Histograms"}
knitr::include_graphics(paste0(.z$PX, "B18P17", "-Churn-mHist-Charges", ".png"))
```

## Q4 

-	Identify the outliers, using:
  - The Z-score method $z \notin [-3, +3] \to \text{Outlier}$
  - The IQR method
  - Shown Above


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B18-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, t_aa, xxB18Churn, B18P01, B18P02, B18P03, B18P04, B18P05, 
          B18P06, B18P07, bins_hh, cap_hh, hh, jj_calls, jj_charge, jj_mins, len_hh, loc_png, 
          mean_hh, median_hh, median_loc_hh, sd_hh, sub_hh, ttl_hh, B18P08, B18P09, B18P10, 
          B18P11, B18P12, B18P13, B18P14, B18P15, B18P16, B18P17)
```

```{r 'B18-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/218-Unsupervised.Rmd-->

# Supervised Learning (B19, Nov-14) {#b19}

```{r 'B19', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview {#mining-b19}

- "Supervised Learning Algorithm: Cluster Analysis"

```{r 'B19D01', comment="", echo=FALSE, results='asis'}
f_getDef("Unsupervised-Methods")
```

```{r 'B19D02', comment="", echo=FALSE, results='asis'}
f_getDef("Supervised-Methods")
```

- [Data Mining Methods](#mining-b19 "b19") and [Definitions](#mining-def-c31 "c31")
  - Data mining methods may be categorized as either supervised or unsupervised.
  - Most data mining methods are supervised methods.
  - Unsupervised : Clustering, PCA, Factor Analysis, Association Rules, RFM
  - Supervised : 
    - Regression (Continuous Target) : Linear Regression, Regularised Regression, Decision trees, Ensemble learning 
      - Linear Regression : Ridge, Lasso and Elastic Regression
      - Ensemble learning : Bagging, Boosting (AdaBoost, XGBoost), Random forests
    - Classification (Categorical Target) : Decision trees, Ensemble learning, Logistic Regression, k-nearest neighbor (k-NN), Naive-Bayes 
    - Deep Learning : Neural Networks

## RFM Analysis

- RFM - Recency, Frequency, and Monetary value
  - It is for customer segmentation
  - Recency -
    - Freshness of customer activity (purchase /visit)
  - Frequency - 
    - Total number of transactions in a given period
  - Monetary value - 
    - Total or Average Transaction value
  - RFM Score is calculated and each parameter is assgined a weightage and based on that all the customers are classified
    - R /F /M Score - Rank them and then score of [1, 5] is allocated
    - Then we can define rules like 
      - 125 Rules
      - Anyone having R[3, 5], F[4,5], M[4,5] is a very important customer for us
      

- (Aside) \textcolor{orange}{Caution:} 
  - RFM ignores other clusters e.g. gender
  - It ignores seasonal /cyclical trends
  - It does not look at the duration of customer engagement i.e. A has done 10 transactions in 10 Years, B has done 9 transactions in 10 Months.

- \textcolor{pink}{Question:} Can the scoring be different from [1, 5] in R
  - ~~Not in R~~
  - (Aside) Defaults can be modified
- \textcolor{pink}{Question:} Can we have fewer than 5 ratings in R e.g. [1, 4]
  - ~~Not in R~~
  - (Aside) Defaults can be modified
- \textcolor{pink}{Question:} What if there are outliers. If Max Recency is 100 while all the other values are 1-40, would we still assign same Rank
  - Yes
  - This is percentile based segregation which is least affected by the outliers
- \textcolor{pink}{Question:} Can we delete that outlier
  - No, we cannot
  - If all customers are purchasing only upto 5000 and one customer is spending 10000, can we afford to overlook that customer
- \textcolor{pink}{Question:} Does the score is affected by Business Context. i.e. a recency value of 100 might be outlier for one business but might not be for another
  - No, 
  - Business context affects the weights assgined to them. However, that is part of later analysis.
- \textcolor{pink}{Question:} For Recency, will there be a maximum limit that we can consider
  - No
  - Most companies do this analysis every quarter and that can be used as a benchmark for next quarter
  - Further, a consumer durable company would not look at it every quarter. They would be looking at broader horizon.
- \textcolor{pink}{Question:} Do we look at the item-wise details
  - No, that is the limitation of RFM
  - We are looking at the Store level (total amount) not the item-wise details. For that we need the 'Market Basket' analysis


## Packages

```{r 'B19-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("rfm", dependencies = TRUE)
  install.packages("lubridate", dependencies = TRUE)
}
```

## Data RFM

\textcolor{pink}{Please import the "B19-Transaction.csv"}

```{r 'B19-Trasaction', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum tools::md5sum(paste0(.z$XL, "B19-Transaction.csv"))
xxB19Transaction <- f_getObject("xxB19Transaction", "B19-Transaction.csv",
                                "5d4f8ef941dba7b25a6630daee81c4e3")
```

```{r 'B19-GetTransaction', include=FALSE}
# #This is same dataset as rfm_data_orders #wwww
xxB19Transaction <- f_getRDS(xxB19Transaction)
bb <- aa <- xxB19Transaction
```

## Transaction vs Customer Level Data

- In the Transaction level data, customer id can be repeated. It is mainly based on each transaction
- In the Customer level data, each row represents a unique customer with no duplicates. It is summarised view of all transactions 

## RFM on Transaction {.tabset .tabset-fade}

- \textcolor{pink}{rfm_table_order()}
  - Transform Transaction level data into Customer level data
  - If Scores are Recency = 3, Frequency = 4, Monetary = 3. Then it report the RFM Score as '343'
  - We can supply bins for all 3 of RFM. See Function Example. These bins are available as attributes of outcome for reference


### Run RFM {.unlisted .unnumbered}

```{r 'B19-RFM-Transaction'}
# #character to date using dmy() #wwww
bb <- aa
str(bb)
bb$order_date <- dmy(bb$order_date)
#
str(bb)
anyNA(bb)
summary(bb)
#
# #Get Analysis Date as the Next Date after the Max Date in the Data
analysis_date <- max(bb$order_date) + 1 #as_date("2006-12-31")
#
# #RFM analysis by rfm_table_order()
rfm_result <- rfm_table_order(bb, customer_id = customer_id, order_date = order_date, 
                              revenue = revenue, analysis_date = analysis_date)
# #Output is a Tibble with some other attributes
loc_src <- paste0(.z$XL, "B19-Transaction-RFM.csv")
# #Save the Result in a CSV
if(FALSE) write_csv(rfm_result$rfm, file = loc_src)
```

### Bins {.unlisted .unnumbered}

```{r 'B19-ResultsBins'}
# #Bins of RFM
str(rfm_result$rfm)
# #Recency: Unlike the other Two its Ranking feels reversed i.e. 5 is assigned to lowest value 
# #However 5 is assigned to 'Most Recent'
rfm_result$rfm %>% 
  group_by(recency_score) %>% 
  summarise(MIN = min(recency_days), MAX = max(recency_days), N = n()) 
# #Frequency
rfm_result$rfm %>% 
  group_by(frequency_score) %>% 
  summarise(MIN = min(transaction_count), MAX = max(transaction_count), N = n()) 
# #Monetrary
rfm_result$rfm %>% 
  group_by(monetary_score) %>% 
  summarise(MIN = min(amount), MAX = max(amount), N = n()) 
```


### Reading Back {.unlisted .unnumbered}

```{r 'B19-ReadBack'}
# #Read CSV
jj <- read_csv(loc_src, show_col_types = FALSE) %>% 
  mutate(across(c(recency_score, frequency_score, monetary_score), as.integer))
ii <- rfm_result$rfm
#
attr(jj, "spec") <- NULL
attr(jj, "problems") <- NULL
# #Verification
all_equal(ii, jj) #TRUE
#
attributes(ii)$class
attributes(jj)$class
#
# #Modify Class Attribute i.e. Remove 1st "spec_tbl_df"
attr(jj, "class") <- attr(jj, "class")[-1]
#
all.equal(ii, jj) #TRUE
identical(ii, jj) #TRUE
#
# #NOTE Position of Attributes does not matter
names(attributes(ii))
names(attributes(jj))
```


### Date Transformation {.unlisted .unnumbered}

```{r 'B19-Dates'}
# #character to date using dmy()
bb <- aa
str(bb)
#
ii <- bb
ii$order_date <- dmy(ii$order_date)
#
# #Equivalent
jj <- bb %>% mutate(order_date = dmy(order_date))
stopifnot(identical(as_tibble(ii), jj))
#
str(jj)
anyNA(jj)
summary(jj)
```

## Develop Segments

Segment rules might look like arbitory however lots of thought goes into this. This is a tedious task.

- \textcolor{pink}{Question:} What happens with overlap e.g. which label will be assigned to the customer with 444. 
  - In this dataset, there is no overlap
  - Further, If some customer score (125 possibilities) is outside the (10) specification, it is classified as 'Others'
  - (Aside) It remains a concern "ForLater"
- \textcolor{pink}{Question:} Is it necessary that all the segments need to be covered
  - No, not necessary but highly recommended
  - We are doing the analysis with some plan of action and it is preferable that we can put them in different and proper buckets so that specific actions can be taken

```{r 'B19-Segments'}
# #Developing segments
segment_titles <- c("First Grade", "Loyal", "Likely to be Loyal", "New Ones", 
                    "Could be Promising", "Require Assistance", "Getting Less Frequent",
                    "Almost Out", "Can not Lose Them", "Do not Show Up at All") 
# #Rules of Minimum and Maximum RFM for each group
r_low  <- c(4, 2, 3, 4, 3, 2, 2, 1, 1, 1)
r_high <- c(5, 5, 5, 5, 4, 3, 3, 2, 1, 2)
f_low  <- c(4, 3, 1, 1, 1, 2, 1, 2, 4, 1)
f_high <- c(5, 5, 3, 1, 1, 3, 2, 5, 5, 2)
m_low  <- c(4, 3, 1, 1, 1, 2, 1, 2, 4, 1)
m_high <- c(5, 5, 3, 1, 1, 3, 2, 5, 5, 2)
#
stopifnot(all(vapply(list(r_low, r_high, f_low, f_high, m_low, m_high), 
                     FUN = function(x) identical(length(x), length(segment_titles)), logical(1))))
```

```{r 'B19-DivBySeg'}
divisions <- rfm_segment(rfm_result, segment_names = segment_titles, 
                       recency_lower = r_low, recency_upper = r_high, 
                       frequency_lower = f_low, frequency_upper = f_high, 
                       monetary_lower = m_low, monetary_upper = m_high)
# #Output is a Tibble 
# #Save the Result in a CSV
loc_src <- paste0(.z$XL, "B19-Transaction-Divisions.csv")
if(FALSE) write_csv(divisions, file = loc_src)
#
# #We defined 10 segments, However only 7 (+1) of them are represented in the data 
# #and 48 customers were not captured by our classifications. These were assigned to 'Others'
divisions %>% 
  count(segment) %>% 
  mutate(PCT = round(100 * n / sum(n), 1)) %>% 
  rename(SEGMENT = segment, FREQ = n) %>% 
  arrange(desc(FREQ)) 
#
```

## Plots

- (Aside) 
  - Problem with these plots is that we ourselves have defined bands for each group. 
  - We can do the if-then-else for complete data and we will get exact values in numbers. 
  - 'First Grade' have high median Frequency because we have defined this label having rank [4, 5] in recency (and combinations of other two).
  - 'Do not Show Up at All' has lower median frequency because we have defined this label having rank [1, 2] in recency (and combinations of other two).

- \textcolor{pink}{Question:} Here we have shown Median can we do this with Mean
  - No, median is more authentic
  - (Aside) These are Ordered Categorical Values [1, 5]. Thus, Median is meaningful, not the Mean.

```{r 'B19-Plots', eval=FALSE}
# #Histogram of Median RFM can be plotted. 
# #These are ggplot graphs so can be improved later by manually plotting
if(FALSE) {#Histograms of Median RFM for each Segment
  hh <- divisions
  rfm_plot_median_recency(hh)
  rfm_plot_median_frequency(hh)
  rfm_plot_median_monetary(hh)
}
if(FALSE) {
  hh <- rfm_result
  rfm_histograms(hh) #Histograms of RFM
  rfm_order_dist(hh) #Histograms of Customer Orders i.e. Frequency
  rfm_heatmap(hh)    #Heatmap of Monetary on Axes of Recency and Frequency. Slighly Useful
  rfm_bar_chart(hh)  #Bar Charts with Facetting of RFM
  # #Scatter Plots among Recency, Monetary, Frequency
  rfm_rm_plot(hh)
  rfm_fm_plot(hh)
  rfm_rf_plot(hh)
}
```

## RFM on Customer

\textcolor{pink}{Please import the "B19-Customer.csv"}

```{r 'B19-Customer', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum tools::md5sum(paste0(.z$XL, "B19-Customer.csv"))
xxB19Customer <- f_getObject("xxB19Customer", "B19-Customer.csv",
                                "e95251e969400f392be76c867b95d890")
```

```{r 'B19-GetCustomer', include=FALSE}
# #This is same dataset as rfm_data_orders #wwww
xxB19Customer <- f_getRDS(xxB19Customer)
```

- \textcolor{pink}{rfm_table_customer()}
  - Use Customer level data 

- \textcolor{pink}{Question:} If we are using Recency Days given in the data then what is the use of providing Analysis Date
  - ""
  - (Aside) I am unclear about the answer to this question given at "2021-11-14 18:08" "ForLater"

```{r 'B19-RFM-Customer'}
# #character to date using dmy() #wwww
bb <- aa <- xxB19Customer
str(bb)
bb$most_recent_visit <- dmy(bb$most_recent_visit)
#
# #Get Analysis Date as the Next Date after the Max Date in the Data
analysis_date <- max(bb$most_recent_visit) + 1 #as_date("2006-12-31")
#
# #RFM analysis by rfm_table_customer()
rfm_customer <- rfm_table_customer(bb, customer_id = customer_id, n_transactions = number_of_orders,
              recency_days = recency_days, total_revenue = revenue, analysis_date = analysis_date)
# #Output is a Tibble with some other attributes
# #Save the Result in a CSV
loc_src <- paste0(.z$XL, "B19-Customer-RFM.csv")
if(FALSE) write_csv(rfm_customer$rfm, file = loc_src)
```

## RFM OnlineRetail

\textcolor{pink}{Please import the "B19-OnlineRetail.csv"}

```{r 'B19-Retail', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum tools::md5sum(paste0(.z$XL, "B19-OnlineRetail.csv"))
xxB19Retail <- f_getObject("xxB19Retail", "B19-OnlineRetail.csv",
                                "bc6183968b08d4b034e08791eaabcf87")
```

```{r 'B19-GetRetail', include=FALSE}
# #This is same dataset as rfm_data_orders #wwww
xxB19Retail <- f_getRDS(xxB19Retail)
```

- About: [541909, 8]
  - However the data has NA in Customer ID. We cannot impute Customer ID. Those rows should be eliminated
  - Unit Price 0 or Quantity 0 or negative (Returns) should be removed
  - InvoiceDate 
     - (Aside) \textcolor{orange}{Caution:} While the lectures on Nov-14 and Nov-21 showed the dates to be converted by assuming that the data is in "dd-mm-yyyy" format. However, the data actually is "mm-dd-yyyy"

- \textcolor{pink}{Question:} If we eliminate returns, should we not remove this related transaction also because we are keeping the data of that transaction as actual revenue
  - We can keep it for the purpose of this analysis. Some 'unforseen circumstance' led to the return. However, the actual transaction did happen. The customer did buy the product.
  - Further, currently we are interested in segregating the customers within different labels /types. We are not analysing the profit or growth, we are analysing customer purchase patterns in terms of RFM only.
  - (Argument) But then we are considering his Monetary Contribution on the higher side. If he has done a single transaction of 1000 dollars but later returned the product, the customer actually did not contribute anything to the company. However we will give him a higher ranking compared to another customer who purchased an item of 500 dollars.

```{r 'B19-PrepRetail'}
bb <- aa <- xxB19Retail
#
# #NOTE dates are in mmddyyyy format
bb$InvoiceDate[5000:5010] 
bb$InvoiceDate <- mdy(bb$InvoiceDate)
#
# #Which Columns have NA
which(vapply(bb, anyNA, logical(1)))
#
# #Remove NA | Remove Unit Price with 0 | Quantity 0 or Negative i.e. Returns | Dropped Columns |
# #Calculate Revenue
ii <- bb %>% 
  drop_na(CustomerID) %>% 
  filter(UnitPrice > 0 & Quantity > 0) %>% 
  select(-c(1:3, 8)) %>% 
  mutate(Revenue = UnitPrice * Quantity)
#
summary(ii)
```

```{r 'B19-Segments-A', ref.label=c('B19-Segments')}
#
```

```{r 'B19-RFM-Retail'}
# #Get Analysis Date as the Next Date after the Max Date in the Data
analysis_date <- max(ii$InvoiceDate) + 1 #as_date("2011-12-10")
rfm_ii <- rfm_table_order(ii, customer_id = CustomerID, order_date = InvoiceDate, 
                              revenue = Revenue, analysis_date = analysis_date)
div_ii <- rfm_segment(rfm_ii, segment_names = segment_titles, 
                       recency_lower = r_low, recency_upper = r_high, 
                       frequency_lower = f_low, frequency_upper = f_high, 
                       monetary_lower = m_low, monetary_upper = m_high)
# #Sorted Count of Segments
div_ii %>% 
  count(segment) %>% 
  mutate(PCT = round(100 *n / sum(n), 1)) %>% 
  rename(SEGMENT = segment, FREQ = n) %>% 
  arrange(desc(FREQ)) 
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B19-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, analysis_date, divisions, loc_src, m_high, m_low, r_high, 
          r_low, rfm_customer, rfm_result, segment_titles, xxB19Customer, xxB19Retail, 
          xxB19Transaction, div_ii, rfm_ii)
```

```{r 'B19-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/219-Supervised.Rmd-->

# K-means (B20, Nov-21) {#b20}

```{r 'B20', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "K-means Cluster Analysis"
  - "ForLater" - The PPT shared in the class was corrupted, need a working file.
  - Refer [Book Merged Here](#c49 "c49")

## Packages

```{r 'B20-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("factoextra", dependencies = TRUE)
}
```

## Clustering

```{r 'B20D01', comment="", echo=FALSE, results='asis'}
f_getDef("Clustering")
```

```{r 'B20D02', comment="", echo=FALSE, results='asis'}
f_getDef("Cluster")
```

- Clustering is an unsupervised learning technique.

## k-means Clustering

- \textcolor{pink}{Question:} Is it same as k-nearest neighbour
  - No, knn is a classification technique and is supervised. k-means is clustering method and is unsupervised.

- Clustering algorithms seek to construct clusters of records such that the between-cluster variation is large compared to the within-cluster variation.
- The variables needs to be scaled before the eculidean distance can be calculated to identify clusters
- Outliers are also a problem. Normalisation does not help with outliers
- The k-means algorithm can be applied only when the mean of cluster is defined.
  - Thus, the limitation is that we cannot apply k-means to categorical values


```{r 'B20D03', comment="", echo=FALSE, results='asis'}
f_getDef("Euclidean-Distance")
```

- \textcolor{pink}{Question:} Should we need to ensure that in each cluster number of datapoints remain same
  - No

## Algorithm {#k-means-b20}

1. Ask the user how many clusters ${k}$ the data set should be partitioned into. 
    - Ex: $k = 3$
1. Randomly assign ${k}$ records to be the initial cluster center locations. 
1. For each record, find the nearest cluster center. 
    - Thus, in a sense, each cluster center "owns" a subset of the records, thereby representing a partition of the data set. 
    - We therefore have ${k}$ clusters, $\{C_1, C_2, \ldots, C_k\}$
      - Ex: ${C_1 = (3, 9), C_2 = (7, 12), C_3 = (6, 18)}$
    - The "nearest" criterion is usually Euclidean distance
1. For each of the ${k}$ clusters, find the cluster \textcolor{pink}{centroid}, and update the location of each cluster center to the new value of the centroid. 
    - Obviously, centroid location need not to be an actual point within data like mean of a set of values need not to exist within that set itself.
1. Repeat 3-5, until convergence or termination.
    - The algorithm terminates when the centroids no longer change. 
      - In other words, the algorithm terminates when for all clusters $\{C_1, C_2, \ldots, C_k\}$, all the records "owned" by each cluster center remain in that cluster. 
    - Alternatively, the algorithm may terminate when some convergence criterion is met, such as no significant shrinkage in the \textcolor{pink}{mean squared error} $\text{MSE} = \frac{\text{SSE}}{N - k}$, where SSE represents the \textcolor{pink}{sum of squares error}.

- \textcolor{pink}{Question:} Does the number of iterations is a function of initial random number
  - Yes
- \textcolor{pink}{Question:} Would we all, with different initial random number, reach the same cluster solutions
  - Yes

## How many k

- What is the number which would be practically feasible and statistically feasible too
  - 'k' should be the 'best guess' on the number of clusters present in the given data.
    - However, we may not have any idea about the possible number of clusters for high dimensional data and for data that is not scatterplotted 
    - There is NO principled way to know what the value of 'k' ought to be.
    - We may try with successive values of 'k' starting with 2.
  - Within Cluster Sum of Squares \textcolor{pink}{(WSQ)} represents within cluster variation i.e. inside cluster homogeneity.
    - we are expecting low value of WSQ (or MSE or SSE)
  - Between Cluster Sum of Sqares \textcolor{pink}{(BSQ)} represents between cluster variation i.e. between cluster heterogeneity
    - we are expection high value of BSQ (or MSB or SSB)
  - SSE vs. k- looks like Scree Plot and Elbow method can be used to identify the optimum number of k

- The iterative process is stopped when two consecutive 'k' values produce more or less identical results in terms of cluster within and between variances
  - However, it is possible that this 'k' value represents a local minima and not the global minima.

## Data Movies {#set-movies-b20 .tabset .tabset-fade}

\textcolor{pink}{Please import the "B20-movie.csv"}

```{r 'B20-Movies', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum tools::md5sum(paste0(.z$XL, "B20-movie.csv"))
xxB20Movies <- f_getObject("xxB20Movies", "B20-movie.csv",
                                "b59d4b0487ce4820ef0893e21b34b417")
```

```{r 'B20-GetMovies', include=FALSE}
xxB20Movies <- f_getRDS(xxB20Movies)
```

- About: [291, 6]
  - Each Row represents a unique customer and the average scores they have given to different movie genres
  - Normalisation
    - (Aside) Normalisation has been done here. However, this data is average rating on a 1-100 scale. So, (probably) it actually does not need the normalisation. "ForLater"

### EDA {.unlisted .unnumbered}

- Refer [Seed for Random Number Generation](#seed-b16 "b16")

```{r 'B20-PrepMovie'}
bb <- aa <- xxB20Movies
# #Drop ID | Scale | 
xw <- aa %>% select(-1) 
zw <- xw %>% mutate(across(everything(), ~ as.vector(scale(.))))
#
summary(xw)
```

## k-means 

- Because I have chosen a different seed than the professor, my algorithm converged through different iterations but to the same clusters. However cluster 1 and cluster 2 got interchanged in the process.
  - In lecture the cluster 1 is cluster 2 here (size 218) and vice-versa.

```{r 'B20-kmeans'}
# #Fix Seed
set.seed(3)
# #Cluster analysis with different k = {2, 3, 4}
k2_zw <- kmeans(zw, centers = 2)
k3_zw <- kmeans(zw, centers = 3)
k4_zw <- kmeans(zw, centers = 4)
#
names(k2_zw)
#
# #Two Clusters
ii <- k2_zw
# #within-cluster sum of squares (Preferred lower value i.e. Homogeneity within cluster)
ii$withinss  
identical(ii$tot.withinss, sum(ii$withinss))
# #between-cluster sum of squares
ii$betweenss 
# #The total sum of squares
ii$totss 
paste0("Between /Total = ",  round(100 * ii$betweenss / ii$totss, 2), "%")
#
# #Members within Clusters
ii$size
#
# #Matrix of cluster centres
round(ii$centers, 3)
#
# #Cluster Membership of each point
str(ii$cluster)
#
# #Save cluster membership of each point back into the dataset
res_movies <- cbind(xw, 
  list(k2 = k2_zw$cluster, k3 = k3_zw$cluster, k4 = k4_zw$cluster)) %>% as_tibble()
```

- Explanation: 
  - In normalised data, average is 0
    - Thus, positive values are above average, negative values are below average
- Two Clusters: Cluster 2 (Size 218) and Cluster 1 (Size 73)
  - Cluster 2 gave Horror & Action movies above average ratings (Favourable)
  - Cluster 2 gave lower than average ratings for Romcom, Comedy, Fantasy (Unfavourable)
  - Behavour of Cluster 1 is completely opposite to Cluster 2
  - However, we cannot make a conclusion here because Between / Total is 47%
    - i.e. Too much heterogeneity within Cluster 2
- Three Clusters: of Sizes 72, 105, 114 with Between /Total = 62% (improved i.e. within reduced)
  - We can analyse these clusters similar to above 
- Four Clusters: of Sizes 73, 51, 69, 98 with Between /Total = 64% 
  - improved i.e. within reduced but not by much 
  - NOTE: Here my cluster sizes are NOT matching with the lecture and the Between /Total is similar but not exactly same.
    - There are 2 reasons for that :
      - I fixed the seed once and then run the commands i.e. (Seed | k=2 | k=3 | k=4). Professor is fixing the seed each time i.e. (Seed | k=2 | Seed | k=3 | Seed | k=4)
      - I used different seed. Effect of starting from different seed is more pronounced as 'k' is increasing

```{r 'B20-kmeans3'}
# #Three Clusters
ii <- k3_zw
ii$size
paste0("Between /Total = ",  round(100 * ii$betweenss / ii$totss, 2), "%")
round(ii$centers, 3)
```

```{r 'B20-kmeans4'}
# #Four Clusters
ii <- k4_zw
ii$size
paste0("Between /Total = ",  round(100 * ii$betweenss / ii$totss, 2), "%")
round(ii$centers, 3)
```

## Elbow Plot of WSS

- \textcolor{pink}{fviz_nbclust()} :
  - Note that it performs the clustering on the original data. It does not take the already created clusters as input.

```{r 'B20-Elbow'}
hh <- zw
cap_hh <- "B20P01"
ttl_hh <- "Movie: Elbow Curve (WSS)"
loc_png <- paste0(.z$PX, "B20P01", "-Movie-Elbow-Wss", ".png")
#
# #factoextra::fviz_nbclust() generates ggplot
# #method = "wss" (for total within sum of square)
B20P01 <- fviz_nbclust(hh, FUNcluster = kmeans, method = "wss") +
  labs(caption = cap_hh, title = ttl_hh)
```

```{r 'B20P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B20P01", "-Movie-Elbow-Wss", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B20P01, device = "png", dpi = 144) 
}
```

```{r 'B20P01', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B20P01", "-Movie-Elbow-Wss", ".png"))
```

```{r 'B20-ElbowBase', include=FALSE}
# Total within-groups sum of squares for k = [1, 10]
maxK <- 10L
within_zw <- numeric(maxK)
#
for (ii in seq_along(within_zw)){
  within_zw[ii] <- kmeans(zw, ii)$tot.withinss
}
#
# Plot total within-groups sum of squares
hh <- tibble(xx = seq_along(within_zw), yy = within_zw)
ttl_hh <- "Movie: ElbowPlot for k=[1, 10]"
cap_hh <- "B20P03"
x_hh <- "Number of clusters k"
y_hh <- "Total Within Sum of Square"
#
B20 <- hh %>% { ggplot(., aes(x = xx, y = yy)) + 
    geom_point() +
    geom_line() + 
    scale_x_continuous(breaks = breaks_pretty()) + 
    theme(panel.grid.major = element_blank()) +
    labs(x = x_hh, y = y_hh, 
         subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B20)
rm(B20)
```

```{r 'B20P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B20P03", "-Movie-Elbow-WSS-Base", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B20P03, device = "png", dpi = 144) 
}
```

```{r 'B20P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B20P03", "-Movie-Elbow-WSS-Base", ".png")) #iiii
```

```{r 'B20P0103', echo=FALSE, ref.label=c('B20P01', 'B20P03'), fig.cap="(B20P01 B20P03) Movie: Elbow Curve (WSS) in FactoExtra and Base R"}
#
```

## Plot Clusters

```{r 'B20-PlotClust', include=FALSE}
# #
hh <- res_movies %>% select(2:6, Clusters = k3) %>% relocate(Clusters)
ttl_hh <- "Movie: Genres with k=3"
cap_hh <- "B20P02"
loc_png <- paste0(.z$PX, "B20P02", "-Movie-k3", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plot(hh[ , 2:ncol(hh)], col = viridis(max(hh$Clusters))[hh$Clusters], main = ttl_hh)
  title(sub = cap_hh, line = 4, adj = 1)
  B20 <- recordPlot()
  dev.off()
  assign(cap_hh, B20)
  rm(B20)
}
```

```{r 'B20P02', echo=FALSE, fig.cap="(B20P02) Movie: Genres with k=3"}
knitr::include_graphics(paste0(.z$PX, "B20P02", "-Movie-k3", ".png"))
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B20-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, B20P01, B20P02, B20P03, cap_hh, hh, k2_zw, k3_zw, k4_zw, loc_png,
          res_movies, ttl_hh, xxB20Movies, within_zw, maxK, x_hh, y_hh, xw, zw)
```

```{r 'B20-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/220-Kmeans.Rmd-->

# Hierarchical Clustering (B21, Nov-28) {#b21}

```{r 'B21', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Hierarchical Clustering"
- [Import Data Movies - B20](#set-movies-b20 "b20")

```{r 'B21-GetMovies', ref.label=c('B20-GetMovies', 'B20-PrepMovie')}
# #xxB20Movies, aa, bb, xw, zw
```

```{r 'B21-GetMoviesKnit', include=FALSE, eval=FALSE}
xxB20Movies <- f_getRDS(xxB20Movies)
bb <- aa <- xxB20Movies
xw <- aa %>% select(-1) 
zw <- xw %>% mutate(across(everything(), ~ as.vector(scale(.))))
```

## Elbow Plot of Silhouette

```{r 'B21D01', comment="", echo=FALSE, results='asis'}
f_getDef("Silhouette")
```

- Refer [Silhouette](#sil-c52 "c52")
  - Range [-1, 1]
  - A good solution has Silhouette value approaching 1

- \textcolor{pink}{Question:} Does the value of 0.2 (positive but near zero) is a good value
  - It is comparitive i.e. at what k you are getting max. silhouette value
  - Further, it might be considered as an indication that the dataset might not be ready for clustering
  - (Aside) a value close to zero is considered a weak assignment
- \textcolor{pink}{Question:} If the value is 0.2, can we claim that no clustering is required
  - No, sometimes the data has inherent heterogeneity. If the value is negative that implies some bad clustering. However small positive value does not imply anything.
- \textcolor{pink}{Question:} wss recommended 3, silhouette is recommending 2, now what
  - Use your own judgement
  - (Aside) \textcolor{pink}{"All models are wrong, but some are useful." - By Someone} 
- \textcolor{pink}{Question:} What is the purpose of this 'Optimal Number of Clusters' when we are using our own judgement anyway e.g. in Crime data, silhouette recommends 2 but we are leaning more towards 3
  - We are doing the k-means clustering when we have some idea about number of clusters. However the data might show something different. It is more about validation of our assumption.
  - However, if we do not have any idea about number of clusters, we should NOT use k-means clustering. Rather the Hierarchical Clustering should be used.
  - (Aside) \textcolor{pink}{"An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem." - By Someone}

```{r 'B21-ElbowSil'}
hh <- zw
cap_hh <- "B21P01"
ttl_hh <- "Movie: Elbow Curve (Silhouette)"
#
# #method = "silhouette" (for average silhouette width)
B21P01 <- fviz_nbclust(hh, FUNcluster = kmeans, method = "silhouette") +
  labs(caption = cap_hh, title = ttl_hh)
```

```{r 'B21P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B21P01", "-Movie-Elbow-Sil", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B21P01, device = "png", dpi = 144) 
}
```

```{r 'B21P01', echo=FALSE, fig.cap="(B21P01) Movie: Elbow Curve of k (Silhouette)"}
knitr::include_graphics(paste0(.z$PX, "B21P01", "-Movie-Elbow-Sil", ".png"))
```

## Data Crime {#set-crime-b21}

\textcolor{pink}{Please import the "B21-state-crime.csv"}

```{r 'B21-Crime', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum tools::md5sum(paste0(.z$XL, "B21-state-crime.csv"))
xxB21Crime <- f_getObject("xxB21Crime", "B21-state-crime.csv",
                                "e68f26d9efd2807ad7ed43c29391e191")
```

```{r 'B21-GetCrime', include=FALSE}
xxB21Crime <- f_getRDS(xxB21Crime)
```

- About: [3115, 21]
  - Source: https://corgis-edu.github.io/corgis/csv/state_crime
  - Either keep Rates Columns or Total Columns
  - We want recent year data i.e. 2019 
  - Also USA Overall Needs to be removed

```{r 'B21-PrepCrime'}
aa <- xxB21Crime
# #Only Year 2019 | Exculte USA Total | Only Rates Variables NOT Total | Scale | 
xw <- aa %>% 
  filter(Year == "2019", State != "United States") %>% 
  select(Data.Population, starts_with("Data.Rates") & !ends_with("All"))
#
# #Rename Columns for ease of use
ii <- names(xw)
ii <- str_replace(ii, pattern = paste0(c("Data.Rates.", "Data."), collapse = "|"), "")
ii <- str_replace_all(ii, c("Violent." = "v_", "Property." = "p_"))
names(xw) <- ii
#
zw <- xw %>% mutate(across(everything(), ~ as.vector(scale(.))))
#
dim(xw)
summary(xw)
```

```{r 'B21-CrimeElbow', include=FALSE}
# #Elbow Plots
hh <- zw
cap_hh <- "B21P02"
ttl_hh <- "Crime: Elbow Curve (Silhouette)"
#
B21P02 <- fviz_nbclust(hh, FUNcluster = kmeans, method = "silhouette") +
  labs(caption = cap_hh, title = ttl_hh)
#
cap_hh <- "B21P03"
ttl_hh <- "Crime: Elbow Curve (WSS)"
B21P03 <- fviz_nbclust(hh, FUNcluster = kmeans, method = "wss") +
  labs(caption = cap_hh, title = ttl_hh)
```

```{r 'B21P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B21P02", "-Crime-Elbow-Sil", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B21P02, device = "png", dpi = 144) 
}
```

```{r 'B21P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B21P03", "-Crime-Elbow-WSS", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B21P03, device = "png", dpi = 144) 
}
```

```{r 'B21P02', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B21P02", "-Crime-Elbow-Sil", ".png"))
```

```{r 'B21P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B21P03", "-Crime-Elbow-WSS", ".png"))
```

```{r 'B21P0203', echo=FALSE, ref.label=c('B21P02', 'B21P03'), fig.cap="(B21P02 B21P03) Crime: Elbow Curve of k Silhouette and WSS"}
knitr::include_graphics(paste0(.z$PX, "B21P01", "-Movie-Elbow-Sil", ".png"))
```

```{r 'B21-kmeans'}
# #Cluster analysis with different k = {3, 4}
set.seed(3)
k3_zw <- kmeans(zw, centers = 3)
k4_zw <- kmeans(zw, centers = 4)
# #Save cluster membership of each point back into the dataset
res_crime <- cbind(xw, list(k3 = k3_zw$cluster, k4 = k4_zw$cluster)) %>% as_tibble()
#
# #Three Clusters
ii <- k3_zw
ii$size
paste0("Between /Total = ",  round(100 * ii$betweenss / ii$totss, 2), "%")
round(ii$centers, 3)
#
# #Four Clusters
ii <- k4_zw
ii$size
paste0("Between /Total = ",  round(100 * ii$betweenss / ii$totss, 2), "%")
round(ii$centers, 3)
```

## Hierarchical Clustering

```{r 'B21D02', comment="", echo=FALSE, results='asis'}
f_getDef("Hierarchical-Clustering")
```

```{r 'B21D03', comment="", echo=FALSE, results='asis'}
f_getDef("Agglomerative-Clustering")
```

```{r 'B21D04', comment="", echo=FALSE, results='asis'}
f_getDef("Divisive-Clustering")
```

- Ex: Flipkart
  - We would do Agglomerative Clustering i.e. start with 1000 customers and then get 10 clusters as final rather than starting with 1 cluster
- \textcolor{pink}{Question:} Are not the B2C companies trying for hyper localised individual level targetting of customers
  - No, they are creating higher number of groups based on wider characteristics. Noone is profiling a single customer rather the groups now are highly specific yet contain high number of customers.

- Hierarchical
  - Distance Matrix is used to decide which clusters to merge or split
  - At least quadratic in number of data points
  - Not usable for large datasets

- Notes on Divisive (Because Agglomerative will be in focus mainly)
  - Monothetic or Polythetic Methods
  - intercluster distance can be measured
  - Computationally intensive

## Linkages

```{r 'B21D05', comment="", echo=FALSE, results='asis'}
f_getDef("Single-Linkage")
```

```{r 'B21D06', comment="", echo=FALSE, results='asis'}
f_getDef("Complete-Linkage")
```

```{r 'B21D07', comment="", echo=FALSE, results='asis'}
f_getDef("Average-Linkage")
```

- How the distance matrix is calculated is the main difference between these Methods
  - Some more methods are Centroid Method, Ward Method etc.
- Single Linkage
  - Positives
    - Can handle non-elliptical shapes
  - Negatives
    - Sensitive to Noise and Outliers
    - It produces long, elongated clusters
- Complete Linkage
  - Positives
    - More balanced clusters (with equal diameters)
    - Less susceptible to noise
  - Negatives
    - Tends to break large clusters
    - All clusters tend to have the same diameter - small clusters are merged with larger ones
- Average Linkage
  - Positives  
    - Less susceptible to noise and outliers
  - Negatives
    - Biased towards globular clusters
- Ward
  - Similar to average and centroid
  - Less susceptible to noise and outliers
  - Biased towards globular clusters
  - Hierarchical analgoue of k-means i.e. can be used to initialise k-means

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B21-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, B21P01, B21P02, B21P03, cap_hh, hh, k3_zw, k4_zw, loc_png, 
          res_crime, ttl_hh, xxB20Movies, xxB21Crime, xw, zw)
```

```{r 'B21-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/221-Hierarchical.Rmd-->

# PCA (B22, Dec-05) {#b22}

```{r 'B22', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Principal Component Analysis"
- [Import Data Movies - B20](#set-movies-b20 "b20")

```{r 'B22-GetMovies', ref.label=c('B20-GetMovies', 'B20-PrepMovie')}
# #xxB20Movies, aa, bb, xw, zw
```

```{r 'B22-GetMoviesKnit', include=FALSE, eval=FALSE}
xxB20Movies <- f_getRDS(xxB20Movies)
bb <- aa <- xxB20Movies
xw <- aa %>% select(-1) 
zw <- xw %>% mutate(across(everything(), ~ as.vector(scale(.))))
```

## Packages

```{r 'B22-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("cluster", dependencies = TRUE)
  install.packages("arules", dependencies = TRUE)
  install.packages("arulesViz", dependencies = TRUE)
}
```

## Hierarchical 

- Agglomerative Clustering is also known as 'Bottom-up' and Devisive Clustering is also known as 'Top-down'
- Upto 16:18 Mathematical Formulation of Linkages which are NOT included here.

- \textcolor{pink}{Question:} Nothing much can be inferred from Cluster 2, even though it has high number of data points (136). Does it call for further split
  - In other words, though the Action & Comedy does not show strong preference and other are negative. Thus there is a high possibility that it is heterogeneous in nature. 
  - Probably Yes
  - So we looked at k=4 also, but the average silhouette value got reduced in Figure \@ref(fig:B22P0304)
- \textcolor{pink}{Question:} When k=2 the average silhouette value improved, so should we accept this as optimum number of clusters
  - The value has improved but one of these two clusters has large size (N =233) which is not that good a solution in terms of clustering
- \textcolor{pink}{Question:} Is it ok to mix and match k-means and hierarchical clustering
  - It does happen but we should not mix them. In one case we assume that we have some idea about cluster numbers, in the other one we do not place any assumption.
  

```{r 'B22-dMat'}
str(zw)
#
# #Create distance matrix
dist_zw <- dist(zw)
#
hclust_com_zw <- hclust(dist_zw, method = "complete")
hclust_avg_zw <- hclust(dist_zw, method = "average")
hclust_sng_zw <- hclust(dist_zw, method = "single")
#
# #Cut Tree by Cluster membership
k2_com_zw <- cutree(hclust_com_zw, 2)
k3_com_zw <- cutree(hclust_com_zw, 3)
k4_com_zw <- cutree(hclust_com_zw, 4)
#
table(k3_com_zw)
str(k3_com_zw)
# #Save cluster membership of each point back into the dataset
res_movies <- cbind(xw, list(k3 = k3_com_zw, k4 = k4_com_zw)) %>% as_tibble()
#
# #Cluster Mean
if(FALSE) aggregate(zw, by = list(k3_com_zw), FUN = function(x) round(mean(x), 3))
# #Equivalent
res_movies %>% select(-k4) %>% group_by(k3) %>% summarise(N = n(), across(everything(), mean))
```

## Dendrogram

```{r 'B22-Ddg', include=FALSE}
# #Only the Complete Linkage has been plotted
hh <- hclust_com_zw #hclust_sng_zw hclust_avg_zw
ttl_hh <- "Movie: Dendrogram (Complete Linkage) with k =3 G, 4 B, 6 R"
cap_hh <- "B22P01"
loc_png <- paste0(.z$PX, "B22P01", "-Movie-Dendrogram", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png, width = k_width, height = k_height, units = "in", res = 144) 
  #dev.control('enable') 
# #hang places the values to below the axis. Otherwise the labels show at the end of dendrogram
  plot(hh, hang = -3, main = ttl_hh)
  # #Add a Rectangle on it based on k clusters
  rect.hclust(hh, k = 3, border = "green")
  rect.hclust(hh, k = 4, border = "blue")
  rect.hclust(hh, k = 6, border = "red")
  #title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B22 <- recordPlot()
  dev.off()
  assign(cap_hh, B22)
  rm(B22)
}
```

```{r 'B22P01', echo=FALSE, out.width='100%', fig.cap="(B22P01) Movie: Dendrogram (Complete Linkage) with k =3 G, 4 B, 6 R"}
knitr::include_graphics(paste0(.z$PX, "B22P01", "-Movie-Dendrogram", ".png"))
```

## Silhouette

```{r 'B22-SilDistK2', include=FALSE}
# Silhouette Plot of K2
hh <- k2_com_zw
dist_hh <- dist_zw
ttl_hh <- "Movie: Silhouette with Distance for k=2"
cap_hh <- "B22P02"
loc_png <- paste0(.z$PX, "B22P02", "-Movie-Sil-Dist-k2", ".png")
```

```{r 'B22-SilDist', include=FALSE, eval=FALSE}
# #Silhouette Plot with Distance
# #IN: hh, dist_hh, ttl_hh, cap_hh, loc_png
if(!file.exists(loc_png)) {
  # #If PNG file looks different than the one Plotted in R, Increase Resolution
  png(filename = loc_png, width = k_width, height = k_height, units = "in", res = 144) 
  #dev.control('enable') 
  plot(silhouette(hh, dist = dist_hh), main = ttl_hh, col = viridis(max(hh)))
  title(sub = cap_hh, line = 4, adj = 1)
  B22 <- recordPlot()
  dev.off()
  assign(cap_hh, B22)
  rm(B22)
}
```

```{r 'B22P02-Save', include=FALSE, ref.label=c('B22-SilDist')}
#
```

```{r 'B22P02', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B22P02", "-Movie-Sil-Dist-k2", ".png"))
```

```{r 'B22-SilDistK3', include=FALSE}
# Silhouette Plot of K3
hh <- k3_com_zw
dist_hh <- dist_zw
ttl_hh <- "Movie: Silhouette with Distance for k=3"
cap_hh <- "B22P03"
loc_png <- paste0(.z$PX, "B22P03", "-Movie-Sil-Dist-k3", ".png")
```

```{r 'B22P03-Save', include=FALSE, ref.label=c('B22-SilDist')}
#
```

```{r 'B22P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B22P03", "-Movie-Sil-Dist-k3", ".png"))
```

```{r 'B22-SilDistK4', include=FALSE}
# Silhouette Plot of K4
hh <- k4_com_zw
dist_hh <- dist_zw
ttl_hh <- "Movie: Silhouette with Distance for k=4"
cap_hh <- "B22P04"
loc_png <- paste0(.z$PX, "B22P04", "-Movie-Sil-Dist-k4", ".png")
```

```{r 'B22P04-Save', include=FALSE, ref.label=c('B22-SilDist')}
#
```

```{r 'B22P04', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B22P04", "-Movie-Sil-Dist-k4", ".png"))
```

```{r 'B22P0304', echo=FALSE, ref.label=c('B22P03', 'B22P04'), fig.cap="(B22P03 B22P04) Movie: Silhouette with Distance for k={3, 4}"}
#
```

```{r 'B22P0302', echo=FALSE, ref.label=c('B22P03', 'B22P02'), fig.cap="(B22P03 B22P02) Movie: Silhouette with Distance for k={3, 2}"}
#
```

## Association Rule Mining {#arules-b22}

```{r 'B22D01', comment="", echo=FALSE, results='asis'}
f_getDef("Affinity-Analysis")
```

- It is unsupervised learning
- Ex: People who purchased Milk, also purchased Bread.

- \textcolor{pink}{Question:} Is it similar to Conjoint Analysis
  - No
  - (Aside) 
    - Conjoint analysis is a survey-based statistical technique used in market research that helps determine how people value different attributes (feature, function, benefits) that make up an individual product or service. 
    - In Conjoint analysis, individual customer /user is distinguished whereas in Affinity analysis or the Market Basket analysis, individuals are not identified.

- Problem: Dimensionality
  - The number of possible association rules grows exponentially in the number of attributes.
  - We can focus on relevant products i.e. high margin or low expiration range etc.
  - Further we can use the a priori algorithm
    - The a priori algorithm for mining association rules takes advantage of structure within the rules themselves to reduce the search problem to a more manageable size. 

- Refer [Association Rules](#arules-c53 "c53")

```{r 'B22D02', comment="", echo=FALSE, results='asis'}
f_getDef("Support")
```

```{r 'B22D03', comment="", echo=FALSE, results='asis'}
f_getDef("Confidence")
```

```{r 'B22D04', comment="", echo=FALSE, results='asis'}
f_getDef("Lift")
```

- We generally try to find rules which have high Support, high Confidence and high Lift.

## Data Makeup {#set-makeup-b22 .tabset .tabset-fade}

\textcolor{pink}{Please import the "B22-Makeup.csv"}

```{r 'B22-Makeup', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum tools::md5sum(paste0(.z$XL, "B22-Makeup.csv"))
xxB22Makeup <- f_getObject("xxB22Makeup", "B22-Makeup.csv",
                                "2c55ec694648cfdd1a8dc40fcf37bf96")
```

```{r 'B22-GetMakeup', include=FALSE}
xxB22Makeup <- f_getRDS(xxB22Makeup)
```

- About: [1000, 14]
  - Each Column represents purchase decisions for each of the 1000 transactions
  - We need to have them as 'factor'

### EDA {.unlisted .unnumbered}

```{r 'B22-PrepMakeup'}
bb <- aa <- xxB22Makeup
#
xw <- aa %>% mutate(across(everything(), factor, levels = c("No", "Yes")))
#str(xw)
dim(xw)
summary(xw)
```

## apriori()

- \textcolor{pink}{arules::apriori()} :
  - The default behavior is to mine rules with minimum support of 0.1, minimum confidence of 0.8, maximum of 10 items (maxlen), and a maximal time for subset checking of 5 seconds (maxtime).
  - 'parameter' : These are Support and Confidence
    - help(`` `ASparameter-class` ``)
  - 'appearance' : These are Antecedents and Consequents
    - It can restrict item appearance
  - \textcolor{orange}{Caution:} Never use \textcolor{pink}{inspect()} without filtering out rows otherwise R may hang.
    - `attributes(summary(rules))$length`

- \textcolor{orange}{Warning:} 
  - "Warning in apriori(xw) : Mining stopped (maxlen reached). Only patterns up to a length of 10 returned!"
  - Increase the 'maxlen' parameter value

```{r 'B22-apriori-basic'}
# #Caution is advised on running inspect() without prior subsetting /filtering the rules
# #Find association rules
#rules <- apriori(xw, maxlen = ncol(xw))
rules <- apriori(xw, parameter = list(maxlen = ncol(xw)))
#
# #More Information
names(attributes(rules))
#
str(attributes(rules)$quality)
str(attributes(rules)$info)
attributes(rules)$lhs
attributes(rules)$rhs
#
summary(rules)
#
names(attributes(summary(rules)))
attributes(summary(rules))$length #Check Number of Rules Here.
attributes(summary(rules))$lengths
#
# #inspect() Do not execute without knowing how many rows will be printed.
#inspect(rules[1:6]) 
#inspect(head(rules, 6))
inspect(head(rules, min(5, attributes(summary(rules))$length)))
```

## Analysis {.tabset .tabset-fade}

### Create {.unlisted .unnumbered}

```{r 'B22-apriori', results='hide'}
# #Rules with more control and oversight.
rr_sup <- 0.7
rr_conf <- 0.8
rules <- apriori(xw, parameter = list(
  minlen = 2, maxlen = ncol(xw), support = rr_sup, confidence = rr_conf))
```

```{r 'B22-pRules', include=FALSE, eval=FALSE}
# #Printable Kable Table from Rules
hh <- inspect(head(rules, min(50, attributes(summary(rules))$length))) %>% 
  as_tibble(.name_repair = 'unique') %>% 
  rename(x = '...2', LHS_Antecedent = lhs, RHS_Consequent = rhs) %>% 
  rename_with(str_to_title, .cols = where(is.numeric)) %>% 
  mutate(SN = row_number()) %>% relocate(SN) %>% 
  mutate(across(where(is.numeric), format, digits = 3, drop0trailing = TRUE, scientific = FALSE)) 
```

```{r 'B22-phh-1', include=FALSE, ref.label=c('B22-pRules')}
#
```

```{r 'B22-SetpRules-1', include=FALSE}
#hh <- hh #Printable Kable Table from Rules
names_hh <- names(hh)
cap_hh <- paste0("(B22T01) ", "Support = ", rr_sup, " & Confidence = ", rr_conf, 
                  " gives Rules = ", attributes(summary(rules))$length)
```

```{r 'B22-RulesKbl', include=FALSE, eval=FALSE}
# #IN: hh, cap_hh, names_hh
kbltbl <- kbl(hh,
  caption = cap_hh,
  col.names = names_hh,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
                #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	) #%>% row_spec(row = 1:nrow(hh), color = "black")
# #Print kk separtely when referencing 
# #so that we will have Table object which can be printed in RMD
kbltbl
```

```{r 'B22T01', echo=FALSE, ref.label=c('B22-RulesKbl')}
#
```

### Inspect {.unlisted .unnumbered}

```{r 'B22-Inspect'}
# #Do not print more than 50 Rules at at time.
inspect(head(rules, min(50, attributes(summary(rules))$length)))
```

### Tibble {.unlisted .unnumbered}

```{r 'B22-Tibble'}
# # Limit Max Rows | To Tibble | Rename | Add Row Numbers | Relocate | Format decimals |
inspect(head(rules, min(50, attributes(summary(rules))$length))) %>% 
  as_tibble(.name_repair = 'unique') %>% 
  rename(x = '...2', LHS_Antecedent = lhs, RHS_Consequent = rhs) %>% 
  rename_with(str_to_title, .cols = where(is.numeric)) %>% 
  mutate(SN = row_number()) %>% relocate(SN) %>% 
  mutate(across(where(is.numeric), format, digits = 3, drop0trailing = TRUE, scientific = FALSE)) 
```

### Summarise Count Binary Columns {.unlisted .unnumbered}

```{r 'B22-YesNo'}
# #If Data has TRUE /FALSE in place of Yes /No, Then more options are available: sum() which()
# #Last Option 'mm' does not use table() and remain as tibble() so fewer steps are required
summary(xw)
# #Count Binary Columns | Transpose | Tibble | Integer | Sort |
ii <- t(vapply(xw, table, numeric(2))) %>% 
  as_tibble(rownames = 'Items') %>% 
  mutate(across(where(is.numeric), as.integer)) %>% 
  arrange(desc(Yes))
# #Match One of the Values | Transpose | Tibble | Rename | Wide | Rename | Sort |
jj <- t(table(xw == 'Yes', names(xw)[col(xw)])) %>% 
  as_tibble(.name_repair = 'unique') %>% 
  rename(Items = 1, Key = 2) %>% 
  pivot_wider(names_from = Key, values_from = n) %>% 
  rename(No = 2, Yes = 3) %>% 
  arrange(desc(Yes))
# #Unlist | Remove Appended Numbers | Count | Transpose | Tibble | Rename | Wide | Rename | Sort |
kk <- t(table(unlist(xw), sub('\\d+', '', names(unlist(xw))))) %>% 
  as_tibble(.name_repair = 'unique') %>% 
  rename(Items = 1, Key = 2) %>% 
  pivot_wider(names_from = Key, values_from = n) %>% 
  rename(No = 2, Yes = 3) %>% 
  arrange(desc(Yes))
# #Long | Table | Tibble | Wide | Rename | Sort | 
ll <- xw %>% 
  pivot_longer(cols = everything(), names_to = 'Items', values_to = 'Key') %>% 
  table() %>% 
  as_tibble() %>% 
  pivot_wider(names_from = Key, values_from = n) %>% 
  rename(No = 2, Yes = 3) %>% 
  arrange(desc(Yes))
# #Long | Count | Wide | Sort | 
mm <- xw %>% 
  pivot_longer(cols = everything(), names_to = 'Items', values_to = 'Key') %>% 
  count(Items, Key) %>% 
  pivot_wider(names_from = Key, values_from = n) %>% 
  arrange(desc(Yes))
stopifnot(all(vapply(list(jj, kk, ll, mm), FUN = function(x) identical(x, ii), logical(1))))
#
# #Option 'mm' is preferable
xw %>% 
  pivot_longer(cols = everything()) %>% 
  count(name, value) %>% 
  pivot_wider(names_from = value, values_from = n) %>% 
  arrange(desc(Yes))
```

### Multiple identical {.unlisted .unnumbered}

```{r 'B22-MultiIdentical'}
mm <- ll <- kk <- jj <- ii <- 1:5
# #Pairwise Identical Check
all(identical(ii, jj), identical(ii, kk), identical(ii, ll), identical(ii, mm))
#
# #Pairwise Identical Check using vapply()
# #It can provide info on which pair does not match OR can be passed to all()
vapply(list(jj, kk, ll, mm), FUN = function(x) identical(x, ii), logical(1))
#
stopifnot(all(vapply(list(jj, kk, ll, mm), FUN = function(x) identical(x, ii), logical(1))))
```

## inspect() {.tabset .tabset-fade}

- 'Foundation' has maximum Yes. It is the highest frequency item in the purchase.
  - So taking it as Consequent, we want to look at what are its antecedents

- \textcolor{pink}{Question:} Why have we reduced the support cutoff
  - If support is high then we will not get many rules because we have already restricted RHS to "Foundation" only
- \textcolor{pink}{Question:} Why have we reduced maxlen to 3, Can we not keep the original higher value
  - We can do that. But rules with too many products are not going to help us much. 
  - (Aside) In general, Beyond 3 items the combination add complexity without enough benefits.
- \textcolor{pink}{Question:} Should not we consider the "Foundation" as LHS
  - OR Why Foundation is taken as Consequent (RHS) and not as Antecedent (LHS)
  - We can do that. For now we have chosen "Foundation" as Consequent (RHS)

- \textcolor{pink}{inspect()}
  - Ex: SN = 1: LHS_Antecedent_A {Lip Gloss=Yes} RHS_Consequent_B {Foundation=Yes}
    - 490 'Lip Gloss' purchased in 1000 Total 
    - 536 'Foundation' purchased in 1000 Total
    - 356 'Foundation' purchased within 490 'Lip Gloss' purchases
    - $\text{Prior Proportion} = \text{Support} = P(A \cap B) = \frac{\text{Number of transactions containing both A and B}}{\text{Total Number of Transactions}} = \frac{356}{1000} = 0.356$
    - $\text{Confidence} = P(B|A) = \frac{P(A \cap B)}{P(A)} = \frac{\text{Number of transactions containing both A and B}}{\text{Total Number of Transactions containing A}} = \frac{356}{490} = 0.727$
      - An item set with higher confidence i.e. value near 1 means that this item set has higher likelihood of purchase
      - (Aside) Confidence however overestimates sometimes and is Asymmetric. So using Lift is the better option.
    - $\text{Coverage} = \text{LHS Support} = P(A) = \frac{\text{Number of transactions containing A}}{\text{Total Number of Transactions}} = \frac{490}{1000} = 0.490$
    - Count = Number of transactions containing both A and B = 356
  - Lift
    - Book $\text{Lift} = \frac{\text{Confidence}}{\text{Prior Proportion of Consequent}} = \frac{\text{Confidence}}{\text{RHS Support}} = \frac{0.727}{0.536} = 1.36$
    - Package $\text{Lift} = \frac{\text{Coverage}}{\text{Support}} = \frac{0.490}{0.356} = 1.37$

- \textcolor{pink}{Question:} There are some Redundant rules. Infact out of total 16 Rules, except SN {2, 3, 4}, Rule 1 is basically duplicated with other items being "No".
  - We need to purify the rules
  - "ForLater"
- \textcolor{pink}{Question:} When restricting the "LHS" to Bag and Blush Yes only, their No rules are still present.
  - Reduce the support and confidence cutoff and use default="none"

- A rule is \textcolor{pink}{redundant} if a more general rules with the same or a higher confidence exists. 
  - That is, a more specific rule is redundant if it is only equally or even less predictive than a more general rule. 
  - A rule is more \textcolor{pink}{general} if it has the same RHS but one or more items removed from the LHS.

### Specify RHS {.unlisted .unnumbered}

```{r 'B22-Rules-2', results='hide'}
# #Rules with more control and oversight. RHS: "Foundation=Yes"
rr_sup <- 0.1
rr_conf <- 0.7
rules <- suppressWarnings(apriori(xw, 
  parameter = list(minlen = 2, maxlen = 3, support = rr_sup, confidence = rr_conf),
  appearance = list(rhs = paste0(names(xw)[11], "=", levels(xw[[11]])[2]), 
                    default = "lhs")))
```

```{r 'B22-phh-2', include=FALSE, ref.label=c('B22-pRules')}
#
```

```{r 'B22-SetpRules-2', include=FALSE}
#hh <- hh #Printable Kable Table from Rules
names_hh <- names(hh)
cap_hh <- paste0("(B22T02) ", "Support = ", rr_sup, " & Confidence = ", rr_conf, 
                  " gives Rules = ", attributes(summary(rules))$length)
```

```{r 'B22T02', echo=FALSE, ref.label=c('B22-RulesKbl')}
#
```

### Verify a Rule {.unlisted .unnumbered}

```{r 'B22-SingleRule'}
# #Specific Rule: SN = 1: LHS_Antecedent {Lip Gloss=Yes} RHS_Consequent {Foundation=Yes}
ii <- xw %>% select(11, 12) %>% rename(Lip_Gloss = 2) %>% count(Foundation, Lip_Gloss)
# #490 'Lip Gloss' purchased in 1000 Total
ii %>% group_by(Lip_Gloss) %>% summarise(SUM = sum(n)) %>% mutate(PROP = SUM/sum(SUM))
# #536 'Foundation' purchased in 1000 Total
ii %>% group_by(Foundation) %>% summarise(SUM = sum(n)) %>% mutate(PROP = SUM/sum(SUM))
# #356 'Foundation' purchased within 490 'Lip Gloss' purchases
ii %>% filter(Lip_Gloss == 'Yes') %>% mutate(PROP = n/sum(n))
```

### Focus Specific LHS {.unlisted .unnumbered}

```{r 'B22-Rules-3', results='hide'}
# #RHS: "Foundation=Yes"
# #LHS: Only Bag Yes, Blush Yes | To identify these Rules supply lower support and confidence
# #(Default) = "both", "lhs", "rhs", "none". Specified the default appearance for all items ...
# #...not explicitly mentioned in the other elements of the list.
# #If default = "lhs" is supplied then redundant rules come up.
rr_sup <- 0.01
rr_conf <- 0.1
rules <- suppressWarnings(apriori(xw, 
  parameter = list(minlen = 2, maxlen = 3, support = rr_sup, confidence = rr_conf),
  appearance = list(rhs = paste0(names(xw)[11], "=", levels(xw[[11]])[2]), 
                    lhs = c("Bag=Yes", "Blush=Yes"), 
                    default = "none")))
```

```{r 'B22-phh-3', include=FALSE, ref.label=c('B22-pRules')}
#
```

```{r 'B22-SetpRules-3', include=FALSE}
#hh <- hh #Printable Kable Table from Rules
names_hh <- names(hh)
cap_hh <- paste0("(B22T03) ", "Support = ", rr_sup, " & Confidence = ", rr_conf, 
                  " gives Rules = ", attributes(summary(rules))$length)
```

```{r 'B22T03', echo=FALSE, ref.label=c('B22-RulesKbl')}
#
```

### Yes LHS Only {.unlisted .unnumbered}

```{r 'B22-Rules-4', results='hide'}
# #RHS: "Foundation=Yes"
# #LHS: All Yes Only 
rr_sup <- 0.01
rr_conf <- 0.1
rr_rhs <- 11L #index of "Foundation"
rules <- suppressWarnings(apriori(xw, 
  parameter = list(minlen = 2, maxlen = 3, support = rr_sup, confidence = rr_conf),
  appearance = list(rhs = paste0(names(xw)[rr_rhs], "=", levels(xw[[rr_rhs]])[2]), 
                    lhs = paste0(names(xw)[-rr_rhs], "=", levels(xw[[rr_rhs]])[2]), 
                    default = "none")))
```

```{r 'B22-phh-4', include=FALSE, ref.label=c('B22-pRules')}
#
```

```{r 'B22-SetpRules-4', include=FALSE}
#hh <- hh #Printable Kable Table from Rules
names_hh <- names(hh)
cap_hh <- paste0("(B22T04) ", "Support = ", rr_sup, " & Confidence = ", rr_conf, 
                  " gives Rules = ", attributes(summary(rules))$length)
```

```{r 'B22T04', echo=FALSE, ref.label=c('B22-RulesKbl')}
#
```

### Paste String to each element {.unlisted .unnumbered}

```{r 'B22-PasteStr'}
# #Paste String "=Yes" to each element of a Vector except "Foundation"
names(xw)
paste0(names(xw)[-11], "=", levels(xw[[11]])[2])
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B22-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, cap_hh, dist_hh, dist_zw, hclust_avg_zw, hclust_com_zw, 
          hclust_sng_zw, hh, k2_com_zw, k3_com_zw, k4_com_zw, kbltbl, loc_png, mm, names_hh, 
          res_movies, rr_conf, rr_rhs, rr_sup, rules, ttl_hh, xw, xxB20Movies, xxB22Makeup, zw)
```

```{r 'B22-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/222-PCA.Rmd-->

# Mid Year Evaluation (B23, Dec-12) {#b23}

```{r 'B23', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Data Champo Carpets {#set-champo-b23 .tabset .tabset-fade}

### Data {.unlisted .unnumbered}

\textcolor{pink}{Please import the "B23-Champo.csv"}

```{r 'B23-ChampoList'}
xxB23Champo <- c("xxB23Champo_2_Both", "xxB23Champo_3_Order", "xxB23Champo_4_Sample",
              "xxB23Champo_6_Cluster", "xxB23Champo_5_Reco", "xxB23Champo_7_Colours", 
              "xxB23Champo_8_SKU", "xxB23Champo_9_RecoTrans")
```

```{r 'B23-ChampoRDS', include=FALSE}
# #Read data from Binary Files
#lst <- lapply(xxChampo, function(x) {readRDS(paste0(XL, x, ".rds"))})
xxB23Champo_2_Both       <- readRDS(paste0(.z$XL, xxB23Champo[1], ".rds"))
xxB23Champo_3_Order      <- readRDS(paste0(.z$XL, xxB23Champo[2], ".rds")) #Subset of All
xxB23Champo_4_Sample     <- readRDS(paste0(.z$XL, xxB23Champo[3], ".rds"))
xxB23Champo_5_Reco       <- readRDS(paste0(.z$XL, xxB23Champo[4], ".rds"))
xxB23Champo_6_Cluster    <- readRDS(paste0(.z$XL, xxB23Champo[5], ".rds"))
xxB23Champo_7_Colours    <- readRDS(paste0(.z$XL, xxB23Champo[6], ".rds"))
xxB23Champo_8_SKU        <- readRDS(paste0(.z$XL, xxB23Champo[7], ".rds"))
xxB23Champo_9_RecoTrans  <- readRDS(paste0(.z$XL, xxB23Champo[8], ".rds"))
```

```{r 'B23-ChampoDim'}
# #Dimensions of all of these datasets
str(lapply(xxB23Champo, function(x) {dim(eval(parse(text = x)))}))
```

### Import Excel {.unlisted .unnumbered}

```{r 'B23-ChampoXL', eval=FALSE}
# #Path to the Excel File #read_delim(clipboard())
loc_src <- paste0(.z$XL, "B23-Champo.xlsx")
#excel_sheets(loc_src)
# #Read Sheets
xxB23Champo_2_Both      <- read_excel(path = loc_src, sheet = 2)
xxB23Champo_3_Order     <- read_excel(path = loc_src, sheet = 3)
xxB23Champo_4_Sample    <- read_excel(path = loc_src, sheet = 4)
xxB23Champo_6_Cluster   <- read_excel(path = loc_src, sheet = 6)
xxB23Champo_5_Reco      <- read_excel(path = loc_src, sheet = 5, range = "A1:U21")
xxB23Champo_7_Colours   <- read_excel(path = loc_src, sheet = 7, range = "A1:H12")
xxB23Champo_8_SKU       <- read_excel(path = loc_src, sheet = 7, range = "J1:Q12")
xxB23Champo_9_RecoTrans <- read_excel(path = loc_src, sheet = 5, range = "X1:AR21")
```

```{r 'B23-ChampoSave', eval=FALSE}
# #Save the Loaded data as Binary Files
for(ii in xxB23Champo){
  saveRDS(eval(parse(text = ii)), paste0(.z$XL, ii, ".rds"))
}
```

### General Information {.unlisted .unnumbered}

Process (3 weeks to 3 months) : Design $\Rightarrow$ CAD (Visual, Material) $\Rightarrow$ Procurement $\Rightarrow$ Warehousing $\Rightarrow$ Dying $\Rightarrow$ Storage of Dyed Yarn $\Rightarrow$ Preparation for Weaving or Hand-Tufting $\Rightarrow$ Wounding $\Rightarrow$ Finishing (edges etc.) $\Rightarrow$ inspection $\Rightarrow$ Dispatch.

Product categories (4 major) - hand-tufted carpets (least effort, most popular), hand knotted carpets (skilled, most expensive), Kilims (woolen, expensive) and Durries (Indian variant)

Company sent samples to the client as per ...

- the latest fiber and color trends
- color and design attributes of their past purchases
- raw material availability in the inventory (preferred, focused effort)
- reproduced the swatches as sent by the client into samples

\textcolor{pink}{Cost-efficient way of selecting appropriate sample designs that could generate maximum revenue.}

Belief: 
carpet attributes could be used for creating customer segments, which in turn could be used for developing models such as classification to identify customer preferences and recommendation systems

\textcolor{pink}{to identify the most important customers and the most important products and find a way to connect the two using suitable attributes from data and appropriate analytics models}

## Data Fanatasy Sports {#set-fantasy-b23 .tabset .tabset-fade}

### Data {.unlisted .unnumbered}

\textcolor{pink}{Please import the "B23-FantasySports.csv"}

```{r 'B23-SportsList'}
xxB23Sports <- c("xxB23Sports_Q3_2T_Paid", "xxB23Sports_Q3_2T_Free", "xxB23Sports_Q4_2T",
  "xxB23Sports_Q5_Chi_Player", "xxB23Sports_Q5_Chi_Captain", "xxB23Sports_Q6_2T_119_Select",
  "xxB23Sports_Q6_2T_119_NotSelect", "xxB23Sports_Q6_2T_6_Select", "xxB23Sports_Q6_2T_6_NotSelect",
  "xxB23Sports_Q7_Anova_NotSelect", "xxB23Sports_Q7_Anova_Captain", "xxB23Sports_Q7_Anova_VC",
  "xxB23Sports_Q8_Regression")
```

```{r 'B23-SportsRDS', include=FALSE}
# #Read data from Binary Files
xxB23Sports_Q3_2T_Paid           <- readRDS(paste0(.z$XL, xxB23Sports[1], ".rds"))
xxB23Sports_Q3_2T_Free           <- readRDS(paste0(.z$XL, xxB23Sports[2], ".rds"))
xxB23Sports_Q4_2T                <- readRDS(paste0(.z$XL, xxB23Sports[3], ".rds"))
xxB23Sports_Q5_Chi_Player        <- readRDS(paste0(.z$XL, xxB23Sports[4], ".rds"))
xxB23Sports_Q5_Chi_Captain       <- readRDS(paste0(.z$XL, xxB23Sports[5], ".rds"))
xxB23Sports_Q6_2T_119_Select     <- readRDS(paste0(.z$XL, xxB23Sports[6], ".rds"))
xxB23Sports_Q6_2T_119_NotSelect  <- readRDS(paste0(.z$XL, xxB23Sports[7], ".rds"))
xxB23Sports_Q6_2T_6_Select       <- readRDS(paste0(.z$XL, xxB23Sports[8], ".rds"))
xxB23Sports_Q6_2T_6_NotSelect    <- readRDS(paste0(.z$XL, xxB23Sports[9], ".rds"))
xxB23Sports_Q7_Anova_NotSelect   <- readRDS(paste0(.z$XL, xxB23Sports[10], ".rds"))
xxB23Sports_Q7_Anova_Captain     <- readRDS(paste0(.z$XL, xxB23Sports[11], ".rds"))
xxB23Sports_Q7_Anova_VC          <- readRDS(paste0(.z$XL, xxB23Sports[12], ".rds"))
xxB23Sports_Q8_Regression        <- readRDS(paste0(.z$XL, xxB23Sports[13], ".rds"))
```

```{r 'B23-SportsDim'}
# #Dimensions of all of these datasets.
#sapply(lapply(xxChampo, function(x) {dim(eval(parse(text = x)))}), "[[", 1)
str(lapply(xxB23Sports, function(x) {dim(eval(parse(text = x)))}))
```

### Import Excel {.unlisted .unnumbered}

```{r 'B23-SportsXL', eval=FALSE}
# #Path to the Excel File
loc_src <- paste0(.z$XL, "B23-FantasySports.xlsx")
# #Read Sheets
xxB23Sports_Q3_2T_Paid     <- read_excel(path = loc_src, sheet = 2, range = "A8:C5188")
xxB23Sports_Q3_2T_Free     <- read_excel(path = loc_src, sheet = 2, range = "E8:G8296")
xxB23Sports_Q4_2T          <- read_excel(path = loc_src, sheet = 3, range = "A8:O80")
xxB23Sports_Q5_Chi_Player  <- read_excel(path = loc_src, sheet = 4, range = "A15:P25")
xxB23Sports_Q5_Chi_Captain <- read_excel(path = loc_src, sheet = 4, range = "A31:P41")
xxB23Sports_Q8_Regression  <- read_excel(path = loc_src, sheet = 7, range = "A7:E55279")
#
# #Create CSV Files because of package failure in reading large excel data
tbl <- read_csv(paste0(.z$XL, "B23-Sports-Q6-2T-119-Select", ".csv"), show_col_types = FALSE)
attr(tbl, "spec") <- NULL
attr(tbl, "problems") <- NULL
xxB23Sports_Q6_2T_119_Select <- tbl
tbl <- read_csv(paste0(.z$XL, "B23-Sports-Q6-2T-119-NotSelect", ".csv"), show_col_types = FALSE)
attr(tbl, "spec") <- NULL
attr(tbl, "problems") <- NULL
xxB23Sports_Q6_2T_119_NotSelect <- tbl
tbl <- read_csv(paste0(.z$XL, "B23-Sports-Q6-2T-6-Select", ".csv"), show_col_types = FALSE)
attr(tbl, "spec") <- NULL
attr(tbl, "problems") <- NULL
xxB23Sports_Q6_2T_6_Select <- tbl
tbl <- read_csv(paste0(.z$XL, "B23-Sports-Q6-2T-6-NotSelect", ".csv"), show_col_types = FALSE)
attr(tbl, "spec") <- NULL
attr(tbl, "problems") <- NULL
xxB23Sports_Q6_2T_6_NotSelect <- tbl
#
tbl <- read_csv(paste0(.z$XL, "B23-Sports-Q7-Anova-NotSelect", ".csv"), show_col_types = FALSE)
attr(tbl, "spec") <- NULL
attr(tbl, "problems") <- NULL
xxB23Sports_Q7_Anova_NotSelect <- tbl
tbl <- read_csv(paste0(.z$XL, "B23-Sports-Q7-Anova-Captain", ".csv"), show_col_types = FALSE)
attr(tbl, "spec") <- NULL
attr(tbl, "problems") <- NULL
xxB23Sports_Q7_Anova_Captain <- tbl
tbl <- read_csv(paste0(.z$XL, "B23-Sports-Q7-Anova-VC", ".csv"), show_col_types = FALSE)
attr(tbl, "spec") <- NULL
attr(tbl, "problems") <- NULL
xxB23Sports_Q7_Anova_VC <- tbl
```

```{r 'B23-SportsSave', eval=FALSE}
# #Save the Loaded data as Binary Files
for(ii in xxB23Sports){
  saveRDS(eval(parse(text = ii)), paste0(.z$XL, ii, ".rds"))
}
```

### General Information {.unlisted .unnumbered}

whether fantasy sports was a game of chance or skill, especially whether skill is a dominant factor in winning fantasy sports competition.

(Legal) The decision between skill and chance was to be decided based on whether the skill-based element was dominant over chance in determining the outcome of the game.

If a fantasy sports is chance based, then every user should have an equal probability of winning.

understand the key difference between skill and chance and how to test it using the data. 

To prove that it is skill dominant, we have to prove that users who are scoring high in fantasy sports are playing a strategic game, their selection of players and captain and vice-captain is more knowledge based than random selection. 

If fantasy games involve skill, then we can expect consistency in the performance of the users both low as well as high. Alternatively, we can also check whether a selection specific player increases probability of winning fantasy sports. 


I think our approach should be to identify and test several possible hypotheses to establish whether fantasy sports is skill dominant or chance dominant. 

- Various rounds/match were played. For example, one IPL match would be one round. 
- There were players who were available to be picked up for a round or match. (This number is more than the number of players who would actually play in that match, so it was possible that a player selected by a user in his team may not actually play.) 
- For every round, multiple contests were opened. The contests were of different categories, from free to paid, and various types of playing and winning options (public, private, special). 
- User selected a team for a round and for a contest. 
- There was a player round performance table which indicated how the player performed in the specific round. 
- Teams selected by users were scored on the basis of the selected performance of player in a contest and those team level scores were provided in the contest user table


Few possible hypotheses are listed below: 

1. Users playing free contests are scoring lower than users playing paid contests. This can prove that when users play paid contests, they play more cautiously and strategically, and do not select teams at random. 
2. Scores of randomly selected players can be tested against scores of the teams based on a specific strategy such as selecting players who have performed well in the recent matches. 
3. Is the selection of captains and vice captains of the team random (equal probability)
4. Selection of players and winning or getting high scores are dependent on each other. 
5. As the user plays more games, his chance of winning increases (learning effect). 


11 Players, 100 credits, Captain 2x, VC 1.5x, Max. players from a Team =7 (C1 ... C7)

## Data RFM {#set-rfm-b23}

\textcolor{pink}{Please import the "B23-RFM.csv"}

```{r 'B23-RFM', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum tools::md5sum(paste0(.z$XL, "B23-RFM.csv"))
xxB23RFM <- f_getObject("xxB23RFM", "B23-RFM.csv",
                                "6bff05ba4e507d3f0cc6e2dec2f7a3c0")
```

```{r 'B23-GetRFM', include=FALSE}
xxB23RFM <- f_getRDS(xxB23RFM)
```

## Q1 RFM {.tabset .tabset-fade}

As a data scientist, you would like to analyse recency, frequency, and monetary value of an online store. Based on the same, you would like to suggest suitable market segments so that the online store can implement marketing actions efficiently and effectively. In this attempt, use the data (See "B23-RFM.csv"), perform the RFM analysis, provide practical /managerial recommendations. 

- About: [2823, 25]
  - There are NA in 4 Columns, However none of these columns are required for RFM so are being kept.
  - Convert ORDERDATE to Date
- Conclusion
  - We are loosing customers and we need to provide incentives for more frequent visits. It is clearly visible that higher frequency have direct positive correlation with higher monetary purchase.

### RFM & Segments {.unlisted .unnumbered}

```{r 'B23-Q1-RFM'}
bb <- aa <- xxB23RFM 
# #Convert to Date
bb$ORDERDATE <- dmy(bb$ORDERDATE)
# #Get Analysis Date as the Next Date after the Max Date in the Data
analysis_date <- max(bb$ORDERDATE) + 1 #as_date("2005-12-02")
#
# #RFM analysis by rfm_table_order()
rfm_result <- rfm_table_order(bb, customer_id = CUSTOMERNAME, order_date = ORDERDATE, 
                              revenue = SALES, analysis_date = analysis_date)
# #Output is a Tibble with some other attributes
loc_src <- paste0(.z$XL, "B23-Results-RFM.csv")
# #Save the Result in a CSV
if(FALSE) write_csv(rfm_result$rfm, file = loc_src)
```

```{r 'B23-Segments'}
# #Developing segments
segment_titles <- c("First Grade", "Loyal", "Likely to be Loyal", "New Ones", 
                    "Could be Promising", "Require Assistance", "Getting Less Frequent",
                    "Almost Out", "Can not Lose Them", "Do not Show Up at All") 
# #Rules of Minimum and Maximum RFM for each group
r_low  <- c(4, 2, 3, 4, 3, 2, 2, 1, 1, 1)
r_high <- c(5, 5, 5, 5, 4, 3, 3, 2, 1, 2)
f_low  <- c(4, 3, 1, 1, 1, 2, 1, 2, 4, 1)
f_high <- c(5, 5, 3, 1, 1, 3, 2, 5, 5, 2)
m_low  <- c(4, 3, 1, 1, 1, 2, 1, 2, 4, 1)
m_high <- c(5, 5, 3, 1, 1, 3, 2, 5, 5, 2)
#
stopifnot(all(vapply(list(r_low, r_high, f_low, f_high, m_low, m_high), 
                     FUN = function(x) identical(length(x), length(segment_titles)), logical(1))))
```

```{r 'B23-DivBySeg'}
divisions <- rfm_segment(rfm_result, segment_names = segment_titles, 
                       recency_lower = r_low, recency_upper = r_high, 
                       frequency_lower = f_low, frequency_upper = f_high, 
                       monetary_lower = m_low, monetary_upper = m_high)
# #Output is a Tibble 
# #Save the Result in a CSV
loc_src <- paste0(.z$XL, "B23-Results-Divisions.csv")
if(FALSE) write_csv(divisions, file = loc_src)
#
# #We defined 10 segments, However only 7 (+1) of them are represented in the data 
# #and 48 customers were not captured by our classifications. These were assigned to 'Others'
divisions %>% 
  count(segment) %>% 
  mutate(PCT = round(100 * n / sum(n), 1)) %>% 
  rename(SEGMENT = segment, FREQ = n) %>% 
  arrange(desc(FREQ)) 
#
```

### Plots Not Plotted {.unlisted .unnumbered}

```{r 'B23-Plots', eval=FALSE}
if(FALSE) {#Histograms of Median RFM for each Segment
  hh <- divisions
  rfm_plot_median_recency(hh)
  rfm_plot_median_frequency(hh)
  rfm_plot_median_monetary(hh)
}
if(FALSE) {
  hh <- rfm_result
  rfm_histograms(hh) #Histograms of RFM
  rfm_order_dist(hh) #Histograms of Customer Orders i.e. Frequency
  rfm_heatmap(hh)    #Heatmap of Monetary on Axes of Recency and Frequency. Slighly Useful
  rfm_bar_chart(hh)  #Bar Charts with Facetting of RFM
  # #Scatter Plots among Recency, Monetary, Frequency
  rfm_rm_plot(hh)
  rfm_fm_plot(hh)
  rfm_rf_plot(hh)
}
```


### NA {.unlisted .unnumbered}

```{r 'B23-NA'}
colSums(is.na(aa)) %>% as_tibble(rownames = "Cols") %>% filter(value > 0)
```

## Q2 Sports P1 {.tabset .tabset-fade}

Dream 11 platform has both free and paid users, that is, users who play games for free with no return and users who pay a fee and obtain returns at the end of the game based on their relative performance. Can the average scores of paid and free users can help Dream 11 in testing skill- based game. (See Sheet "Qns_3_2SampleTTest" of "B23-FantasySports.xlsx")

```{r 'B23D01', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Lower")
```

- (1: Free) ${n}_1 = 8288, {\overline{x}}_1 = 289.5, {\sigma}_1 = 91.6$
- (2: Paid) ${n}_2 = 5180, {\overline{x}}_2 = 301.2, {\sigma}_2 = 74.9$
- Compare with ${\alpha} = 0.05$
  - $\{{}^L\!P_{(t = -8.0797)} = 0 \} < {\alpha} \to {H_0}$ is rejected. Alternative is accepted.
- Conclusion
  - It is a Skill based game because performance of Free users is lower than that of Paid users. 

- \textcolor{pink}{Question:} ContestType was ignored in above analysis. However, the Means are different between public vs. private in case of Free and are same in case of Paid. How should we proceed
  - "ForLater"

### Free vs. Paid {.unlisted .unnumbered}

```{r 'B23-Q2P1-FreePaid'}
# #Data
free <- xxB23Sports_Q3_2T_Free$userpoints
paid <- xxB23Sports_Q3_2T_Paid$userpoints
#
# #Sample Information
round(vapply(f_namedList(free, paid), 
             FUN = function(x) {c(N = length(x), Mean = mean(x), SD = sd(x))}, 
             FUN.VALUE = numeric(3)), 1)
#
# #Welch Two Sample t-test
ha_bb <- "less" #"two.sided" (Default), "less", "greater"
testT_bb <- t.test(x = free, y = paid, alternative = ha_bb)
testT_bb
```

```{r 'B23-Q2P1-Out-1', echo=FALSE, ref.label=c('B23-tTest-Outcome')}
#
```


### Free: Public vs. Private {.unlisted .unnumbered}

```{r 'B23-Q2P1-Free'}
# #Contest Type Comparison for Free
bb <- xxB23Sports_Q3_2T_Free %>% 
  select(key = 2, value = 3) %>% 
  filter(key == "public" | key == "private") %>%
  mutate(across(key, factor))
#
# #Sample Information
bb %>% group_by(key) %>% summarise(N = n(), Mean = round(mean(value), 1), SD = round(sd(value), 1))
#
# #Welch Two Sample t-test
ha_bb <- "two.sided" #"two.sided" (Default), "less", "greater"
testT_bb <- t.test(formula = value ~ key, data = bb, alternative = ha_bb)
testT_bb
```

```{r 'B23-Q2P1-Out-2', echo=FALSE, ref.label=c('B23-tTest-Outcome')}
#
```

### Paid: Public vs. Private {.unlisted .unnumbered}

```{r 'B23-Q2P1-Paid'}
# #Contest Type Comparison for Paid
bb <- xxB23Sports_Q3_2T_Paid %>% 
  select(key = 2, value = 3) %>% 
  filter(key == "public" | key == "private") %>%
  mutate(across(key, factor))
#
# #Sample Information
bb %>% group_by(key) %>% summarise(N = n(), Mean = round(mean(value), 1), SD = round(sd(value), 1))
#
# #Welch Two Sample t-test
ha_bb <- "two.sided" #"two.sided" (Default), "less", "greater"
testT_bb <- t.test(formula = value ~ key, data = bb, alternative = ha_bb)
testT_bb
```

```{r 'B23-Q2P1-Out-3', echo=FALSE, ref.label=c('B23-tTest-Outcome')}
#
```

### Outcome {.unlisted .unnumbered}

```{r 'B23-tTest-Outcome', eval=FALSE}
# #Compare p-value with alpha = 0.05
alpha <- 0.05
if(any(all(ha_bb == "two.sided", testT_bb$p.value >= alpha / 2), 
       all(ha_bb != "two.sided", testT_bb$p.value >= alpha))) {
  cat(paste0("p-value (", round(testT_bb$p.value, 6), ") is greater than alpha (", alpha, 
      "). We failed to reject H0. We cannot conclude that the populations are different.\n")) 
} else {
    cat(paste0("p-value (", round(testT_bb$p.value, 6), ") is less than alpha (", alpha, 
      ").\nWe can reject the H0 with 95% confidence. The populations are different.\n"))
}
```

## Q2 Sports P2 

Scores of users who use some strategy to select players such as recent performance of players are higher than users who select players randomly. (See Sheet "Qns_4_2SampleTTest" of "B23-FantasySports.xlsx")

```{r 'B23D02', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Lower")
```

- (1: Random) ${n}_1 = 36, {\overline{x}}_1 = 249.2, {\sigma}_1 = 55.1$
- (2: Strategy) ${n}_2 = 36, {\overline{x}}_2 = 372.8, {\sigma}_2 = 42.2$
- Compare with ${\alpha} = 0.05$
  - $\{{}^L\!P_{(t = -10.69)} = 0 \} < {\alpha} \to {H_0}$ is rejected. Alternative is accepted.
- Conclusion
  - It is a Skill based game because performance of users with Random strategy is lower than that of  users with some strategy.

```{r 'B23-Q2P2-Strategy'}
# #Team Type Comparison with correction of Typo
bb <- xxB23Sports_Q4_2T %>% 
  select(key = "TeamType", value = "totalpoints") %>% 
  mutate(across(key, str_replace, "Stratergy", "Strategy")) %>% 
  mutate(across(key, factor)) 
#
# #Sample Information
bb %>% group_by(key) %>% summarise(N = n(), Mean = round(mean(value), 1), SD = round(sd(value), 1))
#
# #Welch Two Sample t-test
ha_bb <- "less" #"two.sided" (Default), "less", "greater"
testT_bb <- t.test(formula = value ~ key, data = bb, alternative = ha_bb)
testT_bb
```

```{r 'B23-Q2P2', echo=FALSE, ref.label=c('B23-tTest-Outcome')}
#
```

## Q2 Sports P3

If fantasy-sports is a game of skill, then player performance has a major role in the player getting selected to a team as well as selection of captain or vice-captain. Using the data, can we test if selection of players in a team and getting high scores are dependent on each other. (See Sheet "Qns_5_2SampleTTest" of "B23-FantasySports.xlsx")

- User Category (Top Quartile or Not in Top Quartile) is Categorical 
- Player Selected (Yes or No) is Categorical
- Since both the variables are categorical, we would need to perform Chi-square test
- $P_{\chi^2} < {\alpha} \to {H_0}$ is rejected. Alternative is accepted.
  - Population Proportions are different.
  - The sample results provide sufficient evidence that 'selection of a plyer in the team' and 'user high scores' are dependent on each other.
  
```{r 'B23-Q2P3-ChiSq'}
# #Select | Sum | Long | Separate String | Wide | Relocate | Column To RowNames |
bb <- xxB23Sports_Q5_Chi_Player %>% 
  select(nTop_nSelect = 3, Top_nSelect = 4, nTop_Select = 5, Top_Select = 6) %>% 
  summarise(across(everything(), sum)) %>% 
  pivot_longer(everything()) %>% 
  separate(name, c("isTop", "isSelect")) %>% 
  pivot_wider(names_from = isSelect, values_from = value) %>% 
  relocate(nSelect, .after = last_col()) %>% 
  column_to_rownames('isTop')
bb
# #Chi-squared Test
chisq.test(bb)
```

## Q2 Sports P4

In the data supplied, a few users have selected one of the top three high performing players as captain or vice-captain. There are also users who have not used any of these players as captain or vice-captain. Ramsu claims that choosing high performing players as captain and/or vice-captain has an impact on the scores of the users. Test this claim made by Ramsu and link it to the business problem to make an inference. (See Sheet "Qns_7_Anova" of "B23-FantasySports.xlsx")

- ANOVA Conclusion:
  - Choosing high performing players as captain and/or vice- captain has an impact on the scores of the users.

- \textcolor{pink}{Question:} Coefficients of ANOVA show only "NoSelect" and "ViceCap". "Captain" is missing. What does it mean
  - "ForLater"

```{r 'B23-Q2P4-ANOVA'}
# #NOTE: Because of High number of rows, data was exported to CSV and then Imported
NoSelect <- xxB23Sports_Q7_Anova_NotSelect %>% drop_na(userpoints) %>% select(userpoints)
Captain <- xxB23Sports_Q7_Anova_Captain %>% drop_na(userpoints) %>% select(userpoints)
ViceCap <- xxB23Sports_Q7_Anova_VC %>% drop_na(userpoints) %>% select(userpoints)
#
# #Merge Datasets by Rows
q2p4 <- bind_rows(NoSelect = NoSelect, Captain = Captain, ViceCap = ViceCap, .id = 'Type') 
# ANOVA
anv_q2p4 <- aov(userpoints ~ Type, data = q2p4)
anv_q2p4
#
summary(anv_q2p4)
#
# #Coefficients
anv_q2p4$coefficients
```

## Q3 Champo 

Discuss clustering algorithms that can be used for segmenting customers of Champo Carpets. Apply both k-means and hierarchical clustering techniques and provide insights on the segments we can extract out these data.

Note: Use Sheet "Data Order ONLY" of "B23-Champo.xlsx". It has 13,135 records. Use only the numerical variables (e.g., quantity required, total area, and amount) for performing cluster anlaysis.

- k-means : 3 Clusters looks optimum or we should go with 6
  - Outliers are present and it looks like 6 is better number of clusters
- Hierarchical : Dendrogram was not plotted because of noise
- Conclusion
  - Country USA and Customer TGT have large share on revenue and these should be the focus area.
  - Orders: USA 86% UK 6% by Revenue
  - CustomerCode: "TGT 36%, H-2 12%"
  - Notes:
    - There are Orders having 0 Amount i.e. NO Revenue!
    - 6 Rows are 100% duplicated and some are almost duplicated i.e. DesignName, ColourName changed but no impact on Amount
    - There is a 1-1 Relationship between Country and Customer i.e. a Customer belongs to a specific country only

```{r 'B23-Prep-kMeans'}
xw <- xxB23Champo_3_Order %>% select(Quantity = QtyRequired, Area = TotalArea, Amount)
zw <- xw %>% mutate(across(everything(), ~ as.vector(scale(.))))
str(xw)
```

```{r 'B23-Elbow', eval=FALSE}
# #This is Slow.
hh <- zw
cap_hh <- "B23P01"
ttl_hh <- "Champo: Elbow Curve (WSS)"
#
# #factoextra::fviz_nbclust() generates ggplot
# #method = "wss" (for total within sum of square)
B23P01 <- fviz_nbclust(hh, FUNcluster = kmeans, method = "wss") +
  labs(caption = cap_hh, title = ttl_hh)
```

```{r 'B23P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B23P01", "-Champo-Elbow-Wss", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B23P01, device = "png", dpi = 144) 
}
```

```{r 'B23P01', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B23P01", "-Champo-Elbow-Wss", ".png"))
```

```{r 'B23-ElbowSil', eval=FALSE}
hh <- zw
cap_hh <- "B23P02"
ttl_hh <- "Champo: Elbow Curve (Silhouette)"
#
# #method = "silhouette" (for average silhouette width)
B23P02 <- fviz_nbclust(hh, FUNcluster = kmeans, method = "silhouette") +
  labs(caption = cap_hh, title = ttl_hh)
```

```{r 'B23P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B23P02", "-Champo-Elbow-Sil", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B23P02, device = "png", dpi = 144) 
}
```

```{r 'B23P02', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B23P02", "-Champo-Elbow-Sil", ".png"))
```

### WSS vs. Silhouette {.unlisted .unnumbered}

```{r 'B23P0102', echo=FALSE, ref.label=c('B23P01', 'B23P02'), fig.cap="(B23P01 B23P02) Champo: WSS and Silhouette"}
#
```

### k-means {.unlisted .unnumbered}

```{r 'B23-kmeans'}
# #Fix Seed
set.seed(3)
# #Cluster analysis with k = {3, 6}
k3_zw <- kmeans(zw, centers = 3)
k6_zw <- kmeans(zw, centers = 6)
#
# #Save cluster membership of each point back into the dataset
res_champo <- cbind(xw, 
  list(k3 = k3_zw$cluster, k6 = k6_zw$cluster)) %>% as_tibble()
```

```{r 'B23-k3'}
# #Three Clusters
ii <- k3_zw
ii$size
paste0("Between /Total = ",  round(100 * ii$betweenss / ii$totss, 2), "%")
round(ii$centers, 3)
```

```{r 'B23-k6'}
# #Six Clusters
ii <- k6_zw
ii$size
paste0("Between /Total = ",  round(100 * ii$betweenss / ii$totss, 2), "%")
round(ii$centers, 3)
```

### Plot k=3 {.unlisted .unnumbered}

```{r 'B23-PlotClust3', include=FALSE}
# #
hh <- res_champo %>% select(1:3, Clusters = k3) %>% relocate(Clusters)
ttl_hh <- "Champo: Genres with k=3"
cap_hh <- "B23P03"
loc_png <- paste0(.z$PX, "B23P03", "-Champo-k3", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plot(hh[ , 2:ncol(hh)], col = viridis(max(hh$Clusters))[hh$Clusters], main = ttl_hh)
  title(sub = cap_hh, line = 4, adj = 1)
  B23 <- recordPlot()
  dev.off()
  assign(cap_hh, B23)
  rm(B23)
}
```

```{r 'B23P03', echo=FALSE, fig.cap="(B23P03) Champo: k-means with k=3"}
knitr::include_graphics(paste0(.z$PX, "B23P03", "-Champo-k3", ".png"))
```

### Plot k=6 {.unlisted .unnumbered}

```{r 'B23-PlotClust6', include=FALSE}
# #
hh <- res_champo %>% select(1:3, Clusters = k6) %>% relocate(Clusters)
ttl_hh <- "Champo: Genres with k=6"
cap_hh <- "B23P04"
loc_png <- paste0(.z$PX, "B23P04", "-Champo-k6", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plot(hh[ , 2:ncol(hh)], col = viridis(max(hh$Clusters))[hh$Clusters], main = ttl_hh)
  title(sub = cap_hh, line = 4, adj = 1)
  B23 <- recordPlot()
  dev.off()
  assign(cap_hh, B23)
  rm(B23)
}
```

```{r 'B23P04', echo=FALSE, fig.cap="(B23P04) Champo: k-means with k=6"}
knitr::include_graphics(paste0(.z$PX, "B23P04", "-Champo-k6", ".png"))
```

### Hierarchical {.unlisted .unnumbered}

```{r 'B23-dMat'}
str(zw)
#
# #Create distance matrix
dist_zw <- dist(zw)
#
hclust_com_zw <- hclust(dist_zw, method = "complete")
#hclust_avg_zw <- hclust(dist_zw, method = "average")
#hclust_sng_zw <- hclust(dist_zw, method = "single")
#
# #Cut Tree by Cluster membership
k3_com_zw <- cutree(hclust_com_zw, 3)
k4_com_zw <- cutree(hclust_com_zw, 4)
k6_com_zw <- cutree(hclust_com_zw, 6)
#
# #Save cluster membership of each point back into the dataset
hrc_champo <- cbind(xw, list(k3 = k3_com_zw, k4 = k4_com_zw, k6 = k6_com_zw)) %>% as_tibble()
#
# #Cluster Mean
if(FALSE) aggregate(zw, by = list(k3_com_zw), FUN = function(x) round(mean(x), 3))
# #Equivalent
hrc_champo %>% select(-c(k4, k6)) %>% group_by(k3) %>% 
  summarise(N = n(), across(everything(), mean))
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B23-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, xxB23Champo, xxB23Champo_2_Both, xxB23Champo_3_Order,
  xxB23Champo_4_Sample, xxB23Champo_5_Reco, xxB23Champo_6_Cluster, xxB23Champo_7_Colours,
  xxB23Champo_8_SKU, xxB23Champo_9_RecoTrans, xxB23RFM, xxB23Sports, xxB23Sports_Q3_2T_Free,
  xxB23Sports_Q3_2T_Paid, xxB23Sports_Q4_2T, xxB23Sports_Q5_Chi_Captain, xxB23Sports_Q5_Chi_Player,
  xxB23Sports_Q6_2T_119_NotSelect, xxB23Sports_Q6_2T_119_Select, xxB23Sports_Q6_2T_6_NotSelect,
  xxB23Sports_Q6_2T_6_Select, xxB23Sports_Q7_Anova_Captain, xxB23Sports_Q7_Anova_NotSelect,
  xxB23Sports_Q7_Anova_VC, xxB23Sports_Q8_Regression, alpha, analysis_date, divisions, free, 
  ha_bb, loc_src, m_high, m_low, paid, r_high, r_low, rfm_result, segment_titles, testT_bb, anv_q2p4, cap_hh, Captain, dist_zw, hclust_com_zw, hh, hrc_champo, k3_com_zw, k3_zw, k4_com_zw, k6_com_zw, k6_zw, loc_png, NoSelect, q2p4, res_champo, ttl_hh, ViceCap, xw, zw)
```

```{r 'B23-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/223-MidYearEval.Rmd-->

# Association Rule (B24, Dec-19) {#b24}

```{r 'B24', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Association Rule Mining"
- [Import Data Makeup - B22](#set-makeup-b22 "b22")
- Discussion on R Markdown / R Notebook from 16:45 - 17:20 has not been included.
- "ForLater" Professor will cover "Attribution Model" later.


```{r 'B24-GetMakeup', ref.label=c('B22-GetMakeup', 'B22-PrepMakeup')}
# #xxB22Makeup, aa, bb, xw
```

```{r 'B24-GetMakeupKnit', include=FALSE, eval=FALSE}
xxB22Makeup <- f_getRDS(xxB22Makeup)
bb <- aa <- xxB22Makeup
xw <- aa %>% mutate(across(everything(), factor, levels = c("No", "Yes")))
```

```{r 'B24-AddRed', include=FALSE, eval=FALSE}
# #Tibble excluding Redundant Rules #IN: rules Out: hh, pruned_tbl 
# #Rules | DataFrame | Tibble | Rename | TitleCase | Add Columns | Filter | Drop | SN | Relocate |
pruned_tbl <- DATAFRAME(rules) %>% as_tibble() %>% 
  rename(LHS_Antecedent = LHS, RHS_Consequent = RHS) %>% 
  rename_with(str_to_title, .cols = where(is.numeric)) %>% 
  mutate(isRedConf = isRedConf, isRedLift = isRedLift) %>% 
  filter(!isRedLift) %>% 
  select(-c(isRedConf, isRedLift)) %>% 
  arrange(desc(Lift)) %>% 
  mutate(SN = row_number()) %>% 
  relocate(SN) 
#
hh <- pruned_tbl
```

```{r 'B24-RulesKbl', include=FALSE, eval=FALSE}
# #IN: hh, cap_hh, names_hh
# #Printable Kable Table from Rules
hh <- hh %>% slice(1:10) %>% 
  mutate(across(where(is.numeric), format, digits = 3, drop0trailing = TRUE, scientific = FALSE)) 
#
kbltbl <- kbl(hh,
  caption = cap_hh,
  col.names = names_hh,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
                #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	) #%>% row_spec(row = 1:nrow(hh), color = "black")
# #Print kk separtely when referencing 
# #so that we will have Table object which can be printed in RMD
kbltbl
```

## Packages

```{r 'B24-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("arules", dependencies = TRUE)
  install.packages("arulesViz", dependencies = TRUE)
}
```

## Redundant Rules {.tabset .tabset-fade}

- \textcolor{pink}{Question:} What is the meaning of maxlen = 10. Will there be 10 Rules or 10 items
  - Default is 10. We will look at upto combination of 10 items
  - (Aside) The algorithm will search upto a set of maximum 10 unique items i.e. for maxlen = 2, Rules will be considered upto item pairs {AB, AC, BC} but for maxlen = 3, triplets will also be considered i.e. {ABC} & so on
- \textcolor{pink}{Question:} What is the meaning of minlen = 2
  - Number of items in the Basket
  - (Aside) If minlen is 2 then Antecedent of single item with Consequent of same item would also be considered a rule. i.e. People who bought Potato, also bought Potato. minlen = 2 ensures that at least two unique items would be present in the combination set of Antecedent and Consequent.

- \textcolor{pink}{is.redundant()}
  - It returns a logical vector
  - Rule $\{A, B\} \Rightarrow \{Z\}$ is a more general rule compared to the specific rule $\{A, B, C\} \Rightarrow \{Z\}$. In other words, First one is the superset of second one.
  - If the general rule has higher confidence than the specific rule, we do not need to look at the specific rule. Specific rule is redundant.
- \textcolor{pink}{Question:} What happens if the specific rule $\{A, B, C\} \Rightarrow \{Z\}$ has higher confidence than general rule $\{A, B\} \Rightarrow \{Z\}$
  - Then we cannot delete
  - Refer Table \@ref(tab:B24T01)
    - SN = 1 is Specific {Lip Gloss=Yes, Lipstick=Yes} whereas SN = 2 is General {Lip Gloss=Yes}. Yet, the specific rule is not categorised as redundant because it has higher confidence (0.734) than that of general rule (0.727)


```{definition 'Redundant'}
A rule can be defined as \textcolor{pink}{redundant} if a more general rules with the same or a higher confidence exists.
```

```{r 'B24D05', comment="", echo=FALSE, results='asis'}
f_getDef("A-Priori-Property") #dddd
```

- \textcolor{pink}{Question:} What happens if we put minlen = 0 or (minlen = 2 and maxlen = 2) because triplets are causing a lot of redundancy
  - There is no meaning of Set of items with 0 items
  - We can filter out the rules which are redundant
  - (Aside) Rather than the outright exclusion of set of items with 3 or more items, it is better to get more rules and then filter based on the criteria.

- \textcolor{pink}{Question:} What is the meaning of Lift < 1
  - It means that Antecedent should not be linked with the Consequent. That rule should not be considered.
  - You can also locate the item based on your own margin and other considerations. 
  - Seasonal data would be different. Storewise data would be different. 

### Yes LHS Only {.unlisted .unnumbered}

```{r 'B24-Rules-1', results='hide'}
# #RHS: "Foundation=Yes" 
# #LHS: All Yes Only 
rr_sup <- 0.1
rr_conf <- 0.5
rr_rhs <- 11L #index of "Foundation"
rules <- suppressWarnings(apriori(xw, 
  parameter = list(minlen = 2, maxlen = 3, support = rr_sup, confidence = rr_conf),
  appearance = list(rhs = paste0(names(xw)[rr_rhs], "=", levels(xw[[rr_rhs]])[2]), 
                    lhs = paste0(names(xw)[-rr_rhs], "=", levels(xw[[rr_rhs]])[2]), 
                    default = "none")))
# #Check Redundancy of Rules beased on measure = {"confidence", "oddsRatio", "lift"}
# #Note these are columns names in rules, if these do not exist then creeate them
isRedConf <- is.redundant(rules, measure = "confidence")
isRedLift <- is.redundant(rules, measure = "lift")
#which(isRedConf)
hh <- rules
```

```{r 'B24-AddRed-1', include=FALSE, ref.label=c('B24-AddRed')}
#
```

```{r 'B24-SetpRules-1', include=FALSE}
#hh <- hh #Printable Kable Table from Rules 
names_hh <- names(hh)
cap_hh <- paste0("(B24T01) ", "Support = ", rr_sup, " & Confidence = ", rr_conf, 
                  " Excluding Redundant (Lift) & Sorted (Lift) gives Rules = ", nrow(hh))
```

```{r 'B24T01', echo=FALSE, ref.label=c('B24-RulesKbl')}
#
```

### Prune {.unlisted .unnumbered}

```{r 'B24-AddRed-A', eval=FALSE, ref.label=c('B24-AddRed')}
#
```

```{r 'B24-Prune'}
# #Sort and Subset are available for Rules (Similar to above)
ii <- rules[!isRedLift]
pruned <- sort(ii, by = "lift")
inspect(pruned)
#
# #Qualtity of Rules (Dataframe of Numerics i.e. Support, Confidence, Coverage, Lift, Count) 
# #However it is sorted by confidence only and is of limited use only
quality(pruned)
```

## Support vs Confidence {.tabset .tabset-fade}

```{conjecture 'plot-new'}
\textcolor{brown}{Error in ... : plot.new has not been called yet}
```

- If the error occurs while using plot() or if the resultant png is blank, it might be a ggplot object rather than Base R. 
- Try to use ggplot commands including ggsave() on it.
- OR if it is actually a base R plot, add \textcolor{pink}{plot.new()} before calling plot()
- OR it might be lattice plot object
  - Check the class of the object, check its plotting method, check if the exported or internal plotting method has a ggplot equivalent

```{conjecture 'non-numeric-argument'}
\textcolor{brown}{Error in plot(...) : non-numeric argument to binary operator}
```

- If the error occurs while using ggplot2 syntax, the object might be Base R plot. 


### ScatterPlot {.unlisted .unnumbered}

```{r 'B24-SupConf', include=FALSE}
if(FALSE) pruned_tbl
hh <- pruned_tbl
#
ttl_hh <- "Makeup: Rules Excluding Redundant"
cap_hh <- "B24P01"
sub_hh <- "Note: To stretch the difference, X-axis is only upto 0.5 not 1"
#
if(FALSE) { #To Remove the lowest contrast white from the Palette. This Works.
  #Drop Lowest Two Colours in Palatte which has Max 9 Colours and get a continour function
  k_Palette_B24_O <- colorRampPalette(brewer.pal(9, "Oranges")[-c(1:2)]) 
  # #Use the Function to Rescale the gradients
  k_Scale_B24_O <- scale_colour_gradientn(colours = k_Palette_B24_O(10), 
                                          limits=c(min(hh$Lift), max(hh$Lift)))
}
#
B24 <- hh %>% { 
  ggplot(., aes(x = Support, y = Confidence, colour = Lift, label = LHS_Antecedent)) +
  geom_point() +
  geom_text_repel(max.overlaps = 20) +
  scale_colour_viridis_c(direction = -1) +
  #scale_colour_distiller(palette = "Oranges", direction = 1) +
  #k_Scale_B24_O + 
  scale_y_continuous(breaks = breaks_pretty(), limits = c(0, 1)) + 
  scale_x_continuous(breaks = breaks_pretty(), limits = c(0, 0.5)) + 
  #coord_fixed() +
  theme(plot.title.position = "panel", legend.position = c(0.9, 0.3)) +
  labs(subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B24)
rm(B24)
```

```{r 'B24P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B24P01", "-Makeup-SupConf", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B24P01, device = "png", dpi = 144) 
}
```

```{r 'B24P01', echo=FALSE, fig.cap="(B24P01) Makeup: Support and Confidence with Lift as Gradient"}
knitr::include_graphics(paste0(.z$PX, "B24P01", "-Makeup-SupConf", ".png")) 
```


### Code {.unlisted .unnumbered}

```{r 'B24-SupConf-A', eval=FALSE, ref.label=c('B24-SupConf')}
#
```

## Data Basket {#set-basket-b24 .tabset .tabset-fade}

\textcolor{pink}{Please import the "B24-Basket.csv"}

```{r 'B22-GetBasket', include=FALSE}
xxB24Basket <- f_getRDS(xxB24Basket)
```

- \textcolor{pink}{Question:} What about other type of data like Orderwise or Customerwise
  - We will look at that also.

- \textcolor{pink}{arules::read.transactions()}
  - For reading Transaction Data
- \textcolor{orange}{Warning:} 
  - "In asMethod(object) : removing duplicated items in transactions"
  - The problem is not with duplicated transactions (the same row appearing twice) but duplicated items (the same item appearing more than once in the same transaction)
  - Add `rm.duplicates = TRUE` to remove these objects
  - \textcolor{pink}{Question:} What if you want to show that double the normal amount has been bought in a transaction by showing the items twice in the same tranaction
    - Then it is not the kind of information that 'apriori' handles
    - Further, 'arules' require transactions without duplicated items
      - It stores 'sparse matrix' which can store exist/not-exist for each item and cannot store quantity

### EDA {.unlisted .unnumbered}

```{r 'B24-PrepBasket'}
bb <- aa <- xxB24Basket
#
# #It is a sparse matrix
names(attributes(aa))
#
str(attributes(aa)$itemInfo$labels)
#
names(attributes(summary(aa)))
#
attributes(summary(aa))$Dim
#
attributes(summary(aa))$itemSummary
#
attributes(summary(aa))$lengths
#
#summary(aa)
```

### Import Transactions {.unlisted .unnumbered}

```{r 'B24-Basket', include=FALSE, eval=FALSE}
xxB24Basket <- read.transactions(paste0(.z$XL, "B24-Basket.csv"), 
                                 sep = ',', rm.duplicates = TRUE)
saveRDS(xxB24Basket, paste0(.z$XL, "xxB24Basket", ".rds"))
```


## Item Frequency {.tabset .tabset-fade}

### Bar Plot {.unlisted .unnumbered}

```{r 'B24-ItemFreqAbs', include=FALSE}
# #Absolute Item Frequency Plot using sparse matrix from arules
hh <- aa 
nn_hh <- 15L
type_hh <-  "absolute" # "relative"
ttl_hh <- paste0("Basket: Absolute Item Frequency Plot of Items for Top N = ", nn_hh)
cap_hh <- "B24P02"
x_hh <- NULL
y_hh <- "Item Frequency (Absolute)"
loc_png <- paste0(.z$PX, "B24P02", "-Basket-Freq-Abs", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  itemFrequencyPlot(hh, topN = nn_hh, type = type_hh, 
                    col = viridis(nn_hh), xlab = x_hh, ylab = y_hh, main = ttl_hh)
  title(sub = cap_hh, line = 4, adj = 1)
  B24 <- recordPlot()
  dev.off()
  assign(cap_hh, B24)
  rm(B24)
}
```

```{r 'B24P02', echo=FALSE, fig.cap="(B24P02) Basket: Absolute Item Frequency Plot of Items"}
knitr::include_graphics(paste0(.z$PX, "B24P02", "-Basket-Freq-Abs", ".png"))
```

### Code {.unlisted .unnumbered}

```{r 'B24-ItemFreqAbs-A', eval=FALSE, ref.label=c('B24-ItemFreqAbs')}
#
```

## Rules for Basket

- This did not provide any good result because Count is 2 for most of the rules which is of no use. It is totally random.

```{r 'B24-BasketRules', results='hide'}
# #RHS: "Foundation=Yes"
# #LHS: All Yes Only 
xw <- aa
rr_sup <- 0.0001 #extremely low value used
rr_conf <- 0.5
#rr_rhs <- 11L #index of "Foundation"
rules <- suppressWarnings(apriori(xw, 
  parameter = list(minlen = 2, maxlen = 3, support = rr_sup, confidence = rr_conf)))
# #Check Redundancy of Rules beased on measure = {"confidence", "oddsRatio", "lift"}
isRedConf <- is.redundant(rules, measure = "confidence")
isRedLift <- is.redundant(rules, measure = "lift")
#
#which(isRedConf)
hh <- rules
```

```{r 'B24-AddRed-2', include=FALSE, ref.label=c('B24-AddRed')}
#
```

```{r 'B24-SetpRules-2', include=FALSE}
#hh <- hh #Printable Kable Table from Rules
names_hh <- names(hh)
cap_hh <- paste0("(B24T02) ", "Basket: Not Useful (Count is 2 mostly)")
```

```{r 'B24T02', echo=FALSE, ref.label=c('B24-RulesKbl')}
#
```

## Data Groceries {#set-groceries-b24 .tabset .tabset-fade}

\textcolor{pink}{Please import the "B24-Basket.csv"}

```{r 'B22-GetGroceries', include=FALSE}
xxB24Groceries <- f_getRDS(xxB24Groceries)
```

### EDA {.unlisted .unnumbered}

```{r 'B24-PrepGroceries'}
bb <- aa <- xxB24Groceries
#
str(attributes(aa)$itemInfo$labels)
#
attributes(summary(aa))$Dim
#
attributes(summary(aa))$itemSummary
#
attributes(summary(aa))$lengths
#
#summary(aa)
```

### Import {.unlisted .unnumbered}

```{r 'B24-Groceries', include=FALSE, eval=FALSE}
xxB24Groceries <- read.transactions(paste0(.z$XL, "B24-Groceries.csv"), 
                                    sep = ',', rm.duplicates = TRUE)
saveRDS(xxB24Groceries, paste0(.z$XL, "xxB24Groceries", ".rds"))
```

## Rules for Groceries

- This did not provide any good result because Count is 2 for most of the rules which is of no use. It is totally random.

```{r 'B24-GroceriesRules', results='hide'}
# #RHS: "Foundation=Yes"
# #LHS: All Yes Only 
xw <- aa
rr_sup <- 0.01 #extremely low value used
rr_conf <- 0.5
#rr_rhs <- 11L #index of "Foundation"
rules <- suppressWarnings(apriori(xw, 
  parameter = list(minlen = 2, maxlen = 3, support = rr_sup, confidence = rr_conf)))
# #Check Redundancy of Rules beased on measure = {"confidence", "oddsRatio", "lift"}
isRedConf <- is.redundant(rules, measure = "confidence")
isRedLift <- is.redundant(rules, measure = "lift")
#
#which(isRedConf)
hh <- rules
```

```{r 'B24-AddRed-3', include=FALSE, ref.label=c('B24-AddRed')}
#
```

```{r 'B24-SetpRules-3', include=FALSE}
#hh <- hh #Printable Kable Table from Rules
names_hh <- names(hh)
cap_hh <- paste0("(B24T03) ", "Groceries: ", "Support = ", rr_sup, " & Confidence = ", rr_conf, 
                  " Excluding Redundant (Lift) & Sorted (Lift) gives Rules = ", nrow(hh))
```

```{r 'B24T03', echo=FALSE, ref.label=c('B24-RulesKbl')}
#
```


### ScatterPlot {.unlisted .unnumbered}

```{r 'B24-SupConf-Gro', include=FALSE}
# #
hh <- pruned_tbl
#
ttl_hh <- "Groceries: Rules Excluding Redundant"
cap_hh <- "B24P03"
sub_hh <- NULL #"Note: To stretch the difference, X-axis is only upto 0.5 not 1"
#
B24 <- hh %>% { 
  ggplot(., aes(x = Support, y = Confidence, colour = Lift, label = LHS_Antecedent)) +
  geom_point(aes(shape = RHS_Consequent), size =  2) + 
  geom_text_repel(max.overlaps = 20) +
  scale_colour_viridis_c(direction = -1) +
  scale_shape_manual(values=c(1, 4)) +
  scale_y_continuous(breaks = breaks_pretty(), limits = c(0, 1)) + 
  scale_x_continuous(breaks = breaks_pretty(), limits = c(0, 0.025)) + 
  theme(plot.title.position = "panel", legend.position = c(0.8, 0.85), 
        legend.key.size = unit(0.4, 'cm'), legend.box = "horizontal") +
  labs(subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B24)
rm(B24)
```

```{r 'B24P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B24P03", "-Groceries-SupConf", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B24P03, device = "png", dpi = 144) 
}
```

```{r 'B24P03', echo=FALSE, fig.cap="(B24P03) Groceries: Support vs. Confidence with Lift as Gradient and Two RHS"}
knitr::include_graphics(paste0(.z$PX, "B24P03", "-Groceries-SupConf", ".png")) 
```


## Regression and Classification Framework

```{r 'B24D01', comment="", echo=FALSE, results='asis'}
f_getDef("Unsupervised-Methods")
```

```{r 'B24D02', comment="", echo=FALSE, results='asis'}
f_getDef("Supervised-Methods")
```

- [Data Mining Methods](#mining-b19 "b19") and [Definitions](#mining-def-c31 "c31")
  - Data mining methods may be categorized as either supervised or unsupervised.
  - Most data mining methods are supervised methods.
  - Unsupervised : Clustering, PCA, Factor Analysis, Association Rules, RFM
  - Supervised : 
    - Regression (Continuous Target) : Linear Regression, Regularised Regression, Decision trees, Ensemble learning 
      - Linear Regression : Ridge, Lasso and Elastic Regression
      - Ensemble learning : Bagging, Boosting (AdaBoost, XGBoost), Random forests
    - Classification (Categorical Target) : Decision trees, Ensemble learning, Logistic Regression, k-nearest neighbor (k-NN), Naive-Bayes 
    - Deep Learning : Neural Networks

```{r 'B24D03', comment="", echo=FALSE, results='asis'}
f_getDef("Estimation")
```

```{r 'B24D04', comment="", echo=FALSE, results='asis'}
f_getDef("Classification")
```

- \textcolor{pink}{Question:} Why so many different types of algorithms
  - We would need to apply all of them one by one on our dataset and would need to analyse which of them provide better performance measure on our specific dataset 
- \textcolor{pink}{Question:} What is the performance measure
  - We calculate the predictive power /accuracy of various algorithms
  - Example: Cross-validation (Train & Test)
- \textcolor{pink}{Question:} AutoML functionality provided by various tools apply various algorithms on the dataset and suggest the best possible Algorithm. Are there any drawbacks or limitations of that
  - Mostly Proprietary, EDA and Pre-processing is difficult, limitations of what they allow
  - R, Python : No restriction, Noone else has access to your data
  - Longterm availability and applicability of past learnings based on your specific datasets
  - Automated R Package: rattle: Easy but many restrictions
  - Kaggle : Many competitions happen based on different algorithms

## Groceries (Visualisations)

```{r 'B24-GroceriesViz'}
set.seed(3)
data(Groceries)
xw <- Groceries
#
rr_sup <- 0.001
rr_conf <- 0.5
rules <- apriori(xw, parameter=list(support = rr_sup, confidence = rr_conf))
#
# #Number of Rules
attributes(summary(rules))$length
#
# #Filter Rules with low confidence score.
subrules <- rules[quality(rules)$confidence > 0.8]
attributes(summary(subrules))$length
#
# #Top10 for Graph Based visualisation
t10rules <- head(rules, n = 10, by = "lift")
```


```{conjecture 'Not-Exported-Object'}
\textcolor{brown}{Error: 'plot' is not an exported object from 'namespace:arulesViz'}
```

- Package 'arulesViz' provides S3 method for plot(). To avoid this overloading we can explicitly call the function like \textcolor{pink}{arulesViz::plot()}. However, this is NOT an exported function.
- So, we can use the \textcolor{pink}{:::} operator. For a package 'pkg' :
  - \textcolor{pink}{pkg::name} returns the value of the exported variable 'name' in namespace 'pkg'
  - \textcolor{pink}{pkg:::name} returns the value of the internal variable 'name'. 
  - The package namespace will be loaded if it was not loaded before the call, but the package will not be attached to the search path. 
  - i.e. \textcolor{pink}{arulesViz:::plot.rules()} - Note the intenal function name is NOT plot().


### ScatterPlot {.tabset .tabset-fade}

A straight-forward visualization of association rules is to use a scatter plot with two interest measures on the axes. We can see that rules with high lift have typically a relatively low support. It has been argued that the most interesting rules reside on the support /confidence border.

- All 5 quality measures are available for scatterplot i.e. support, confidence, coverage, lift, count.
- Other measures can be added by \textcolor{pink}{interestMeasure()} and then plotted

#### Image {.unlisted .unnumbered}

```{r 'B24-SuppConfLift', include=FALSE}
hh <- rules
cap_hh <- "B24P04"
ttl_hh <- "Groceries: Support vs Confidence with Shading by Lift"
#
if(FALSE) {# #Accessing Internal Plotting Method plot.rules()
  B24 <- suppressMessages(
  arulesViz:::plot.rules(hh, measure = c("support", "confidence"), shading = "lift")) +
  labs(subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
if(FALSE){# #Help for different methods 
  plot(hh, method = "paracoord", control = "help")  
}
#
B24 <- suppressMessages(
  plot(hh, measure = c("support", "confidence"), shading = "lift")) +
  labs(subtitle = NULL, caption = cap_hh, title = ttl_hh)
assign(cap_hh, B24)
rm(B24)
#
# #S3 Method Used by arulesViz::plot()
#Registered S3 methods overwritten by 'registry':
#  method               from 
#  print.registry_field proxy
#  print.registry_entry proxy
```

```{r 'B24P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B24P04", "-Groceries-SuppConfLift", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B24P04, device = "png", dpi = 144) 
}
```

```{r 'B24P04', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B24P04", "-Groceries-SuppConfLift", ".png"))
```

```{r 'B24-SuppLiftConf', include=FALSE}
hh <- rules
cap_hh <- "B24P05"
ttl_hh <- "Groceries: Support vs Lift with Shading by Confidence"
#
B24 <- suppressMessages(
  plot(hh, measure = c("support", "lift"), shading = "confidence")) +
  labs(subtitle = NULL, caption = cap_hh, title = ttl_hh)
assign(cap_hh, B24)
rm(B24)
```

```{r 'B24P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B24P05", "-Groceries-SuppLiftConf", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B24P05, device = "png", dpi = 144) 
}
```

```{r 'B24P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B24P05", "-Groceries-SuppLiftConf", ".png")) 
```


```{r 'B24P0405', echo=FALSE, ref.label=c('B24P04', 'B24P05'), fig.cap="(B24P04 B24P05) Groceries: Support, Confidence and Lift"}
#
```

#### Code {.unlisted .unnumbered}

```{r 'B24-SuppConfLift-A', eval=FALSE, ref.label=c('B24-SuppConfLift')}
#
```

```{r 'B24-SuppLiftConf-A', eval=FALSE, ref.label=c('B24-SuppLiftConf')}
#
```


### Two-key Plot {.tabset .tabset-fade}

Here support and confidence are used for the x and y-axes and the color of the points is used to indicate "order", i.e., the number of items contained in the rule (Both LHS and RHS).

It would be better to plot this with all rules.

From the plot it is clear that order and support have a very strong inverse relationship, which is a known fact for association rules.

#### Image {.unlisted .unnumbered}

```{r 'B24-TwoKey', include=FALSE}
hh <- rules
cap_hh <- "B24P06"
ttl_hh <- "Groceries: Two-key Plot"
#
B24 <- suppressMessages(
  plot(hh, method = "two-key plot")) +
  labs(subtitle = NULL, caption = cap_hh, title = ttl_hh)
assign(cap_hh, B24)
rm(B24)
```

```{r 'B24P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B24P06", "-Groceries-Two-key", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B24P06, device = "png", dpi = 144) 
}
```

```{r 'B24P06', echo=FALSE, fig.cap="(B24P06) Groceries: Two-key Plot: Support and Confidence with Items"}
knitr::include_graphics(paste0(.z$PX, "B24P06", "-Groceries-Two-key", ".png")) 
```

#### Code {.unlisted .unnumbered}

```{r 'B24-TwoKey-A', eval=FALSE, ref.label=c('B24-TwoKey')}
#
```


### Matrix Plot {.tabset .tabset-fade}

2D matrix is used and the interest measure is represented by color shading of squares at the intersection.
For this type of visualization the number of rows/columns depends on the number of unique itemsets in the consequent/antecedent in the set of rules.

#### Image {.unlisted .unnumbered}

```{r 'B24-Matrix', include=FALSE}
hh <- subrules #rules
cap_hh <- "B24P07"
ttl_hh <- "Groceries: Matrix"
#
B24 <- plot(hh, method = "matrix", measure = "lift") +
  labs(subtitle = NULL, caption = cap_hh, title = ttl_hh)
assign(cap_hh, B24)
rm(B24)
```

```{r 'B24P07-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B24P07", "-Groceries-Matrix", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B24P07, device = "png", dpi = 144) 
}
```

```{r 'B24P07', echo=FALSE, fig.cap="(B24P07) Groceries: Matrix"}
knitr::include_graphics(paste0(.z$PX, "B24P07", "-Groceries-Matrix", ".png")) 
```

#### Code {.unlisted .unnumbered}

```{r 'B24-Matrix-A', eval=FALSE, ref.label=c('B24-Matrix')}
#
```


### Grouped Matrix {.tabset .tabset-fade}

Matrix-based visualization is limited in the number of rules it can visualize effectively since
large sets of rules typically also have large sets of unique antecedents /consequents. We can enhance matrix-based visualization using grouping of rules via clustering to handle a larger number of rules. 

Grouped rules are presented as an aggregate in the matrix and can be explored interactively by zooming into and out of groups.

- To group the column vectors fast and efficient into k groups we use k-means clustering. 
- The default interest measure used is lift. 
- The idea is that antecedents that are statistically dependent on the same consequents are similar and thus can be grouped together. 
  - Compared to other clustering approaches for itemsets, this method enables us to even group antecedents containing substitutes (e.g., butter and margarine) which are rarely purchased together since they will have similar dependence to the same consequents.

- To visualize the grouped matrix we use a balloon plot with antecedent groups as columns and consequents as rows.
  - The color of the balloons represent the aggregated interest measure in the group with a certain consequent and the size of the balloon shows the aggregated support. 
  - The default aggregation function is the median value in the group. 
  - The number of antecedents and the most important (frequent) items in the group are displayed as the labels for the columns. 
  - Furthermore, the columns and rows in the plot are reordered such that the aggregated interest measure is decreasing from top down and from left to right, placing the most interesting group in the top left corner.
  
- The group of most interesting rules according to lift (the default measure) are shown in the top-left corner of the plot. 
  - There are 3 rules which contain "Instant food products" and up to 2 other items in the antecedent and the consequent is "hamburger meat."
  - To increase the number of groups we can change k which defaults to 20.

#### Image {.unlisted .unnumbered}

```{r 'B24-GroupMatrix', include=FALSE}
hh <- rules #rules #subrules 
cap_hh <- "B24P08"
ttl_hh <- "Groceries: Grouped Matrix"
#
B24 <- plot(hh, method = "grouped") +
  labs(subtitle = NULL, caption = cap_hh, title = ttl_hh)
assign(cap_hh, B24)
rm(B24)
```

```{r 'B24P08-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B24P08", "-Groceries-GroupMat", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B24P08, device = "png", dpi = 144) 
}
```

```{r 'B24P08', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B24P08", "-Groceries-GroupMat", ".png")) 
```

```{r 'B24-GroupMatrixK', include=FALSE}
hh <- rules #rules #subrules 
cap_hh <- "B24P09"
ttl_hh <- "Groceries: Grouped Matrix with k = 50"
#
B24 <- plot(hh, method = "grouped", control = list(k = 50)) +
  labs(subtitle = NULL, caption = cap_hh, title = ttl_hh)
assign(cap_hh, B24)
rm(B24)
```

```{r 'B24P09-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B24P09", "-Groceries-GroupMat-50k", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B24P09, device = "png", dpi = 144) 
}
```

```{r 'B24P09', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B24P09", "-Groceries-GroupMat-50k", ".png")) 
```


```{r 'B24P0809', echo=FALSE, ref.label=c('B24P08', 'B24P09'), fig.cap="(B24P08 B24P09) Groceries: Grouped Matrix with default k = 20 and 50"}
#
```


#### Code {.unlisted .unnumbered}

```{r 'B24-GroupMatrixK-A', eval=FALSE, ref.label=c('B24-GroupMatrixK')}
#
```


### Graph Based {.tabset .tabset-fade}

- Graph-based techniques visualize association rules using vertices and edges where vertices annodated with item labels represent items, and itemsets or rules are reptesented as a second set of vertices. 
  - Items are connected with itemsets/rules using arrows. 
  - For rules arrows pointing from items to rule vertices indicate LHS items and an arrow from a rule to an item indicates the RHS. 
  - Interest measures are typically added to the plot by using color or size of the vertices representing the itemsets/rules. 
  - Graph-based visualization offers a very clear representation of rules but they tend to easily become cluttered and thus are only viable for very small sets of rules. 
  - For the following plots we select the 10 rules with the highest lift.
  -  The following plot represents items and rules as vertices connecting them with directed edges. 
  - This representation focuses on how the rules are composed of individual items and shows which rules share items.
  - By default 'igraph' is being used by 'arulesViz'.


#### Image {.unlisted .unnumbered}

```{r 'B24-Graph', include=FALSE}
hh <- t10rules #subrules #rules
cap_hh <- "B24P10"
ttl_hh <- "Groceries: Graph of Top 10 Rules by Lift"
#
B24 <- plot(hh, method = "graph") +
  labs(subtitle = NULL, caption = cap_hh, title = ttl_hh)
assign(cap_hh, B24)
rm(B24)
```

```{r 'B24P10-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B24P10", "-Groceries-Graph", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B24P10, device = "png", dpi = 144) 
}
```

```{r 'B24P10', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B24P10", "-Groceries-Graph", ".png")) 
```

```{r 'B24-GraphCircle', include=FALSE}
hh <- t10rules #subrules #rules
cap_hh <- "B24P11"
ttl_hh <- "Groceries: Circle Graph of Top 10 Rules by Lift"
loc_png <- paste0(.z$PX, "B24P11", "-Groceries-Graph-Circle", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plot(hh, method = "graph", engine = "igraph", 
       control = list(layout = igraph::in_circle()), main = NULL) 
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B24 <- recordPlot()
  dev.off()
  assign(cap_hh, B24)
  rm(B24)
}
```

```{r 'B24P11', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B24P11", "-Groceries-Graph-Circle", ".png")) 
```


```{r 'B24P1011', echo=FALSE, ref.label=c('B24P10', 'B24P11'), fig.cap="(B24P10 B24P11) Groceries: Graph of Top 1o Rules by Lift"}
#
```


#### Code Graph {.unlisted .unnumbered}

```{r 'B24-Graph-A', eval=FALSE, ref.label=c('B24-Graph')}
#
```

#### Code Circle {.unlisted .unnumbered}

```{r 'B24-GraphCircle-A', eval=FALSE, ref.label=c('B24-GraphCircle')}
#
```


### Parallel coordinates plot {.tabset .tabset-fade}

- Parallel coordinates plots are designed to visualize multidimensional data where each dimension is displayed separately on the x-axis and the y-axis is shared. 
  - Each data point is represented by a line connecting the values for each dimension. 
  - Items are on the y-axis as nominal values and the x-axis represents the positions in a rule, i.e., first item, second item, etc. 
  - Instead of a simple line an arrow is used where the head points to the consequent item. 
  - Arrows only span enough positions on the x-axis to represent all the items in the rule, i.e., rules with less items are shorter arrows.
  - The width of the arrows represents support and the intensity of the color represent confidence.
  - The number of crossovers can be significantly reduced by reordering the items on the y-axis.
  

#### Image {.unlisted .unnumbered}

```{r 'B24-ParaCoord', include=FALSE}
hh <- t10rules #subrules #rules
cap_hh <- "B24P12"
ttl_hh <- "Groceries: Parallel Coordinates Plot"
loc_png <- paste0(.z$PX, "B24P12", "-Groceries-Paracoord", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plot.new()
  plot(hh, method = "paracoord", main = ttl_hh) 
  title(sub = cap_hh, line = 4, adj = 1)
  B24 <- recordPlot()
  dev.off()
  assign(cap_hh, B24)
  rm(B24)
}
```

```{r 'B24P12', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B24P12", "-Groceries-Paracoord", ".png")) 
```

```{r 'B24-ParaCoordReorder', include=FALSE}
hh <- t10rules #subrules #rules
cap_hh <- "B24P13"
ttl_hh <- "Groceries: Parallel Coordinates Plot"
loc_png <- paste0(.z$PX, "B24P13", "-Groceries-Paracoord-Reorder", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plot.new()
  plot(hh, method = "paracoord", control = list(reorder = TRUE), main = ttl_hh) 
  title(sub = cap_hh, line = 4, adj = 1)
  B24 <- recordPlot()
  dev.off()
  assign(cap_hh, B24)
  rm(B24)
}
```

```{r 'B24P13', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B24P13", "-Groceries-Paracoord-Reorder", ".png")) 
```

```{r 'B24P1213', echo=FALSE, ref.label=c('B24P12', 'B24P13'), fig.cap="(B24P12 B24P13) Groceries: Parallel Coordinates Plots (Default and Reordered)"}
#
```

#### Code {.unlisted .unnumbered}

```{r 'B24-ParaCoordReorder-A', eval=FALSE, ref.label=c('B24-ParaCoordReorder')}
#
```


### Double Decker plots {.tabset .tabset-fade}

- A double decker plot is a variant of a mosaic plot. 
  - A mosaic plot displays a contingency table using tiles on a rectangle created by recursive vertical and horizontal splits. 
    - The size of each tile is proportional to the value in the contingency table. 
  - Double decker plots use only a single horizontal split.
    - To visualize a single association rule. 
    - Here the displayed contingency table is computed for a rule by counting the occurrence frequency for each subset of items in the antecedent and consequent from the original data set. 
    - The items in the antecedent are used for the vertical splits and the consequent item is used for horizontal highlighting.
- The area of blocks gives the support and the height of the "yes" blocks is proportional to the confidence for the rules consisting of the antecedent items marked as "yes". 
- Items that show a significant jump in confidence when changed from "no" to "yes" are interesting. 


#### Image {.unlisted .unnumbered}

```{r 'B24-DoubleDeck', include=FALSE}
# #Double Decker Plots need original dataset also (xw)
# #Select One of the Rule
set.seed(3)
hh <- sample(rules, 1) 
inspect(hh)
#
cap_hh <- "B24P14"
ttl_hh <- "Groceries: Double Decker Plot of Single Rule"
loc_png <- paste0(.z$PX, "B24P14", "-Groceries-Double-Decker", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plot.new()
  plot(hh, method = "doubledecker", data = xw, main = ttl_hh)
  title(sub = cap_hh, line = 0, adj = 1)
  B24 <- recordPlot()
  dev.off()
  assign(cap_hh, B24)
  rm(B24)
}
```


```{r 'B24P14', echo=FALSE, fig.cap="(B24P14) Groceries: Double Decker Plot of Single Rule"}
knitr::include_graphics(paste0(.z$PX, "B24P14", "-Groceries-Double-Decker", ".png")) 
```

#### Code {.unlisted .unnumbered}

```{r 'B24-DoubleDeck-A', eval=FALSE, ref.label=c('B24-DoubleDeck')}
#
```

### From Rules to Graph

```{r 'B24-RuleGraph', eval=FALSE}
# #convert rules into a graph with rules as nodes
hh <- associations2igraph(rules)
if(FALSE) plot(hh)
#
# #convert the graph into a tidygraph
if(FALSE) {
  #library("tidygraph")
  as_tbl_graph(hh)
  #Error: `trunc_mat()` was deprecated in tibble 3.1.0. 
  #Printing has moved to the pillar package.
}
# #convert the generating itemsets of the rules into a graph with itemsets as edges
itemsets <- generatingItemsets(rules)
hh <- associations2igraph(itemsets, associationsAsNodes = FALSE)
if(FALSE) plot(hh, layout = igraph::layout_in_circle)
#
# #save rules as a graph so they can be visualized using external tools
if(FALSE) saveAsGraph(rules, "rules.graphml")
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B24-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, cap_hh, hh, isRedConf, isRedLift, kbltbl, names_hh, rr_conf, 
          rr_rhs, rr_sup, rules, rules_tbl, xw, xxB22Makeup, pruned, B24P01, loc_png, pruned_tbl, 
          sub_hh, ttl_hh, nn_hh, type_hh, x_hh, xxB24Basket, y_hh, B24P03, xxB24Groceries, B24P04,
          B24P05, B24P06, B24P07, B24P08, B24P09, B24P10, Groceries, subrules, t10rules)
```

```{r 'B24-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/224-Association.Rmd-->

# Regresssion (B25, Dec-26) {#b25}

```{r 'B25', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Supervised Learning Algorithm: Regression"

## Packages

```{r 'B25-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("fastDummies", dependencies = TRUE)
  install.packages("carData", dependencies = TRUE)
}
```

## Linear Regression {.tabset .tabset-fade}

```{r 'B25D01', comment="", echo=FALSE, results='asis'}
f_getDef("Simple-Linear-Regression")
```

- ${Y}$: Scalar response, Dependent variable, Outcome variable, Target, Predicted
- ${X}$: Explanatory variables, Independent variables, Antecendent variables, Predictors

- It is applied when the objective is to predict the outcome variable based on the antecendent variables
- Predicted (Y) should be continuous, but Predictors (X) can be either continuous or categorical 
  - Ex: Salary (Y) is a function of Age, Gender, Education, Years of Experience
  - Ex: Consumption (Y) is a function of Income (X)
  - We are interested in $Y = mX + C$ where ${m}$ is the slope of the line and C is the y-intercept
    - Slope: What is the change in Y for unit change in X
  - Because there will be some error ${\epsilon}$, thus the equation is given by $y = {\alpha} + {\beta}x + {\epsilon}$ where ${\alpha}$ is the average Y when X are zero.
  - NOTE Equation can also be given by ${y} = {\beta}_0 + {\beta}_1 {x} + {\epsilon}$. In that case assume ${\alpha} = {\beta}_0$ and ${\beta} = {\beta}_1$.

- \textcolor{pink}{Question:} ${\alpha}$ is constant for a given set of data
  - Yes

- Refer [Slope is tested for Significance](#beta1-c14 "c14")
- Simple Linear Regression is also known as \textcolor{pink}{Bivariate Regression}. i.e. Single Y, Single X
- When there are Single Y and Multiple X, it is called \textcolor{pink}{Multiple Linear Regression}
  - Equation: ${y} = {\beta}_0 + {\beta}_1 {x}_1 + {\beta}_2 {x}_2 + \ldots + {\epsilon}$

- \textcolor{pink}{lm()}
  - Base R Function to run linear model or regression
  - Tilde "~" means regressed on i.e. Dependent (Y) ~ Independents (X)
    - "linear regression model of y on x"
  - Models for lm are specified symbolically. A typical model has the form \textcolor{pink}{response ~ terms} where \textcolor{pink}{response} is the (numeric) response vector and \textcolor{pink}{terms} is a series of terms which specifies a linear predictor for response. 
  - \textcolor{pink}{summary()}
    - The standard error is variability to expect in coefficient which captures sampling variability so the variation in intercept can be up 0 and variation in IncomeX will be 0 not more than that
    - t-value: t value is Coefficient divided by standard error.
      - It is basically how big is estimated relative to error.
      - Bigger the coefficient relative to Std. error the bigger the t score and t score comes with a p-value because it is a distribution.
    - p-value is how statistically significant the variable is to the model for a confidence level of 95%
      - If the p-value is less than alpha (0.05) for both intercept and X then it implies that both are statistically significant to our model.
    - Residual standard error or the standard error of the model is basically the average error for the model. It means the average value by which our model can deviate while predicting the Y. 
      - Lesser the error the better the model while predicting.
    - Multiple R-squared is the ratio of (1-(sum of squared error/sum of squared total))
    - Adjusted R-squared:
      - If we add variables no matter if its significant in prediction or not the value of R-squared will increase which the reason Adjusted R-squared is used because if the variable added is not significant for the prediction of the model the value of Adjusted R-squared will reduce, it is one of the most helpful tools to avoid overfitting of the model.
   - F-statistic is the ratio of the mean square of the model and mean square of the error, in other words, it is the ratio of how well the model is doing and what the error is doing, and the higher the F value is the better the model is doing as compared to the error.
   
### lm() {.unlisted .unnumbered}

```{r 'B25-Model'}
bb <- tibble(ConsumptionY = seq.int(80, by = 20, length.out = 5),
             IncomeX = seq.int(100, by = 100, length.out = 5))
#
# #Build the model
mod_bb <- lm(formula = ConsumptionY ~ ., data = bb)
#
# #Model
suppressWarnings(mod_bb)
#
# #Summarise the model
if(FALSE) suppressWarnings(summary(mod_bb))
#
# #ANOVA Table
if(FALSE) suppressWarnings(anova(mod_bb))
```

### Model {.unlisted .unnumbered}

```{r 'B25-lmModel'}
names(mod_bb)
#
# #Coefficients (Model Parameters): Different headers than summary(mod_bb)$coefficients 
mod_bb$coefficients #coefficients(mod_bb)
#
# #Residuals
mod_bb$df.residual
#
f_pNum(mod_bb$residuals) #residuals(mod_bb) #summary(mod_bb)$residuals 
#
# #What is Effects
f_pNum(mod_bb$effects) #effects(mod_bb)
#
# #Rank
mod_bb$rank
#
# #Fitted Values
mod_bb$fitted.values #fitted.values(mod_bb)
#
# #Assign
mod_bb$assign
#
# #qr
mod_bb$qr[[1]] %>% as_tibble()
#
# #Others
if(FALSE) mod_bb$xlevels
if(FALSE) mod_bb$call #summary(mod_bb)$call
if(FALSE) mod_bb$terms
#
mod_bb$model
```

### Summary {.unlisted .unnumbered}

```{r 'B25-lmSummary'}
#summary(mod_bb)
names(suppressWarnings(summary(mod_bb)))
# [1] "call"          "terms"         "residuals"     "coefficients"  "aliased"       "sigma"        
# [7] "df"            "r.squared"     "adj.r.squared" "fstatistic"    "cov.unscaled" 
#
# #Coefficients: Different headers than mod_bb$coefficients
f_pNum(suppressWarnings(summary(mod_bb))$coefficients) %>% as_tibble()
#
#
# #Better Printing
if(FALSE) f_pNum(summary(mod_bb)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
# #R^2 and Adjusted R^2
suppressWarnings(summary(mod_bb))$r.squared
suppressWarnings(summary(mod_bb))$adj.r.squared
#
# #F-Statistic
suppressWarnings(summary(mod_bb))$fstatistic 
#
# #Covariance
suppressWarnings(summary(mod_bb))$cov.unscaled
#
# #Sigma
f_pNum(suppressWarnings(summary(mod_bb))$sigma)
```

## Prediction

- \textcolor{pink}{Question:} After generating the equation can we calculate what would be the Consuption if the Income is 600
  - The model can be used to predict the dependent variable
  - (Aside) \textcolor{orange}{Caution:} Prediction beyond the values of min(x) and max(x) of original dataset is inference and it is discouraged. i.e. Y for X= 150 can be predicted but Y for X >500 should be avoided.

```{r 'B25-lmPredict'}
# #Predict the Outcome variable using the Model
test_bb <- tibble(IncomeX = c(150, 600, 700))
#res_bb <- predict(mod_bb, test_bb)
res_bb <- test_bb %>% mutate(ConsumptionY = predict(mod_bb, .))
res_bb
```

## MS Excel: Regression Analysis

- We need Data Analysis AddIn.
- In Windows 10 & Microsoft Excel 2016 
  - Menu | File | Options | Add-ins | Manage = Excel Add-ins | Go | Add-ins Popup 
    - Tick the Analysis ToolPak | Go
    - As shown by Professr, the Sequence might be Menu | File | More | Options | ...
  - Confirmation
    - Menu | Data | 
      - Right Most section would have been added called "Analysis" and it will have one button "Data Analysis"

- Regression Analysis
  - Enter the data
    - Menu | Data | Analysis | Data Analysis | Popup | Regression | OK
    - Select Input Y Range | Select Input X Range | Tick Labels | OK 

```{r 'B25P01', echo=FALSE, out.width='100%', fig.cap="(B25P01) Regression in MS Excel"}
knitr::include_graphics(paste0(.z$PX, "B25P01-Excel-Analysis.jpg"))
```

```{r 'B25P02', echo=FALSE, out.width='100%', fig.cap="(B25P02) Regression Result in MS Excel"}
knitr::include_graphics(paste0(.z$PX, "B25P02-Excel-Regression-Result.jpg"))
```

## Explanation of Terms


```{r 'B25D02', comment="", echo=FALSE, results='asis'}
f_getDef("H-SimpleRegression")
```

- t-value:
  - The slope model parameter $(\beta_1)$ needs to be tested for significance.
  - Refer [Slope is tested for Significance](#beta1-c14 "c14")
  - $t = \frac{b_1}{s_{b_1}}$
  - If ${}^2\!P_{(t)} \leq {\alpha} \to {H_0}$ Rejected.

```{r 'B25D03', comment="", echo=FALSE, results='asis'}
f_getDef("Standard-Error-B1")
```

- \textcolor{pink}{Question:} In case of multiple variables, do we need to do this for all variables
  - No, we will not do it individually. We will do multiple regression. We will incorporate all the variables simultaneously.
  - In multiple regression, we get $\{\beta_1, \beta_2, \beta_3, \ldots \}$ and each will have its own standard error i.e. $\{s_{b_1}, s_{b_2}, s_{b_3}, \ldots \}$.
  - If a variable is NOT signifact, it means that it is not contributing to the model in a meaningful manner.

- F-Statistic:
  - For simple linear regression i.e. single X, single Y; the F-test and t-test provide same result. However, in multiple regression model F-test is used as the test for \textcolor{pink}{overall significance} and t-tests are used as tests for \textcolor{pink}{individual significance}.

```{r 'B25D07', comment="", echo=FALSE, results='asis'}
f_getDef("H-MultipleRegression-F")
```

```{r 'B25D08', comment="", echo=FALSE, results='asis'}
f_getDef("H-MultipleRegression-t") 
```


- \textcolor{pink}{Question:} Which one should come first Joint Significance or Individual Significance
  - In multiple regression analysis, the joint significance needs to be looked at first.
  - i.e. First the variables jointly able to predict the outcome variable then later on we can check contribution of individual variable
  - If the model is not good then there is no point in looking at individual variables
  - F should be greater than 0.05 for us to consider that model is valid.

- \textcolor{pink}{Question:} Can we drop the variables which are not contributing much to the model
  - We may find that out of 4 independent variable A, B, C, D; C & D are not contributing much to the model. We can drop C & D. However, there is a possibility that A & B are performing well because of the presence of C & D. Though C & D contribution, by itself, is insignificant, it makes A & B contribution significant. C or D might be influencing A or B.
  - If there are high number of variables e.g. 20 or 30, then we will drop the insignificant variables because the model complexity becomes an issue.
    - Practically how many independent variable we can handle in our business case is also a consideration
  
- \textcolor{pink}{Question:} Here we have single $\beta_1$ then how the F-test is applied
  - We are only checking $\beta_1 = 0$

- Any model is a good model if it has minimum number of predictors and maximum predictive power.


```{r 'B25D04', comment="", echo=FALSE, results='asis'}
f_getDef("Coefficient-of-Determination") 
```

- $r^2$ is the 'Coefficient of Determination' or 'Goodness of Fit'
  - $r^2 = 1$ means model is able to explain 100% relationship between independent and dependent variables.
  - $r^2 = 0$ means model is not able to explain anything
  


```{r 'B25D05', comment="", echo=FALSE, results='asis'}
f_getDef("Error-Term")
```

- Error Term $\epsilon$ denotes unexplained variance
  
- \textcolor{pink}{Question:} What would be the acceptable value of $r^2$
  - No rule, context dependent
  - In general, we want $r^2$ to be as high as possible
  - (Aside) Further, there is a 'model overfitting' concern also with very high $r^2$

- Total = Explained (by independent variables) + Unexplained ($\epsilon$)
  - Join Significance is about all the independent variables


- \textcolor{pink}{Question:} Is there a possibility of situation where independent variables are not performing jointly but $r^2$ is high
  - It is possible
  - There might be some internal issue which results in high $r^2$ but low model performance
  - If the predictors are highly correlated (Multicollinearity)
    - Individual performance gets reduced, however, the $r^2$ value might increase
    - Multicollinearity reduces the robustness of model
    - Individually the either variable can explain the dependent variable very well. However, due to multicollinearity, together they fail to perform at same level.
    - Multicollinearity is not a problem between depdendent and independent variable. It is applicable only between independent variables.
  - Regression analysis require that the variables should be non-overlapping (high correlation should not be there)


```{r 'B25D06', comment="", echo=FALSE, results='asis'}
f_getDef("Multicollinearity")
```

- \textcolor{pink}{Question:} How do we deal with Multicollinearity and other such problems
  - First of all, you need to select those independent variables which are not correlated to each other

- Multiple R (in Excel)
  - Average correlation between all the variables (dependent and independent)


```{r 'B25D09', comment="", echo=FALSE, results='asis'}
f_getDef("RSq-Adj")
```


- Adjusted R square $R_a^2$
  - If we want to compare different models (and specially those with different number of independent variables), we should use Adjusted $R_a^2$
  
## Application of Regression {#app-reg-b25}

- Two Applications of Regression
  - Estimation (Descriptive)
    - Which of these independent variables significantly affect the dependent variable
    - e.g. Which of these factors are influencing the employee performance
    - When we are doing estimation, data partition into train and test datasets is not required.
  - Prediction
    - Partition the Sample data randomly into Train and Test datasets in ratio of 80:20, 70:30 etc.
    - Do not predict beyond the min(x) and max(x) range because no data is available outside these limits.
    - Validation 
      - Randomly Partition data | Build the Model on Train | Run it on Test | For Test we will have actual Y and predicted Y $(\hat{y})$ | Evaluate the difference between Actual and Predicted ${(y_i - \hat{y}_i)}$
      - The error between Actual and Predicted is called \textcolor{pink}{loss function}.
      - For all the models do this and compare them and select the one which have lowest loss function.


- \textcolor{pink}{Question:} More number of observations in Train dataset would lead to better model
  - Yes. That is why Train would have the major chunk. For small datasets it would be around 80%
- \textcolor{pink}{Question:} What is the drawback if Train contains 80% for large datasets
  - No issue.
  - Partition should be done randomly, otherwise no limitation.


## Bias-Variance Trade-off

- Refer [Bias-Variance Trade-off](#bias-var-c37 "c37")


```{r 'B25D10', comment="", echo=FALSE, results='asis'}
f_getDef("Bias-Variance-Trade-off")
```

```{r 'B25D11', comment="", echo=FALSE, results='asis'}
f_getDef("Overfitting")
```

```{r 'B25P03', echo=FALSE, ref.label=c('C37P01'), fig.cap="(C37P01) Bias-Variance Trade-off"}
# #Ref another file chunk
```

## Categorical Independent Variables

- Refer [Categorical Independent Variables](#reg-cat-c15 "c15")

```{r 'B25D12', comment="", echo=FALSE, results='asis'}
f_getDef("Dummy-Variables") #dddd
```

- When the independent variable is categorical e.g. Gender (M, F)
  - Then "one unit change in X" is not applicable 
  - It is more about "change in state of X" from one level to another.
  - Convert Categorical Independent Variable into Dummy Variables.
  - The category that is not assigned an indicator variable is denoted the \textcolor{pink}{reference category} (or the Benchmark). 
    - If 'M' is assigned 0 then it will be the benchmark
    - If 'F' is assigned 0 then it will be the benchmark
    - Performance of all other levels of the variable is given as compared to the referenced level
  - In the example above 'high' is the benchmark
  - "dummy coding" leads to the creation of a table called \textcolor{pink}{contrast matrix}.
  - "dummy coding" is also known as "one-hot encoding"

- \textcolor{pink}{Question:} What if we have a dataset with both categorical and continuoud dependent varaibles
  - Convert categorical to dummies, no need to do anything with continous variables

## Example 

```{conjecture 'lm-non-numeric-y'}
\textcolor{brown}{Warning messages: In model.response(mf, "numeric") : using type = "numeric" with a factor response will be ignored}
```

- Additional Warnings
  - \textcolor{brown}{"In Ops.factor(y, ...) : '-' not meaningful for factors"} 
- Running summary() on this Model will result in Error
  - \textcolor{brown}{"Error in quantile.default(resid) : (unordered) factors are not allowed"} 
  - \textcolor{brown}{"In addition: Warning message: In Ops.factor(r, 2) : '^' not meaningful for factors"}
- Running step() on this Model will resutl in Error
  - \textcolor{brown}{"Error in step(...) : AIC is -infinity for this model, so 'step' cannot proceed"}
- Dependent Variable is Non-numeric. Found to be Factor in this case.
- Y needs to be Numeric for Regression Analysis


```{r 'B25-lmExample'}
# #Create Data Set with Factor Column having the its First Level as the Reference for Later
bb <- tibble(Performance = c(35, 36, 40, 45, 60, 66, 67, 78, 80, 87, 78, 89, 89, 90),
             Class = factor(c(rep("A", 4), rep("B", 6), rep("C", 4)), levels = c("C", "B", "A")))
#
# #Create Dummies | Drop Original | Drop Reference Variable 
dum_bb <- dummy_cols(bb, select_columns = "Class", 
                     remove_selected_columns = TRUE, remove_first_dummy = TRUE)
str(dum_bb)
#
mod_bb <- lm(Performance ~ ., data = bb)
if(FALSE) summary(mod_bb)$coefficients
# #Better Printing
if(TRUE) f_pNum(summary(mod_bb)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
# #Anova Table 
if(FALSE) anova(mod_bb) 
if(TRUE) anova(mod_bb) %>% as_tibble(rownames = "Predictors") %>% 
  rename(pVal = "Pr(>F)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
```


## fastDummies

- \textcolor{pink}{dummy_cols()}
  - To generate dummy columns
  - If NA are present, then by default, an NA dummy column is also created.
    - \textcolor{orange}{Caution:} The other columns would have {0, 1, NA} in this case. For now those NA are being set to 0 manually. Impact of keeping these NA as it is and handling it inside model would be observed "ForLater"
  
  
```{r 'B25-Dummy'}
set.seed(3) 
bb <- tibble(Performance = sample(1:100, size = 20),
             Grade = sample(LETTERS[1:3], size = 20, replace = TRUE))
str(bb, vec.len = 10)
#
# #Convert character to dummy columns 
if(FALSE) dum_bb <- dummy_cols(bb, select_columns = c("Grade"))
# #To keep only (k-1) columns to avoid multicollinearity
if(FALSE) dum_bb <- dummy_cols(bb, select_columns = c("Grade"),  
                              remove_first_dummy = FALSE, remove_selected_columns = TRUE)
if(TRUE) dum_bb <- dummy_cols(bb, select_columns = c("Grade"), 
                              remove_most_frequent_dummy = TRUE, remove_selected_columns = TRUE)
str(dum_bb, vec.len = 10)
#
# #Multiple Linear Regression with Categorical to Dummy Variables
if(FALSE) mod_bb <- lm(formula = Performance ~ Grade_A + Grade_B, data = dum_bb)
if(TRUE) mod_bb <- lm(formula = Performance ~ ., data = dum_bb)
#
# #Model
mod_bb
#
# #Summarise the model
if(TRUE) summary(mod_bb)
#
# #ANOVA Table
if(TRUE) anova(mod_bb)
```

## Data: CarDekho 

- Covered in Next Lecture.

## Data: Salaries {.tabset .tabset-fade}

### Salaries {.unlisted .unnumbered}

```{r 'B25-DataSalaries'}
# #Load Data "Salaries". It is NOT included in "car". It is included in the "carData" Package
if(FALSE) data(package = "car")$results[ , "Item"]
data("Salaries", package = "carData")
str(Salaries)
```

### Categorical X with 2 levels {.unlisted .unnumbered}


```{r 'B25-1X2l'}
# #y = b0 + b1 * x : Y (Salaries), X (Sex) with 2 levels
#
# #In R "Factor" Notation (Default Alphabetical Ordering): Female = 1, Male = 2 
levels(Salaries$sex)
#
# #To encode categorical variables, known as 'contrast coding systems'. 
# #R can directly convert the Categorical Variable into dummy with Female = 0, Male = 1
# #The default option in R is to use the first level of the factor as a reference 
# #and interpret the remaining levels relative to this level.
# #contrasts() lists the dummy variables that would be created for k levels i.e. k-1 dummy 
contrasts(Salaries$sex)
#
# #Compute the model
mod_sal_f <- lm(salary ~ sex, data = Salaries)
if(TRUE) f_pNum(summary(mod_sal_f)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
# #Interpretation of Coefficients with 'Female' as the reference level i.e. 0 within the dummy 
# # b0 : average salary among Female (Reference): (Intercept, Estimate) 101002
# # b0 + b1 : average salary among Male : (101002) + (14088) = 115090
# # b1 : average difference in salary of Male & Female (Reference): (sexMale, Estimate) 14088
#
# #The p-value is 0 (significant), suggesting that there is a statistical evidence 
# #of a difference in average salary between the genders
#
# #We can change the Factor Levels and thus change the Reference Variable
m_Salaries <- as_tibble(Salaries) %>% mutate(across(sex, factor, levels = c("Male", "Female")))
levels(m_Salaries$sex)
contrasts(m_Salaries$sex)
#
mod_sal_m <- lm(salary ~ sex, data = m_Salaries)
if(TRUE) f_pNum(summary(mod_sal_m)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
# #Interpretation of Coefficients with 'Male' as the reference level i.e. 0 within the dummy 
# # b0 : average salary among Male (Reference): (Intercept, Estimate) 115090
# # b0 + b1 : average salary among Female : (115090) + (-14088) = 101002
# # b1 : average difference in salary of Female & Male (Reference): (sexFemale, Estimate) -14088
#
# #The fact that the coefficient for sexFemale in the regression output is negative 
# #indicates that being a Female is associated with decrease in salary (relative to Male).
#
# #The p-value is 0 (significant)
```


### Categorical X with 3 levels {.unlisted .unnumbered}

```{r 'B25-1X3l'}
# #y = b0 + b1 * x : Y (Salaries), X (Rank) with 3 levels
#
# #Change Factor Levels | Treating Professor as Reference Variable 
# #Ordering is done in decreasing Rank but NOT as 'Ordered Factor' for now
r_Salaries <- as_tibble(Salaries) %>% 
  mutate(across(rank, factor, levels = c("Prof", "AssocProf", "AsstProf")))
levels(r_Salaries$rank)
#
# #contrasts() lists the dummy variables that would be created for k levels i.e. k-1 dummy 
# #Two dummies were created against the reference of "Prof"
contrasts(r_Salaries$rank)
#
mod_sal_r <- lm(salary ~ rank, data = r_Salaries)
if(TRUE) f_pNum(summary(mod_sal_r)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
# #Interpretation of Coefficients with 'Prof' as the reference level i.e. {0 0} within the dummies
# # b0 : average salary among Prof (Reference): (Intercept) 126772
# # b0 + b1 : average salary among AssocProf : (126772) + (-32895) 
# # b0 + b2 : average salary among AsstProf  : (126772) + (-45996) 
# # b1 : average difference in salary of AssocProf & Prof (Reference): (rankAssocProf) -32895
# # b2 : average difference in salary of AsstProf & Prof (Reference): (rankAssocProf) -45996
#
# #The p-value is 0 (significant), suggesting that there is a statistical evidence 
# #of a difference in average salary between the ranks

# #The fact that the coefficient for rankAssocProf & rankAsstProf in the regression output are 
# #negative indicates that lower ranks are associated with lower salary (relative to Prof).
```

### Complete Model {.unlisted .unnumbered}

```{r 'B25-AllX'}
# #y = b0 + b1 * x : Y (Salaries), X (Rank) with 3 levels
#
# #Change Factor Levels | Reference: Rank = Professor, Sex = Male, Discipline = A
bb <- as_tibble(Salaries) %>% 
  mutate(across(sex, factor, levels = c("Male", "Female"))) %>% 
  mutate(across(rank, factor, levels = c("Prof", "AssocProf", "AsstProf")))
#
# #contrasts() lists the dummy variables that would be created for k levels i.e. k-1 dummy 
contrasts(bb$rank)
contrasts(bb$sex)
contrasts(bb$discipline)
#
mod_bb <- lm(salary ~ ., data = bb)
if(TRUE) f_pNum(summary(mod_sal_r)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
# #Anova Table of Base R
if(TRUE) anova(mod_bb) %>% as_tibble(rownames = "Predictors") %>% 
  rename(pVal = "Pr(>F)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
# #Anova Table of Car Package which automatically takes care of unbalanced designs
if(TRUE) Anova(mod_bb) %>% as_tibble(rownames = "Predictors") %>% 
  rename(pVal = "Pr(>F)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
# #Taking other variables into account, it can be seen that the categorical variable sex 
# #is no longer significantly associated with the variation in salary between individuals. 
# #Significant variables are rank and discipline.
```

### anova() vs. Anova() {.unlisted .unnumbered}

```{r 'B25-ANOVA'}
# #Anova Table of Base R (Type I : each variable is added in sequential order)
anova(mod_bb) %>% as_tibble(rownames = "Predictors")
#
# #Anova Table of Car Package which automatically takes care of unbalanced designs (Type II)
# #Type II tests each variable after all the others
# #There is a Type III also. However, its usage is highly controversial 
Anova(mod_bb, type = 2) %>% as_tibble(rownames = "Predictors")
```



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B25-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, dum_bb, m_Salaries, mod_bb, mod_sal_f, mod_sal_m, mod_sal_r, 
          r_Salaries, res_bb, Salaries, test_bb, xxB25CarDekho)
```

```{r 'B25-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/225-RegressionL.Rmd-->

# Linear Regression (B26, Jan-02) {#b26}

```{r 'B26', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Machine learning using linear regression"


## Packages

```{r 'B26-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("car", dependencies = TRUE)
}
```

## Data: CarDekho {#set-cardekho-b26 .tabset .tabset-fade}

\textcolor{pink}{Please import the "B26-CarDekho.csv"}

```{r 'B26-CarDekho', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum tools::md5sum(paste0(.z$XL, "B26-CarDekho.csv"))
xxB26CarDekho <- f_getObject("xxB26CarDekho", "B26-CarDekho.csv",
                                "92770bca1b81e9339def909673097b97")
```

```{r 'B26-GetCarDekho', include=FALSE}
xxB26CarDekho <- f_getRDS(xxB26CarDekho)
```

### EDA {.unlisted .unnumbered}

- About: [4340, 8]
  - Source: https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho
  - Objective: Build a Model to predict Selling Price
  - From Year we need to calculate Age (At 2022 level). 
    - Obviously the prices are not of 2022 but anyway we just need any year
  - Convert all Character to Factors
  - Create Dummies


```{r 'B26-PrepCar'}
aa <- xxB26CarDekho
# #Split String | Rename | Filter | Factor | Age | Drop |
# #To prevent long dummy names each variable name and its levels have been shorten
# #Each Factor Level has been modified so that levels are in decreasing order of occurance
# #i.e. Fuel Diesel has the most frequent and thus has been converted to Reference Dummy
bb <- aa %>% 
  separate(name, c("brand", NA), sep = " ", remove = FALSE, extra = "drop") %>% 
  filter(fuel != "Electric") %>% 
  #mutate(across(where(is.character), factor)) %>% 
  mutate(across(fuel, factor, levels = c("Diesel", "Petrol", "CNG", "LPG"))) %>% 
  mutate(across(transmission, factor, levels = c("Manual", "Automatic"), 
                labels = c("Manual", "Auto"))) %>% 
  mutate(across(owner, factor, 
levels = c("First Owner", "Second Owner", "Third Owner", "Fourth & Above Owner", "Test Drive Car"), 
labels = c("I", "II", "III", "More", "Test"))) %>% 
  mutate(across(seller_type, factor, levels = c("Individual", "Dealer", "Trustmark Dealer"), 
                labels = c("Indiv", "Dealer", "mDealer"))) %>% 
  rename(price = selling_price, km = km_driven, 
         s = seller_type, o = owner, t = transmission, f = fuel) %>% 
  mutate(age = 2022 - year) %>% 
  select(-c(year, name, brand))
# 
# #Older Pattern
xsyw <- bb
xw <- xsyw %>% select(-price)
# #New Pattern
xfw <- bb
zfw <- xfw %>% mutate(across(where(is.numeric), ~ as.vector(scale(.)))) 
xnw <- xfw %>% select(where(is.numeric))
znw <- zfw %>% select(where(is.numeric))
```

## Model {.tabset .tabset-fade}

- \textcolor{pink}{Question:} There are 29 Brands. Should we do the analysis separately for them e.g. When we are creating a Model for Price of 'Maruti', should we not remove 'BMW' 
  - Yes, if we have a large set of data, it would be good to separate each car /brand /model
- \textcolor{pink}{Question:} What is the Top Brand
  - Maruti with 1280 out of 4340 cars
- \textcolor{pink}{Question:} Can we drop "Petrol" in place of "CNG" i.e. converting it to reference
  - Options available are for 'First' or 'Most Frequent'
  - (Aside) We would need to convert to factor with 'Petrol' being the First Level then it can be treates as Reference.

```{r 'B26D01', comment="", echo=FALSE, results='asis'}
f_getDef("Overfitting")
```

```{r 'B26D02', comment="", echo=FALSE, results='asis'}
f_getDef("Underfitting")
```

- \textcolor{pink}{Question:} "1" in the sample() syntax means 1 sample and if we want Two sets we would modify the \textcolor{pink}{sample(1:nrow(bb), size = 0.8 * nrow(bb))} to \textcolor{pink}{sample(2:nrow(bb), size = 0.8 * nrow(bb))}
  - No
  - (Aside) No 
    - sample(1:10, size = 3) means pick 3 items out of the set of 10 items which are in [1, 10]
    - sample(11:20, size = 3) means pick 3 items out of the set of 10 items which are in [11, 20]
    - sample(2:10, size = 3) means pick 3 items out of the set of 9 items which are in [2, 10]
    - We are using the selected indices to partition the data in two sets selected (i.e. Train) and not selected (i.e. Test)
    - To create 3 or more sets Refer [Partition data in Train & Test](#train-test-c34 "c34")
  - While the sample() can take a dataframe i.e. it would not throw error if size is less than number of columns, the outcome is not meaningful and would not match with the expectation.
  - probability given to the sample() is not the partitionining range. It is the proabability of each index to be chosen.

- Form of the Function
  - $y \sim x :$ "linear regression model of y on x"
    - Tilde "~" means regressed on i.e. Dependent (Y) ~ Independents (X)
  - $y \sim x_1 + x_2 :$ "linear regression model of y on $\{x_1, x_2\}$"
  - $y \sim . :$ "linear regression model of y on $\{x_1, x_2, \ldots, x_p\}$"
    - Dot (.) means all variables in the dataset except the dependent variable

- \textcolor{pink}{Question:} How to check count of values for each level i.e. each dummy variable
  - (Aside) See Below
- \textcolor{pink}{Question:} Is it better to take care of this type of problem in the original dataset
  - Yes
  - There is no point in building a model with 1 "Electric" Car.
- \textcolor{pink}{Question:} Is it possible to generalise this process of checking the variable levels and if some have single occurance eliminate them
  - We need to do it by ourselves on a case to case basis
- \textcolor{pink}{Question:} If the single data point is highly relevant, then how can we ensure that it is in the training set and not in the test set
  - Ex: if the dataset has gender and it is 100% populated by "Male", do we need to include that as variable
    - No
  - (Argument) But here we have at least 1 observation and not including that we will lead to the model failing while predicting "Electric"
    - It needs to have some minimum number of observations for model to be based on that
- \textcolor{pink}{Question:} What happens if we keep this observation
  - Model may give us wrong fit
  - We should not keep a predictor (dummy variable here) which actually have no relevancy to our model
- \textcolor{pink}{Question:} What happens if we decide to create a train dataset where we include at least a minimum number of observations (e.g. 2) for each level i.e. for each dummy variable. Would the Model be considered biased in that case because we have not created the training set randomly
  - Generally, that kind of splitting we will do when we are doing classification problem (Categorical Y)
  - It is \textcolor{pink}{"Stratified Random Sampling"}. There we ensure that we have equal proportion of each group
  - For Independent variable of categorical nature generally we do not do stratified sampling
  - (Further) But can we do this stratified sampling here so that 'NA' does not come up in the model outcome because of few or none observations of a level 
    - If you have a categorical predictor and majority of datapoints are highly unique (e.g. 99% "Male"), then it is better not to consider that as a predictor
    - Infact it might look like 'Outlier'
- \textcolor{pink}{Question:} Can we extrapolate this logic and say there are only 23 LPG so drop those also
  - We should not drop this one because it has at least some observations
- \textcolor{pink}{Question:} So is there a thumb rule of how many minimum observations should be present 
  - Judgement i.e. Case by case basis
  - We can look at the significance of the predictor and then we can decide
    - \textcolor{pink}{"Stepwise Regression"} can remove those predictors which are insignificant
- \textcolor{pink}{Question:} What happens if all the 23 observations of 'LPG' are grouped into one of the datasets i.e. either train or test because it is a random selection
  - This is not the final model. There will be multiple iterations. Subsequently, it will be included if it is a good predictor or would be dropped if it is insignificant

- Refer [Application of Regression](#app-reg-b25 "b25")
  - Estimation (Descriptive)
    - Which of these independent variables significantly affect the dependent variable
    - e.g. Which of these factors are influencing the employee performance
    - When we are doing estimation, data partition into train and test datasets is not required.
  - Prediction
    - Partition the Sample data randomly into Train and Test datasets in ratio of 80:20, 70:30 etc.

- Refer [How to Disable Scientic Notation in R](#scipen-b09 "b09")

### Build {.unlisted .unnumbered}

```{r 'B26-ModelCar'}
# #Removed Fuel "Electric"
unique(xsyw$f)
str(xsyw)
#
# #Dummy | Drop First Level i.e. Reference | Drop Selected Columns i.e. Original |
dum_xsyw <- xsyw %>% dummy_cols(.data = ., 
                  select_columns = c("f", "s", "t", "o"), 
                  remove_first_dummy = TRUE, remove_selected_columns = TRUE)
names(dum_xsyw)
#
# #Partition Data
set.seed(3)
#idx_xsyw <- sample(x = 1:nrow(dum_xsyw), size = 0.8 * nrow(dum_xsyw)) #Equivalent
idx_xsyw <- sample.int(n = nrow(dum_xsyw), size = floor(0.8 * nrow(dum_xsyw)), replace = FALSE)
train_xsyw <- dum_xsyw[idx_xsyw, ]
test_xsyw  <- dum_xsyw[-idx_xsyw, ]
#
mod_xsyw <- lm(price ~ ., data = train_xsyw)
if(TRUE) f_pNum(summary(mod_xsyw)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
# #Anova Table 
if(FALSE) anova(mod_xsyw) %>% as_tibble(rownames = "Predictors") %>% 
  rename(pVal = "Pr(>F)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
```

### Top Brands {.unlisted .unnumbered}

```{r 'B26-TopBrands'}
# #What are the Top Brands (29 levels)
aa %>% 
  separate(name, c("brand", NA), sep = " ", remove = FALSE, extra = "drop") %>% 
  count(brand) %>% arrange(desc(n))
```

### Change Reference Level {.unlisted .unnumbered}


```{r 'B26-ChangeRef'}
# #To Make different level as the reference i.e. Petrol in place of CNG
ii <- xsyw
levels(ii$f) 
jj <- ii %>% mutate(f = relevel(f, ref = "CNG"))
levels(jj$f)
```

### sample() {.unlisted .unnumbered}


```{r 'B26-Sample'}
set.seed(3)
# #Create a chracter set of 10 items. 
# #Initial 10 letters were not chosen to show difference between 
# #indexing numbers /position and actual item values.
ii <- letters[11:20]
ii
# #Note: length() is used on vectors & nrow() is used on dataframes
# #Pick 3 letters out of these 10 items and indexing numbers can be within [1, 10]
idx <- sample(1:length(ii), size = 3)
ii[idx] #If ii is dataframe then a comma would be required i.e. ii[idx, ]
#
# #We can directly pick up 3 items if it is a Vector
sample(ii, size = 3) #Not applicable for dataframes etc. and thus should be avoided.
#
# #Going back to using length() or nrow()
#
# #Pick 3 letters out of these 10 items and indexing numbers can be within [2, 10]
# #i.e. First Index letter "k" can never be chosen
idx <- sample(2:length(ii), size = 3)
ii[idx]
#
# #Pick 3 letters out of these 10 items and indexing numbers can be within [6, 10]
# #First 5 letters can never be chosen i.e. "k" "l" "m" "n" "o" excluded
# #Further, Some remaining letters have different probabilities /weightage
# #Note Sum of Probabilities need not to be 1 it can be less than that
# #Length of Probability Vector needs to match the Range Length of Indexing i.e. 6-10 has 5 items
idx <- sample(6:length(ii), size = 3, prob = c(0.2, 0.3, 0.1, 0.1, 0.1))
ii[idx]
#
# #Note, using this we are always getting an index of items chosen thus resulting in a set of two
# #To get 3 sets, the syntax is different and covered elesewhere.
```

### Count of Each Level {.unlisted .unnumbered}

```{r 'B26-FactorCount'}
# #For any Given Column, What is the Count for each Level
xw %>% count(f) %>% arrange(desc(n))
#
# #What is the Count for each Dummy Variable
dum_xsyw %>% summarise(across(4:ncol(.), sum)) %>% pivot_longer(everything())
```


## Explanation of Estimates

- With Referenced Variabls and Keeping others constant
- Significant and inversely affecting the Selling Price 
  - km_driven 
  - age 
- Significant and positively affecting the Selling Price
  - Diesel has higher selling price compared to CNG
  - Other Fuels do not have significant impact (when CNG is the Reference)
- \textcolor{pink}{Question:} Are the km_driven and age not correlated. Would these together not cause the multicollinearity issue
  - We can check by 'cor()', however it is not very high

## Correlation Plots {.tabset .tabset-fade}

- (Aside) Scaling of Dummy Variables would not impact negatively to the analysis. However scaling is done to adjust for large variances. As all the dummy variables are {0, 1}, these have not been scaled.
- (Aside) The referenced dummy variable is NOT included for example Diesel
  - It would be highly correlated (inversely) to the 2nd-most frequent level (Petrol) because when it occurs the other one does not happen and when that happens 1st-one would not happen


```{r 'B26-Scale'}
# #Exclude Y | Scale Continuous NOT Dummies |
zw <- dum_xsyw %>% select(-price) %>% 
  mutate(across(c(km, age), ~ as.vector(scale(.))))
# #Long
#f_wl(zw)
```

### Images {.unlisted .unnumbered}

```{r 'B26-CorGG-Set', include=FALSE}
# #IN: zw
cap_hh <- "B26P01"
ttl_hh <- "CarDekho: GGplot: Corrplot of Dummies (Scaled)"
sub_hh <- "showing only the correlation not significance"
lgd_hh <- "Correlation"
#
# #Correlation Matrix pXp | Tibble pX(p+1) | Long | Unique Factor | Remove duplicates AB = BA |
# #Factor with Unique is better to keep the sequence as occurred, default is alphabetical
hh <- cor(zw) %>% 
  as_tibble(rownames = "dummies") %>% 
  pivot_longer(cols = -dummies) %>% 
  mutate(across(where(is.character), factor, levels = unique(name))) %>% 
  filter(!duplicated(paste0(pmax(as.character(dummies), as.character(name)), 
                            pmin(as.character(dummies), as.character(name)))))
```

```{r 'B26-CorGG-plot', include=FALSE}
# #IN: hh[dummies, names, value] (Correlation Tibble Long, Triangle with Diagonal) 
B26 <- hh %>% { ggplot(., aes(x = dummies, y = name, fill = value)) + 
    geom_tile(color = "white") + 
    geom_text(aes(label = round(value, 2)), color = "black", size = 4) +
    coord_fixed() +
    scale_fill_distiller(palette = "BrBG", direction = 1, limits = c(-1, 1)) +
    #scale_x_discrete(position = "top") +
    scale_y_discrete(limits = rev) +
    guides(fill = guide_colourbar(barwidth = 0.5, barheight = 15)) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          axis.title = element_blank(), 
          axis.line = element_blank(), 
          axis.ticks = element_blank(),
          panel.grid.major = element_blank(), 
          panel.border = element_blank()) +
	  labs(fill = lgd_hh, subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B26)
rm(B26)
```

```{r 'B26P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B26P01", "-CarDekho-GGplot-Corrplot-z", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B26P01, device = "png", dpi = 144) 
}
```

```{r 'B26P01', echo=FALSE, fig.cap="(B26P01) CarDekho: GGplot: Corrplot of Dummies (Scaled)"}
knitr::include_graphics(paste0(.z$PX, "B26P01", "-CarDekho-GGplot-Corrplot-z", ".png"))
```


```{r 'B26P02-Save', include=FALSE}
if(FALSE){#Testing
  ii_dum_xsyw <- xsyw %>% dummy_cols(.data = ., 
                  select_columns = c("f", "s", "t", "o"), 
                  remove_first_dummy = FALSE, remove_selected_columns = TRUE)
  ii <- ii_dum_xsyw %>% select(-price) %>% 
    mutate(across(c(km, age), ~ as.vector(scale(.))))
  hh <- cor(ii)
  corr_hh <- corrplot::cor.mtest(ii)
}
#
hh <- cor(zw)
corr_hh <- corrplot::cor.mtest(zw)
#
cap_hh <- "B26P02"
ttl_hh <- "CarDekho: corrplot: Corrplot of Dummies (Scaled)"
loc_png <- paste0(.z$PX, "B26P02", "-CarDekho-corrplot-Corrplot-z", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  corrplot::corrplot(hh, method = "circle", type = "lower", diag = FALSE, 
                   cl.pos = 'n', tl.pos = 'ld', addCoef.col = "black", 
                   p.mat = corr_hh$p, sig.level = 0.05, insig = 'blank', 
        #order = "hclust", hclust.method = "ward.D", addrect = 2, rect.col = 3, rect.lwd = 3, 
                   title = NULL #, col = RColorBrewer::brewer.pal(3, "BrBG")
				   )
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B26 <- recordPlot()
  dev.off()
  assign(cap_hh, B26)
  rm(B26)
}
```

```{r 'B26P02', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B26P02", "-CarDekho-corrplot-Corrplot-z", ".png"))
```

```{r 'B26P03-Save', include=FALSE}
hh <- psych::corr.test(zw)
#
cap_hh <- "B26P03"
ttl_hh <- "CarDekho: psych: Corrplot of Dummies (Scaled)"
loc_png <- paste0(.z$PX, "B26P03", "-CarDekho-Psych-Corrplot-z", ".png")
#
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  psych::corPlot(hh$r, pval = hh$p, upper = FALSE, diag = FALSE, show.legend = TRUE, 
                 xlas = 2, cex = 0.6,
                 #keep.par = FALSE, 
    gr = colorRampPalette(RColorBrewer::brewer.pal(3, "BrBG")), main = ttl_hh)
  title(sub = cap_hh, line = 4, adj = 1)
  B26 <- recordPlot()
  dev.off()
  assign(cap_hh, B26)
  rm(B26)
}
```

```{r 'B26P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B26P03", "-CarDekho-Psych-Corrplot-z", ".png"))
```


```{r 'B26P0203', echo=FALSE, ref.label=c('B26P02', 'B26P03'), fig.cap="(B26P02 B26P03) CarDekho: corrplot vs. psych"}
#
```

### Code GGplot {.unlisted .unnumbered}


```{r 'B26-CorGG-Set-A', eval=FALSE, ref.label=c('B26-CorGG-Set', 'B26-CorGG-plot')}
#
```

### Code Corrplot {.unlisted .unnumbered}


```{r 'B26P02-Save-A', eval=FALSE, ref.label=c('B26P02-Save')}
#
```


### Code psych {.unlisted .unnumbered}


```{r 'B26P03-Save-A', eval=FALSE, ref.label=c('B26P03-Save')}
#
```

### Why NOT SPLOM {.unlisted .unnumbered}

- (Aside) 
  - There are 2 Packages for SPLOM Plots Psych and GGally. 
    - Examples of these plots can be found in other chapters. - "ForLater" Add Links
  - The Problem with these plots is that screen space is limited and as soon as the number of variables go beyond 4, it becomes highly difficult to make sense of anything. 
    - And it takes a long time to plot them.
    - These plots will only be included if these make sense.
  - There are different plots which would show only the correlation number and those are more efficient.

## VIF

```{r 'B26D03', comment="", echo=FALSE, results='asis'}
f_getDef("VIF")
```

- If the multicollinearity is present then model performance decreases
  - We can check correlation or VIF
  - $R_i^2 = 0.80 \to \text{VIF} \geq 5$ to be an indicator of moderate multicollinearity
  - $R_i^2 = 0.90 \to \text{VIF} \geq 10$ to be an indicator of severe multicollinearity
  - (Not Shown Here) But if the model is created with reference level of 'CNG' in fuel, then Petrol and Diesel will show very high VIF and high correlation
    - Because, when a Car runs on Petrol it does not run on Diesel and when it runs on Diesel it does not run on Petrol.
    - Number of observations for both of these levels are similar
    - Thus, it is better to convert the most frequent level (Diesel) as the reference level


```{r 'B26-VIF'}
# #vif() To check VIF of the Model. All values should be < 5 (desirable) or < 10 (recommended)
vif(mod_xsyw)
```

## Stepwise Regression {.tabset .tabset-fade}

```{r 'B26D04', comment="", echo=FALSE, results='asis'}
f_getDef("Stepwise-Regression") #dddd
```

- The stepwise procedure represents a modification of the forward selection procedure. 
  - In Forward Selection, we start with no variables in model, add most highly correlated variable (correlated to Y), check for significance, keep doing this for other variables in decreasing order of  correltaion until the model remains significant.
  - In Backward Elimination, we start with all the variables in the model, select the variable with smallest partial F-statistic, remove it if it is insignificant, keep doing this for other variables in the increasing order of partial F-statistic until these remain insignificant.

- There are 3 dummy variables which have low p-value. 
  - All were kept by "forward"
  - Only 1 of them (o_More) was dropped by "backward"
  - Only 1 of them (o_More) was dropped by "both"
  - "ForLater" Theoretically it is understandable that some insignificant variables were kept because algorithm run differently than the simplistic p-value based approach. However, does it means that elimination or retaintion of variables should NOT be done based on p-value
  - "ForLater" It has been observed that selection of reference level changes the model outcome, significance of dummy variables etc. Which Level should be chosen as Reference. Currently, I am going with the idea that most frequent level should be the reference.

- \textcolor{pink}{Question:} What are 12 elements / 13 elements shown about these Models
  - Elements are Attributes of the Model, not the number of variables in the model
  - (Aside) Base Model has 12 attributes. Model returned by step() has 1 more attibute (anova)

### Model Stepwise {.unlisted .unnumbered}

```{r 'B26-Step'}
# #step() can provide Stepwise Regression # "forward" "backward" "both"
stp_xsyw <- step(mod_xsyw, direction = "backward", trace = 0)
#stp_xsyw
#summary(stp_xsyw)
# #It adds another attribute (anova) to the model and thus shows 13 attributes
names(stp_xsyw) #13
#
if(TRUE) f_pNum(summary(stp_xsyw)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
# #anova table attribute for the iterations performed (not present in Base Model)
stp_xsyw$anova
```

### Model Original {.unlisted .unnumbered}

```{r 'B26-OriginalModel'}
#mod_xsyw
#summary(mod_xsyw)
# #Base Model has 12 attributes
names(mod_xsyw) #12
#
if(TRUE) f_pNum(summary(mod_xsyw)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
```

## Model Validation

- For some data points, the error is Huge, otherwise the Models look OK.
  - Later, we would compare RMSE of different algorithms to identify the best algorithm applicable to the secific dataset

```{r 'B26-ModelValidation'}
# #predict() on test dataset by Base Model and Stepwise corrected Model
#pred_mod_xsyw <- predict(mod_xsyw, test_xsyw)
res_mod_xsyw <- test_xsyw %>% mutate(CalY = predict(mod_xsyw, .), Y_Yc = price - CalY)
res_stp_xsyw <- test_xsyw %>% mutate(CalY = predict(stp_xsyw, .), Y_Yc = price - CalY)
#
res_w <- tibble(Model = res_mod_xsyw$Y_Yc, Step = res_stp_xsyw$Y_Yc) 
f_wl(res_w)
#
summary(res_w)
#
# #RMSE: Root Mean Squared Error for Both Models (Loss Function)
res_w %>% summarise(across(everything(), ~ sqrt(mean((.)^2))))
#
# #MAE: Mean Absolute Error (MAE)
res_w %>% summarise(across(everything(), ~ mean(abs(.))))
```


```{r 'B26-Box', include=FALSE}
hh <- res_l
#
ttl_hh <- "CarDekho: BoxPlot of Results"
cap_hh <- "B26P04"
sub_hh <- NULL 
lgd_hh  <- NULL
```

```{r 'B26-Box-Plot', include=FALSE}
# #IN: hh(Keys, Values), 
B26 <- hh %>% { ggplot(data = ., mapping = aes(x = Keys, y = Values, fill = Keys)) +
    geom_boxplot(outlier.shape = NA) +
    geom_point(position = position_jitterdodge(jitter.width = 0.2), 
                   size = 0.5, alpha = 0.4, colour = "#21908CFF"
               ) + 
    k_gglayer_box +
    scale_y_continuous(breaks = breaks_pretty()) + 
    #coord_flip() +
    theme(legend.position = 'none') +
    labs(x = NULL, y = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B26)
rm(B26)
```

```{r 'B26P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B26P04", "-CarDekho-Results-BoxPlot", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B26P04, device = "png", dpi = 144) 
}
```

```{r 'B26P04', echo=FALSE, fig.cap="(B26P04) CarDekho: BoxPlot of Results"}
knitr::include_graphics(paste0(.z$PX, "B26P04", "-CarDekho-Results-BoxPlot", ".png"))
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B26-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, ff, model1_jj, namesXL, test_ii, test_jj, train_ii, train_jj,
          train_row_ii, train_row_jj, xxB26CarDekho, xxB26Hdesc, xxB26Hmod, xxB26Hraw, xxB26KC, 
          xxflights, dum_xsyw, idx_bb, mod_bb, test_bb, train_bb, xxB25CarDekho, B26P01, B26P03, 
          cap_hh, corr_hh, hh, idx, idx_xsyw, lgd_hh, loc_png, mod_xsyw, sub_hh, test_xsyw, 
          train_xsyw, ttl_hh, xsyw, xw, zw, B26P04, res_l, res_mod_xsyw, res_stp_xsyw, res_w, 
          stp_xsyw)
```

```{r 'B26-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/226-RegressionL.Rmd-->

# Linear Regression (B27, Jan-09) {#b27}

```{r 'B27', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Machine learning using linear regression"
  - Skipped "15:48-15:56" because interactive function 'file.choose()' or Import is difficult to show in Notebook.

## Data: Hospital {.tabset .tabset-fade}

\textcolor{pink}{Please import the "B27-Hospital.xlsx"}

```{r 'B27-namesHospital', include=FALSE}
# #Object Names for each sheet
namesXL <- c("xxB27Hdesc", "xxB27Hraw", "xxB27Hmod")
```

```{r 'B27-getHospital', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum
#tools::md5sum(paste0(.z$XL, "B27-Hospital.xlsx"))
xxB27Hospital <- f_getObject("xxB27Hospital", "B27-Hospital.xlsx", "176788c58511ea4e98739a5314a55aab")
```

```{r 'B27-XLobjects', include=FALSE, eval=FALSE}
# #Create Separate Tibbles for Each Sheet
# #Separate Objects, excluding the First Sheet
for(ii in seq_along(namesXL)){
  assign(namesXL[ii], xxB27Hospital[[ii + 1]])
}
# #Save Binary Files
for(ii in seq_along(namesXL)){
  saveRDS(eval(parse(text = namesXL[ii])), paste0(.z$XL, namesXL[ii], ".rds"))
}
```

```{r 'B27-GetHospital', include=FALSE}
for(ii in seq_along(namesXL)){
  assign(namesXL[ii], readRDS(paste0(.z$XL, namesXL[ii], ".rds")))
}
```

```{r 'B27-List', include=FALSE, eval=FALSE}
# #Dimensions of these datasets
str(lapply(namesXL, function(x) {dim(eval(parse(text = x)))}))
```

### EDA {.unlisted .unnumbered}

- About: [248, 24]
  - Sanitise Column Names for ease of use
  - NA in 7 columns 
  - ~~If implant was not used then implant cost was changed from 0 to NA~~
    - Regression really does not like NA.
  - Dropped "State at the Time of Arrival" because only 1/248 has "Confused", all others are "Alert"
  - Converted Low BP from Character to Numeric
    - Low BP and High BP have 23 NA for same patients
  - Converted Binary Variables to dummy variables i.e. iFemale, iMarried, iEmergency, iImplant
  - History has Typo in case so convert all to lower, change labels and made "ht1" as reference because it is the most frequent
    - NA in History were merged with already available "other" category


```{r 'B27-PrepHospital'}
# #Sanitise Headers
aa <- xxB27Hraw %>% 
  rename_with(make.names) %>% rename_with(~ tolower(gsub(".", "_", .x, fixed = TRUE))) 
names(aa) <- c("ID", "Age", "iFemale", "iMarried", "Complaints", "Weight", "Height", "Pulse", 
               "HighBP", "LowBP", "Respiratory", "History", "Hemoglobin", "Urea", "Creatinine", 
               "Arrival", "State", "iEmergency", "TC", "Stay", "ICU", "Ward", "iImplant", "ImpCost")
#
# #Drop | To Numeric | Relocate | Flags | Lower Case | Replace NA | 
# #Levels & Labels | Replace String | To Factor
# #To Remove Multicollinearity: Dropped Arrival, Dropped iImplant, Drop Stay (~ ICU + Ward)
# #Reverted ImpCost to original (0) for No implant 
# #Changed NA of Complaints to 'other'
# #Remove Rows containing NA in HighBP, LowBP, Hemoglobin, Urea, Creatinine
#
bb <- aa %>% 
  select(-State) %>% 
  mutate(across(LowBP, as.numeric)) %>% 
  relocate(TC, .after = ID) %>% 
  mutate(iFemale = ifelse(iFemale == "F", 1, 0)) %>% 
  mutate(iMarried = ifelse(iMarried == "MARRIED", 1, 0)) %>% 
  mutate(iEmergency = ifelse(iEmergency == "EMERGENCY", 1, 0)) %>% 
  mutate(iImplant = ifelse(iImplant == "Y", 1, 0)) %>% 
  mutate(across(Complaints, coalesce, "other")) %>% 
  mutate(across(History, tolower)) %>% 
  mutate(across(History, coalesce, "other")) %>% 
  mutate(across(History, factor, 
levels = c("hypertension1", "hypertension2", "hypertension3", "diabetes1", "diabetes2", "other"), 
labels = c("ht1", "ht2", "ht3", "db1", "db2", "other"))) %>% 
  mutate(Arrival = str_replace_all(Arrival, c(" " = "_"))) %>% 
  mutate(Arrival = factor(Arrival, levels = c("WALKED_IN", "AMBULANCE", "TRANSFERRED"), 
                          labels = c("Walked", "Ambulance", "Transferred"))) %>% 
  mutate(Complaints = str_replace_all(Complaints, 
                        c(" " = "_", "-" = "_", "__" = "_", "other_" = "o_"))) %>% 
  mutate(across(Complaints, factor)) %>% 
  mutate(across(Complaints, relevel, ref = "o_heart")) %>% 
  #select(-c(Arrival, iImplant, Stay)) %>% 
  drop_na(HighBP, LowBP, Hemoglobin, Urea, Creatinine)
#
xsyw <- bb
```

### Structure {.unlisted .unnumbered}

```{r 'B27-Temps', include=FALSE, eval=FALSE}
# #Count NA 
if(FALSE) colSums(is.na(bb)) %>% as_tibble(rownames = "Cols") %>% filter(value > 0)
#
if(FALSE) bb %>% select(where(is.numeric)) %>% summary()
if(FALSE) bb %>% select(isFemale) %>% slice(1:10)
if(FALSE) cat('"', paste0(levels(bb$Arrival), collapse = '", "'), '"\n', sep = '')
if(FALSE) bb %>% count(Complaints)
if(FALSE) bb %>% filter(is.na(bpl)) %>% select(id, bph, bpl)
if(FALSE) bb %>% select(isImplant, imp_c) %>% summary()
if(FALSE) bb %>% select(iFemale, iMarried, iEmergency, iImplant) %>% 
  pivot_longer(everything()) %>% 
  count(name, value) %>% 
  pivot_wider(names_from = value, values_from = n) 
```

```{r 'B27-strH'}
str(bb)
```

### Summary {.unlisted .unnumbered}

```{r 'B27-summaryH'}
summary(bb)
```

## Dummies {.tabset .tabset-fade}

- Refer [Omitting the rows with missing data is not recommended](#immputation-b16 "b16")

- Following is applicable to First Run of the Model which had Multicollinearity issues and thus was modified further.
  - Note: The dummy object created here contains 40 columns. It is slightly different from what professor has created with 41 columns.
  - I have skipped 'state on arrival' and kept 'ID' for now
  - I have merged NA in 'history' column to 'other' category
  - Further, the Reference Level is different in some dummy variables
  - I have NOT removed rows containg NA. Those will be removed just before the Model building 
    - Ideally these should be imputed but that is "ForLater"
      - Medical information is difficult to impute
    - Number of Rows with NA is higher (211) in my case because I have replaced 0 in 'Implant Cost' to NA
  - If the original categorical column has 'NA' the dummy creation lead to extra 'NA' column. 
    - The other columns would have {0, 1, NA} in this case. For now those NA are being set to 0 manually. Impact of keeping these NA as it is and handling it inside model would be observed "ForLater"

### Create {.unlisted .unnumbered}

```{r 'B27-Dummy'}
# #Create Dummies | Replaced All NA in original data so no replacement in dummy columns |
dum_xsyw <- dummy_cols(xsyw, remove_first_dummy = TRUE, remove_selected_columns = TRUE) 
```

### Structure {.unlisted .unnumbered}

```{r 'B27-StrHd'}
names(dum_xsyw)
str(dum_xsyw)
```

### NA {.unlisted .unnumbered}

```{r 'B27-NA'}
sum(!complete.cases(dum_xsyw))
colSums(is.na(dum_xsyw)) %>% as_tibble(rownames = "Cols") %>% filter(value > 0)
```

## Model {.tabset .tabset-fade}

- First Run of Model with 40 variables
  - Multicollinear

```{conjecture 'vif-aliased'}
\textcolor{brown}{Error in vif.default(...) : there are aliased coefficients in the model}
```

- There is Perfect Mullicollinearity issue in the Model Variables
- To identify which variables are the culprits, run use \textcolor{pink}{alias()}
  - It tells that 'Arrival_Ambulance' has perfect collinearity with 'iEmergency'
    - ~~In this case we should drop the Arrival Variable completely~~
    - It turns out that the NA were the main problem. Multicollinearity can be handled by step()
  - Ultimately all NA were removed from Continuous Variables and NA in Complaints were converted to Other

```{conjecture 'step-rows-changed'}
\textcolor{brown}{Error in step(...) : number of rows in use has changed: remove missing values}
```

- If the original model was created with using 'na.action' of lm(), step() throws this error
  - Remove all NA from original dataset

### Model 4 {.unlisted .unnumbered}

```{r 'B27-ModelH'}
# #Partition Data
set.seed(3)
idx_xsyw <- sample.int(n = nrow(dum_xsyw), size = floor(0.8 * nrow(dum_xsyw)), replace = FALSE)
#
# #Drop the ID Column from Training
train_xsyw <- dum_xsyw[idx_xsyw, ] %>% select(-ID) 
test_xsyw  <- dum_xsyw[-idx_xsyw, ]
#
mod_xsyw <- lm(TC ~ ., data = train_xsyw)
if(FALSE) f_pNum(summary(mod_xsyw)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
stp_xsyw <- step(mod_xsyw, direction = "backward", trace = 0)
if(TRUE) f_pNum(summary(stp_xsyw)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
summary(stp_xsyw)$adj.r.squared
names(stp_xsyw$coefficients)
```

### VIF {.unlisted .unnumbered}

```{r 'B27-VifH'}
ii <- vif(mod_xsyw) 
ii[ii > 5]
#
ii <- vif(stp_xsyw) 
ii[ii > 5]
```

### Multicollinear {.unlisted .unnumbered}

```{r 'B27-Model2Coll', eval=FALSE}
# #Had to Remove all NA from Original Dataset to handle the Error
mm <- alias(mod_xsyw)$Complete
mm[rowSums(mm^2) != 0, colSums(mm^2) != 0]
#
if(FALSE) xsyw %>% filter(History == "db1") %>% select(ID, History, Complaints, iEmergency)
```

## Model 5

- To Explore: What happens if we remove the NA of "Complaints" Column (earlier those were converted to 'Other')
  - Different Variables become significant 
    - However, it might be due to change in Train Data because number of Rows has changed. The included and excluded rows have changed even if we are re-setting the seed immediately before the sampling.
    - "ForLater" Take a dataset do the same exercise but filter on Train data (not the original dataset as done here). It would show whether the variables included remain same or not.

- Comparison of Models
  - ~~Model 2 (Excluding Arrival, iImplant, Stay) & (Complaints NA to Other)~~ 
    - $R_a^2 = 0.873$
    - No VIF > 5
    - Variables: 10
      - (Intercept), Pulse, Hemoglobin, Urea, Creatinine, ICU, Ward, ImpCost, Complaints_CAD_TVD, Complaints_o_general, History_db2
      - Common: Pulse, ICU, Ward, ImpCost, Complaints_o_general, History_db2
      - NOT in Model 3: Hemoglobin, Urea, Creatinine, Complaints_CAD_TVD
  - ~~Model 3 (Excluding Arrival, iImplant, Stay) & (Excluding NA of Complaints)~~ 
    - $R_a^2 = 0.863$
    - No VIF > 5
    - Variables: 9
      - (Intercept), Age, Height, Pulse, ICU, Ward, ImpCost, Complaints_o_general, Complaints_PM_VSD, History_db2
      - Common: Pulse, ICU, Ward, ImpCost, Complaints_o_general, History_db2
      - NOT in Model 2: Age, Height, Complaints_PM_VSD
  - Model 4 (Complaints NA to Other)
    - $R_a^2 = 0.873$
    - No VIF > 5
    - Variables: 10 (Only Stay replaced Ward compared to Model 2)
      - (Intercept), Pulse, Hemoglobin, Urea, Creatinine, Stay, ICU, ImpCost, Complaints_CAD_TVD, Complaints_o_general, History_db2
      - Common: Pulse, ICU, Stay, ImpCost, Complaints_o_general, History_db2
      - NOT in Model 5: Hemoglobin, Urea, Creatinine, Complaints_CAD_TVD
  - Model 5 (Excluding NA of Complaints) 
    - $R_a^2 = 0.861$
    - No VIF > 5
    - Variables: 8 (Stay replaced Ward, Tertology replaced VSD and Dropped Height compared to Model 3)
      - (Intercept), Age, Stay, ICU, ImpCost, Complaints_o_general, Complaints_o_tertalogy, History_db2
      - Common: Pulse, ICU, Stay, ImpCost, Complaints_o_general, History_db2
      - NOT in Model 4: Age, Height, Complaints_o_tertalogy


```{r 'B27-Model3'}
# #Remove the Original NA (that were converted to other) in Complaints
ii <- xsyw %>% filter(Complaints != "other") %>% mutate(across(Complaints, droplevels))
dum_ii <- dummy_cols(ii, remove_first_dummy = TRUE, remove_selected_columns = TRUE) 
# #Partition Data
set.seed(3)
idx_ii <- sample.int(n = nrow(dum_ii), size = floor(0.8 * nrow(dum_ii)), replace = FALSE)
#
# #Drop the ID Column from Training
train_ii <- dum_ii[idx_ii, ] %>% select(-ID) 
test_ii  <- dum_ii[-idx_ii, ]
#
mod_ii <- lm(TC ~ ., data = train_ii)
if(FALSE) f_pNum(summary(mod_ii)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
stp_ii <- step(mod_ii, direction = "backward", trace = 0)
if(FALSE) f_pNum(summary(stp_ii)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
vif_ii <- vif(stp_ii)
vif_ii[vif_ii > 5]
#
summary(stp_ii)$adj.r.squared
names(stp_ii$coefficients)
```


## Normality 

```{r 'B27D01', comment="", echo=FALSE, results='asis'}
f_getDef("Simple-Linear-Regression-Assumption-Summary") #dddd
```

- To check the normality assumtion of each $x_i$, we can look at the normality of error term $\epsilon$
  - The $\epsilon$ will vary depending upon the $\beta_i$ and the $\beta_i$ vary depending upon the $x_i$
  - So if $x_i$ follows an assumption, then $\epsilon$ should follow that assumption
  - If we presume that $x_i$ is follow Normal Distribution then it implies that $\epsilon$ will also follow Normal Distribution i.e. $x_i \in \mathcal{N} : \epsilon \in \mathcal{N}$
  - Explanation
    - $\beta_i$ should explain maximum variability, so there is an association between $\beta_i$ and $\epsilon$. Change in $\beta_i$ would impact $\epsilon$.
    - $\epsilon$ is the part unexplained by $x_i$. So, it gets affected by $x_i$
  - If $\epsilon$ is not distributed normally then at least one of the $x_i$ is not distributed normally and we would need to take corrective action


- \textcolor{pink}{Question:} What are the causes of non-normality in error term $\epsilon$
  - One of the common cause is Outliers

- \textcolor{pink}{Question:} "ForLater" This will tell us about only those $x_i$ which were included in the model. It would not comment on the variables that got dropped during the model or step model creation


```{r 'B27-Normal'}
# #Normality Test
test_shapiro <- shapiro.test(mod_xsyw$residuals)
test_shapiro
#
if(test_shapiro$p.value > 0.05) cat("Normal. H0.\n") else cat("Not Normal. Ha.\n")
```

## Semi-Log Transformation

- To decrease the skewness, we need to transform Y.
  - Semi-log Transformation: Take Log only on Y
  - Log-log Transformation: If situation does not improve, we will take log of Y and all $x_i$

- \textcolor{pink}{Question:} Can we do the Normalisation
  - No, Scaling or Normalisation does not reduce the skewness.

- \textcolor{pink}{Question:} "ForLater" During log-log how would we take log of dummy variables {0, 1}
  - (Aside) Probably, we do not apply log on dummy variables. As we have seen earlier, their variance is already controlled.

- Semi-log Transformation 
  - Model still is NOT following Normality
  - $R_a^2 = 0.828$
    - VIF > 5 is for Two Variables: Age, iMarried (It can be ignored)
    - Variables: 18 (8 more than original Model 4)

- \textcolor{pink}{Question:} "ForLater" Now both iImplant and ImpCost are present in the Model without any problem. Even though iImplant = 0 (Flag) directly corresponds with ImpCost = 0 (Continuous). This might be the correct way to do regression after all!

```{r 'B27-TransformSemi'}
# #Run Log Transformed Y
mod_xsyw_log <- lm(log(TC) ~ ., data = train_xsyw)
stp_xsyw_log <- step(mod_xsyw_log, direction = "backward", trace = 0)
if(FALSE) f_pNum(summary(stp_xsyw_log)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
vif_ii <- vif(stp_xsyw_log)
vif_ii[vif_ii > 5]
#
summary(stp_xsyw_log)$adj.r.squared
names(stp_xsyw_log$coefficients)
#
# #Normality
test_shapiro <- shapiro.test(stp_xsyw_log$residuals)
test_shapiro
#
if(test_shapiro$p.value > 0.05) cat("Normal. H0.\n") else cat("Not Normal. Ha.\n")
```

## Log-Log Transformation - Normal

- \textcolor{pink}{Question:} Sometimes step() does not remove some variable which are not significant (as given by p-value)
  - Presence of these variables help other variables in making significant contribution to the model

- Log-log Transformation 
  - \textcolor{pink}{Model is NORMAL.}
    - Added +1 to 4 variables with 0 values to handle 'log(0) = -Inf' problem. Two of them anyway were dropped due to multicollinearity.
  - $R_a^2 = 0.814$
    - No VIF > 5
    - Variables: 16 

```{r 'B27-TransformLogLog'}
# #Run Log Transformed Y and X
train_log <- train_xsyw %>% 
  mutate(across(c(Stay, ICU, Ward, ImpCost), ~ . + 1)) %>% 
  mutate(across(c(TC, Age, Weight, Height, Pulse, HighBP, LowBP, Respiratory, Hemoglobin, Urea,
                  Creatinine, Stay, ICU, Ward, ImpCost), log)) %>% 
  select(-c(ImpCost, Stay))
#
mod_xsyw_lll <- lm(TC ~ ., data = train_log)
stp_xsyw_lll <- step(mod_xsyw_lll, direction = "backward", trace = 0)
if(FALSE) f_pNum(summary(stp_xsyw_lll)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
vif_ii <- vif(stp_xsyw_lll)
vif_ii[vif_ii > 5]
#
summary(stp_xsyw_lll)$adj.r.squared
names(stp_xsyw_lll$coefficients)
#
# #Normality
test_shapiro <- shapiro.test(stp_xsyw_lll$residuals)
test_shapiro
#
if(test_shapiro$p.value > 0.05) cat("Normal. H0.\n") else cat("Not Normal. Ha.\n")
```


## Predict

- Subtract Y from 'exp(predicted)' to get the RMSE etc. in case of log transformed Y.

```{r 'B27-Predict'}
# #Apply Same Transformation on Test Data (except the Actual Y)
test_log <- test_xsyw %>% 
  mutate(across(c(Stay, ICU, Ward, ImpCost), ~ . + 1)) %>% 
  mutate(across(c(#TC, 
                  Age, Weight, Height, Pulse, HighBP, LowBP, Respiratory, Hemoglobin, Urea,
                  Creatinine, Stay, ICU, Ward, ImpCost), log)) %>% 
  select(-c(ImpCost, Stay))
#
res_lll <- test_log %>% 
  mutate(CalYlog = predict(stp_xsyw_lll, .), Y_Yc = TC - exp(CalYlog))
#
summary(res_lll$Y_Yc)
#
# #RMSE: Root Mean Squared Error
#res_w %>% summarise(across(everything(), ~ sqrt(mean((.)^2))))
sqrt(mean((res_lll$Y_Yc)^2))

#
# #MAE: Mean Absolute Error (MAE)
#res_w %>% summarise(across(everything(), ~ mean(abs(.))))
mean(abs(res_lll$Y_Yc))
```


## Glance

```{r 'B27-Glance'}
lapply(list(stp_xsyw_lll, stp_xsyw_log, mod_xsyw, stp_ii), glance)
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B27-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, ff, namesXL, dum_ii, dum_xsyw, idx_ii, idx_xsyw, mod_ii, 
          mod_xsyw, mod_xsyw_lll, mod_xsyw_log, res_lll, stp_ii, stp_xsyw, stp_xsyw_lll, 
          stp_xsyw_log, test_ii, test_log, test_shapiro, test_xsyw, train_ii, train_log, 
          train_xsyw, vif_ii, xsyw, xxB27Hdesc, xxB27Hmod, xxB27Hraw)
```

```{r 'B27-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/227-RegressionL.Rmd-->

# Linear Regression (B28, Jan-16) {#b28}

```{r 'B28', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Machine learning using linear regression"

## Packages

```{r 'B28-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("caret", dependencies = TRUE)
  install.packages("glmnet", dependencies = TRUE)
  install.packages("mlbench", dependencies = TRUE)
}
```

## Review

- Refer [Bias-Variance Trade-off](#bias-var-c37 "c37")

```{r 'B28D01', comment="", echo=FALSE, results='asis'}
f_getDef("Bias-Variance-Trade-off")
```

```{r 'B28D02', comment="", echo=FALSE, results='asis'}
f_getDef("Coefficient-of-Determination") 
```

```{r 'B28D03', comment="", echo=FALSE, results='asis'}
f_getDef("Overfitting")
```

```{r 'B28D04', comment="", echo=FALSE, results='asis'}
f_getDef("Underfitting")
```

```{r 'B28P01', echo=FALSE, ref.label=c('C37P01'), fig.cap="(C37P01) Bias-Variance Trade-off"}
# #Ref another file chunk
```

```{r 'B28D05', comment="", echo=FALSE, results='asis'}
f_getDef("Residuals")
```

```{r 'B28D06', comment="", echo=FALSE, results='asis'}
f_getDef("RSq-Adj")
```

```{r 'B28D07', comment="", echo=FALSE, results='asis'}
f_getDef("Multicollinearity")
```

```{r 'B28D08', comment="", echo=FALSE, results='asis'}
f_getDef("Principle-of-Parsimony") 
```

- \textcolor{pink}{Question:} Bias can be mathematically represented by $R^2$. What represents the Variance
  - RMSE represents the Variance
  - "ForLater" $R^2$ is linked with RMSE also so how can it represent bias

## Explanation of Bias-Variance Trade-off

- \textcolor{pink}{Question:} What does $\beta = 0$ represents
  - It will be a line parallel to x-axis. Its slope will be zero. 
  - It means that Y does not depend on X. There is No correlation between Y and X.
  - X can not explain the changes in Y. 
  - Inclusion of this X in the model would lead to increase in complexity without any reduction in error
    - i.e. Bias will increase but variance will not decrease
    - i.e. It would cause overfitting
- Higher the absolute slope of the line, stronger would be the effect of unit change in X on Y
  - Positive or Negative would depend on the sign of the slope

```{r 'B28P02', echo=FALSE, ref.label=c('C14P01'), fig.cap="(C14P01) Linear Regression"}
# #Ref another file chunk
```

## Regularised Regression

- 3 Approaches of Regularised Regression
  - Ridge Regression $(L_2) : \lambda_R \displaystyle \sum_{j=1}^{p} \beta_j^2$
  - Lasso Regression $(L_1) : \lambda_L \displaystyle \sum_{j=1}^{p} |\beta_j|$
    - LASSO i.e. Least Absolute Shrinkage and Selection Operator
  - Elastic Net Regression $: \lambda_E \displaystyle \sum_{j=1}^{p} \left ( \left ( 1 - \alpha \right ) \beta_j^2 + \alpha |\beta_j| \right )$
    - $\alpha = 0 : \lambda_E \to \lambda_L$
    - $\alpha = 1 : \lambda_E \to \lambda_R$
    - The Elastic Net selects variables like the Lasso, and shrinks together the coefficients of correlated predictors like Ridge.
    - "ForLater" Book shows the equation with multipliers interchanged 
      - Overall exaplanation would not change. Only the effect of ${\alpha}$ being 0 and 1 would be reversed.
  - $\{\alpha, \lambda\}$ are sometimes called \textcolor{pink}{hyperparameters}

- train() with method = "glmnet" is used for all 3
  - Ridge $(\alpha = 0)$, Lasso $(\alpha = 1)$, Elastic Net $(\alpha \in \{0, 1\})$
  
## Cross-validation {#cross-b28}

```{r 'B28D10', comment="", echo=FALSE, results='asis'}
f_getDef("Cross-Validation") #dddd
```

- Refer [Cross-validation C37](#cross-c37 "c37")
- \textcolor{pink}{Two-fold cross-validation}
  - Randomly assign 9 observations (10% of total 90 observations) to Test and remaining to Train.
  - Build the Model on Train, validate on Test
  - \textcolor{pink}{The thus obtained performance evaluation depends on the actual split of the data set.}
    - Use k-fold cross-validation

- \textcolor{pink}{k-fold cross-validation} (Assume k = 10)
  - Let us assume there are 90 observations
  - We can create create k=10 sets each of 9 observations randomly assigned
  - Thus, we can create 10 models where each set acts as 'test' one time and 9 times it is part of 'train' along with others
    - Model 1: Test 1:9, Train 10:90
    - Model 2: Test 10:18, Train {1:9, 19:90} 
    - Model 10: Test 82:90, Train 1:81
  - This ensures that each observation gets to be part of train & test in proportion same as other  variables.
  - Then the best performing model gets selected.
  - \textcolor{pink}{Still the division into the k subsets involves a degree of randomness.}
    - Iterate by Repeated cross-validation

- \textcolor{pink}{Repeated cross-validation} (Example)
  - Do the random k-fold splits 5 times, thus we will have 50 models to choose from
    - Iteration 1: Splits are sequential i.e. {{1:9}, {10:18}, ..., {81:90}}
    - Iteration 2: Splits are along units i.e. {{1, 11, 21, ..., 81}, {2, 12, 22, ..., 82}, ..., {10, 20, 30, ..., 90}}
    - So on


## Ridge Regression (L2)

```{r 'B28D09', comment="", echo=FALSE, results='asis'}
f_getDef("Ridge-Estimator")
```

- For small enough values of $\lambda$, with increse in $\lambda$, the decrease in variance of the ridge regression estimator exceeds the increase in its bias. 
  - Each choice of $\lambda$ leads to a different ridge regression estimate.
    - So, for each model we can get RMSE for many values of $\lambda$


- \textcolor{pink}{Parameter Tuning} is the process of finding the optimum $\lambda$.
  - Let us assume we are doing k-fold cross-validation with k=10 on a datset of 90 observations
  - We can get 10 Models where each set gets to be 'test' once i.e. $\{M_1, M_2, \ldots, M_k\}$
  - For any given model $M_i$, we can also choose a range of $\lambda$
    - ex: $\lambda \in \{0.1, 0.2, 0.3, 0.4, 0.5\}$
    - So, we can get 5 Models for each Model i.e. $\{\{M_{11}, M_{12}, \ldots, M_{15}\}, \{M_{21}, M_{22}, \ldots, M_{25}\}, \cdots, \{M_{k1}, M_{k2}, \ldots, M_{k5}\}\}$
  - Each Model can provide a RMSE value on its own Test Data.
  - We can get average of RMSE associated with each $\lambda$ across all Models i.e. $\{M_{12}, M_{22}, \ldots, M_{k2}\}$
  - Whichever value of $\lambda$ provide robust RMSE, we will assume to be the optimum $\lambda$

- \textcolor{pink}{Question:} Penalisation is done on X only not the Y
  - Yes
  - $\epsilon = \sum {(Y_i - \hat{Y}_i)} + \lambda \sum \beta_i^2$

- \textcolor{pink}{Question:} After doing the iteration, suppose if iteration 1 gives optimum $\lambda = 0.1$ and iteration 2 gives optimum $\lambda = 0.2$. Which one should we consider
  - (Aside) If we have 10 models and 7 iterations then we have actually 70 models. All of them would be run with different (example 5) values of $\lambda$. Thus out of 5 sets of RMSE would be available each having 70 values. Average of each set of RMSE would be done and then optimum $\lambda$ would be chosen based on these 5 sets.

## Data: BostonHousing {#set-boston-b28 .tabset .tabset-fade}

### Load {.unlisted .unnumbered}

```{r 'B28-Boston'}
data("BostonHousing")
xsyw <- bb <- aa <- as_tibble(BostonHousing)
xw <- xsyw %>% select(-medv)
zw <- xw %>% mutate(across(where(is.numeric), ~ as.vector(scale(.)))) %>% 
  mutate(across(chas, strtoi))
```

### Structure {.unlisted .unnumbered}

```{r 'B28-BostonStr'}
str(bb)
```

### Summary {.unlisted .unnumbered}

```{r 'B28-BostonSummary'}
summary(bb)
```

## Correlation

```{r 'B28P03-Save', include=FALSE}
# #IN: zw
hh <- cor(zw)
corr_hh <- corrplot::cor.mtest(zw)
#
cap_hh <- "B28P03"
ttl_hh <- "Boston: Corrplot (Scaled)"
loc_png <- paste0(.z$PX, "B28P03", "-Boston-Corrplot-z", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  corrplot::corrplot(hh, method = "circle", type = "lower", diag = FALSE, 
                   cl.pos = 'n', tl.pos = 'ld', addCoef.col = "black", 
                   p.mat = corr_hh$p, sig.level = 0.05, insig = 'blank', 
        #order = "hclust", hclust.method = "ward.D", addrect = 2, rect.col = 3, rect.lwd = 3, 
                   title = NULL #, col = RColorBrewer::brewer.pal(3, "BrBG")
				   )
  title(main = ttl_hh, line = 2, adj = 0.1)
  title(sub = cap_hh, line = 4, adj = 1)
  B28 <- recordPlot()
  dev.off()
  assign(cap_hh, B28)
  rm(B28)
}
```

```{r 'B28P03', echo=FALSE, fig.cap="(B28P03) Boston Housing Corrplot (Scaled)"}
knitr::include_graphics(paste0(.z$PX, "B28P03", "-Boston-Corrplot-z", ".png"))
```

## Partition Data

```{r 'B28-BostonPart'}
# #Partition Data
set.seed(3)
dum_xsyw <- xsyw
idx_xsyw <- sample.int(n = nrow(dum_xsyw), size = floor(0.8 * nrow(dum_xsyw)), replace = FALSE)
train_xsyw <- dum_xsyw[idx_xsyw, ] 
test_xsyw  <- dum_xsyw[-idx_xsyw, ]
```

## Model lm()

- No Cross Validation

```{r 'B28-BostonLm'}
# #Linear Regression without any Cross-validation
mod_xsyw <- lm(medv ~ ., data = train_xsyw)
if(FALSE) f_pNum(summary(mod_xsyw)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
stp_xsyw <- step(mod_xsyw, direction = "backward", trace = 0)
if(TRUE) f_pNum(summary(stp_xsyw)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
vif_xsyw <- vif(stp_xsyw)
vif_xsyw[vif_xsyw > 5]
#
summary(stp_xsyw)$adj.r.squared
names(stp_xsyw$coefficients)
```

## Repeated CV {.tabset .tabset-fade}

- Repeated Cross-validation (CV)
- \textcolor{pink}{trainControl()}
  - Parameters for train()
- \textcolor{pink}{train()}
  - For Cross-validation
  - tuneGrid : To pass parameters (\textcolor{pink}{hyperparameters}) to the regression i.e. $\{\alpha, \lambda\}$
  - Note: Train Set was passed to it not the full set.
  - Output
    - Rsquared is the average $R^2$ across all models 
    - RsquaredSD is the standard deviation of$R^2$ across all models 

- \textcolor{pink}{Question:} What is the $\lambda$ in linear regression
  - $\lambda = 0$ in linear regression
  
### train() {.unlisted .unnumbered}

```{r 'B28-TrainControl'}
set.seed(3)
# #trainControl() To Set Custom Control Parameters for k=10, iteration = 5
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
#
# #train() can do the crossvalidation
mod_cv <- train(medv ~ ., train_xsyw, method = "lm", trControl = custom)
mod_cv$results
#
mod_cv
#
if(TRUE) f_pNum(summary(mod_cv)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
```

### Details {.unlisted .unnumbered}

```{r 'B28-trainNames'}
names(mod_cv)
# #
mod_cv$method
# #
mod_cv$modelType
# #
mod_cv$results
# #
mod_cv$metric
# #
names(mod_cv$control)
# #
mod_cv$finalModel
# #
names(mod_cv$finalModel)
# #
str(mod_cv$resample)
# #
mod_cv$yLimits
# #
attributes(mod_cv$terms)$term.labels
# #
attributes(mod_cv$terms)$dataClasses
# #
attributes(mod_cv$terms)$variables
# #
mod_cv$coefnames
```

## Ridge Regression {.tabset .tabset-fade}

- \textcolor{pink}{Question:} When RMSE values are similar across multiple $\lambda$, should we use this RMSE as the criteria to choose optimum $\lambda$
  - Yes, we decided to run the algorithm with the given range of $\lambda$ values.

```{conjecture 'invalid-graphics'}
\textcolor{brown}{Error in value[[3L]](cond) : invalid graphics state}
```

- This is probably the RStudio error because of smaller than required plotting Area. 
  - Increase the size
  - OR export PNG etc.

### Model & Lambda {.unlisted .unnumbered}

```{r 'B28-Ridge'}
# #Ridge Regression i.e. alpha = 0, lambda = (0, Inf)
set.seed(3)
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
mod_ridge <- train(medv ~ ., train_xsyw, method = "glmnet", trControl = custom,
    tuneGrid = expand.grid(alpha = 0, 
                           lambda = seq(from = 0.1, to = 1, length.out = 10)))
#
# #Sample Subset of Results
mod_ridge$results %>% as_tibble() %>% slice_sample(n = 5)
#
# #Best Tuned Model
mod_ridge$results %>% as_tibble() %>% filter(lambda == mod_ridge$bestTune$lambda)
```

```{r 'B28-TrainModel-Plot', include=FALSE}
# #Plot Lambda
hh <- mod_ridge
#plot(hh)
#
cap_hh <- "B28P04"
ttl_hh <- "Boston: Ridge Lambda vs RMSE"
#
B28 <- hh %>% { ggplot(., highlight = TRUE) + 
    scale_x_continuous(breaks = breaks_pretty()) + 
	  labs(caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B28)
rm(B28)
```

```{r 'B28P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B28P04", "-Boston-Ridge-Lambda", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B28P04, device = "png", dpi = 144) 
}
```

```{r 'B28P04', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B28P04", "-Boston-Ridge-Lambda", ".png"))
```


```{r 'B28P05-Save', include=FALSE}
# #Plot Coefficients
hh <- mod_ridge$finalModel
#
cap_hh <- "B28P05"
ttl_hh <- "Boston: Ridge Trace Plot of Final Model"
loc_png <- paste0(.z$PX, "B28P05", "-Boston-Ridge-Model", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plot(hh, xvar = "lambda", label = TRUE, ylim = c(-5, 5))
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B28 <- recordPlot()
  dev.off()
  assign(cap_hh, B28)
  rm(B28)
}
```

```{r 'B28P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B28P05", "-Boston-Ridge-Model", ".png"))
```


```{r 'B28P0405', echo=FALSE, ref.label=c('B28P04', 'B28P05'), fig.cap="(B28P04 B28P05) Boston Ridge Lambda and Trace Plot of Final Model"}
# 
```


```{r 'B28-VarImportance-Plot', include=FALSE}
# #Variable Importance Plot
hh <- mod_ridge
#plot(varImp(hh, scale = TRUE))
#
cap_hh <- "B28P06"
ttl_hh <- "Boston: Ridge Variable Importance Plot"
#
B28 <- hh %>% { ggplot(varImp(., scale = TRUE)) + 
	  labs(caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B28)
rm(B28)
```

```{r 'B28P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B28P06", "-Boston-Ridge-Vars", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B28P06, device = "png", dpi = 144) 
}
```

```{r 'B28P06', echo=FALSE, fig.cap="(B28P06) Boston: Ridge Variable Importance Plot"}
knitr::include_graphics(paste0(.z$PX, "B28P06", "-Boston-Ridge-Vars", ".png"))
```


### Code Lambda {.unlisted .unnumbered}


```{r 'B28-TrainModel-Plot-A', eval=FALSE, ref.label=c('B28-TrainModel-Plot')}
#
```

### Code Trace Plot {.unlisted .unnumbered}

```{r 'B28P05-Save-A', eval=FALSE, ref.label=c('B28P05-Save')}
#
```

### Code Variable Importance {.unlisted .unnumbered}

```{r 'B28-VarImportance-Plot-A', eval=FALSE, ref.label=c('B28-VarImportance-Plot')}
#
```

## To be continued ...

- Discussion continued in next lecture on Lasso and Elastic Net Regression


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B28-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, ff, B28P04, B28P06, BostonHousing, cap_hh, corr_hh, custom, 
          dum_xsyw, hh, idx_xsyw, loc_png, mod_cv, mod_ridge, mod_xsyw, stp_xsyw, test_xsyw, 
          train_xsyw, ttl_hh, vif_xsyw, xsyw, xw, zw)
```

```{r 'B28-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/228-RegressionL.Rmd-->

# Linear Regression (B29, Jan-23) {#b29}

```{r 'B29', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```


## Overview

- "Ridge, Lasso and Elastic Net regressions"


## Review {.tabset .tabset-fade}

- Discussion continued from previous lecture from Ridge Regression onwards on the [Data: Boston Housing](#set-boston-b28 "b28")

### Regularised Regression {.unlisted .unnumbered}

- 3 Approaches of Regularised Regression
  - Ridge Regression $(L_2) : \lambda_R \displaystyle \sum_{j=1}^{p} \beta_j^2$
  - Lasso Regression $(L_1) : \lambda_L \displaystyle \sum_{j=1}^{p} |\beta_j|$
    - LASSO i.e. Least Absolute Shrinkage and Selection Operator
  - Elastic Net Regression $: \lambda_E \displaystyle \sum_{j=1}^{p} \left ( \left ( 1 - \alpha \right ) \beta_j^2 + \alpha |\beta_j| \right )$
    - $\alpha = 0 : \lambda_E \to \lambda_L$
    - $\alpha = 1 : \lambda_E \to \lambda_R$
    - The Elastic Net selects variables like the Lasso, and shrinks together the coefficients of correlated predictors like Ridge.
    - "ForLater" Book shows the equation with multipliers interchanged 
      - Overall exaplanation would not change. Only the effect of ${\alpha}$ being 0 and 1 would be reversed.
  - $\{\alpha, \lambda\}$ are sometimes called \textcolor{pink}{hyperparameters}

- train() with method = "glmnet" is used for all 3
  - Ridge $(\alpha = 0)$, Lasso $(\alpha = 1)$, Elastic Net $(\alpha \in \{0, 1\})$

### Previous Lecture Code {.unlisted .unnumbered}

```{r 'B29-GetBoston', include=FALSE, ref.label=c('B28-Boston', 'B28-BostonPart', 'B28-BostonLm', 'B28-TrainControl', 'B28-Ridge')}
#
```


```{r 'B29-GetBostonKnit', eval=FALSE}
data("BostonHousing")
xsyw <- bb <- aa <- as_tibble(BostonHousing)
xw <- xsyw %>% select(-medv)
zw <- xw %>% mutate(across(where(is.numeric), ~ as.vector(scale(.)))) %>% 
  mutate(across(chas, strtoi))
#
# #Partition Data
set.seed(3)
dum_xsyw <- xsyw
idx_xsyw <- sample.int(n = nrow(dum_xsyw), size = floor(0.8 * nrow(dum_xsyw)), replace = FALSE)
train_xsyw <- dum_xsyw[idx_xsyw, ] 
test_xsyw  <- dum_xsyw[-idx_xsyw, ]
#
# #Linear Regression without any Cross-validation
mod_xsyw <- lm(medv ~ ., data = train_xsyw)
stp_xsyw <- step(mod_xsyw, direction = "backward", trace = 0)
#
set.seed(3)
# #trainControl() To Set Custom Control Parameters for k=10, iteration = 5
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
#
# #train() can do the crossvalidation
mod_cv <- train(medv ~ ., train_xsyw, method = "lm", trControl = custom)
#
# #Ridge Regression i.e. alpha = 0, lambda = (0, Inf)
set.seed(3)
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
mod_ridge <- train(medv ~ ., train_xsyw, method = "glmnet", trControl = custom,
    tuneGrid = expand.grid(alpha = 0, 
                           lambda = seq(from = 0.1, to = 1, length.out = 10)))
```

## Lasso Regression (L1)

```{r 'B29-Lasso'}
# #Lasso Regression i.e. alpha = 1, lambda = (0, Inf)
set.seed(3)
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
mod_lasso <- train(medv ~ ., train_xsyw, method = "glmnet", trControl = custom,
    tuneGrid = expand.grid(alpha = 1, 
                           lambda = seq(from = 0.001, to = 0.01, length.out = 10)))
#
# #Sample Subset of Results
mod_lasso$results %>% as_tibble() %>% slice_sample(n = 5)
#
# #Best Tuned Model
mod_lasso$results %>% as_tibble() %>% filter(lambda == mod_lasso$bestTune$lambda)
```


```{r 'B29-TrainLasso-Plot', include=FALSE}
# #Plot Lambda
hh <- mod_lasso
#plot(hh)
#
cap_hh <- "B29P04"
ttl_hh <- "Boston: Lasso Lambda vs RMSE"
#
B29 <- hh %>% { ggplot(., highlight = TRUE) + 
    scale_x_continuous(breaks = breaks_pretty()) + 
	  labs(caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B29)
rm(B29)
```

```{r 'B29P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B29P04", "-Boston-Lasso-Lambda", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B29P04, device = "png", dpi = 144) 
}
```

```{r 'B29P04', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B29P04", "-Boston-Lasso-Lambda", ".png"))
```

```{r 'B29-VarImportance-Lasso', include=FALSE}
# #Variable Importance Plot
hh <- mod_lasso
#plot(varImp(hh, scale = TRUE))
#
cap_hh <- "B29P05"
ttl_hh <- "Boston: Lasso Variable Importance Plot"
#
B29 <- hh %>% { ggplot(varImp(., scale = TRUE)) + 
	  labs(caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B29)
rm(B29)
```

```{r 'B29P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B29P05", "-Boston-Lasso-Vars", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B29P05, device = "png", dpi = 144) 
}
```

```{r 'B29P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B29P05", "-Boston-Lasso-Vars", ".png"))
```


```{r 'B29P03-Save', include=FALSE}
# #Plot Coefficients
hh <- mod_lasso$finalModel
#
cap_hh <- "B29P03"
ttl_hh <- "Boston: Lasso Trace Plot of Final Model"
loc_png <- paste0(.z$PX, "B29P03", "-Boston-Lasso-Model", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plot(hh, xvar = "lambda", label = TRUE, ylim = c(-5, 5))
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B29 <- recordPlot()
  dev.off()
  assign(cap_hh, B29)
  rm(B29)
}
```

```{r 'B29P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B29P03", "-Boston-Lasso-Model", ".png"))
```

## Elastic Net

```{r 'B29-Elastic'}
# #Elastic Net Regression i.e. alpha = [0, 1], lambda = (0, Inf)
set.seed(3)
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
mod_enet <- train(medv ~ ., train_xsyw, method = "glmnet", trControl = custom,
    tuneGrid = expand.grid(alpha = seq(from = 0.001, to = 0.01, length.out = 10), 
                           lambda = seq(from = 0.02, to = 0.2, length.out = 10)))
#
# #Sample Subset of Results
mod_enet$results %>% as_tibble() %>% slice_sample(n = 5)
#
# #Best Tuned Model
mod_enet$results %>% as_tibble() %>% 
  filter(alpha == mod_enet$bestTune$alpha & lambda == mod_enet$bestTune$lambda)
```


```{r 'B29-TrainEnet-Plot', include=FALSE}
# #Plot Lambda
hh <- mod_enet
#plot(hh)
#
cap_hh <- "B29P06"
ttl_hh <- "Boston: Elastic Net Lambda vs RMSE"
#
B29 <- hh %>% { ggplot(., highlight = TRUE) + 
    scale_x_continuous(breaks = breaks_pretty()) + 
	  labs(caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B29)
rm(B29)
```

```{r 'B29P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B29P06", "-Boston-Elastic-Lambda", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B29P06, device = "png", dpi = 144) 
}
```

```{r 'B29P06', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B29P06", "-Boston-Elastic-Lambda", ".png"))
```

```{r 'B29-VarImportance-Enet', include=FALSE}
# #Variable Importance Plot
hh <- mod_enet
#plot(varImp(hh, scale = TRUE))
#
cap_hh <- "B29P07"
ttl_hh <- "Boston: Elastic Net Importance Plot"
#
B29 <- hh %>% { ggplot(varImp(., scale = TRUE)) + 
	  labs(caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B29)
rm(B29)
```

```{r 'B29P07-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B29P07", "-Boston-Elastic-Vars", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B29P07, device = "png", dpi = 144) 
}
```

```{r 'B29P07', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B29P07", "-Boston-Elastic-Vars", ".png"))
```


```{r 'B29P08-Save', include=FALSE}
# #Plot Coefficients
hh <- mod_enet$finalModel
#
cap_hh <- "B29P08"
ttl_hh <- "Boston: Elastic Net Trace Plot of Final Model"
loc_png <- paste0(.z$PX, "B29P08", "-Boston-Elastic-Model", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plot(hh, xvar = "lambda", label = TRUE, ylim = c(-5, 5))
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B29 <- recordPlot()
  dev.off()
  assign(cap_hh, B29)
  rm(B29)
}
```

```{r 'B29P08', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B29P08", "-Boston-Elastic-Model", ".png")) #iiii
```


## Variable Importance

```{r 'B29P05-A', echo=FALSE, ref.label=c('B28P06', 'B29P05'), fig.cap="(B28P06, B29P05) Boston: Variable Importance Plot of Ridge vs Lasso"}
#
```


- \textcolor{pink}{Question:} Now we have identified some variables as having low influence (importance) on model. Should we exclude these variables and run the algorithm again
  - Yes
- \textcolor{pink}{Question:} And if we remove the variables with low influence, would the collinearity be reduced
  - If the collinearity is a severe issue, the penalty parameters would suppress that accordignly
  - Ridge is generally treated as a model for shrinkage only
  - Lasso is generally treated as a model for shrinkage and deletion (of low importance)
    - So Lasso helps in feature selection

- \textcolor{pink}{Question:} Did the Lasso helped us here in feature selection
  - If we use Lasso for something then the variables with low importance would not influence the model
  - "ForLater" (Aside) It does not look like that Lasso did any feature selection compared to Ridge

- \textcolor{pink}{Question:} Does it mean that by using Lasso, we can remove the independent variables with low influence /importance and then rerun the analysis to create a model
  - No, we do not need to rerun the analysis. We have the model which is not being affected by those variables. 

- \textcolor{pink}{Question:} But, in Lasso, example variable 'b' does appear in the graph but it is not included in model
  - Effectively it would not be considered by the Model 

- \textcolor{pink}{Question:} If we do not remove the variables having low influence, then what is the use of identifying those variables
  - In stepwise regression, the variables which do not have any influence on the Model are dropped.
  - Similarly, in Lasso, effect of those variables would not be present when we apply this model for prediction.
  
- \textcolor{pink}{Question:} Can we do processing like run Lasso, findout that 'b' is not important, drop the 'b' from original dataset and then run either ridge or linear regression
  - Yes

- \textcolor{pink}{Question:} The dependent variable, medv i.e.	'median value of owner-occupied homes in USD 1000', has median value of 21.2. The model has RMSE of 4.7 which is 22% of the median value of the dependent variable. Is this model a good model
  - We have applied Linear Regression, Ridge, Lasso, Elastic Net algorthms on the dataset. Later we will apply Decision Trees, Random forest etc. Then we will select one of the model.

- \textcolor{pink}{Question:} Is there any benchmark for good model e.g. 90% Y explained etc.
  - No, generally we compare across algorithms for a particular loss function
  - Then we select the algorithm /model which provides the lowest loss function
  

## Coefficients

```{r 'B29P03-A', echo=FALSE, ref.label=c('B28P05', 'B29P03'), fig.cap="(B28P05, B29P03) Boston: Trace Plot of Ridge vs Lasso"}
#
```


- As the $\log(\lambda)$ increases, penalisation increases
  - It shows the effect of penalisation on all variables
    - Highly Negative: 5 (nox)
    - Highly Positive: 6 (rm), 4 (chas)


- \textcolor{pink}{Question:} In the image, 2nd x-axis at the Top shows consistent 13 across 
  - There are 13 independent variables. In Ridge, none of the variables are eliminated. In Lasso, situation would be different
  
- \textcolor{pink}{Question:} "ForLater" These 3 are the variables which are at the top of the list of importance. What is the explanation 

  
- \textcolor{pink}{Question:} "ForLater" If the plot shows $\log(\lambda)$ then why it is showing x-axis labels as 0-8 because professor supplied 5 values of lambda and all were less than 1. I supplied 10 values of lambda and all were less than 1. Log should be negative for all values less than 1.
  - These might be having extrapolated $\log(\lambda)$
 

```{r 'B29-CoefficientsCompare'}
# #Coefficients
# #Check whether the number of coefficients are same between the two models
ii <- names(mod_cv$finalModel$coefficients)
jj <- rownames(coef(mod_ridge$finalModel, s = mod_ridge$bestTune$lambda))
stopifnot(identical(ii, jj))
#
# #Get Coefficients of CV lm model
bb <- mod_cv
ii <- bb$finalModel$coefficients
#
# #Get Coefficients of Best Ridge Model | Sparse Matrix | Matrix | Tibble
bb <- mod_ridge
jj <- coef(bb$finalModel, s = bb$bestTune$lambda) %>% 
  as.matrix() %>% as_tibble(rownames = "DummyParVsRef") %>% rename(mod_ridge = s1)
#
# #Merge Named Vector into the Tibble by matching names to a column
jj$mod_cv <- ii[jj$DummyParVsRef]
#
# #Get Coefficients of Best Lasso Model
bb <- mod_lasso
kk <- coef(bb$finalModel, s = bb$bestTune$lambda) %>% 
  as.matrix() %>% as_tibble(rownames = "DummyParVsRef") %>% rename(mod_lasso = s1)
ll <- full_join(jj, kk, by = "DummyParVsRef")
#
# #Get Coefficients of Best Elastic Net Model
bb <- mod_enet
mm <- coef(bb$finalModel, s = bb$bestTune$lambda) %>% 
  as.matrix() %>% as_tibble(rownames = "DummyParVsRef") %>% rename(mod_enet = s1)
nn <- full_join(ll, mm, by = "DummyParVsRef")
#
coef_mods <- nn
```


## Comparison of Models

### Coefficients

```{r 'B29T01', echo=FALSE}
# #Print Kable Table
hh <- coef_mods %>% 
  mutate(across(where(is.numeric), round, digits = 4)) %>% 
  mutate(across(where(is.numeric), format, digits = 3, drop0trailing = TRUE)) 
#
cap_hh <- paste0("(B29T01)", "Boston: Coefficients of CV, Ridge, Lasso, Elastic Net") 
f_pKbl(x = hh, caption = cap_hh, debug = FALSE)
```



### Results

```{r 'B29-ModelResults'}
#mod_xsyw
#stp_xsyw
mod_cv$results
#
mod_ridge$results %>% as_tibble() %>% filter(lambda == mod_ridge$bestTune$lambda)
#
mod_lasso$results %>% as_tibble() %>% filter(lambda == mod_lasso$bestTune$lambda)
#
mod_enet$results %>% as_tibble() %>% 
  filter(alpha == mod_enet$bestTune$alpha & lambda == mod_enet$bestTune$lambda)
```

### Resample

```{r 'B29-Resample'}
# #Resample the Models for Comparison
mods <- list(mod_cv = mod_cv, mod_ridge = mod_ridge, mod_lasso = mod_lasso, mod_enet = mod_enet)
#
# #Resample all Models with common dataset (subset of original data)
# #Number of Resamples = 50 i.e. 50 sets will be created with 'random sampling with replacement'
res_mods <- resamples(mods)
summary(res_mods)
```

### Tuning Parameters

```{r 'B29-BestTune'}
mod_cv$bestTune
mod_ridge$bestTune
mod_lasso$bestTune
mod_enet$bestTune
```

### The Best

```{r 'B26-BestModel'}
# #Examine Coefficients
coef(mod_enet$finalModel, s = mod_enet$bestTune$lambda)
```


```{r 'B29P0608', echo=FALSE, ref.label=c('B29P06', 'B29P08'), fig.cap="(B29P06 B29P08) Boston Elastic Net Lambda and Trace Plot of Final Model"}
#
```

### Save Model

```{r 'B29-SaveModel'}
# #Save Model as Rds File
if(FALSE) saveRDS(mod_enet, "xxB29-mod-enet.rds")
if(FALSE) mod_best <- readRDS("xxB29-mod-enet.rds")
```

## Predict

```{r 'B29-Predict'}
# #Apply Same Transformation on Test Data (except the Actual Y)
#
res_enet <- test_xsyw %>% 
  mutate(CalY = predict(mod_enet, .), Y_Yc = medv - CalY)
#
summary(res_enet$Y_Yc)
#
# #RMSE: Root Mean Squared Error
sqrt(mean((res_enet$Y_Yc)^2))
#
# #MAE: Mean Absolute Error (MAE)
mean(abs(res_enet$Y_Yc))
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B29-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, ff, B29P04, B29P05, B29P06, B29P07, BostonHousing, cap_hh, 
          coef_mods, custom, dum_xsyw, hh, idx_xsyw, loc_png, mm, mod_cv, mod_enet, mod_lasso, 
          mod_ridge, mod_xsyw, mods, nn, res_enet, res_mods, stp_xsyw, test_xsyw, train_xsyw, 
          ttl_hh, xsyw, xw, zw, vif_xsyw)
```

```{r 'B29-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/229-RegressionL.Rmd-->

# Linear Regression (B30, Jan-30) {#b30}

```{r 'B30', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Decision Tree Algorithm"

## Data: KC House {#set-kc-b30 .tabset .tabset-fade}

\textcolor{pink}{Please import the "B30-KC-House.csv"}

```{r 'B30-KC', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum
#tools::md5sum(paste0(.z$XL, "B30-KC-House.csv"))
xxB30KC <- f_getObject("xxB30KC", "B30-KC-House.csv", "13e2be1e90780e7331b6a24ad950799d")
```

```{r 'B26-GetKC', include=FALSE}
# #Load Data: KC House
xxB30KC <- f_getRDS(xxB30KC)
```

### EDA

- About: [21613, 21]
  - Source: https://www.kaggle.com/harlfoxem/housesalesprediction
  - Description
    1. id: A notation for a house
    1. date: Date house was sold
    1. price: Price is prediction target
    1. bedrooms: Number of bedrooms
    1. bathrooms: Number of bathrooms
    1. sqft_living: Square footage of the home
    1. sqft_lot: Square footage of the lot
    1. floors: Total floors (levels) in house
    1. waterfront: House which has a view to a waterfront
    1. view: Has been viewed
    1. condition: How good the condition is overall
    1. grade: overall grade given to the housing unit, based on King County grading system
    1. sqft_above: Square footage of house apart from basement
    1. sqft_basement: Square footage of the basement
    1. yr_built: Built Year
    1. yr_renovated: Year when house was renovated
    1. zipcode: Zip code
    1. lat: Latitude coordinate
    1. long: Longitude coordinate
    1. sqft_living15: Living room area in 2015 (implies some renovations) This might or might not have     affected the lotsize area
    1. sqft_lot15: LotSize area in 2015 (implies some renovations)
  - Date 
    - Depending upon the import method, date might be character and need conversion
  - Integer to Categorical conversion is needed
  - yr_renovated needs to be handled 0 means no renovation - We can convert to Factor of Yes/No
  - sqft_basement - Similarly Yes/No
  - There are big area houses without any bedroom or bathroom
  - Renovated House is NOT a new house.
  - Calculate Age = Date of Sales - Year Built
    - There are 8 houses with negative age i.e. sold first completed later
    - There are 430 houses with 0 age i.e. sold in the year it was built
    - These can happen anyway

- Rows 21613
  - sqft_above: NA: -2 
  - Beds: 33: -1 : 
    - ~~No point in predicting price of a castle~~
    - With Price at 640000, and single floor with low sqft, it might be an Error for 3 beds
- Columns
  - Dropped sqft_living because of Multicollinearity issue with sqft_above. Together the sqft_above and sqft_basement take care of sqft_living
  - Dropped sqft_living15 and sqft_lot15 

- \textcolor{pink}{Question:} Property price are affected by location. Why are we removing lat/long
  - We are already including different types like waterfront etc.
  - It would have been better if we have rural, urban, city centre, market type categories 
  - (Aside) We should identify clusters of zipcodes, some of them are clearly different from others

- \textcolor{pink}{Question:} Based on description of sqft_living15, would this not cause Multicollinearity issue
  - We will check "ForLater"

- \textcolor{pink}{Question:} Average Price over zipcode has clear distinctions
  - We will check "ForLater"
  - (Aside) We should identify clusters of zipcodes, some of them are clearly different from others

- \textcolor{pink}{Question:} Why the age is not taken as Today
  - Price is of the date it was sold. Our analysis date does not change the price.

```{r 'B30-PrepKC'}
aa <- xxB30KC
# #Drop NA | Get Age | Dummy | Rename | Filter | Factor | Relevel Most Frequent Reference | 
# #Drop | Relocate | 
bb <- aa %>% 
  drop_na() %>% 
  #mutate(across(date, as_date)) %>% 
  mutate(sold = year(date), age = sold - yr_built) %>% 
  mutate(iRenew = ifelse(yr_renovated == 0, 0, 1)) %>% 
  rename(beds = bedrooms, baths = bathrooms, sqAbove = sqft_above, sqLot = sqft_lot, 
         iWater = waterfront) %>% 
  filter(beds != 33) %>% 
  mutate(across(c(beds, baths, floors, iWater, condition, grade, iRenew), factor)) %>% 
  mutate(across(beds, relevel, ref = "3")) %>% 
  mutate(across(baths, relevel, ref = "2")) %>% 
  mutate(across(floors, relevel, ref = "1")) %>% 
  mutate(across(condition, relevel, ref = "3")) %>% 
  mutate(across(grade, relevel, ref = "7")) %>% 
  select(-c(id, view, zipcode, lat, long, date, sold, yr_renovated, yr_built, 
            sqft_living, sqft_living15, sqft_lot15)) %>% 
  relocate(price)
#
xfw <- bb
zfw <- xfw %>% mutate(across(where(is.numeric), ~ as.vector(scale(.)))) 
xnw <- xfw %>% select(where(is.numeric))
znw <- zfw %>% select(where(is.numeric))
f_wl(znw)
```

### Structure {.unlisted .unnumbered}

```{r 'B30-Temp-KC', include=FALSE, eval=FALSE}
# #Count NA 
if(FALSE) colSums(is.na(aa)) %>% as_tibble(rownames = "Cols") %>% filter(value > 0)
#
if(FALSE) bb %>% select(isFemale) %>% slice(1:10)
if(FALSE) cat('"', paste0(levels(bb$Arrival), collapse = '", "'), '"\n', sep = '')
if(FALSE) bb %>% filter(is.na(bpl)) %>% select(id, bph, bpl)
if(FALSE) bb %>% select(isImplant, imp_c) %>% summary()
if(FALSE) bb %>% select(iFemale, iMarried, iEmergency, iImplant) %>% 
  pivot_longer(everything()) %>% 
  count(name, value) %>% 
  pivot_wider(names_from = value, values_from = n) 
# #Count Unique of all Columns to decide which should be Factors
if(FALSE) bb %>% summarise(across(everything(), ~ length(unique(.)))) %>% pivot_longer(everything())
# #Summary of Columns of a class: is.factor is.numeric is.character
if(FALSE) bb %>% select(where(is.factor)) %>% summary()
# #Names and Indices of Columns of class: is.factor is.numeric is.character
if(FALSE) which(sapply(bb, is.factor))
# #Comma separated string having each item within quotes for easy pasting as character not objects
if(FALSE) cat('"', paste0(names(which(sapply(bb, is.factor))), collapse = '", "'), '"\n', sep = '')
# #Levels of Factor Columns
if(FALSE) lapply(bb[c(3, 6:9, 15)], levels)
# #Frequency of Each level of Factor
if(FALSE) bb %>% count(baths) %>% arrange(desc(n))
```

```{r 'B30-Str-KC'}
str(bb)
```

### Summary {.unlisted .unnumbered}

```{r 'B30-Summary-KC'}
summary(bb)
```

### 33 Beds {.unlisted .unnumbered}

```{r 'B30-Beds'}
# #With Price at 640000, and single floor with low sqft, it might be an Error for 3 beds
aa %>% filter(bedrooms == 33 | price > 5000000) %>% 
  mutate(across(price, num, notation = "dec"))
```

### zipcode {.unlisted .unnumbered}

```{r 'B30-zipcode'}
# #zipcode might be converted into clusters
# #Some of these have clearly higher than average prices
aa %>% filter(zipcode == 98112) %>% mutate(across(price, num, notation = "eng"))
#
aa %>% filter(zipcode == 98039) %>% mutate(across(price, num, notation = "eng"))
```

### relevel() {.unlisted .unnumbered}

```{r 'B30-Relevel'}
ii <- aa %>% select(beds = bedrooms) %>% mutate(across(beds, factor))
levels(ii$beds)
#
# #Relevel 3rd Index as Reference
jj <- ii %>% mutate(across(beds, relevel, ref = 3))
levels(jj$beds)
#
# #Relevel Value 3 as Reference
kk <- ii %>% mutate(across(beds, relevel, ref = "3"))
levels(kk$beds)
```


## SPLOM

- Note: Scatterplot Matrix (SPLOM) of all variables was not plotted because it takes long time and does not provide good actionable insight. Notes of earlier lectures have the code to generate them. - "ForLater" add link


## Correlation {.tabset .tabset-fade}

- \textcolor{pink}{Question:} The dependent variable (price) is skewed towards left but some independent variables (e.g. sqft_living) also are left skewed. Just like we are focusing on skewness of Y, should we also be concerned about X
  - Skewness of Y is more important. X can have skewness.

- \textcolor{pink}{Question:} Price and Age have correlation value of -0.05 and is significant. This is really small. Should this not show stronger correlation
  - Age might not come out as stronger predictor. 
  - Sometimes a variable might show a strong influence with the help of another independent variable
  
- \textcolor{pink}{Question:} If the correlation between Bedrooms and Price is 0.3 and is significant, then 30% of price can be explained by bedrooms
 - When we are looking for corrleation between two variables, it does not consider presence or absence of other variables. However, regression consider that.
  - Correlation tells us if the variables are related and what type of relationship that is. Correlation is only about the association. It does not talk about whether the X leads to or explains Y.
  - Correlation is bidirectional i.e. $Y \Leftrightarrow X$, whereas regression is unidirectional i.e. $Y \Leftarrow X$
 - (Aside) No, Bedroom does not explain 30% of price. Sum of all correlations to Y is not 100%.


```{conjecture 'cor-numeric'}
\textcolor{brown}{Error in cor(...) : 'x' must be numeric}
```

- cor() can take only the numeric columns, remove any factor or character
  
### Image {.unlisted .unnumbered}

```{r 'B30-SetCorr-KC', include=FALSE}
# #Setup for Corrplot
ii <- znw
hh <- cor(ii)
corr_hh <- cor.mtest(ii)
# #p-value Higher than this is insignificant and should be skipped
sig_corr_hh <- 0.05 
#
cap_hh <- "B30P01"
ttl_hh <- "KC: Corrplot (Scaled)"
loc_png <- paste0(.z$PX, "B30P01", "-KC-Corrplot-z", ".png")
```

```{r 'B30P01-Save', include=FALSE, ref.label=c('B30-Corrplot')}
#
```

```{r 'B30P01', echo=FALSE, fig.cap="(B30P01) KC: Corrplot (Scaled)"}
knitr::include_graphics(paste0(.z$PX, "B30P01", "-KC-Corrplot-z", ".png"))
```

### Code {.unlisted .unnumbered}

```{r 'B30-SetCorr-KC-A', eval=FALSE, ref.label=c('B30-SetCorr-KC')}
#
```

```{r 'B30-Corrplot', eval=FALSE}
# #IN: hh, corr_hh, sig_corr_hh
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  corrplot(hh, method = "circle", type = "lower", diag = FALSE, col = COL2('BrBG', 200),
                   cl.pos = 'r', tl.pos = 'ld', addCoef.col = "black", tl.col = "black",
                   p.mat = corr_hh$p, sig.level = sig_corr_hh, insig = 'blank', 
        #order = "hclust", hclust.method = "ward.D", addrect = 2, rect.col = 3, rect.lwd = 3, 
                   title = NULL #, col = RColorBrewer::brewer.pal(3, "BrBG")
				   )
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B30 <- recordPlot()
  dev.off()
  assign(cap_hh, B30)
  rm(B30)
}
```


## Boxplot {.tabset .tabset-fade}

### Image {.unlisted .unnumbered}

```{r 'B30-SetBox-KC', include=FALSE}
hh <- znl
#levels(hh$Keys) <- names(znw)
#
ttl_hh <- "KC: Boxplot (Scaled)"
cap_hh <- "B30P02"
sub_hh <- NULL 
lgd_hh  <- NULL
```

```{r 'B30-KCBox', include=FALSE, ref.label=c('B30-ScaleBox')}
#
```

```{r 'B30P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B30P02", "-KC-Box-Z", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B30P02, device = "png", dpi = 144) 
}
```

```{r 'B30P02', echo=FALSE, fig.cap="(B30P02) KC: Boxplot (Scaled)"}
knitr::include_graphics(paste0(.z$PX, "B30P02", "-KC-Box-Z", ".png"))
```

### Code {.unlisted .unnumbered}

```{r 'B30-SetBox-KC-A', eval=FALSE, ref.label=c('B30-SetBox-KC')}
#
```

```{r 'B30-ScaleBox', eval=FALSE}
# #IN: hh(Keys, Values) 
B30 <- hh %>% { ggplot(data = ., mapping = aes(x = Keys, y = Values, fill = Keys)) +
    geom_boxplot() +
    k_gglayer_box +
    scale_y_continuous(breaks = breaks_pretty()) + 
    coord_flip() +
    theme(legend.position = 'none') +
    labs(x = NULL, y = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B30)
rm(B30)
```


## Outliers

- Multiple criteria to identify Outliers
  - Scaling 406
  - IQR 406 + 733
  - MAD 406 + 733 + 32
- Note: None of the identified outliers are actually deleted. All have been kept for processing.
  - I am interested in looking at the RMSE of the model before deleting any outlier

- \textcolor{pink}{Question:} Instead of deleting, can we separate the outliers and treat them as a different dataset. This will give us two models for the two distributions
  - Yes, we can do that. Anyway removal of more than 5% datapoints is not recommended.

```{r 'B30-Outliers'}
# # Select Price | Rename | Get MAD | Outliers by Z, IQR, MAD | Count |
xnw %>% select(Y = price) %>% 
  mutate(MADY = (Y - median(Y))/mad(Y)) %>% 
  mutate(isOut_Z = ifelse(abs(scale(Y)) > 3, TRUE, FALSE), 
         isOut_MAD = ifelse(abs(MADY) > 3, TRUE, FALSE),
         isOut_IQR = ifelse(Y < {quantile(Y)[2] - 1.5 * IQR(Y)} | 
                            Y > {quantile(Y)[4] + 1.5 * IQR(Y)}, TRUE, FALSE)) %>% 
  count(isOut_Z, isOut_IQR, isOut_MAD)
#
# #NOT DONE: But To Show How to Remove anythind beyond 2 SD of Price
if(FALSE) ii <- xnw %>% filter(!(abs(price - median(price)) > 2 * sd(price)))
#
# #NOT DONE: But To Show How to Remove anythind beyond 2.5 Scaled of Price
if(FALSE) ii <- znw %>% filter(between(price, -2.5, +2.5))
```

## Partition Data 

```{r 'B30-Partition-KC'}
# #Partition Data
set.seed(3)
# #Create Dummies | Replaced All NA in original data so no replacement in dummy columns |
dum_xfw <- dummy_cols(xfw, remove_first_dummy = TRUE, remove_selected_columns = TRUE) 
idx_xsyw <- sample.int(n = nrow(dum_xfw), size = floor(0.8 * nrow(dum_xfw)), replace = FALSE)
train_xfw <- dum_xfw[idx_xsyw, ] 
test_xfw  <- dum_xfw[-idx_xsyw, ]
```

## Model

```{r 'B30-Model-KC'}
# #Linear Regression
mod_xfw <- lm(price ~ ., data = train_xfw)
if(FALSE) f_pNum(summary(mod_xfw)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
#names(mod_xfw$coefficients)
```

## Stepwise Model

- Stepwise Regression did not remove multicollinearity 

```{r 'B30-StepModel-KC'}
# #Stepwise Regression #"both", "backward", "forward"
stp_xfw <- step(mod_xfw, direction = "backward", trace = 0)
if(TRUE) f_pNum(summary(stp_xfw)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
summary(stp_xfw)$adj.r.squared
names(stp_xfw$coefficients)
```

## VIF

- \textcolor{pink}{Question:} If we keep the outliers and get $R_a^2 \approx 72$ and after removing the outliers we get $R_a^2 \approx 58$, which model would be considered better
  - Do not compare $R_a^2$ built on different number of observations. $R_a^2$ is to compare models built on same dataset but with different number of independent variables


- Multicollinearity is an issue that needs to be handled 

```{r 'B30-VIF-KC'}
ii <- vif(stp_xfw) 
ii[ii > 2]
```


## Normality

```{conjecture 'shapiro-5000'}
\textcolor{brown}{Error in shapiro.test(...) : sample size must be between 3 and 5000}
```

- 5000 is a safety limit on Shapiro. Randomly choose a sample of 5000 observations for testing

```{r 'B30-Normal-KC'}
# #Normality Test
set.seed(3)
test_shapiro <- shapiro.test(sample(x = stp_xfw$residuals, size = 5000))
test_shapiro
#
if(test_shapiro$p.value > 0.05) cat("Normal. H0.\n") else cat("Not Normal. Ha.\n")
```

## Semi Log

```{r 'B30-SemiLog-KC'}
# #Run Log Transformed Y
mod_xfw_log <- lm(log(price) ~ ., data = train_xfw)
stp_xfw_log <- step(mod_xfw_log, direction = "backward", trace = 0)
if(FALSE) f_pNum(summary(stp_xfw_log)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
vif_ii <- vif(stp_xfw_log)
vif_ii[vif_ii > 5]
#
summary(stp_xfw_log)$adj.r.squared
names(stp_xfw_log$coefficients)
#
# #Normality Test
set.seed(3)
test_shapiro <- shapiro.test(sample(x = stp_xfw_log$residuals, size = 5000))
test_shapiro
#
if(test_shapiro$p.value > 0.05) cat("Normal. H0.\n") else cat("Not Normal. Ha.\n")
```

## Log-Log

```{r 'B30-LogLog-KC'}
# #Run Log Transformed Y and X
train_log <- train_xfw %>% 
  mutate(across(c(age), ~ . + 2)) %>% 
  mutate(across(c(sqft_basement), ~ . + 1)) %>% 
  mutate(across(c(price, sqLot, sqAbove, sqft_basement, age), log)) 
#
mod_xfw_lll <- lm(price ~ ., data = train_log)
stp_xfw_lll <- step(mod_xfw_lll, direction = "backward", trace = 0)
if(FALSE) f_pNum(summary(stp_xfw_lll)$coefficients) %>% as_tibble(rownames = "DummyParVsRef") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
vif_ii <- vif(stp_xfw_lll)
vif_ii[vif_ii > 5]
#
summary(stp_xfw_lll)$adj.r.squared
names(stp_xfw_lll$coefficients)
#
# #Normality Test
set.seed(3)
test_shapiro <- shapiro.test(sample(x = stp_xfw_lll$residuals, size = 5000))
test_shapiro
#
if(test_shapiro$p.value > 0.05) cat("Normal. H0.\n") else cat("Not Normal. Ha.\n")
```

## Glance

```{r 'B30-Glance'}
lapply(list(stp_xfw_lll, stp_xfw_log, stp_xfw, mod_xfw), glance)
```

## Predict

- Subtract Y from 'exp(predicted)' to get the RMSE etc. in case of log transformed Y.

```{r 'B30-Predict-KC'}
# #Apply Same Transformation on Test Data (except the Actual Y)
test_log <- test_xfw %>% 
  mutate(across(c(age), ~ . + 2)) %>% 
  mutate(across(c(sqft_basement), ~ . + 1)) %>% 
  mutate(across(c(sqLot, sqAbove, sqft_basement, age), log)) 
#
res_lll <- test_log %>% 
  mutate(CalYlog = predict(stp_xfw_lll, .), Y_Yc = price - exp(CalYlog), MAPE = Y_Yc / price)
#
summary(res_lll$Y_Yc)
#
# #RMSE: Root Mean Squared Error
#res_w %>% summarise(across(everything(), ~ sqrt(mean((.)^2))))
sqrt(mean((res_lll$Y_Yc)^2))

#
# #MAE: Mean Absolute Error (MAE)
#res_w %>% summarise(across(everything(), ~ mean(abs(.))))
mean(abs(res_lll$Y_Yc))
#
# #MAPE: Mean Absolute Percentage Error
mean(abs(res_lll$MAPE))
#
# #Accuracy 
1 - mean(abs(res_lll$MAPE))
```

## Ridge Regression

```{r 'B30-Ridge-KC'}
# #Ridge Regression i.e. alpha = 0, lambda = (0, Inf)
set.seed(3)
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
mod_ridge <- train(price ~ ., train_log, method = "glmnet", trControl = custom,
    tuneGrid = expand.grid(alpha = 0, 
                           lambda = seq(from = 0.01, to = 0.1, length.out = 10)))
#
# #Sample Subset of Results
mod_ridge$results %>% as_tibble() %>% slice_sample(n = 5)
#
# #Best Tuned Model
mod_ridge$results %>% as_tibble() %>% filter(lambda == mod_ridge$bestTune$lambda)
```


```{r 'B30-Predict-KC-Ridge'}
res_lll_ridge <- test_log %>% 
  mutate(CalYlog = predict(mod_ridge, .), Y_Yc = price - exp(CalYlog), MAPE = Y_Yc / price)
#
sqrt(mean((res_lll_ridge$Y_Yc)^2))
mean(abs(res_lll_ridge$Y_Yc))
mean(abs(res_lll_ridge$MAPE))
1 - mean(abs(res_lll_ridge$MAPE))
```

## Lasso Regression

```{r 'B30-Lasso-KC'}
# #Lasso Regression i.e. alpha = 1, lambda = (0, Inf)
set.seed(3)
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
mod_lasso <- train(price ~ ., train_log, method = "glmnet", trControl = custom,
    tuneGrid = expand.grid(alpha = 1, 
                           lambda = seq(from = 0.0005, to = 0.005, length.out = 10)))
#
# #Sample Subset of Results
mod_lasso$results %>% as_tibble() %>% slice_sample(n = 5)
#
# #Best Tuned Model
mod_lasso$results %>% as_tibble() %>% filter(lambda == mod_lasso$bestTune$lambda)
```

```{r 'B30-Predict-KC-Lasso'}
res_lll_lasso <- test_log %>% 
  mutate(CalYlog = predict(mod_lasso, .), Y_Yc = price - exp(CalYlog), MAPE = Y_Yc / price)
#
sqrt(mean((res_lll_lasso$Y_Yc)^2))
mean(abs(res_lll_lasso$Y_Yc))
mean(abs(res_lll_lasso$MAPE))
1 - mean(abs(res_lll_lasso$MAPE))
```

## Elastic Net

- It takes long time to run and multiple attempts resulted in different values of alpha and lambda so on hold for now - "ForLater"
  - Need a better method than providing different ranges

```{r 'B30-Elastic', eval=FALSE}
# #Elastic Net Regression i.e. alpha = [0, 1], lambda = (0, Inf)
set.seed(3)
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
mod_enet <- train(price ~ ., train_log, method = "glmnet", trControl = custom,
    tuneGrid = expand.grid(alpha = seq(from = 0.001, to = 0.1, length.out = 100), 
                           lambda = seq(from = 0.01, to = 0.1, length.out = 10)))
#
# #Sample Subset of Results
mod_enet$results %>% as_tibble() %>% slice_sample(n = 5)
#
# #Best Tuned Model
mod_enet$results %>% as_tibble() %>% 
  filter(alpha == mod_enet$bestTune$alpha & lambda == mod_enet$bestTune$lambda)
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B30-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, B30P02, cap_hh, corr_hh, custom, dum_xfw, hh, idx_xsyw, lgd_hh, 
          loc_png, mod_lasso, mod_ridge, mod_xfw, mod_xfw_lll, mod_xfw_log, res_lll, 
          res_lll_lasso, res_lll_ridge, sig_corr_hh, stp_xfw, stp_xfw_lll, stp_xfw_log, sub_hh, 
          test_log, test_shapiro, test_xfw, train_log, train_xfw, ttl_hh, vif_ii, xfw, xnw, 
          xxB30KC, zfw, znl, znw)
```

```{r 'B30-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/230-RegressionL.Rmd-->

# Decision Tree Algorithm (B31, Feb-06) {#b31}

```{r 'B31', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Decision Tree Algorithm"

## Packages

```{r 'B31-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("rpart", dependencies = TRUE)
  install.packages("rpart.plot", dependencies = TRUE)
  install.packages("partykit", dependencies = TRUE)
  install.packages("treeheatr", dependencies = TRUE)
}
```

## Data

- [Import Data CarDekho - B26](#set-cardekho-b26 "b26")


```{r 'B31-GetCarDekho', ref.label=c('B26-GetCarDekho', 'B26-PrepCar')}
# #xxB26CarDekho, aa, bb, xsyw, xw,xsw, zfw, xnw, znw
```

```{r 'B31-GetCarDekhoKnit', include=FALSE, eval=TRUE}
xxB26CarDekho <- f_getRDS(xxB26CarDekho)
aa <- xxB26CarDekho
bb <- aa %>% 
  separate(name, c("brand", NA), sep = " ", remove = FALSE, extra = "drop") %>% 
  filter(fuel != "Electric") %>% 
  #mutate(across(where(is.character), factor)) %>% 
  mutate(across(fuel, factor, levels = c("Diesel", "Petrol", "CNG", "LPG"))) %>% 
  mutate(across(transmission, factor, levels = c("Manual", "Automatic"), 
                labels = c("Manual", "Auto"))) %>% 
  mutate(across(owner, factor, 
levels = c("First Owner", "Second Owner", "Third Owner", "Fourth & Above Owner", "Test Drive Car"), 
labels = c("I", "II", "III", "More", "Test"))) %>% 
  mutate(across(seller_type, factor, levels = c("Individual", "Dealer", "Trustmark Dealer"), 
                labels = c("Indiv", "Dealer", "mDealer"))) %>% 
  rename(price = selling_price, km = km_driven, 
         s = seller_type, o = owner, t = transmission, f = fuel) %>% 
  mutate(age = 2022 - year) %>% 
  select(-c(year, name, brand))
# 
xfw <- bb
zfw <- xfw %>% mutate(across(where(is.numeric), ~ as.vector(scale(.)))) 
xnw <- xfw %>% select(where(is.numeric))
znw <- zfw %>% select(where(is.numeric))
```

## Definitions


```{r 'B31D01', comment="", echo=FALSE, results='asis'}
f_getDef("Decision-Tree-Methods")
```


```{r 'B31D02', comment="", echo=FALSE, results='asis'}
f_getDef("Decision-Tree")
```


```{r 'B31D03', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node")
```


```{r 'B31D04', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node-Parent-Child")
```


```{r 'B31D05', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node-Internal-Leaf")
```


```{r 'B31D06', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node-Root")
```


```{r 'B31D07', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node-Degree")
```


```{r 'B31D08', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node-Depth")
```


```{r 'B31D09', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node-Height")
```


```{r 'B31D10', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Breadth")
```


```{r 'B31D11', comment="", echo=FALSE, results='asis'}
f_getDef("Decision-Trees-Summary")
```


```{r 'B31D12', comment="", echo=FALSE, results='asis'}
f_getDef("Splitting-Pruning")
```


```{r 'B31D13', comment="", echo=FALSE, results='asis'}
f_getDef("Recursive-Partitioning") #dddd
```



## Decision Trees

- Decision Trees can be used for both regression (Continuous Y) and classification (Categorical Y)
  - Objective is to partition the dependent variable based on given set of predictors
  - At each step The algorithm try to use a variable which can split the dataset into two groups A & B which have maximum heterogeneity between them (between A & B) and minimum heterogeneity within the groups (within A & within B).
  - The process stops when there is no variance in data that can be separated further i.e. All leaf nodes are homogeneous (within variance is zero)

- \textcolor{pink}{Question:} At each decision node, would the variable be changed
  - At each decision node, the algorithm loops through all the variabls, selects one of them and does a binary split on the value.
  - It is a sequential process, not simultaneous

- Note: If there is a variable in the dataset which does not change i.e. its variance is zero e.g. gender is male for all observations
  - In linear regression model the variable would be kept unless we eliminate it in stepwise regression
  - In Decision Tree model the variable will not be included


## Recursive Tree Algorithm {.tabset .tabset-fade}

### Data {.unlisted .unnumbered}

```{r 'B31-Tree'}
# #24 Observations with 12 who defaulted (isDefaulted = 1) and 12 who paid their loan
bb <- tibble(Income = c(51000, 63000, 59000, 47000, 64000, 84000, 49000, 66000, 
                        33000, 75000, 43000, 53000, 85000, 108000, 60000, 110000, 
                        69000, 81000, 93000, 61000, 65000, 52000, 83000, 87000), 
             Credit = c(524, 548, 596, 602, 627, 640, 638, 667, 680, 708, 730, 748, 
                        620, 648, 682, 701, 731, 716, 749, 752, 767, 788, 796, 840), 
             iDefault = factor(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)))
str(bb)
```

### Split 01 {.unlisted .unnumbered}

```{r 'B31-Split-1'}
# #Split: 01 Credit=681
mod_tree <- rpart(iDefault ~ ., data = bb, method = 'class', 
                  control = rpart.control(cp = 0.5, minsplit = 2))
```

```{r 'B31-PrintTree-1', ref.label=c('B31-PrintTree')}
#
```

### 02 {.unlisted .unnumbered}

```{r 'B31-Split-2'}
# #Split: 01 Credit=681 |02 Income=84500
mod_tree <- rpart(iDefault ~ ., data = bb, method = 'class', 
                  control = rpart.control(cp = 0.15, minsplit = 2))
```

```{r 'B31-PrintTree-2', ref.label=c('B31-PrintTree')}
#
```

### 03 {.unlisted .unnumbered}

```{r 'B31-Split-3'}
# #Split: 01 Credit=681 |02 Income=56500 |03 Income=84500
mod_tree <- rpart(iDefault ~ ., data = bb, method = 'class', 
                  control = rpart.control(cp = 0.083, minsplit = 4))
```

```{r 'B31-PrintTree-3', ref.label=c('B31-PrintTree')}
#
```

### 04 {.unlisted .unnumbered}

```{r 'B31-Split-4'}
# #Split: 01 Credit=681 |02 Income=56500 |03 Income=84500 |04 Credit=768
mod_tree <- rpart(iDefault ~ ., data = bb, method = 'class', 
                  control = rpart.control(cp = 0.08, minsplit = 2))
```

```{r 'B31-PrintTree-4', ref.label=c('B31-PrintTree')}
#
```

### Full Split {.unlisted .unnumbered}

```{r 'B31-Split-Full'}
# #Split: 01 Credit=681 |02 Income=56500 |03 Income=84500 |04 Credit=768 | ... | FULL
mod_tree <- rpart(iDefault ~ ., data = bb, method = 'class', 
                  control = rpart.control(cp = 0.005, minsplit = 3))
```

```{r 'B31-PrintTree-Full', ref.label=c('B31-PrintTree')}
#
```


### as.party() {.unlisted .unnumbered}

```{r 'B31-PrintTree', eval=FALSE}
# #Printing Tree
ii <- as.party(mod_tree)
length(ii)
width(ii)
depth(ii)
ii
```

## Plot Tree {.tabset .tabset-fade}

### Images {.unlisted .unnumbered}

```{r 'B31-Tree-Base-Set', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P01"
ttl_hh <- "Example Tree Basic"
loc_png <- paste0(.z$PX, "B31P01", "-Tree-Basic", ".png")
```

```{r 'B31P01-Save', include=FALSE, ref.label=c('B31-Tree-Base')}
#
```

```{r 'B31P01', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P01", "-Tree-Basic", ".png"))
```

```{r 'B31-Tree-rpart-Set', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P02"
ttl_hh <- "Example Tree by rpart"
loc_png <- paste0(.z$PX, "B31P02", "-Tree-rpart", ".png")
```

```{r 'B31P02-Save', include=FALSE, ref.label=c('B31-Tree-rpart')}
#
```

```{r 'B31P02', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P02", "-Tree-rpart", ".png"))
```

```{r 'B31-Tree-fancy-Set', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P03"
ttl_hh <- "Example Tree by fancyRpartPlot"
loc_png <- paste0(.z$PX, "B31P03", "-Tree-fancy", ".png")
```

```{r 'B31P03-Save', include=FALSE, ref.label=c('B31-Tree-fancy')}
#
```

```{r 'B31P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P03", "-Tree-fancy", ".png"))
```

```{r 'B31-Tree-prp-Set', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P04"
ttl_hh <- "Example Tree by prp"
loc_png <- paste0(.z$PX, "B31P04", "-Tree-prp", ".png")
```

```{r 'B31P04-Save', include=FALSE, ref.label=c('B31-Tree-prp')}
#
```

```{r 'B31P04', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P04", "-Tree-prp", ".png"))
```

```{r 'B31-Tree-heat-Set', include=FALSE}
hh <- as.party(mod_tree)
#
cap_hh <- "B31P05"
ttl_hh <- "(B31P05) Example Tree by heat_tree"
```

```{r 'B31-Tree-heat-Plot', include=FALSE, ref.label=c('B31-Tree-heat')}
#
```

```{r 'B31P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B31P05", "-Tree-treeheatr", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B31P05, device = "png", dpi = 144) 
}
```

```{r 'B31P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P05", "-Tree-treeheatr", ".png"))
```

```{r 'B31-Tree-ggparty-Set', include=FALSE}
hh <- ctree(iDefault ~ ., data = bb, 
      control = ctree_control(alpha = 0.9999, minsplit = 1, minbucket = 1)) 
#
cap_hh <- "B31P06"
ttl_hh <- "Example Tree (& counts) by ctree & ggparty"
```

```{r 'B31-Tree-ggparty-Plot', include=FALSE, ref.label=c('B31-Tree-ggparty')}
#
```

```{r 'B31P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B31P06", "-Tree-ctree-ggparty", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B31P06, device = "png", dpi = 144) 
}
```

```{r 'B31P06', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P06", "-Tree-ctree-ggparty", ".png"))
```

```{r 'B31P0106', echo=FALSE, ref.label=c('B31P01', 'B31P02', 'B31P03', 'B31P04', 'B31P05', 'B31P06'), fig.cap="(B31P01 B31P02 B31P03 B31P04 B31P05 B31P06) Example Tree: Basic, rpart, fancyRpartPlot, prp, treeheatr and ctree & ggparty"}
#
```

### Code Base Tree {.unlisted .unnumbered}

```{r 'B31-Tree-Base-Set-A', eval=FALSE, ref.label=c('B31-Tree-Base-Set')}
#
```

```{r 'B31-Tree-Base', eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh <- mod_tree
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plot(hh)
  text(hh, pretty = 1)
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B31 <- recordPlot()
  dev.off()
  assign(cap_hh, B31)
  rm(B31)
}
```

### rpart.plot() {.unlisted .unnumbered}

```{conjecture 'rpart-plot-extra'}
\textcolor{brown}{Error: extra=2 is legal only for "class", "poisson" and "exp" models (you have an "anova" model)}
```

- Use extra = 'auto' or appropriate code for extra information


```{r 'B31-Tree-rpart-Set-A', eval=FALSE, ref.label=c('B31-Tree-rpart-Set')}
#
```

```{r 'B31-Tree-rpart', eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh <- mod_tree
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  rpart.plot(hh, extra = 'auto')
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B31 <- recordPlot()
  dev.off()
  assign(cap_hh, B31)
  rm(B31)
}
```

### fancyRpartPlot() {.unlisted .unnumbered}

```{r 'B31-Tree-fancy-Set-A', eval=FALSE, ref.label=c('B31-Tree-fancy-Set')}
#
```

```{r 'B31-Tree-fancy', eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh <- mod_tree
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  fancyRpartPlot(hh)
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B31 <- recordPlot()
  dev.off()
  assign(cap_hh, B31)
  rm(B31)
}
```

### prp() {.unlisted .unnumbered}

```{conjecture 'prp-extra'}
\textcolor{brown}{Error: extra=104 is legal only for "class" models (you have an "anova" model)}
```

- Use extra = 0 or appropriate code for extra information

```{r 'B31-Tree-prp-Set-A', eval=FALSE, ref.label=c('B31-Tree-prp-Set')}
#
```

```{r 'B31-Tree-prp', eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh <- mod_tree
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  prp(hh, type = 2, extra = 0, nn = TRUE, fallen.leaves = TRUE, 
      faclen = 0, varlen = 0, shadow.col = "grey", branch.lty = 3)
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B31 <- recordPlot()
  dev.off()
  assign(cap_hh, B31)
  rm(B31)
}
```

### heat_tree() {.unlisted .unnumbered}

```{r 'B31-Tree-heat-Set-A', eval=FALSE, ref.label=c('B31-Tree-heat-Set')}
#
```

```{r 'B31-Tree-heat', eval=FALSE}
# #IN: cap_hh, ttl_hh, hh <- as.party(mod_tree)
#
B31 <- hh %>% {# cannot use labs because it is a gtable object
  suppressWarnings(heat_tree(x = ., label_map = c('0'= 'Paid', '1'= 'Fault'), 
                             target_cols = c('#FDE725FF', '#440154FF'), 
      panel_space = 0.03, target_space = 0.2, tree_space_bottom = 0.05, title = ttl_hh))
}
assign(cap_hh, B31)
rm(B31)
#
if(FALSE){# Modifying a gtable object. Finding the Coordinates of specific grob is Manual.
  ii <- suppressWarnings(heat_tree(x = hh, label_map = c('0'= 'Paid', '1'= 'Fault'), 
                             target_cols = c('#FDE725FF', '#440154FF'), 
      panel_space = 0.03, target_space = 0.2, tree_space_bottom = 0.05, title = ttl_hh))
  # #Works
  if(FALSE) ii 
  if(FALSE) grid.draw(ii)
  # #Plot with grey squares in backgroud if plot or save (like base R plot objects)
  if(FALSE) plot(ii)
  #
  # #Explore gtable Object
  names(ii)
  # #layout is a data.frame indicating the position of each grob.
  # #z defines drawing order i.e. overlap
  if(FALSE) ii$layout #t = 9, l = 7, b = 9, r = 7
  # # t   l  b  r   z   clip       name
  # # 1   2  1  14  21  0          on background
  left.foot = textGrob("Caption", x = 0, y = 0.8, 
                       just = c("left", "top"), gp = gpar(fontsize = 11, col =  "black"))
  labs.foot = gTree("LabsFoot", children = gList(left.foot))
  jj <- gtable_add_grob(ii, labs.foot, t = 3, l = 2, b = 3, r = 2, z = 13, clip = 'off')
  jj 
}
```

### ggparty() {.unlisted .unnumbered}

```{r 'B31-Tree-ggparty-Set-A', eval=FALSE, ref.label=c('B31-Tree-ggparty-Set')}
#
```

```{r 'B31-Tree-ggparty', eval=FALSE}
# #IN: cap_hh, ttl_hh, hh as ctree object
#
B31 <- hh %>% { ggparty(.) + 
    geom_edge() + 
    geom_edge_label() + 
    geom_node_label(aes(label = splitvar), ids = "inner") + 
    geom_node_label(aes(label = paste("n =", nodesize)), nudge_y = 0.03, ids = "terminal") +
    geom_node_plot(shared_legend = FALSE,
                   gglist = list(geom_bar(aes(x = !!.$terms[[2]], 
                                              fill = !!.$terms[[2]])),
                                 theme_minimal(), theme(legend.position = "none"))) + 
    theme(plot.background = element_rect(fill = 'white', colour = 'white')) +
    labs(caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B31)
rm(B31)
#
if(FALSE) {#With Bars
  B31 <- hh %>% { ggparty(.) + 
    geom_edge() + 
    geom_edge_label() + 
    geom_node_label(aes(label = splitvar), ids = "inner") + 
    geom_node_label(aes(label = paste("n =", nodesize)), nudge_y = 0.03, ids = "terminal") +
    geom_node_plot(gglist = list(geom_bar(aes(x = "", fill = !!.$terms[[2]]), 
                                          position = position_fill()), 
        theme_classic(), theme(axis.title = element_blank(), axis.ticks.x = element_blank()))) + 
    theme(plot.background = element_rect(fill = 'white', colour = 'white')) +
    labs(caption = cap_hh, title = ttl_hh)
  }
}
```

## Gini Index

```{definition 'Gini-Index'}
\textcolor{pink}{Gini index} is the probability of misclassifying an observation. It is a measure of total variance across the 'K' classes. If $\hat{p}_{mk}$ represents the proportion of training observations in the $m^{\text{th}}$ region that are from the $k^{\text{th}}$ class. Then \textcolor{pink}{Gini index: $G = \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk}) \to G \in [0, 0.5]$}. 
```

- To determine the particular split that maximized homogeneity the algorithm used a purity measure, Gini impurity. 
  - If a set was very mixed (impure), the Gini impurity would be high. 
  - As the set become more pure, the Gini impurity would decrease. 
  - Gini index is referred to as a measure of node purity. A small Gini value indicates that a node contains predominantly observations from a single class.


```{definition 'Information-Gain'}
The difference in Gini impurity from the prior state to the next is called the \textcolor{pink}{information gain}. It is inversely related with Gini impurity.
```


- \textcolor{pink}{Question:} What are the chances of a person being defaulter if the dataset has 12 defaulters and 12 not defaulters
  - For being defaulter : 12/24 = 50%
- \textcolor{pink}{Question:} What are the chances of a person being defaulter if the dataset has 6 defaulters and 18 not defaulters
  - For being defaulter : 6/24 = 25% (more homogeneour sample compared to above, thus Gini will be reduced)

- \textcolor{pink}{Question:} If the Tree splits at Credit Score = 681, what is the Gini index
  - $R_1$ [2] Credit >= 681: 0 (n = 13, err = 23.1%) 3 deaulters, 10 not defaulters
  - $R_2$ [3] Credit < 681: 1 (n = 11, err = 18.2%) 9 defaulters, 2 not defaulters
  - Gini impurity of $R_1 : G_{R_1} = \frac{10}{13} \times \frac{3}{13} + \frac{3}{13} \times \frac{10}{13} = 0.356$
  - Gini impurity of $R_2 : G_{R_2} = \frac{2}{11} \times \frac{9}{11} + \frac{9}{11} \times \frac{2}{11} = 0.298$
  - Proportion of $R_1 : P_{R_1} = \frac{13}{24}$
  - Proportion of $R_2 : P_{R_2} = \frac{11}{24}$
  - $G = P_{R_1} \times G_{R_1} + P_{R_2} \times G_{R_2} = \frac{13}{24} \times 0.356 + \frac{11}{24} \times 0.298 = 0.3286$
  - Thus, Gini reduced from 0.5 to 0.3286 after the first split.

- \textcolor{pink}{Question:} Why the first cut is at Credit Score = 681, why can it not be at 850, 700, 650, 500
  - At this value the Gini becomes lowest for single cut compared to all other cuts (Horizontal or Vertical)
  - (Aside) At the extreme ends (higher than max or lower than min) the Gini index will be 50% (maximum for binary choices)
    - As we start to move towards inside (from max to lower or from min to higher) the Gini would start decreasing once and then reaching a local minima and then increasing again
    - The algorithm has identified this local minima at the value of Credit Score = 681
    - This is the line that reduces that variances within $(R_1)$ and within $(R_2)$. And is maximises the variance between $(R_1 , R_2)$

- \textcolor{pink}{Question:} For a completely pure node, what would be the Gini impurity
  - Zero

## Pruning

- If we allow to grow to any extent, it might result in overfitting and if we prune it too much, it might lead to underfitting
- We can use complexity parameter $C_p \in [0, 1]$ for pruning
  - It decides the minimum information gain. Any branch having lower gain than that will be pruned
  - High value of $C_p$ will result in small tree

## Building a Regression Tree

1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations. 
2. Apply cost complexity pruning $(C_p)$ to the large tree in order to obtain a sequence of best subtrees, as a function of ${\alpha}$. 
3. Use K-fold cross-validation to choose ${\alpha}$. 
    - Divide the training observations into K folds. 
    - For each $k = \{1, 2, \ldots, K\}$: 
      - Repeat Steps 1 and 2 on all but the $k^{\text{th}}$ fold of the training data. i.e. keep at least one separate as Test. 
      - Evaluate the mean squared prediction error on the data in the left-out $k^{\text{th}}$ fold, as a function of ${\alpha}$. 
    - Average the results for each value of ${\alpha}$, and pick ${\alpha}$ to minimize the average error. 
4. Return the subtree from Step 2 that corresponds to the chosen value of ${\alpha}$.


## Dummy Variable Creation

- Some algorithms in R and Python can automatically convert factors to dummy variable and perform the analysis. Ex: lm()
  - However, it is recommended to perform the conversion yourself for fewer bugs and better control over reference variables etc. 
    - Most of other software platform would not do this automatically. We need factor variables to be converted to the dummy variables
    - Some of the advanced modelling methods in R or Python would not do this conversion automatically


```{r 'B31-Dummies'}
# #Dataset: iris (Assume Y = Petal.Width, 3 numerical X and 1 factor X)
str(iris)
# #Create Linear Model without converting the factor variable to dummies
mod_lm <- lm(Petal.Width ~ ., data = iris)
# #Note that it automatically converted factor to dummy variables with "setosa" as reference
summary(mod_lm)
```

## Car Dekho

```{r 'B31-Dummies-Car'}
# #Dataset: Car Dekho
str(xfw)
#
# #Dummy | Drop First Level i.e. Reference | Drop Selected Columns i.e. Original |
dum_xfw <- xfw %>% dummy_cols(.data = ., 
                  select_columns = c("f", "s", "t", "o"), 
                  remove_first_dummy = TRUE, remove_selected_columns = TRUE)
#
# #Partition Data
set.seed(3)
idx_xfw <- sample.int(n = nrow(dum_xfw), size = floor(0.8 * nrow(dum_xfw)), replace = FALSE)
train_xfw <- dum_xfw[idx_xfw, ]
test_xfw  <- dum_xfw[-idx_xfw, ]
#
# #Decision Trees: Convert Dummy Variables to Factor Variables of two levels "0", "1"
train_xfw_ii <- train_xfw %>% mutate(across(starts_with(c("s_", "f_", "t_", "o_")), factor))
test_xfw_ii  <- test_xfw  %>% mutate(across(starts_with(c("s_", "f_", "t_", "o_")), factor))
#
str(train_xfw_ii)
```

## Build Tree by rpart

- \textcolor{pink}{rpart()}
  - For Regression Tree (Y Continuous), Method needs to be 'anova'
  - For Classification Tree (Y Categorical), Method needs to be 'class'
  - If there is a problem in plotting then mention the cp parameter in rpart() with the earlier identified value
  
- \textcolor{orange}{Caution:} Unlike Regression, for Tree Model: Convert Dummy Variables to Factor Variables of two levels "0", "1"
  - Otherwise these would be treated as numeric i.e. Petrol < 0.5 etc.

```{r 'B31-Tree-Car'}
# #rpart() for Tree Model
mod_tree <- rpart(price ~ ., data = train_xfw_ii, method = 'anova')
```

```{r 'B31-PrintTree-Car', ref.label=c('B31-PrintTree')}
#
```

## Plot Tree

- "ForLater"
  - Unable to run heat_tree() on this dataset.
  - Error in get_fit.party : Please ensure the tree was trained on a dataset with dependent variable of class factor or switch task to regression.


```{r 'B31-Tree-Base-Set-Car', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P07"
ttl_hh <- "CarDekho: Tree Basic"
loc_png <- paste0(.z$PX, "B31P07", "-Car-Basic", ".png")
```

```{r 'B31P07-Save', include=FALSE, ref.label=c('B31-Tree-Base')}
#
```

```{r 'B31P07', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P07", "-Car-Basic", ".png"))
```

```{r 'B31-Tree-rpart-Set-Car', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P08"
ttl_hh <- "CarDekho: Tree by rpart"
loc_png <- paste0(.z$PX, "B31P08", "-Car-rpart", ".png")
```

```{r 'B31P08-Save', include=FALSE, ref.label=c('B31-Tree-rpart')}
#
```

```{r 'B31P08', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P08", "-Car-rpart", ".png"))
```

```{r 'B31-Tree-fancy-Set-Car', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P09"
ttl_hh <- "CarDekho: Tree by fancyRpartPlot"
loc_png <- paste0(.z$PX, "B31P09", "-Car-fancy", ".png")
```

```{r 'B31P09-Save', include=FALSE, ref.label=c('B31-Tree-fancy')}
#
```

```{r 'B31P09', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P09", "-Car-fancy", ".png"))
```

```{r 'B31-Tree-prp-Set-Car', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P10"
ttl_hh <- "CarDekho: Tree by prp"
loc_png <- paste0(.z$PX, "B31P10", "-Car-prp", ".png")
```

```{r 'B31P10-Save', include=FALSE, ref.label=c('B31-Tree-prp')}
#
```

```{r 'B31P10', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P10", "-Car-prp", ".png"))
```

```{r 'B31-Tree-heat-Set-Car', include=FALSE, eval=FALSE}
# #NOT WORKING. THROWING ERROR
hh <- as.party(mod_tree)
#
cap_hh <- "B31P11"
ttl_hh <- "(B31P11) CarDekho: Tree by heat_tree"
# 'B31-Tree-heat-Regression'
# #IN: cap_hh, ttl_hh, hh <- as.party(mod_tree)
#
B31 <- hh %>% {# cannot use labs because it is a gtable object
  suppressWarnings(heat_tree(x = ., task = 'regression', title = ttl_hh))
}
assign(cap_hh, B31)
rm(B31)
# 'B31P11-Save'
loc_png <- paste0(.z$PX, "B31P11", "-Car-treeheatr", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B31P11, device = "png", dpi = 144) 
}
# 'B31P11'
knitr::include_graphics(paste0(.z$PX, "B31P11", "-Car-treeheatr", ".png"))
```

```{r 'B31-Tree-ggparty-Set-Car', include=FALSE}
# #, minsplit = 1, minbucket = 1
hh <- ctree(price ~ ., data = train_xfw_ii, 
      control = ctree_control(alpha = 0.000001)
      ) 
#
cap_hh <- "B31P12"
ttl_hh <- "CarDekho: Tree (& counts) by ctree & ggparty"
```

```{r 'B31-Tree-ggparty-Plot-Car'}
# #'B31-Tree-ggparty'
# #IN: cap_hh, ttl_hh, hh as ctree object
#
B31 <- hh %>% { ggparty(.) + 
    geom_edge() + 
    geom_edge_label() + 
    geom_node_label(aes(label = splitvar), ids = "inner") + 
    geom_node_label(aes(label = paste("n =", nodesize)), nudge_y = 0.03, ids = "terminal") +
    #geom_node_plot(shared_legend = FALSE,
    #               gglist = list(geom_bar(aes(x = !!.$terms[[2]], 
    #                                          fill = !!.$terms[[2]])),
    #                             theme_minimal(), theme(legend.position = "none"))) + 
    #theme_bw() +
    #theme_minimal() +
    theme(plot.background = element_rect(fill = 'white', colour = 'white')) +
    labs(caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B31)
rm(B31)
```

```{r 'B31P12-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B31P12", "-Car-ctree-ggparty", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B31P12, device = "png", dpi = 144) 
}
```

```{r 'B31P12', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P12", "-Car-ctree-ggparty", ".png"))
```

```{r 'B31P0708', echo=FALSE, ref.label=c('B31P07', 'B31P08'), fig.cap="(B31P07 B31P08) CarDekho: Tree: Basic and rpart"}
#
```

```{r 'B31P0910', echo=FALSE, ref.label=c('B31P09', 'B31P10'), fig.cap="(B31P09 B31P10) CarDekho: Tree: fancyRpartPlot and prp"}
#
```

```{r 'B31P12-A', echo=FALSE, ref.label=c('B31P12'), fig.cap="(B31P12) CarDekho: Tree: ctree & ggparty"}
#
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B31-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, B31P01, B31P02, B31P03, B31P04, B31P05, B31P06, B31P07, B31P08, 
          B31P09, B31P10, B31P12, cap_hh, dum_xfw, hh, idx_xfw, loc_png, mod_lm, mod_tree, 
          test_xfw, test_xfw_ii, train_xfw, train_xfw_ii, ttl_hh, xfw, xnw, xxB26CarDekho, zfw, 
          znw)
```

```{r 'B31-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/231-Trees.Rmd-->

# Decision Tree Algorithm (B32, Feb-13) {#b32}

```{r 'B32', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Decision Tree Algorithm"

## Packages

```{r 'B32-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("rattle", dependencies = TRUE)
  install.packages("janitor", dependencies = TRUE)
  install.packages("randomForest", dependencies = TRUE)
}
```

## Car Dekho - Data

- [Import Data CarDekho - B26](#set-cardekho-b26 "b26")

```{r 'B32-GetCarDekho', ref.label=c('B26-GetCarDekho', 'B26-PrepCar', 'B31-Dummies-Car')}
# #xxB26CarDekho, aa, bb, xsyw, xw,xsw, zfw, xnw, znw, 
# #train_xfw, test_xfw, train_xfw_ii, test_xfw_ii
```

```{r 'B32-GetCarDekhoKnit', include=FALSE, eval=FALSE}
xxB26CarDekho <- f_getRDS(xxB26CarDekho)
aa <- xxB26CarDekho
bb <- aa %>% 
  separate(name, c("brand", NA), sep = " ", remove = FALSE, extra = "drop") %>% 
  filter(fuel != "Electric") %>% 
  #mutate(across(where(is.character), factor)) %>% 
  mutate(across(fuel, factor, levels = c("Diesel", "Petrol", "CNG", "LPG"))) %>% 
  mutate(across(transmission, factor, levels = c("Manual", "Automatic"), 
                labels = c("Manual", "Auto"))) %>% 
  mutate(across(owner, factor, 
levels = c("First Owner", "Second Owner", "Third Owner", "Fourth & Above Owner", "Test Drive Car"), 
labels = c("I", "II", "III", "More", "Test"))) %>% 
  mutate(across(seller_type, factor, levels = c("Individual", "Dealer", "Trustmark Dealer"), 
                labels = c("Indiv", "Dealer", "mDealer"))) %>% 
  rename(price = selling_price, km = km_driven, 
         s = seller_type, o = owner, t = transmission, f = fuel) %>% 
  mutate(age = 2022 - year) %>% 
  select(-c(year, name, brand))
# 
xfw <- bb
zfw <- xfw %>% mutate(across(where(is.numeric), ~ as.vector(scale(.)))) 
xnw <- xfw %>% select(where(is.numeric))
znw <- zfw %>% select(where(is.numeric))
#
# #Dummy | Drop First Level i.e. Reference | Drop Selected Columns i.e. Original |
dum_xfw <- xfw %>% dummy_cols(.data = ., 
                  select_columns = c("f", "s", "t", "o"), 
                  remove_first_dummy = TRUE, remove_selected_columns = TRUE)
#
# #Partition Data
set.seed(3)
idx_xfw <- sample.int(n = nrow(dum_xfw), size = floor(0.8 * nrow(dum_xfw)), replace = FALSE)
train_xfw <- dum_xfw[idx_xfw, ]
test_xfw  <- dum_xfw[-idx_xfw, ]
#
# #Decision Trees: Convert Dummy Variables to Factor Variables of two levels "0", "1"
train_xfw_ii <- train_xfw %>% mutate(across(starts_with(c("s_", "f_", "t_", "o_")), factor))
test_xfw_ii  <- test_xfw  %>% mutate(across(starts_with(c("s_", "f_", "t_", "o_")), factor))
```

```{r 'B32-PrintTree', include=FALSE, eval=FALSE}
# #Printing Tree
ii <- as.party(hh)
length(ii)
width(ii)
depth(ii)
ii
```

## Backticks

- `` `Header Names containing Spaces or Symbols are referred by surrounding them in Backticks` ``
  - As shown in the above line
  - Backtick can be found on the Tilde "~" Key, just below the Escape Key.
  - \textcolor{orange}{Caution:} R sometimes can handle special symbols within single quotes and sometimes it fails. 
    - Thus it is recommended to use backticks to handle special symbols
    - Mostly it is found when Column Names contain Spaces

```{r 'B32-Backticks'}
# #There is NO difference between Double Quotes (" ") and Single Quotes (' ')
identical("a", 'a')
#
# #However Backticks (` `) may behave differently than Single Quotes (' ')
if(FALSE) identical(`a`, 'a') #Error 
#
# #Backticks are needed to Create /Manipulate Header Names with special symbols 
# #Column 1 needed to be surrounded with Backticks whereas Column 2 does not require this
bb <- tibble(`header'has'single_quotes` = 1:3, header_no_single_quote = 4:6)
str(bb)
#
# #Convert Columns from integer to character by using Column Names
bb$header_no_single_quote   <- as.character(bb$header_no_single_quote)
#bb$header'has'single_quotes   <- as.character(bb$`header'has'single_quotes`)  #Error
#bb$'header'has'single_quotes' <- as.character(bb$`header'has'single_quotes`)  #Error
# #Works
bb$`header'has'single_quotes`  <- as.character(bb$`header'has'single_quotes`)
#
str(bb)
```

## Model Parameters

- \textcolor{pink}{modelLookup()}
  - It provides information about the parameters avaialble for the given model and their applicability on Regression or Classification problem

```{r 'B32-Lookup-Model'}
# #modelLookup() for information about models and packages that are accessible via train()
modelLookup('rpart')
```

## Build Tree

- Plots have been generated in previous lecture - "ForLater" Link
- "dummy coding" is also known as "one-hot encoding"
- We can use complexity parameter $C_p \in [0, 1]$ for pruning
  - It decides the minimum information gain. Any branch having lower gain than that will be pruned 
  - High value of $C_p$ will result in small tree
  - Pruning also helps in avoiding the problem of outliers

```{r 'B32-Tree-Car'}
# #rpart() for Tree Model
mod_tree <- rpart(price ~ ., data = train_xfw_ii, method = 'anova')
#
# #Very Long Output 
#summary(mod_tree)
hh <- mod_tree
```

```{r 'B32-PrintTree-Car', ref.label=c('B32-PrintTree')}
#
```

## Parameter Tuning 

- Limitation of Train and Test Concept
  - When we are using 80% of data for Training purpose, we are not able to utilise the information in the other 20% for model building
  - This might lead to bias in the model because we have not considered one portion of data
- k-fold Cross-validation helps in utilising all the information available
  - Refer [Cross-validation B28](#cross-b28 "b28")

```{r 'B32D01', comment="", echo=FALSE, results='asis'}
f_getDef("Cross-Validation") #dddd
```

- \textcolor{pink}{Question:} Which one to choose from the Simple Two-fold, k-fold CV and repeated CV
  - Depends upon the available computational resources (time, memory, cost etc.)

- It is possible for each model (repeated CV) to run with different values of $C_p$
  - For each $C_p$ of each model we can calculate RMSE
  - For each value of $C_p$ we can calculate Average RMSE across all Models
  - The $C_p$ value having lowest average RMSE across all models would be the optimum one.
  - This process for identification of optimum $C_p$ is \textcolor{pink}{parameter tuning}

- \textcolor{pink}{Question:} shoule we do this process only for identification of $C_p$
  - In general, Cross-validation provides better result
  - Some people prefer to split the dataset and then on the training set do the cross-validation
  - However, cross-validation on complete dataset can also be done 

- If we find that we are unable to improve accuracy between two-fold and k-fold, we can do the k-fold on complete data.
  - However, in that case, it would not be comparable models because base model would be trained on only 80% of data.

## Predict

- Prediction Accuracy is only 35.9% 

```{r 'B32-Predict-Car'}
# #Model Validation
#res_tree <- predict(mod_tree, test_xfw_ii, type  = "vector") 
res_tree <- test_xfw_ii %>% 
  mutate(CalY = predict(mod_tree, ., type = "vector"), Y_Yc = price - CalY, MAPE = Y_Yc / price)
#
ii <- res_tree
summary(ii$Y_Yc)
#
# #Correlation 
cor(ii$price, ii$CalY)
#
# #RMSE: Root Mean Squared Error
sqrt(mean((ii$Y_Yc)^2))
#
# #MAE: Mean Absolute Error (MAE)
mean(abs(ii$Y_Yc))
#
# #MAPE: Mean Absolute Percentage Error
mean(abs(ii$MAPE))
#
# #Accuracy 
1 - mean(abs(ii$MAPE))
```

## Cleaning Names

- A syntactically valid name consists of letters, numbers and the dot or underline characters and starts with a letter or the dot not followed by a number.
- 'janitor' or 'dplyr' packages can be used to rename the headers to R standards

```{r 'B32-Rename'}
# #Rename All Column Headers to proper names
bb <- tibble("A_Underscore" = 1, "B.Dot" = 2, "C Space" = 3, "D-Dash" = 4, `E'apostrophe` = 5) #'
ii <- janitor::clean_names(bb)
jj <- bb %>% dplyr::rename_with(make.names) %>% 
  rename_with(~ tolower(gsub(".", "_", .x, fixed = TRUE))) 
#
names(bb)
names(ii)
names(jj)
```

## Setup Parallel

- Allow Parallel

```{r 'B32-ModelControl'}
cv <- trainControl(method = "repeatedcv", number = 10, repeats = 5, allowParallel = TRUE)
```


## Crossvalidation

- \textcolor{orange}{Warning:} 
  - "Warning in nominalTrainWorkflow : There were missing values in resampled performance measures."
  - If this is regression, the most likely case is that the tree did not find a good split and used the average of the outcome as the predictor. It is fine but you cannot calculate $R^2$ since the variance of the predictions is zero. 
  - For now, assume that the warning is not importnat - "ForLater"

- Accuracy improved to 42.5% (from 35.9%) with cp = 0.008528435

```{r 'B32-CV-Car'}
# #train() tuneLength decides the granularity in the tuning parameter grid
mod_cv <- suppressWarnings(train(price ~ ., data = train_xfw_ii, method = 'rpart', 
                trControl = cv, tuneLength = 10))
#mod_cv
#
# #Best Tuning Cp value
mod_cv$bestTune
```


```{r 'B32-Predict-cv-Car'}
# #Model Validation
# #Class of Both Models are different. rpart() require "vector" but "raw" is used with train()
class(mod_tree)
class(mod_cv)
#
#res_tree <- predict(mod_cv, test_xfw_ii, type  = "raw") 
res_cv <- test_xfw_ii %>% 
  mutate(CalY = predict(mod_cv, ., type = "raw"), Y_Yc = price - CalY, MAPE = Y_Yc / price)
#
ii <- res_cv
summary(ii$Y_Yc)
#
# #RMSE: Root Mean Squared Error
sqrt(mean((ii$Y_Yc)^2))
#
# #MAE: Mean Absolute Error (MAE)
mean(abs(ii$Y_Yc))
#
# #MAPE: Mean Absolute Percentage Error
mean(abs(ii$MAPE))
#
# #Accuracy 
1 - mean(abs(ii$MAPE))
```

```{r 'B32-CP-Car'}
# #rpart() for Tree Model using Tuned cp obtained from Cross-validation
# #Needed to plot this CV model 
mod_tree_cp <- rpart(price ~ ., data = train_xfw_ii, method = 'anova', cp = 0.008528435)
#
# #Very Long Output 
#summary(mod_tree_cp)
```

```{r 'B32-Tree-rpart', include=FALSE, eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh <- mod_tree
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  rpart.plot(hh, extra = 'auto')
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B31 <- recordPlot()
  dev.off()
  assign(cap_hh, B31)
  rm(B31)
}
```


```{r 'B32-Tree-rpart-Set', include=FALSE}
hh <- mod_tree_cp
#
cap_hh <- "B32P01"
ttl_hh <- "CarDekho: Tree by Crossvalidation"
loc_png <- paste0(.z$PX, "B32P01", "-Tree-cv", ".png")
```

```{r 'B32P01-Save', include=FALSE, ref.label=c('B32-Tree-rpart')}
#
```

```{r 'B32P01', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B32P01", "-Tree-cv", ".png"))
```


```{r 'B32P01-A', echo=FALSE, ref.label=c('B31P08', 'B32P01'), fig.cap="(B31P08, B32P01) CarDekho: Two-part vs 10-part CV Tree"}
#
```

## KC House - Data  {.tabset .tabset-fade}

- [Import Data KC House - B30](#set-kc-b30 "b30")
- Note: More columns have been included in Decision Trees, compared to Linear Regression, because no checks have been done on normality or multicollinearity (VIF)

### Preprocessing {.unlisted .unnumbered}

```{r 'B32-GetKC-Tree'}
# #Load Data: KC House
xxB30KC <- f_getRDS(xxB30KC)
# #Drop NA | Get Age | Dummy | Rename | Filter | Factor | Relevel Most Frequent Reference | 
# #Drop | Relocate |
xfw <- xxB30KC %>% 
  drop_na() %>% 
  #mutate(across(date, as_date)) %>% 
  mutate(sold = year(date), age = sold - yr_built) %>% 
  mutate(iRenew = ifelse(yr_renovated == 0, 0, 1)) %>% 
  rename(beds = bedrooms, baths = bathrooms, sqAbove = sqft_above, sqLot = sqft_lot, 
         iWater = waterfront, iView = view) %>% 
  filter(beds != 33) %>% 
  mutate(across(c(beds, baths, floors, iWater, iView, condition, grade, iRenew), factor)) %>% 
  mutate(across(c(zipcode), factor)) %>% 
  mutate(across(beds, relevel, ref = "3")) %>% 
  mutate(across(baths, relevel, ref = "2")) %>% 
  mutate(across(floors, relevel, ref = "1")) %>% 
  mutate(across(condition, relevel, ref = "3")) %>% 
  mutate(across(grade, relevel, ref = "7")) %>% 
  mutate(across(zipcode, relevel, ref = "98103")) %>% 
  select(-c(id, lat, long, date, sold, yr_renovated, yr_built, 
            sqft_living, sqft_living15, sqft_lot15)) %>% 
  select(-c(zipcode)) %>% 
  relocate(price)
```

```{r 'B32-Dummies-KC'}
# #Partition Data
set.seed(3)
# #Create Dummies | Replaced All NA in original data so no replacement in dummy columns |
dum_xfw <- dummy_cols(xfw, remove_first_dummy = TRUE, remove_selected_columns = TRUE) 
idx_xsyw <- sample.int(n = nrow(dum_xfw), size = floor(0.8 * nrow(dum_xfw)), replace = FALSE)
train_xfw <- dum_xfw[idx_xsyw, ] 
test_xfw  <- dum_xfw[-idx_xsyw, ]
```

```{r 'B32-TreeFact-KC'}
# #Decision Trees: Convert Dummy Variables to Factor Variables of two levels "0", "1"
train_xfw_ii <- train_xfw %>% mutate(across(starts_with(c(
  "beds_", "baths_", "floors_", "iWater_", "iView_", "condition_", "grade_", "iRenew_")), factor))
test_xfw_ii <- test_xfw %>% mutate(across(starts_with(c(
  "beds_", "baths_", "floors_", "iWater_", "iView_", "condition_", "grade_", "iRenew_")), factor))
```

### Structure {.unlisted .unnumbered}

```{r 'B32-Structure-KC'}
str(train_xfw_ii)
```

### Summary {.unlisted .unnumbered}

```{r 'B32-Summary-KC'}
#summary(train_xfw_ii)
train_xfw_ii %>% select(where(is.numeric)) %>% summary()
#
train_xfw_ii %>% select(!where(is.numeric)) 
```


## Decision Tree 

- Prediction Accuracy is only 63.4%

```{r 'B32-Tree-KC'}
# #Decision Tree Model
mod_tree <- rpart(price ~ ., data = train_xfw_ii, method = 'anova')
#
# #Very Long Output 
#summary(mod_tree)
hh <- mod_tree
```

```{r 'B32-PrintTree-KC', ref.label=c('B32-PrintTree')}
#
```

```{r 'B32-Tree-rpart-Set-KC', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B32P02"
ttl_hh <- "KC: Tree"
loc_png <- paste0(.z$PX, "B32P02", "-Tree-KC-Rpart", ".png")
```

```{r 'B32P02-Save', include=FALSE, ref.label=c('B32-Tree-rpart')}
#
```

```{r 'B32P02', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B32P02", "-Tree-KC-Rpart", ".png")) #iiii
```

```{r 'B32-Predict-KC'}
# #Model Validation
#res_tree <- predict(mod_tree, test_xfw_ii, type  = "vector") 
res_tree <- test_xfw_ii %>% 
  mutate(CalY = predict(mod_tree, ., type = "vector"), Y_Yc = price - CalY, MAPE = Y_Yc / price)
#
ii <- res_tree
summary(ii$Y_Yc)
#
# #Correlation 
cor(ii$price, ii$CalY)
#
# #RMSE: Root Mean Squared Error
sqrt(mean((ii$Y_Yc)^2))
#
# #MAE: Mean Absolute Error (MAE)
mean(abs(ii$Y_Yc))
#
# #MAPE: Mean Absolute Percentage Error
mean(abs(ii$MAPE))
#
# #Accuracy 
1 - mean(abs(ii$MAPE))
```

## Get Cp

```{r 'B32-CalCp-KC'}
# #Calculate cp
printcp(mod_tree)
```

```{r 'B32-Cp-Set-KC', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B32P04"
ttl_hh <- "KC: Plot Cp"
loc_png <- paste0(.z$PX, "B32P04", "-KC-CP", ".png")
```

```{r 'B32-Cp-KC', include=FALSE, eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh <- mod_tree
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plotcp(mod_tree)
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B31 <- recordPlot()
  dev.off()
  assign(cap_hh, B31)
  rm(B31)
}
```

```{r 'B32P04-Save', include=FALSE, ref.label=c('B32-Cp-KC')}
#
```

```{r 'B32P04', echo=FALSE, fig.cap="(B32P04) KC: Plot Cp"}
knitr::include_graphics(paste0(.z$PX, "B32P04", "-KC-CP", ".png")) #iiii
```

## Pruning by Cp

- Prediction Accuracy is 61% (reduced from 63.4%)

```{r 'B32-Pruned-KC'}
# #Decision Tree Model with pruning
mod_prune <- prune.rpart(mod_tree, cp = 0.028)
#
# #Very Long Output 
#summary(mod_prune)
hh <- mod_prune
```

```{r 'B32-PrintTree-KC-Pruned', include=FALSE, eval=FALSE, ref.label=c('B32-PrintTree')}
#
```

```{r 'B32-Predict-Prune-KC'}
# #Model Validation
#res_prune <- predict(mod_prune, test_xfw_ii, type  = "vector") 
res_prune <- test_xfw_ii %>% 
  mutate(CalY = predict(mod_prune, ., type = "vector"), Y_Yc = price - CalY, MAPE = Y_Yc / price)
#
ii <- res_prune
summary(ii$Y_Yc)
#
# #Correlation 
cor(ii$price, ii$CalY)
#
# #RMSE: Root Mean Squared Error
sqrt(mean((ii$Y_Yc)^2))
#
# #MAE: Mean Absolute Error (MAE)
mean(abs(ii$Y_Yc))
#
# #MAPE: Mean Absolute Percentage Error
mean(abs(ii$MAPE))
#
# #Accuracy 
1 - mean(abs(ii$MAPE))
```

## Crossvalidation

- Accuracy is 63.4% (similar to original) with cp = 0.01105875	

```{r 'B32-ModelControl-KC'}
cv <- trainControl(method = "repeatedcv", number = 10, repeats = 5, allowParallel = TRUE)
```

```{r 'B32-CV-KC'}
# #train() tuneLength decides the granularity in the tuning parameter grid
mod_cv <- suppressWarnings(train(price ~ ., data = train_xfw_ii, method = 'rpart', 
                trControl = cv, tuneLength = 10))
#mod_cv
#
# #Best Tuning Cp value
mod_cv$bestTune
```


```{r 'B32-Predict-cv-KC'}
# #Model Validation
# #Class of Both Models are different. rpart() require "vector" but "raw" is used with train()
class(mod_tree)
class(mod_cv)
#
#res_tree <- predict(mod_cv, test_xfw_ii, type  = "raw") 
res_cv <- test_xfw_ii %>% 
  mutate(CalY = predict(mod_cv, ., type = "raw"), Y_Yc = price - CalY, MAPE = Y_Yc / price)
#
ii <- res_cv
summary(ii$Y_Yc)
#
# #Correlation 
cor(ii$price, ii$CalY)
#
# #RMSE: Root Mean Squared Error
sqrt(mean((ii$Y_Yc)^2))
#
# #MAE: Mean Absolute Error (MAE)
mean(abs(ii$Y_Yc))
#
# #MAPE: Mean Absolute Percentage Error
mean(abs(ii$MAPE))
#
# #Accuracy 
1 - mean(abs(ii$MAPE))
```

## Plots

```{r 'B32-Tree-KC-Cp', include=FALSE}
# #rpart() for Tree Model using Tuned cp obtained from Cross-validation
# #Needed to plot this CV model 
mod_tree_cp <- rpart(price ~ ., data = train_xfw_ii, method = 'anova', cp = mod_cv$bestTune)
#
# #Very Long Output 
#summary(mod_tree)
hh <- mod_tree
```

```{r 'B32-Tree-KC-Cp-Set', include=FALSE}
hh <- mod_tree_cp
#
cap_hh <- "B32P03"
ttl_hh <- "KC: Tree by Crossvalidation"
loc_png <- paste0(.z$PX, "B32P03", "-KC-cv", ".png")
```

```{r 'B32P03-Save', include=FALSE, ref.label=c('B32-Tree-rpart')}
#
```

```{r 'B32P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B32P03", "-KC-cv", ".png"))
```

```{r 'B32P0203', echo=FALSE, ref.label=c('B32P02', 'B32P03'), fig.cap="(B32P02, B32P03) KC: Two-part vs 10-part CV Tree"}
#
```

## Submission {.tabset .tabset-fade}

- Complete Code for EDA, Dummies, Partition, Decision Tree, Crossvalidation & Validation on KC House Data

### A {.unlisted .unnumbered}

### Code {.unlisted .unnumbered}

```{r 'B32-Submission-KC', eval=FALSE}
if(FALSE) {# #Data: KC House: Decision Trees: 2022-Feb-13
  #install.packages("dplyr", dependencies = TRUE)
  # #Load Packages (Install Packages if there is any error when loading the packages)
  library("dplyr")
  library("tibble")
  library("readr")
  library("tidyr")
  library("lubridate")
  library("fastDummies")
  library("rpart")
  library("rpart.plot")
  library("rattle")
  library("randomForest")
  library("caret")
  #
  # #Import from Clipboard - KC House Data - [21613 x 21]
  xxKC <- read_delim(clipboard())
  #
  # #Drop NA | Get Age | Dummy | Rename | Filter | Factor | Relevel Most Frequent Reference | 
  # #Drop | Relocate |
  xfw <- xxKC %>% 
    drop_na() %>% 
    #mutate(across(date, as_date)) %>% 
    mutate(sold = year(date), age = sold - yr_built) %>% 
    mutate(iRenew = ifelse(yr_renovated == 0, 0, 1)) %>% 
    rename(beds = bedrooms, baths = bathrooms, sqAbove = sqft_above, sqLot = sqft_lot, 
           iWater = waterfront, iView = view) %>% 
    filter(beds != 33) %>% 
    mutate(across(c(beds, baths, floors, iWater, iView, condition, grade, iRenew), factor)) %>% 
    mutate(across(c(zipcode), factor)) %>% 
    mutate(across(beds, relevel, ref = "3")) %>% 
    mutate(across(baths, relevel, ref = "2")) %>% 
    mutate(across(floors, relevel, ref = "1")) %>% 
    mutate(across(condition, relevel, ref = "3")) %>% 
    mutate(across(grade, relevel, ref = "7")) %>% 
    mutate(across(zipcode, relevel, ref = "98103")) %>% 
    select(-c(id, lat, long, date, sold, yr_renovated, yr_built, sqft_living, sqft_living15, sqft_lot15)) %>% 
    select(-c(zipcode)) %>% 
    relocate(price)
  #
  # #Structure: tibble [21,610 x 13]
  str(xfw)
  #
  # #Partition Data
  #
  # #Set Seed
  set.seed(3)
  #
  # #Create Dummies | Replaced All NA in original data so no replacement in dummy columns |
  dum_xfw <- dummy_cols(xfw, remove_first_dummy = TRUE, remove_selected_columns = TRUE) 
  idx_xfw <- sample.int(n = nrow(dum_xfw), size = floor(0.8 * nrow(dum_xfw)), replace = FALSE)
  train_xfw <- dum_xfw[idx_xfw, ] 
  test_xfw  <- dum_xfw[-idx_xfw, ]
  #
  # #Structure: tibble [17,288 x 50]
  #str(train_xfw)
  dim(train_xfw)
  #
  # #Decision Trees: Convert Dummy Variables to Factor Variables of two levels "0", "1"
  train_xfw_ii <- train_xfw %>% mutate(across(starts_with(c(
    "beds_", "baths_", "floors_", "iWater_", "iView_", "condition_", "grade_", "iRenew_")), factor))
  test_xfw_ii <- test_xfw %>% mutate(across(starts_with(c(
    "beds_", "baths_", "floors_", "iWater_", "iView_", "condition_", "grade_", "iRenew_")), factor))
  #
  # #Decision Tree Model
  mod_tree <- rpart(price ~ ., data = train_xfw_ii, method = 'anova')
  #
  # #Print Model - Long Text so not executed here.
  #mod_tree
  #summary(mod_tree)
  #
  # #Plot Tree
  #fancyRpartPlot(mod_tree, cex = .5)
  #
  # #Predict
  res_tree <- test_xfw_ii %>% 
    mutate(CalY = predict(mod_tree, ., type  = "vector"), Y_Yc = price - CalY, MAPE = Y_Yc / price)
  #
  ii <- res_tree
  summary(ii$Y_Yc)
  #
  # #Correlation 70.2% 
  cor(ii$price, ii$CalY)
  #
  # #RMSE: Root Mean Squared Error
  sqrt(mean((ii$Y_Yc)^2))
  #
  # #MAE: Mean Absolute Error (MAE)
  mean(abs(ii$Y_Yc))
  #
  # #MAPE: Mean Absolute Percentage Error
  mean(abs(ii$MAPE))
  #
  # #Accuracy 63.4%
  1 - mean(abs(ii$MAPE))
  #
  # #Setup Parallel Crossvalidation
  cv <- trainControl(method = "repeatedcv", number = 10, repeats = 5, allowParallel = TRUE)
  #
  # #Decision Tree Model with Crossvalidation
  # #Ignore the Warning message: 
  # #In nominalTrainWorkflow ... : There were missing values in resampled performance measures.
  mod_cv <- suppressWarnings(train(price ~ ., data = train_xfw_ii, method = 'rpart', 
                  trControl = cv, tuneLength = 10))
  #
  # #Print Model
  #mod_cv
  #
  # #Best Tuning Cp value = 0.01111514
  mod_cv$bestTune
  #
  # #Predict
  res_cv <- test_xfw_ii %>% 
    mutate(CalY = predict(mod_cv, ., type  = "raw"), Y_Yc = price - CalY, MAPE = Y_Yc / price)
  #
  ii <- res_cv
  summary(ii$Y_Yc)
  #
  # #Correlation 66.8% (decreased from 70.2 of mod_tree)
  cor(ii$price, ii$CalY)
  #
  # #RMSE: Root Mean Squared Error
  sqrt(mean((ii$Y_Yc)^2))
  #
  # #MAE: Mean Absolute Error (MAE)
  mean(abs(ii$Y_Yc))
  #
  # #MAPE: Mean Absolute Percentage Error
  mean(abs(ii$MAPE))
  #
  # #Accuracy 63.2% (decreased from 63.4% of mod_tree)
  1 - mean(abs(ii$MAPE))
  #
  # #Random Forest 
  if(FALSE) {# Random Forest is Horribly Slow. Run this code only if you hate your computer.
  # #NOTE: Following Parameter values are not too slow and explain 68.8% Var
    mod_frst <- randomForest(price ~ ., data = train_xfw_ii, ntrees = 500, sampsize = 5000, mtry = 8, nodesize = 10)
    print(mod_frst)
  #Call:
  # randomForest(formula = price ~ ., data = train_xfw_ii, ntrees = 500,      sampsize = 5000, mtry = 8, nodesize = 10) 
  #               Type of random forest: regression
  #                     Number of trees: 500
  #No. of variables tried at each split: 8
  #
  #          Mean of squared residuals: 41840392284
  #                    % Var explained: 68.85
  }
  #
  if(FALSE) {# Execute this code if you are trying to run the model on Test and following ERROR is observed
  # #ERROR: Type of predictors in new data do not match that of the training data.
    common <- intersect(names(train_xfw_ii), names(test_xfw_ii)) 
    for (p in common) { 
      if (class(train_xfw_ii[[p]]) == "factor") { 
        levels(test_xfw_ii[[p]]) <- levels(train_xfw_ii[[p]]) 
      } 
    }
  }
  #
  # #Accuracy of Random Forest
  if(FALSE){# Run This if mod_frst has been created
    res_frst <- test_xfw_ii %>% 
    mutate(CalY = predict(mod_frst, ., type  = "response"), Y_Yc = price - CalY, MAPE = Y_Yc / price)
  #
  summary(res_frst$Y_Yc)
  sqrt(mean((res_frst$Y_Yc)^2)) # 211970.5
  mean(abs(res_frst$Y_Yc))      # 127049.8
  mean(abs(res_frst$MAPE))      # 0.2690059
  #
  # #Accuracy 73.2% (increased from 63.4% of mod_tree and 63.2% of mod_cv)
  1 - mean(abs(res_frst$MAPE))  # 0.7309941
  }
  #
  if(FALSE) {# #Variable Importance
    randomForest::importance(mod_frst)
    varImpPlot(mod_frst, sort = TRUE, n.var = 20, main = 'Top 20 Variables')
    # how many times these variables are used in building the trees
    #varUsed(mod_frst) 
  }
  #
  # #Model Performance on Test Data
  # #mod_tree : 63.4%
  # #mod_cv   : 63.2%
  # #mod_frst : 73.1%
}
```

## Postponed 

### Random forest

```{r 'B32-Forest', eval=FALSE}
# Random Forest is Horribly Slow. Keep it in separate chunk.
# #NOTE: Following Parameter values are not too slow and explain 68.8% Var
mod_frst <- randomForest(price ~ ., data = train_xfw_ii, 
                         ntrees = 500, sampsize = 5000, mtry = 8, nodesize = 10)
```

```{r 'B32-Print-Forest', eval=FALSE}
print(mod_frst)
```

### Error Rate

```{r 'B32-ErrorRate', eval=FALSE}
# #Plot to see Error Rates
plot(mod_frst)
```

### Tuning

- Parameter Tuning
  - This is for identifying optimal paraemets (mtree, and mtry)
  - First specify the independent variables (x) and dependent variables (y)
  - stepFactor: each iteration 'mtry' is inflated or deflated with the mentioned factor
  - plot: we need a plot to identify mtry or not (whether to plot out of bag error as a function of mtry)
  - ntreeTry: identified number of trees
  - trace allows to insert debugging code
  - improve: the relative improvement in OBB error must be by this much for the search to continue
  - Use extra = 'auto' or appropriate code for extra information

```{conjecture 'randomForest-length'}
\textcolor{brown}{Error in randomForest.default : length of response must be the same as predictors}
```

- Y needs to be vector. A tibble subsetted normally returns a tibble (even for single column) (unlike dataframe)

```{r 'B32-TuneModel', eval=FALSE}
tuneRF(x = data_train_ii[, -1], y = data_train_ii$price, stepFactor = 2, plot = TRUE, 
       ntreeTry = 150, improve = .05)
```

### Revised

- Use the tuned valued identified for revised model

```{r 'B32-Forest-Tuned', eval=FALSE}
# Random Forest is Horribly Slow. Keep it in separate chunk.
# #NOTE: ntree = 150
mod_frst_tuned <- randomForest(price ~ ., data = train_xfw_ii, 
                         ntree = 500, sampsize = 5000, mtry = 8, nodesize = 10)
```

```{r 'B32-Print-Forest-A', eval=FALSE}
print(mod_frst_tuned)
```

### Prediction

- Accuracy 56% - Improved on previous

```{r 'B32-Forest-Predict', eval=FALSE}
# #Model Validation
res_frst_tuned <- test_xfw_ii %>% 
  mutate(CalY = predict(mod_frst_tuned, ., type  = "response"), 
         Y_Yc = price - CalY, MAPE = Y_Yc / price)
#
ii <- res_frst_tuned
summary(ii$Y_Yc)
#
# #Correlation 
cor(ii$price, ii$CalY)
#
# #RMSE: Root Mean Squared Error
sqrt(mean((ii$Y_Yc)^2))
#
# #MAE: Mean Absolute Error (MAE)
mean(abs(ii$Y_Yc))
#
# #MAPE: Mean Absolute Percentage Error
mean(abs(ii$MAPE))
#
# #Accuracy 
1 - mean(abs(ii$MAPE))
```

### Importance

```{r 'B32-VarImp', eval=FALSE}
# #Variable Importance
randomForest::importance(mod_frst)
varImpPlot(mod_frst, sort = TRUE, n.var = 20, main = 'Top 20 Variables')
# how many times these variables are used in building the trees
#varUsed(mod_frst) 
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B32-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, cap_hh, cv, dum_xfw, hh, idx_xfw, idx_xsyw, loc_png, mod_cv, 
          mod_prune, mod_tree, mod_tree_cp, res_cv, res_prune, res_tree, test_xfw, test_xfw_ii, 
          train_xfw, train_xfw_ii, ttl_hh, xfw, xnw, xxB26CarDekho, xxB30KC, zfw, znw)
```

```{r 'B32-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/232-Trees.Rmd-->

# Decision Tree Algorithm (B33, Feb-20) {#b33}

```{r 'B33', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

> "Incomplete"

## Packages

```{r 'B33-Installations', include=FALSE, eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("phangorn", dependencies = TRUE)
  install.packages("ape", dependencies = TRUE)
  install.packages("ROSE", dependencies = TRUE)
}
```


## Review {.tabset .tabset-fade}

- \textcolor{pink}{Question:} In regression, we had to check for normality and multicollinearity (VIF). The KC House data was not normal, so did semi-log transformation and then log-log transformation. Does that not affect model performance in Decision Trees
  - Tree does not assume Normality so there is no requirement of transformation
  - (Aside) 
    - Interaction effect is already assumed in Trees
    - Probably outliers would have low cp value and would be dropped 
    - But some outliers are actually important observations
    - If those have more than 3 z-value, model might drop them

- $\text{\{full, overfit\} } 0 \leq \text{Complexity Parameter (cp)} \leq 1 \text{\{pruned, underfit\}}$
  - A Node would be split if the information gain is more than the cp value, otherwise it would remain as leaf node.

- \textcolor{pink}{Question:} At each split why the split is happening with one variable. Why not more than one variable at same time OR why not split by some other variable 
  - Split is sequential and recursive
  - (Aside)
    - Sequential (step by step, not in one step) : At each step, all variables are evaluated for split and the one with maximum reduction in Gini is selected as the best candidate
    - Recursive : At each subsequent step, again, all variables are evaluated for split and the one with maximum reduction in Gini is selected as the best candidate 
  - (Clarification) If the split happened on variable 'Age', Is it ('Age') is avaiable for further splits in susequent steps 
    - Yes (If the 'cp' criteria allows it)
  - (Clarification) Why the split is happening by 'Age' and not by 'Transmission'
    - On that step 'Age' provides maximum information gain (maximum reduction in Gini) so the split happens on this variable.

### Data {.unlisted .unnumbered}

- [Import Data CarDekho - B26](#set-cardekho-b26 "b26")
- "ForLater" - train() does not take method = 'anova' or 'class' for 'rpart'. (Probably) it handles them based on class of Y but How to perform both explicitly. 

### Previous Lecture Code {.unlisted .unnumbered}

```{r 'B33-GetCarDekho', ref.label=c('B26-GetCarDekho', 'B26-PrepCar', 'B31-Dummies-Car')}
# #xxB26CarDekho, aa, bb, xsyw, xw,xsw, zfw, xnw, znw, 
# #train_xfw, test_xfw, train_xfw_ii, test_xfw_ii
```

```{r 'B33-GetCarDekhoKnit', include=FALSE, eval=TRUE}
xxB26CarDekho <- f_getRDS(xxB26CarDekho)
aa <- xxB26CarDekho
bb <- aa %>% 
  separate(name, c("brand", NA), sep = " ", remove = FALSE, extra = "drop") %>% 
  filter(fuel != "Electric") %>% 
  #mutate(across(where(is.character), factor)) %>% 
  mutate(across(fuel, factor, levels = c("Diesel", "Petrol", "CNG", "LPG"))) %>% 
  mutate(across(transmission, factor, levels = c("Manual", "Automatic"), 
                labels = c("Manual", "Auto"))) %>% 
  mutate(across(owner, factor, 
levels = c("First Owner", "Second Owner", "Third Owner", "Fourth & Above Owner", "Test Drive Car"), 
labels = c("I", "II", "III", "More", "Test"))) %>% 
  mutate(across(seller_type, factor, levels = c("Individual", "Dealer", "Trustmark Dealer"), 
                labels = c("Indiv", "Dealer", "mDealer"))) %>% 
  rename(price = selling_price, km = km_driven, 
         s = seller_type, o = owner, t = transmission, f = fuel) %>% 
  mutate(age = 2022 - year) %>% 
  select(-c(year, name, brand))
# 
xfw <- bb
zfw <- xfw %>% mutate(across(where(is.numeric), ~ as.vector(scale(.)))) 
xnw <- xfw %>% select(where(is.numeric))
znw <- zfw %>% select(where(is.numeric))
#
# #Dummy | Drop First Level i.e. Reference | Drop Selected Columns i.e. Original |
dum_xfw <- xfw %>% dummy_cols(.data = ., 
                  select_columns = c("f", "s", "t", "o"), 
                  remove_first_dummy = TRUE, remove_selected_columns = TRUE)
#
# #Partition Data
set.seed(3)
idx_xfw <- sample.int(n = nrow(dum_xfw), size = floor(0.8 * nrow(dum_xfw)), replace = FALSE)
train_xfw <- dum_xfw[idx_xfw, ]
test_xfw  <- dum_xfw[-idx_xfw, ]
#
# #Decision Trees: Convert Dummy Variables to Factor Variables of two levels "0", "1"
train_xfw_ii <- train_xfw %>% mutate(across(starts_with(c("s_", "f_", "t_", "o_")), factor))
test_xfw_ii  <- test_xfw  %>% mutate(across(starts_with(c("s_", "f_", "t_", "o_")), factor))
```

```{r 'B33-ModelControl'}
cv <- trainControl(method = "repeatedcv", number = 10, repeats = 5, allowParallel = TRUE)
```

### Trees {.unlisted .unnumbered}

```{r 'B33-CV-Car'}
set.seed(3)
# #Basic Regression Tree using Best Tuning Cp Value
mod_tree_anv <- rpart(price ~ ., data = train_xfw_ii, method = 'anova', cp = 0.008528435)
#
# #Formula Interface (Does not match with Basic Regression Tree because of Factors)
mod_cv_form <- suppressWarnings(train(price ~ ., data = train_xfw_ii, 
                                      method = 'rpart', trControl = cv, tuneLength = 10))
#
# #With Non-Formula Interface: Use this if Factors are Present
mod_cv_nonf <- suppressWarnings(train(y = train_xfw_ii$price, 
                                      x = as.data.frame(train_xfw_ii[ , -1]), 
                                      method = 'rpart', trControl = cv, tuneLength = 10))
#
# #Using Formula Interface even with Factors present needs formula() & model.frame()
formula_xfw <- formula(price ~ .)
mod_cv_frame <- suppressWarnings(train(y = model.frame(formula_xfw, train_xfw_ii)[ , 1], 
                                       x = model.frame(formula_xfw, train_xfw_ii)[ , -1], 
                                       method = 'rpart', trControl = cv, tuneLength = 10))
# #Best Tuning Cp value
if(FALSE) mod_cv_frame$bestTune
#
# #Best Model
if(FALSE) mod_cv_frame$finalModel
```

### Simple {.unlisted .unnumbered}

```{r 'B33-PrintTree-Simple-1'}
# #Printing Tree
ii <- as.party(mod_tree_anv)
if(TRUE) length(ii)
if(TRUE) width(ii)
if(TRUE) depth(ii)
if(TRUE) ii
```

### CrossValidated {.unlisted .unnumbered}

```{r 'B33-PrintTree-CV'}
# #Printing Tree
ii <- as.party(mod_cv_frame$finalModel)
if(TRUE) length(ii)
if(TRUE) width(ii)
if(TRUE) depth(ii)
if(TRUE) ii
```

### Factor Problem {.unlisted .unnumbered}

- The problem is that the already converted dummy variables have been further treated as factors with levels and recoded again
  - t_Auto1 itself is a problem and then it is being treated as continuous i.e. split is at 0.5

```{r 'B33-PrintTree-Simple-2'}
# #Printing Tree
ii <- as.party(mod_cv_form$finalModel)
if(TRUE) length(ii)
if(TRUE) width(ii)
if(TRUE) depth(ii)
if(TRUE) ii
```

### Non-Formula {.unlisted .unnumbered}

```{r 'B33-PrintTree-Simple-3'}
# #Printing Tree
ii <- as.party(mod_cv_nonf$finalModel)
if(TRUE) length(ii)
if(TRUE) width(ii)
if(TRUE) depth(ii)
if(TRUE) ii
```



## x

```{r 'B33-Predict-cv-Car', eval=FALSE}
# #Model Validation
# #Class of Both Models are different. rpart() require "vector" but "raw" is used with train()
class(mod_tree)
class(mod_cv)
#
#res_tree <- predict(mod_cv, test_xfw_ii, type  = "raw") 
res_cv <- test_xfw_ii %>% 
  mutate(CalY = predict(mod_cv, ., type = "raw"), Y_Yc = price - CalY, MAPE = Y_Yc / price)
#
ii <- res_cv
summary(ii$Y_Yc)
#
# #RMSE: Root Mean Squared Error
sqrt(mean((ii$Y_Yc)^2))
#
# #MAE: Mean Absolute Error (MAE)
mean(abs(ii$Y_Yc))
#
# #MAPE: Mean Absolute Percentage Error
mean(abs(ii$MAPE))
#
# #Accuracy 
1 - mean(abs(ii$MAPE))
```

## WIP

## Plots

```{r 'B33-PrintTree', include=TRUE, eval=FALSE}
# #Printing Tree
ii <- as.party(hh)
length(ii)
width(ii)
depth(ii)
ii
```

```{r 'B33-Tree-rpart', include=TRUE, eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh <- mod_tree
#
if(!file.exists(loc_png)|TRUE) {#xxxx
  png(filename = loc_png) 
  dev.control('enable') 
  rpart.plot(hh, extra = 'auto')
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B31 <- recordPlot()
  dev.off()
  assign(cap_hh, B31)
  rm(B31)
}
eval(parse(text = cap_hh))
```


```{r 'B33-Tree-KC-Cp-Set', include=FALSE, eval=FALSE}
# #3 models here
#mod_tree, mod_tree_cp, mod_cv
hh <- mod_tree_cp
#
cap_hh <- "B33P01"
ttl_hh <- "Car: Cp"
loc_png <- paste0(.z$PX, "B33P01", "-Car-cp", ".png")
```

```{r 'B33P03-Save', include=FALSE, ref.label=c('B33-Tree-rpart'), eval=FALSE}
#
```

## Ensemble Methods

- Collection of Models

## Bagging

## Random Forest

```{r 'B33-Forest', eval=FALSE}
# Random Forest is Horribly Slow. Keep it in separate chunk.
# #ntrees = 100, sampsize = 1000, mtry = 4, nodesize = 10 : Var 61.17
# #ntrees = 200, sampsize = 1000, mtry = 4, nodesize = 10 : Var 61.07
# #ntrees = 100, sampsize = 1000, mtry = 8, nodesize = 10 : Var 62.33
# #ntrees = 200, sampsize = 1000, mtry = 8, nodesize = 10 : Var 62.64
# #ntrees = 500, sampsize = 1000, mtry = 8, nodesize = 10 : Var 62.3
# #ntrees = 1000, sampsize = 1000, mtry = 8, nodesize = 10: Var 62.37
# #ntrees = 500, sampsize = 1000, mtry = 10, nodesize = 10: Var 62.37
# #ntrees = 500, sampsize = 1000, mtry = 10, nodesize = 5: 63.52
# #ntrees = 500, sampsize = 1000, mtry = 10, nodesize = 3: 64.39
# #ntrees = 500, sampsize = 1000, mtry = 10, nodesize = 2: 64.53
# #ntrees = 500, sampsize = 2000, mtry = 10, nodesize = 2: 64.88
# #ntrees = 500, sampsize = 3000, mtry = 10, nodesize = 2: 64.41
mod_frst <- randomForest(price ~ ., data = train_xfw_ii, 
                         ntrees = 500, sampsize = 3000, mtry = 10, nodesize = 2)
```

```{r 'B33-Print-Forest', eval=FALSE}
print(mod_frst)
```

## Tune RF

- step factor : how many features it should step through

```{r 'B33-TuneModel', eval=FALSE}
# 1: 4
# 2: 4, 8, 13
# 3: 2, 4, 12
tuneRF(x = train_xfw_ii[, -1], y = train_xfw_ii$price, stepFactor = 10, plot = TRUE, trace = TRUE, 
       ntreeTry = 10, improve = .05)
```



## x



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B33-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, cv, dum_xfw, formula_xfw, idx_xfw, mod_cv_form, mod_cv_frame,
          mod_cv_nonf, mod_tree_anv, test_xfw, test_xfw_ii, train_xfw, train_xfw_ii, xfw, xnw,
          xxB26CarDekho, zfw, znw)
```

```{r 'B33-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/233-Trees.Rmd-->

# WIP (B34) {#b34 .unlisted .unnumbered}

```{r 'B34', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

> "Incomplete"

## Overview

- Classification

## Logistic Regression

## Data: Bank {#set-bank-b34 .tabset .tabset-fade}

\textcolor{pink}{Please import the "B34-bank-full.csv"}

```{r 'B34-Bank', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum tools::md5sum(paste0(.z$XL, "B34-bank-full.csv"))
#xxB34Bank <- f_getObject("xxB34Bank", "B34-bank-full.csv",
                                #"e6b0ca77f3f200ec5428e04dd104da53")
loc_src <- paste0(.z$XL, "B34-bank-full", ".csv")
loc_rds <- paste0(.z$XL, "xxB34Bank", ".rds")
tbl <- read_delim(file = paste0(.z$XL, "B34-bank-full.csv"), delim = ";")
attr(tbl, "spec") <- NULL
attr(tbl, "problems") <- NULL
saveRDS(tbl, loc_rds)
```

```{r 'B34-GetBank', include=FALSE, eval=FALSE}
xxB34Bank <- f_getRDS(xxB34Bank)
```

### EDA {.unlisted .unnumbered}


```{r 'B34-T1', eval=FALSE}
aa <- xxB34Bank
#
bb <- aa %>% 
  mutate(across(where(is.character) & !c(y), factor)) %>% 
  relocate(y) %>% 
  select(-c(job, month))
#
xfw <- bb
```


```{r 'B34-T2', eval=FALSE}
# #Partition Data
set.seed(3)
# #Create Dummies | Replaced All NA in original data so no replacement in dummy columns |
#dum_xfw <- dummy_cols(xfw, 
#  select_columns = c("marital", "education", "default", "housing", "loan", "contact", "poutcome"), 
#                      remove_first_dummy = TRUE, remove_selected_columns = TRUE) 
dum_xfw <- dummy_cols(xfw, 
                      remove_first_dummy = TRUE, remove_selected_columns = TRUE) 
idx_xsyw <- sample.int(n = nrow(dum_xfw), size = floor(0.8 * nrow(dum_xfw)), replace = FALSE)
train_xfw <- dum_xfw[idx_xsyw, ] 
test_xfw  <- dum_xfw[-idx_xsyw, ]
```


### Structure {.unlisted .unnumbered}


```{r 'B34-Temp-Bank', include=TRUE, eval=FALSE}
# #Count NA 
if(FALSE) colSums(is.na(aa)) %>% as_tibble(rownames = "Cols") %>% filter(value > 0)
#
if(FALSE) bb %>% select(isFemale) %>% slice(1:10)
if(FALSE) cat('"', paste0(levels(bb$Arrival), collapse = '", "'), '"\n', sep = '')
if(FALSE) bb %>% filter(is.na(bpl)) %>% select(id, bph, bpl)
if(FALSE) bb %>% select(isImplant, imp_c) %>% summary()
if(FALSE) bb %>% select(iFemale, iMarried, iEmergency, iImplant) %>% 
  pivot_longer(everything()) %>% 
  count(name, value) %>% 
  pivot_wider(names_from = value, values_from = n) 
# #Count Unique of all Columns to decide which should be Factors
if(FALSE) bb %>% summarise(across(everything(), ~ length(unique(.)))) %>% pivot_longer(everything())
# #Summary of Columns of a class: is.factor is.numeric is.character
if(TRUE) bb %>% select(!where(is.factor)) %>% summary()
# #Names and Indices of Columns of class: is.factor is.numeric is.character
if(FALSE) which(sapply(bb, is.factor))
# #Comma separated string having each item within quotes for easy pasting as character not objects
if(FALSE) cat('"', paste0(names(which(sapply(bb, is.factor))), collapse = '", "'), '"\n', sep = '')
# #Levels of Factor Columns
if(FALSE) lapply(bb[c(3, 6:9, 15)], levels)
# #Frequency of Each level of Factor
if(FALSE) bb %>% count(y) %>% arrange(desc(n))
```

```{r 'B34-Structure-Bank', eval=FALSE}
str(bb)
```

### Summary {.unlisted .unnumbered}

```{r 'B34-Summary-Bank', eval=FALSE}
summary(bb)
```


## PERFORM LOGISTIC REGRESSION


```{r 'B34-T3', eval=FALSE}
mod_xfw_glm <- glm(y_yes ~ ., family = binomial, data = train_xfw)
```


```{r 'B34-T4', eval=FALSE}
#summary(mod_xfw_glm)
stargazer(mod_xfw_glm, type = "text", single.row = TRUE, intercept.bottom = FALSE)
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B34-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'B34-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z2LectureNotes/234-WIP.Rmd-->

# Data and Statistics {#c01}
> Definitions and Exercises are from the Book [@Anderson]

```{r 'C01', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
invisible(lapply(f_getPathR(A07getObject), knitr::read_chunk))
```

## Overview

- This chapter covers introductory information and basic definitions.
  - [Statistical Inference](#stat-inference-c01 "c01"), [Population vs Sample](#population-sample-c01 "c01")
  - [Scales of Measurement](#scales-c01 "c01")
  - [Exercises](#exercises-c01 "c01"): Mean, Percentage, Transpose, Rowwise Calculations
  - Graphs: [Multiple Time Series](#cars-c01 "c01"), [Pareto Chart](#cars-c01 "c01")

## Introduction

```{definition 'Data'}
\textcolor{pink}{Data} are the facts and figures collected, analysed, and summarised for presentation and interpretation.
```

```{definition 'Elements'}
\textcolor{pink}{Elements} are the entities on which data are collected. (Generally ROWS)
```

```{definition 'Variable'}
A \textcolor{pink}{variable} is a characteristic of interest for the elements. (Generally COLUMNS)
```

```{definition 'Observation'}
The set of measurements obtained for a particular element is called an \textcolor{pink}{observation}.
```

Hence, the total number of data items can be determined by multiplying the number of observations by the number of variables.

```{definition 'Statistics'}
\textcolor{pink}{Statistics} is the art and science of collecting, analysing, presenting, and interpreting data. 
```

## Scales of Measurement {#scales-c01}

Data collection requires one of the following scales of measurement: nominal, ordinal, interval, or ratio. 
```{definition 'Scale-of-Measurement'}
The \textcolor{pink}{scale of measurement} determines the amount of information contained in the data and indicates the most appropriate data summarization and statistical analyses.  
```

```{definition 'Nominal-Scale'}
When the data for a variable consist of labels or names used to identify an attribute of the element, the scale of measurement is considered a \textcolor{pink}{nominal scale}.
```

For Example, Gender as Male and Female. In cases where the scale of measurement is nominal, a numerical code as well as a nonnumerical label may be used. For example, 1 denotes Male, 2 denotes Female. The scale of measurement is nominal even though the data appear as numerical values. Only \textcolor{pink}{Mode} can be calculated.

```{definition 'Ordinal-Scale'}
The scale of measurement for a variable is considered an \textcolor{pink}{ordinal scale} if the data exhibit the properties of nominal data and in addition, the order or rank of the data is meaningful.
```

For example, Size as small, medium, large. Along with the labels, similar to nominal data, the data can also be ranked or ordered, which makes the measurement scale ordinal. Ordinal data can also be recorded by a numerical code. \textcolor{pink}{Median} can be calculated but not the Mean.

```{definition 'Interval-Scale'}
The scale of measurement for a variable is an \textcolor{pink}{interval scale} if the data have all the properties of ordinal data and the interval between values is expressed in terms of a fixed unit of measure.
```

Interval data are always numerical. These can be ranked or ordered like ordinal. In addition, the differences between them are meaningful.   

```{definition 'Ratio-Scale'}
The scale of measurement for a variable is a \textcolor{pink}{ratio scale} if the data have all the properties of interval data and the ratio of two values is meaningful.
```

Variables such as distance, height, weight, and time use the ratio scale of measurement. This scale requires that a zero value be included to indicate that nothing exists for the variable at the zero point. \textcolor{pink}{Mean} can be calculated.

For example, consider the cost of an automobile. A zero value for the cost would indicate that the automobile has no cost and is free. In addition, if we compare the cost of 30,000 dollars for one automobile to the cost of 15,000 dollars for a second automobile, the ratio property shows that the first automobile is 30000/15000 = 2 times, or twice, the cost of the second automobile. 

See Table \@ref(tab:C01V01) for more details.

### Interval scale vs. Ratio scale

Interval scale is a measure of continuous quantitative data that has an arbitrary 0 reference point. This is contrasted with ratio scaled data which have a non-arbitrary 0 reference point. Ex: When we look at "profit" we see that negative profit does make sense to us. So while, the 0 for "profit" is meaningful (just like temperature measurements of Celsius) it is arbitrary. Therefore, it is Interval scale of measurement.

In an interval scale, you can take difference of two values. You may not be able to take ratios of two values. Ex: Temperature in Celsius. You can say that if temperatures of two places are 40 °C and 20 °C, then one is hotter than the other (taking difference). But you cannot say that first is twice as hot as the second (not allowed to take ratio).

In a ratio scale, you can take a ratio of two values. Ex: 40 kg is twice as heavy as 20 kg (taking ratios).

Also, "0" on ratio scale means the absence of that physical quantity. "0" on interval scale does not mean the same. 0 kg means the absence of weight. 0 °C does not mean absence of heat.

Table: (\#tab:C01V01) (C01V01) Interval scale Vs Ratio scale

| Features | Interval scale | Ratio scale |
| :--- | :--- | :---  | 
| Variable property | Addition and subtraction | Multiplication and Division i.e. calculate ratios. Thus, you can leverage numbers on the scale against 0. |
| Absolute Point Zero | Zero-point in an interval scale is arbitrary. For example, the temperature can be below 0 °C and into negative temperatures. | The ratio scale has an absolute zero or character of origin. Height and weight cannot be zero or below zero. |
| Calculation | Statistically, in an interval scale, the Arithmetic Mean is calculated. Statistical dispersion permits range and standard deviation. The coefficient of variation is not permitted. | Statistically, in a ratio scale, the Geometric or Harmonic mean is calculated. Also, range and coefficient of variation are permitted for measuring statistical dispersion. |
| Measurement | Interval scale can measure size and magnitude as multiple factors of a defined unit. | Ratio scale can measure size and magnitude as a factor of one defined unit in terms of another. |
| Example | Temperature in Celsius, Calendar years and time, Profit | These possesses an absolute zero characteristic, like age, weight, height, or Sales |

## Categorical and Quantitative Data

```{definition 'Categorical-Data'}
Data that can be grouped by specific categories are referred to as \textcolor{pink}{categorical data}. Categorical data use either the nominal or ordinal scale of measurement.  
```

```{definition 'Quantitative-Data'}
Data that use numeric values to indicate 'how much' or 'how many' are referred to as \textcolor{pink}{quantitative data}. Quantitative data are obtained using either the interval or ratio scale of measurement.
```

If the variable is categorical, the statistical analysis is limited. We can summarize categorical data by counting the number of observations in each category or by computing the proportion of the observations in each category. However, even when the categorical data are identified by a numerical code, arithmetic operations do not provide meaningful results.  

Arithmetic operations provide meaningful results for quantitative variables. For example, quantitative data may be added and then divided by the number of observations to compute the average value.  

Quantitative data may be discrete or continuous. 

```{definition 'Discrete'}
Quantitative data that measure 'how many' are \textcolor{pink}{discrete}. 
```

```{definition 'Continuous'}
Quantitative data that measure 'how much' are \textcolor{pink}{continuous} because no separation occurs between the possible data values.
```

## Cross-Sectional and Time Series Data

```{definition 'Cross-Sectional-Data'}
\textcolor{pink}{Cross-sectional data} are data collected at the same or approximately the same point in time.  
```

```{definition 'Time-Series-Data'}
\textcolor{pink}{Time-series data} data are data collected over several time periods. 
```

## Observational Study and Experiment

```{definition 'Observational-Study'}
In an \textcolor{pink}{observational study} we simply observe what is happening in a particular situation, record data on one or more variables of interest, and conduct a statistical analysis of the resulting data.  
```

```{definition 'Experiment'}
The key difference between an observational study and an \textcolor{pink}{experiment} is that an experiment is conducted under controlled conditions.
```

As a result, the data obtained from a well-designed experiment can often provide more information as compared to the data obtained from existing sources or by conducting an observational study.  

## Caution

1. Time and Cost - The cost of data acquisition and the subsequent statistical analysis should not exceed the savings generated by using the information to make a better decision.  
1. Data Acquisition Errors - An error in data acquisition occurs whenever the data value obtained is not equal to the true or actual value that would be obtained with a correct procedure. Ex: recording error, misinterpretation etc. Blindly using any data that happen to be available or using data that were acquired with little care can result in misleading information and bad decisions. 

## Descriptive Statistics

```{definition 'Descriptive-Statistics'}
Most of the statistical information is summarized and presented in a form that is easy to understand. Such summaries of data, which may be tabular, graphical, or numerical, are referred to as \textcolor{pink}{descriptive statistics}.  
```

## Population and Sample 

```{definition 'Population'}
A \textcolor{pink}{population} is the set of all elements of interest in a particular study.  
```

```{definition 'Sample'}
A \textcolor{pink}{sample} is a subset of the population. 
```

```{definition 'Parameter-vs-Statistic'}
The measurable quality or characteristic is called a \textcolor{pink}{Population Parameter} if it is computed from the population. It is called a \textcolor{pink}{Sample Statistic} if it is computed from a sample.
```

Refer [Sample](#sample-c07 "c07") For More ... 

## Difference between a population and a sample {#population-sample-c01}

The population is the set of entities under study. 

- For example, the mean height of men. (Population "men", parameter of interest "height")
  - We choose the population that we wish to study. 
  - Typically it is impossible to survey/measure the entire population because not all members are observable. 
  - If it is possible to enumerate the entire population it is often costly to do so and would take a great deal of time. 

Instead, we could take a subset of this population called a sample and use this sample to draw inferences about the population under study, given some conditions. 

- It is an inference because there will be some uncertainty and inaccuracy involved in drawing conclusions about the population based upon a sample.
  - In Simple Random Sampling (SRS) each member of the population has an equal probability of being included in the sample, hence the term "random". There are many other sampling methods e.g. stratified sampling, cluster sampling, etc.

## Statistical Inference {#stat-inference-c01}

```{definition 'Census'}
The process of conducting a survey to collect data for the entire population is called a \textcolor{pink}{census}. 
```

```{definition 'Sample-Survey'}
The process of conducting a survey to collect data for a sample is called a \textcolor{pink}{sample survey}. 
```

```{definition 'Statistical-Inference'}
Statistics uses data from a sample to make estimates and test hypotheses about the characteristics of a population through a process referred to as \textcolor{pink}{statistical inference}.  
```

Whenever statisticians use a sample to estimate a population characteristic of interest, they usually provide a statement of the quality, or precision, associated with the estimate.  

Inferential statistics are used for Hypothesis Testing. 

- It is often used to compare the differences between the treatment groups.
- It uses measurements from the sample of subjects in the experiment to compare the treatment groups and make generalizations about the larger population of subjects.
- Most inferential statistics are based on the principle that a test-statistic value is calculated on the basis of a particular formula. 
  - That value along with the degrees of freedom, and the rejection criteria are used to determine whether differences exist between the treatment groups. 
  - The larger the sample size, the more likely a statistic is to indicate that differences exist between the treatment groups.


The two most common types of Statistical Inference are -

1. Confidence Intervals
    - To estimate a population parameter
1. Test of Significance
    - To assess the evidence provided by data about some claim concerning a population
    - i.e. To compare observed data with a claim (Hypothesis)
    - The results of a significance test are expressed in terms of a probability that measures how well the data and the claim agree

Reasoning for Tests of Significance

- Example: Is the sample mean ${\overline{x}}$ \textcolor{pink}{significantly different }from population mean ${\mu}$
- To determine if two numbers are significantly different, a statistical test must be conducted to provide evidence

## Analytics

```{definition 'Analytics'}
\textcolor{pink}{Analytics} is the scientific process of transforming data into insight for making better decisions.
```

Analytics is used for data-driven or fact-based decision making, which is often seen as more objective than alternative approaches to decision making. The tools of analytics can aid decision making by creating insights from data, improving our ability to more accurately forecast for planning, helping us quantify risk, and yielding better alternatives through analysis.  
Analytics is now generally thought to comprise three broad categories of techniques. These categories are descriptive analytics, predictive analytics, and prescriptive analytics.  

```{definition 'Descriptive-Analytics'}
\textcolor{pink}{Descriptive analytics} encompasses the set of analytical techniques that describe what has happened in the past. 
```

Examples of these types of techniques are data queries, reports, descriptive statistics, data visualization, data dash boards, and basic what-if spreadsheet models.  

```{definition 'Predictive-Analytics-301'}
\textcolor{pink}{Predictive analytics} consists of analytical techniques that use models constructed from past data to predict the future or to assess the impact of one variable on another.
```

Linear regression, time series analysis, and forecasting models fall into the category of predictive analytics. \textcolor{pink}{Simulation}, which is the use of probability and statistical computer models to better understand risk, also falls under the category of predictive analytics.

Prescriptive analytics differs greatly from descriptive or predictive analytics. What distinguishes prescriptive analytics is that prescriptive models yield a best course of action to take. That is, the output of a prescriptive model is a best decision. 

```{definition 'Prescriptive-Analytics'}
\textcolor{pink}{Prescriptive analytics} is the set of analytical techniques that yield a best course of action. 
```

Optimization models, which generate solutions that maximize or minimize some objective subject to a set of constraints, fall into the category of prescriptive models. 

## Big Data and Data Mining

```{definition 'Big-Data'}
Larger and more complex data sets are now often referred to as \textcolor{pink}{big data}. 
```

\textcolor{pink}{Volume} refers to the amount of available data; \textcolor{pink}{velocity} refers to the speed at which data is collected and processed; and \textcolor{pink}{variety} refers to the different data types.
The term \textcolor{pink}{data warehousing} is used to refer to the process of capturing, storing, and maintaining the data.

```{definition 'Data-Mining-301'}
\textcolor{pink}{Data Mining} deals with methods for developing useful decision-making information from large databases. It can be defined as the automated extraction of predictive information from (large) databases.
```

Data mining relies heavily on statistical methodology such as multiple regression, logistic regression, and correlation. 

## Exercises {#exercises-c01}

- Table: \@ref(tab:C01T02) 
- Table: \@ref(tab:C01T03)  
- Table: \@ref(tab:C01T04)  
  - Who appears to be the market share leader and how the market shares are changing over time
    - \textcolor{orange}{Caution:} Trend Analysis should be done by linear regression with `cor(), lm()` etc.

### R {.unlisted .unnumbered .tabset .tabset-fade}

#### Load Data {.unlisted .unnumbered}

```{r 'C01-Load'}
xxComputers <- f_getObject("xxComputers", "C01-Computers.csv", "971fb6096e4f71e8185d3327a9033a10")
xxCordless <- f_getObject("xxCordless", "C01-Cordless.csv", "9991f612fe44f1c890440bd238084679")
```

#### f_getObject() {.unlisted .unnumbered}
```{r 'C01F-getObject', ref.label=c('A07-getObject'), eval=FALSE}
# #
```

#### Print Table {.unlisted .unnumbered}

```{r 'C01T01-KBL', eval=FALSE}
bb <- xxComputers
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
# #Kable Table
kbl(bb,
  caption = "(C01T01)",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```

#### Transpose Tibble {.unlisted .unnumbered}

```{r 'C01-L-Assignment'}
bb <- tibble(Company = c("Hertz", "Dollar", "Avis"), 
              `2007` = c(327, 167, 204), `2008` = c(311, 140, 220),
              `2009` = c(286, 106, 300), `2010` = c(290, 108, 270))
```

```{r 'C01-L-Long'}
# #Transpose Tibble: Note that the First Column Header is lost after Transpose
# #Longer
bb %>% pivot_longer(!Company, names_to = "Year", values_to = "Values")
```

```{r 'C01-L-Transpose'}
# #Transpose
(ii <- bb %>% 
  pivot_longer(!Company, names_to = "Year", values_to = "Values") %>% 
  pivot_wider(names_from = Company, values_from = Values))
# #Equivalent
stopifnot(identical(ii, 
                    bb %>% pivot_longer(-1) %>% 
                      pivot_wider(names_from = 1, values_from = value) %>% 
                      rename(., Year = name)))
```

### Computers {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C01-I-Assignment', include=FALSE}
bb <- xxComputers
```

```{r 'C01T02', echo=FALSE}
bb <- xxComputers
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
# #Kable Table
kbl(bb,
  caption = "(C01T02) xxComputers",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```

#### Mean {.unlisted .unnumbered}

```{r 'C01-I-Mean'}
# #What is the average cost for the tablets #$582.90
cat(paste0("Avg. Cost for the tablets is = $", round(mean(bb$cost), digits = 1), "\n"))
#
# #Compare the average cost of tablets with different OS (Windows /Android) #$723.20 $428.5
(ii <- bb %>%
  group_by(os) %>%
  summarise(Mean = round(mean(cost), digits =1)) %>%
  arrange(desc(Mean)) %>% 
	mutate(Mean = paste0("$", Mean)))
#
cat(paste0("Avg. Cost of Tablets with Windows OS is = ", 
  ii %>% filter(os == "Windows") %>% select(Mean), "\n"))
```

#### Percentage {.unlisted .unnumbered}

```{r 'C01-I-Percentage'}
# #What percentage of tablets use an Android operating system #40%
(ii <- bb %>%
  group_by(os) %>%
  summarise(PCT = n()) %>%
  mutate(PCT = 100 * PCT / sum(PCT)) %>% 
  arrange(desc(PCT)) %>% 
  mutate(PCT = paste0(PCT, "%")))
#
cat(paste0("Android OS is used in ", 
  ii %>% filter(os == "Android") %>% select(PCT), " Tablets\n"))
```

### Cordless {.unlisted .unnumbered .tabset .tabset-fade} 

```{r 'C01-J-Assignment', include=FALSE}
bb <- xxCordless
```

```{r 'C01T03', echo=FALSE}
bb <- xxCordless
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
# #Kable Table
kbl(bb,
  caption = "(C01T03) xxCordless",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```

#### Mean {.unlisted .unnumbered}

```{r 'C01-J-Mean'}
# #What is the average price for the cordless telephones 
cat(paste0("Avg. Price is = $", round(mean(bb$price), digits = 1), "\n"))
#
# #What is the average talk time for the cordless telephones
cat(paste0("Avg. Talk Time is = ", round(mean(bb$talk_time_hh), digits = 1), " Hours \n"))
```

#### Percentage {.unlisted .unnumbered}

```{r 'C01-J-Percentage'}
# #What percentage of the cordless telephones have a voice quality of excellent 
(hh <- bb %>%
  group_by(voice_quality) %>%
  summarise(PCT = n()) %>%
  mutate(PCT = 100 * PCT / sum(PCT)) %>% 
	mutate(voice_quality = factor(voice_quality, 
	                              levels = c("Very Good", "Excellent"), ordered = TRUE)) %>% 
	arrange(desc(voice_quality)) %>% 
	mutate(PCT = paste0(PCT, "%")))
#
cat(paste0("Percentage of 'Excellent' Voice Quality is = ", 
  hh %>% filter(voice_quality == "Excellent") %>% select(PCT), "\n"))
#
# #Equivalent
print(bb %>%
 group_by(voice_quality) %>%
 summarise(PCT = n()) %>%
 mutate(PCT = prop.table(PCT) * 100))
```

#### PCT 2 {.unlisted .unnumbered}

```{r 'C01-J-PercentageMore'}
# #What percentage of the cordless telephones have a handset on the base 
bb %>%
  group_by(handset_on_base) %>%
  summarise(PCT = n()) %>%
  mutate(PCT = 100 * PCT / sum(PCT)) %>% 
  arrange(desc(PCT)) %>% 
  mutate(PCT = paste0(PCT, "%")) %>%
  filter(handset_on_base == "Yes") 
```

### Cars {#cars-c01 .unlisted .unnumbered .tabset .tabset-fade} 

```{r 'C01-K-Assignment', include=FALSE}
bb <- tibble(Company = c("Hertz", "Dollar", "Avis"), 
              `2007` = c(327, 167, 204), `2008` = c(311, 140, 220),
              `2009` = c(286, 106, 300), `2010` = c(290, 108, 270))
# #Transpose Tibble: Note that the First Column Header is lost after Transpose
# #Longer
hh <- bb %>% pivot_longer(!Company, names_to = "Year", values_to = "Values")
# #Transpose
ii <- bb %>% 
  pivot_longer(!Company, names_to = "Year", values_to = "Values") %>% 
  pivot_wider(names_from = Company, values_from = Values)
```

#### Transform {.unlisted .unnumbered}

```{r 'C01T04', echo=FALSE}
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kk_bb <- kbl(bb,
  caption = "(C01T04) Cars in Service",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
#
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kk_hh <- kbl(hh,
  caption = "(C01T04A) Cars (Longer)",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
#
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kk_ii <- kbl(ii,
  caption = "(C01T04B) Cars (Transposed)",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
# #Multiple Kable Tables
knitr::kables(list(kk_bb, kk_ii))
```

```{r 'C01-K-Transform', ref.label=c('C01-K-Assignment'), eval=FALSE}
# #
```

#### TimeSeries {.unlisted .unnumbered}

```{r 'C01-K-TimeSeries', include=FALSE}
# #Time series graph for all companies on the same graph
C01P01 <- hh %>% { ggplot(data = ., aes(x = Year, y = Values, colour = Company, group = Company)) + 
    geom_line() +
    labs(x = "Year", y = "Values", subtitle = NULL, 
         caption = "C01P01", title = "Multiple Time Series Chart")
  }
```

```{r 'C01P01-Save', echo=c(2,4)}
# #Save an Image
loc_png <- paste0(.z$PX, "C01P01", "-Cars-TimeSeries", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C01P01, device = "png", dpi = 144) 
}
```

```{r 'C01P01', fig.cap="(C01P01) Multiple Time Series Graph"}
# #Load an Image
knitr::include_graphics(paste0(.z$PX, "C01P01", "-Cars-TimeSeries", ".png"))
```

#### Rowwise {.unlisted .unnumbered}

```{r 'C01-K-Rowwise'}
# #who appears to be the market share leader
# #how the market shares are changing over time
print(ii)
# #Row Total
jj <- ii %>% rowwise() %>% mutate(SUM = sum(c_across(where(is.numeric)))) %>% ungroup()
kk <- ii %>% mutate(SUM = rowSums(across(where(is.numeric))))
stopifnot(identical(jj, kk))
#
# #Rowwise Percentage Share 
ii %>% 
  rowwise() %>% 
  mutate(SUM = sum(c_across(where(is.numeric)))) %>% 
  ungroup() %>%
  mutate(across(2:4, ~ round(. * 100 / SUM, digits = 1), .names = "{.col}.{.fn}")) %>%
  mutate(across(ends_with(".1"), ~ paste0(., "%")))
```

#### Pareto {.unlisted .unnumbered}

```{r 'C01-K-Pareto'}
# #Bar Plot
aa <- bb %>% 
  select(Company, `2010`) %>% 
  rename("Y2010" = `2010`) %>% 
  arrange(desc(.[2])) %>% 
  mutate(cSUM = cumsum(Y2010)) %>%
  mutate(PCT = 100 * Y2010 / sum(Y2010)) %>% 
  mutate(cPCT = 100 * cumsum(Y2010) / sum(Y2010)) %>% 
  mutate(across(Company, factor, levels = unique(Company), ordered = TRUE))
# #
pareto_chr <- setNames(c(aa$Y2010), aa$Company)
stopifnot(identical(pareto_chr, aa %>% pull(Y2010, Company)))
stopifnot(identical(pareto_chr, aa %>% select(1:2) %>% deframe()))
```

```{r 'C01P02-Save'}
# #Save without using ggsave()
hh <- pareto_chr
loc_png <- paste0(.z$PX, "C01P02", "-Cars-Pareto", ".png")
cap_hh <- "C01P02"
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  pareto.chart(hh, xlab = "Company", ylab = "Cars", cumperc = seq(0, 100, by = 20),  
               ylab2 = "Cumulative Percentage", main = "Pareto Chart")  
  #title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  C01P02 <- recordPlot()
  dev.off()
}
```

```{r 'C01P02', echo=FALSE, fig.cap="(C01P02) Pareto of Cars in 2010"}
knitr::include_graphics(paste0(.z$PX, "C01P02", "-Cars-Pareto", ".png")) #iiii
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C01-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, hh, ii, jj, kk, kk_bb, kk_hh, kk_ii, pareto_chr, C01P01, C01P02, 
          xxComputers, xxCordless, cap_hh, loc_png)
```

```{r 'C01-Validation', include=FALSE, cache=FALSE}
# #Summarised Packages and Objects
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/301-Data.Rmd-->

# Descriptive Statistics {#c02}

```{r 'C02', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
invisible(lapply(f_getPathR(A05ThemeGG), knitr::read_chunk))
```

## Overview

- This chapter covers 
  - [CrossTab](#crosstab-c02 "c02") : Transpose, Attributes, Total Row, Cut, Clipboard, Replace NA
  - Graphs: [Bar Chart](#bar-pie-c02 "c02"), [Pie Chart](#bar-pie-c02 "c02"), [Histogram](#histogram-c02 "c02"), [Scatterplot](#scatterplot-c02  "c02"), [cut() for creating bins](#cut-c02 "c02")

## Summarizing Data for a Categorical Variable {.tabset .tabset-fade}

```{definition 'Frequency-Distribution'}
A \textcolor{pink}{frequency distribution} is a tabular summary of data showing the number (frequency) of observations in each of several non-overlapping categories or classes.
```

The \textcolor{pink}{relative frequency} of a class equals the fraction or proportion of observations belonging to a class i.e. it is out of 1 whereas 'percent frequency' is out of 100%.

Rather than showing the frequency of each class, the \textcolor{pink}{cumulative frequency distribution} shows the number of data items with values less than or equal to the upper class limit of each class. 

- Bar Chart
  - Pareto Chart - `ggplot()` does not allow easy setup of dual axis
  - Stacked Bar Chart - do not use it if there are more than 2 categories
- Pie Chart
  - Only use it if total is 100% and Categories are fewer than 5 or 6.
  
### Bar & Pie {#bar-pie-c02 .unlisted .unnumbered}

```{r 'C02-I-Drinks', include=FALSE}
# #Frequency Distribution
aa <- tibble(softdrink = c("Coca-Cola", "Diet Coke", "Dr. Pepper", "Pepsi", "Sprite"), 
             Frequency = c(19, 8, 5, 13, 5))
#
# #Sort, Cummulative Sum, Percentage, and Cummulative Percentage
bb <- aa %>% 
  arrange(desc(Frequency)) %>% 
  mutate(cSUM = cumsum(Frequency)) %>%
  mutate(PROP = 100 * Frequency / sum(Frequency)) %>% 
  mutate(PCT = paste0(PROP, "%")) %>% 
  mutate(cPCT = paste0(100 * cumsum(Frequency) / sum(Frequency), "%"))
```

```{r 'C02T02', echo=FALSE}
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kbl(bb,
  caption = "(C02T02) Frequency Distribution",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```

```{r 'C02-I-Bar', include=FALSE}
# #Sorted Bar Chart of Frequencies (Needs x-axis as Factor for proper sorting)
C02P01 <- bb %>% mutate(across(softdrink, factor, levels = rev(unique(softdrink)))) %>% {
  ggplot(data = ., aes(x = softdrink, y = Frequency)) +
  geom_bar(stat = 'identity', aes(fill = softdrink)) + 
  scale_y_continuous(sec.axis = sec_axis(~ (. / sum(bb$Frequency))*100, name = "Percentages", 
                       labels = function(b) { paste0(round(b, 0), "%")})) +
  geom_text(aes(label = paste0(Frequency, "\n(", PCT, ")")), vjust = 2, 
            colour = c(rep("black", 2), rep("white", nrow(bb)-2))) +
  k_gglayer_bar +	
  labs(x = "Soft Drinks", y = "Frequency", subtitle = NULL, 
         caption = "C02P01", title = "Bar Chart of Categorical Data")
}
```

```{r 'C02P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C02P01", "-Drinks-Bar", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C02P01, device = "png", dpi = 144) 
}
```

```{r 'C02P01', echo=FALSE, fig.cap="(C02P01) geom_bar(): Frequency Bar Chart", include=FALSE}
knitr::include_graphics(paste0(.z$PX, "C02P01", "-Drinks-Bar", ".png"))
```

```{r 'C02-I-Pie', include=FALSE}
# #Pie Chart of Frequencies (Needs x-axis as Factor for proper sorting)
C02P02 <- bb %>% mutate(across(softdrink, factor, levels = unique(softdrink))) %>% {
  ggplot(data = ., aes(x = '', y = Frequency, fill = rev(softdrink))) +
  geom_bar(stat = 'identity', width = 1, color = "white") +
  coord_polar(theta = "y", start = 0) +
  geom_text(aes(label = paste0(softdrink, "\n", Frequency, " (", PCT, ")")), 
            position = position_stack(vjust = 0.5), 
            colour = c(rep("black", 2), rep("white", nrow(bb)-2))) +
  k_gglayer_pie +	
  labs(caption = "C02P02", title = "Pie Chart of Categorical Data")
}
```

```{r 'C02P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C02P02", "-Drinks-Pie", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C02P02, device = "png", dpi = 144) 
}
```

```{r 'C02P02', echo=FALSE, fig.cap="(C02P02) Polar geom_bar(): Frequency Pie Chart", include=FALSE}
knitr::include_graphics(paste0(.z$PX, "C02P02", "-Drinks-Pie", ".png"))
```

```{r 'C02P0102', echo=FALSE, ref.label=c('C02P01', 'C02P02'), fig.cap="(C02P01 C02P02) Bar Chart and Pie Chart of Frequency"}
#
```

### Data {.unlisted .unnumbered}

```{r 'C02-I-Drinks-A', eval=FALSE, ref.label=c('C02-I-Drinks')}
# #
```

### Bar {.unlisted .unnumbered}

```{r 'C02-I-Bar-A', ref.label=c('C02-I-Bar'), eval=FALSE}
# #
```

### Pie {.unlisted .unnumbered}

```{r 'C02-I-Pie-A', ref.label=c('C02-I-Pie'), eval=FALSE}
# #
```

### f_theme_gg() {.unlisted .unnumbered}

```{r 'C02F-theme-gg', ref.label=c('A05-ThemeGG'), eval=FALSE}
# #
```

```{r 'C02F-A05-SetThemeNLists', ref.label=c('A05-SetThemeNLists'), eval=FALSE}
# #
```

### Errors {.unlisted .unnumbered}

```{conjecture 'gg-stat-count-geom-bar'}
\textcolor{brown}{Error: stat_count() can only have an x or y aesthetic.}
```

Solution: Use `geom_bar(stat = "identity")`

## Summarizing Data for a Quantitative Variable {.tabset .tabset-fade}

A \textcolor{pink}{histogram} is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. 

The three steps necessary to define the classes for a frequency distribution with quantitative data are

1. Determine the number of nonoverlapping classes (Bins)
    - Classes are formed by specifying ranges that will be used to group the data.
    - Approx. 5-20
1. Determine the width of each class
    - The bins are usually specified as consecutive, non-overlapping intervals of a variable. 
    - The bins (intervals) must be adjacent and are often (but not required to be) of equal size.
    - Approx. Bin Width = (Max - Min) / Number of Bins
    - Ex: For a dataset with min =12 & max =33, 5 bins of 10-14, ..., 30-34 can be selected
1. Determine the class 
    - Class limits must be chosen so that each data item belongs to one and only one class
    - For categorical data, this was not required because each item naturally fell into a separate class
    - But with quantitative data, class limits are necessary to determine where each data value belongs
    - The 'class midpoint' is the value halfway between the lower and upper class limits. For a Bin of 10-14, 12 will be its mid-point. 
  
- Dot Plot 
  - A horizontal axis shows the range for the data. Each data value is represented by a dot placed above the axis.
  - \textcolor{orange}{Caution:} Avoid! Y-Axis is deceptive. 
- Histogram
  - Unlike a bar graph, a histogram contains no natural separation between the rectangles of adjacent classes.
- Stem-and-Leaf Display (Not useful)

### Histogram {#histogram-c02 .unlisted .unnumbered}

```{r 'C02-K-NormalRandom'}
set.seed(3)
# #Get Normal Data
bb <- tibble(aa = rnorm(n = 10000)) 
```

```{r 'C02-K-Histogram'}
# #Histogram
# # '..count..' or '..x..'
C02P03 <- bb %>% {
  ggplot(data = ., aes(x = aa, fill = ..count..)) + 
  geom_histogram(bins = 50, position = "identity") +	
  k_gglayer_hist +
  labs(x = "Normal Data", y = "Count", subtitle = paste0("n = ", format(nrow(.), big.mark = ",")), 
       caption = "C02P03", title = "Histogram")
}
```

```{r 'C02P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C02P03", "-Normal-Histogram", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C02P03, device = "png", dpi = 144) 
}
```

```{r 'C02P03', echo=FALSE, fig.cap="(C02P03) geom_histogram(): Histogram"}
knitr::include_graphics(paste0(.z$PX, "C02P03", "-Normal-Histogram", ".png"))
```

### Dot Plot  {.unlisted .unnumbered}

```{r 'C02-J-Random'}
# #Random Data
aa <- c(26, 35, 22, 47, 37, 5, 50, 49, 42, 2, 8, 7, 4, 47, 44, 35, 17, 49, 1, 48, 
        1, 27, 13, 26, 18, 44, 31, 4, 23, 47, 38, 28, 28, 5, 35, 39, 29, 13, 17, 
        38, 1, 8, 3, 30, 18, 37, 29, 39, 7, 28)
```

```{r 'C02-J-Dot'}
bb <- tibble(aa)
# #Dot Chart of Frequencies
C02P04 <- bb %>% {
  ggplot(., aes(x = aa)) +
  geom_dotplot(binwidth = 5, method = "histodot") + 
  theme(axis.line.y = element_blank(), panel.grid = element_blank(), axis.text.y = element_blank(),
        axis.ticks.y = element_blank(), axis.title.y =  element_blank()) + 
  labs(x = "Bins", subtitle = "Caution: Avoid! Y-Axis is deceptive.", 
       caption = "C02P04", title = "Dot Plot")
}
```

```{r 'C02P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C02P04", "-Random-Dot", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C02P04, device = "png", dpi = 144) 
}
```

```{r 'C02P04', echo=FALSE, fig.cap="(C02P04) geom_dotplot(): Frequency Dot Chart"}
knitr::include_graphics(paste0(.z$PX, "C02P04", "-Random-Dot", ".png"))
```

### Get Frequency  {.unlisted .unnumbered}

```{r 'C02-J-Frequency', include=FALSE}
#set.seed(3)
#aa <- paste0(sample(1:50, size = 50, replace = TRUE), collapse = ", ")
aa <- c(26, 35, 22, 47, 37, 5, 50, 49, 42, 2, 8, 7, 4, 47, 44, 35, 17, 49, 1, 48, 
        1, 27, 13, 26, 18, 44, 31, 4, 23, 47, 38, 28, 28, 5, 35, 39, 29, 13, 17, 
        38, 1, 8, 3, 30, 18, 37, 29, 39, 7, 28)
# #Get Frequency in another column
agg_aa <- aggregate(aa, by = list(Group = aa), FUN = length)
df_aa <- data.frame(Group = unique(aa), 
                    x = vapply(unique(aa), function(y) sum(aa == y), integer(1))) %>% 
  arrange(Group)
cnt_aa <- tibble(aa) %>% count(aa) %>% rename(Group = aa, x = n)
grp_aa <- tibble(aa) %>% group_by(aa) %>% tally() %>% rename(Group = aa, x = n) 
stopifnot(all(identical(df_aa, agg_aa), 
              identical(as_tibble(df_aa), cnt_aa), 
              identical(grp_aa, cnt_aa)))
#
# #Any of the above can be summarised to get the Maximum Frequency
bb <- cnt_aa
bb %>% summarise(max(x)) %>% pull()
#
# # Get Frequncy Vector. Note this uses some tricky syntax and tabulate() has limitations
str(match(aa, aa) %>% `[`(tabulate(.), .))
#
# #by() gives a list that later would need subsetting, so it is not useful here
str(by(aa, INDICES = list(Group = aa), FUN = length))
# 
# #This looks similar to above but it retains original number of rows (not the unique)
str(tibble(aa) %>% group_by(aa) %>% mutate(Count = n()))
#
# #Another way to find all the values which have maximum frequency i.e. Mode
unq <- unique(aa)
tab <- tabulate(match(aa, unq))
unq[tab == max(tab)]
#
```

## Summarizing Data for Two Variables Using Tables {#crosstab-c02 .tabset .tabset-fade}

```{definition 'Cross-Tab'}
A \textcolor{pink}{crosstabulation} is a tabular summary of data for two variables. It is used to investigate the relationship between them. Generally, one of the variable is categorical. 
```

- Simpson Paradox
  - The reversal of conclusions based on aggregate and unaggregated data is called Simpson paradox.
  - Ex: Table \@ref(tab:C02T01) shows the count of judgements that were 'upheld' or 'reversed' on appeal for two judges 
    - 86% of the verdicts were upheld for Judge Abel, while 88% of the verdicts were upheld for Judge Ken. From this aggregated crosstabulation, we conclude that Judge Ken is doing the better job because a greater percentage of his verdicts are being upheld.
    - However, unaggregated crosstabulations show that in both types of courts (Common, Municipal) Judge Abel has higher percentage of 'Upheld' Verdicts (90.6% and 84.7%) - compared to Judge Ken (90% and 80%)
    - Thus, Abel has a better record because a greater percentage of his verdicts are being upheld in both courts. 
    - This reversal of conclusions based on aggregated and unaggregated data illustrates Simpson paradox.
  - Cause
    - Note that for both judges the percentage of appeals that resulted in reversals was much higher in 'Municipal' than in 'Common' Court i.e. 15.3% vs. 9.4% for Abel and 20% vs. 10% for Ken.
    - Because Judge Abel tried a much higher percentage of his cases in 'Municipal', the aggregated data favoured Judge Ken i.e. 118/150 for Abel vs. 25/125 for Ken.
    - Thus, for the original crosstabulation, we see that the 'type of court' is a \textcolor{pink}{hidden variable} that cannot be ignored when evaluating the records of the two judges.
  
```{r 'C02-K-Judges', include=FALSE}
# #Judges: Because we are evaluating 'Judges', they are the 'elements' and thus are the 'rows'
xxJudges <- tibble(Judge_Verdict = c('Abel', 'Ken'), Upheld = c(129, 110), Reversed = c(21, 15))
# #Uaggregated crosstab for both Judges in different types of Courts
xxKen <- tibble(Ken = c("Common", "Municipal "), 
                    Upheld = c(90, 20), Reversed = c(10, 5))
xxAbel <- tibble(Abel = c("Common", "Municipal "), 
                    Upheld = c(29, 100), Reversed = c(3, 18))
```

```{r 'C02T01', echo=FALSE}
aa <- xxJudges
bb <- aa %>% 
  bind_rows(aa %>% summarise(across(where(is.numeric), sum))) %>%
  mutate(across(1, ~replace(., . %in% NA, "Total"))) %>%
  mutate(SUM = rowSums(across(where(is.numeric)))) %>% 
  mutate(upPCT = round(Upheld * 100 / SUM, 1)) %>% 
  mutate(revPCT = round(Reversed * 100 / SUM, 1)) %>% 
  mutate(xUpheld = paste0(Upheld, " (", upPCT, "%)")) %>% 
  mutate(xReversed = paste0(Reversed, " (", revPCT, "%)")) %>% 
  select(1, SUM, 7:8) %>% 
  relocate(SUM, .after = last_col())
#
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kk_both <- kbl(bb,
  caption = "(C02T01) Both Judges",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
                position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
#
aa <- xxAbel
bb <- aa %>% 
  bind_rows(aa %>% summarise(across(where(is.numeric), sum))) %>%
  mutate(across(1, ~replace(., . %in% NA, "Total"))) %>%
  mutate(SUM = rowSums(across(where(is.numeric)))) %>% 
  mutate(upPCT = round(Upheld * 100 / SUM, 1)) %>% 
  mutate(revPCT = round(Reversed * 100 / SUM, 1)) %>% 
  mutate(xUpheld = paste0(Upheld, " (", upPCT, "%)")) %>% 
  mutate(xReversed = paste0(Reversed, " (", revPCT, "%)")) %>% 
  select(1, SUM, 7:8) %>% 
  relocate(SUM, .after = last_col())
#
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kk_Abel <- kbl(bb,
  caption = "(C02T01A) Abel",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
                position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
#
aa <- xxKen
bb <- aa %>% 
  bind_rows(aa %>% summarise(across(where(is.numeric), sum))) %>%
  mutate(across(1, ~replace(., . %in% NA, "Total"))) %>%
  mutate(SUM = rowSums(across(where(is.numeric)))) %>% 
  mutate(upPCT = round(Upheld * 100 / SUM, 1)) %>% 
  mutate(revPCT = round(Reversed * 100 / SUM, 1)) %>% 
  mutate(xUpheld = paste0(Upheld, " (", upPCT, "%)")) %>% 
  mutate(xReversed = paste0(Reversed, " (", revPCT, "%)")) %>% 
  select(1, SUM, 7:8) %>% 
  relocate(SUM, .after = last_col())
#
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kk_Ken <- kbl(bb,
  caption = "(C02T01B) Ken",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
                position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
# #Multiple Kable Tables
knitr::kables(list(kk_both, kk_Abel, kk_Ken))
```

### Judges {.unlisted .unnumbered}

```{r 'C02-K-Judges-A', ref.label=c('C02-K-Judges'), eval=FALSE}
#
```

### Transpose {.unlisted .unnumbered}

```{r 'C02-K-Transpose'}
# #Judges
aa <- tibble(Judge_Verdict = c('Abel', 'Ken'), Upheld = c(129, 110), Reversed = c(21, 15))
bb <- tibble(Verdict_Judge = c('Upheld', 'Reversed'), Abel = c(129, 21), Ken = c(110, 15))
aa
# #Transpose, Assuming First Column Header has "Row_Col" Type Format
ii <- aa %>% 
  `attr<-`("ColsLost", unlist(strsplit(names(.)[1], "_"))[1]) %>% 
  `attr<-`("RowsKept", unlist(strsplit(names(.)[1], "_"))[2]) %>% 
  pivot_longer(c(-1), 
               names_to = paste0(attributes(.)$RowsKept, "_", attributes(.)$ColsLost), 
               values_to = "Values") %>% 
  pivot_wider(names_from = 1, values_from = Values) %>% 
  `attr<-`("ColsLost", NULL) %>% `attr<-`("RowsKept", NULL) 
stopifnot(identical(bb, ii))
ii
# #Testing for Reverse
ii <- bb %>% 
  `attr<-`("ColsLost", unlist(strsplit(names(.)[1], "_"))[1]) %>% 
  `attr<-`("RowsKept", unlist(strsplit(names(.)[1], "_"))[2]) %>% 
  pivot_longer(c(-1), 
               names_to = paste0(attributes(.)$RowsKept, "_", attributes(.)$ColsLost), 
               values_to = "Values") %>% 
  pivot_wider(names_from = 1, values_from = Values) %>% 
  `attr<-`("ColsLost", NULL) %>% `attr<-`("RowsKept", NULL) 
stopifnot(identical(aa, ii))
```

### String Split {.unlisted .unnumbered}

```{r 'C02-K-StringSplit'}
bb <- "Judge_Verdict"
# #Split String by strsplit(), output is list
(ii <- unlist(strsplit(bb, "_")))
#
# #Split on Dot 
bb <- "Judge.Verdict"
# #Using character classes
ii <- unlist(strsplit(bb, "[.]"))
# #By escaping special characters
jj <- unlist(strsplit(bb, "\\."))
# #Using Options
kk <- unlist(strsplit(bb, ".", fixed = TRUE))
stopifnot(all(identical(ii, jj), identical(ii, kk)))
```

### Attributes {.unlisted .unnumbered}

- Tibble 
  - 'problems' attribute contains List of All Problems
    - \textcolor{pink}{problems(bb)} 
  - 'spec' attribute contains List of Columns and Types
    - \textcolor{pink}{spec(bb)}
    - \textcolor{orange}{Caution:} SHOW Snapshot at Import NOT the current Status, Better To Be Removed

```{r 'C02-K-Attributes'}
jj <- ii <- bb <- aa
# #attr() adds or removes an attribute
attr(bb, "NewOne") <- "abc"
# #Using Backticks
ii <- `attr<-`(ii, "NewOne", "abc")
# #Using Pipe
jj <- jj %>% `attr<-`("NewOne", "abc")
#
stopifnot(all(identical(bb, ii), identical(bb, jj)))
#
# #List Attributes
names(attributes(bb))
#
# #Specific Attribute Value
attributes(bb)$NewOne
#
# #Remove Attributes
attr(bb, "NewOne") <- NULL
ii <- `attr<-`(ii, "NewOne", NULL)
jj <- jj %>% `attr<-`("NewOne", NULL)
stopifnot(all(identical(bb, ii), identical(bb, jj)))
```

### Total Row {.unlisted .unnumbered}

```{r 'C02-K-RowTotal'}
# #(Deprecated) Issues: 
# #(1) bind_rows() needs two dataframes. Thus, first can be skipped in Pipe, But...
# #The second dataframe cannot be replaced with dot (.), it has to have a name
# #(2) Pipe usage inside function call was working but was a concern
# #(3) It introduced NA for that replace was needed as another step
ii <- aa %>% bind_rows(aa %>% summarise(across(where(is.numeric), sum))) %>%
	mutate(across(1, ~ replace(., . %in% NA, "Total"))) 
#
# #(Deprecated) Works but needs ALL Column Names individually
jj <- aa %>% add_row(Judge_Verdict = "Total", Upheld = sum(.[ , 2]), Reversed = sum(.[ , 3]))
kk <- aa %>% add_row(Judge_Verdict = "Total", Upheld = sum(.$Upheld), Reversed = sum(.$Reversed))
#
# #(Deprecated) Removed the Multiple call to sum(). However, it needs First Column Header Name
ll <- aa %>% add_row(Judge_Verdict = "Total", summarise(., across(where(is.numeric), sum)))
# #(Deprecated) Replaced Column Header Name using "Tilde"
mm <- aa %>% add_row(summarise(., across(where(is.character), ~"Total")), 
               summarise(., across(where(is.numeric), sum)))
stopifnot(all(identical(ii, jj), identical(ii, kk), identical(ii, ll), identical(ii, mm)))
#
# #(Working): Minimised
aa %>% add_row(summarise(., across(1, ~"Total")), summarise(., across(where(is.numeric), sum)))
```

### Replace NA {.unlisted .unnumbered}

```{r 'C02-K-ReplaceNA'}
# # USE '%in%' for NA, otherwise '==' works
bb <- aa %>% bind_rows(aa %>% summarise(across(where(is.numeric), sum)))
#
ii <- bb %>% mutate(across(1, ~ replace(., . %in% NA, "Total"))) 
mm <- bb %>% mutate(across(1, ~ replace(., is.na(.), "Total"))) 
jj <- bb %>% mutate(Judge_Verdict = replace(Judge_Verdict, is.na(Judge_Verdict), "Total"))
kk <- bb %>% mutate(across(1, coalesce, "Total")) 
ll <- bb %>% mutate(across(1, ~ replace_na(.x, "Total")))
nn <- bb %>% mutate(across(1, replace_na, "Total"))
stopifnot(all(identical(ii, jj), identical(ii, kk), identical(ii, ll), 
              identical(ii, mm), identical(ii, nn)))
#
#	#Replace NA in a Factor
bb %>% 
  mutate(Judge_Verdict = factor(Judge_Verdict)) %>% 
  mutate(across(1, fct_explicit_na, na_level = "Total"))
```

### To Factor {.unlisted .unnumbered}

```{r 'C02-K-ToFactor'}
#	#Convert to Factor
aa %>% mutate(Judge_Verdict = factor(Judge_Verdict))
```

### Clipboard  {.unlisted .unnumbered}

```{r 'C02-K-Clipboard', eval=FALSE}
# #Paste but do not execute
aa <- read_delim(clipboard())
# #Copy Excel Data, then execute the above command
#
# #Print its structure
dput(aa)
# #Copy the relevant values, headers in tibble()
bb <- tibble(  )
# #The above command will be the setup to generate this tibble anywhere
```

## Exercise 

### C02E27 {.tabset .tabset-fade .unlisted .unnumbered}

#### Data {.unlisted .unnumbered}

```{r 'C02-L-Ex27'}
ex27 <- tibble(Observation = 1:30, 
             x = c("A", "B", "B", "C", "B", "C", "B", "C", "A", "B", "A", "B", "C", "C", "C", 
                   "B", "C", "B", "C", "B", "C", "B", "C", "A", "B", "C", "C", "A", "B", "B"), 
             y = c(1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 
                   2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2))
```

#### CrossTab {.unlisted .unnumbered}

```{r 'C02-L-CrossTab'}
bb <- ex27
str(bb)
# #Create CrossTab
bb <- bb %>% 
  count(x, y) %>% 
  pivot_wider(names_from = y, values_from = n, values_fill = 0)
```

#### PCT {.unlisted .unnumbered}

```{r 'C02-L-PCT'}
bb
# #Rowwise Percentage in Separate New Columns
bb %>% 
  mutate(SUM = rowSums(across(where(is.numeric)))) %>% 
  mutate(across(where(is.numeric), ~ round(. * 100 /SUM, 1), .names = "{.col}_Row" )) 
#
# #Rowwise Percentage in Same Columns
bb %>% 
  mutate(SUM = rowSums(across(where(is.numeric)))) %>% 
  mutate(across(where(is.numeric), ~ round(. * 100 /SUM, 1))) 
#
# #Equivalent
bb %>% 
  mutate(SUM = rowSums(across(where(is.numeric))),
         across(where(is.numeric), ~ round(. * 100 /SUM, 1))) 
#
# #Columnwise Percentage in Separate New Columns
bb %>% 
  mutate(across(where(is.numeric), ~ round(. * 100 /sum(.), 1), .names = "{.col}_Col" ))
# #Columnwise Percentage in Same Columns
bb %>% 
  mutate(across(where(is.numeric), ~ round(. * 100 /sum(.), 1)))
```

### C02E28 {.tabset .tabset-fade .unlisted .unnumbered}

#### Data {.unlisted .unnumbered}

```{r 'C02-M-Ex28'}
ex28 <- tibble(Observation = 1:20, 
        x = c(28, 17, 52, 79, 37, 71, 37, 27, 64, 53, 13, 84, 59, 17, 70, 47, 35, 62, 30, 43), 
        y = c(72, 99, 58, 34, 60, 22, 77, 85, 45, 47, 98, 21, 32, 81, 34, 64, 68, 67, 39, 28))
```

#### CrossTab {.unlisted .unnumbered}

```{r 'C02-M-CrossTab'}
bb <- ex28
# #Rounding to the lowest 10s before min and to the highest 10s after max
nn <- 10L	
n_x <- seq(floor(min(bb$x) / nn) * nn, ceiling(max(bb$x) / nn) * nn, by = 20)
n_y <- seq(floor(min(bb$y) / nn) * nn, ceiling(max(bb$y) / nn) * nn, by = 20)
#
# #Labels in the format of [10-29]
lab_x <- paste0(n_x, "-", n_x + 20 - 1) %>% head(-1)
lab_y <- paste0(n_y, "-", n_y + 20 - 1) %>% head(-1)

# #Wider Table without Totals
ii <- bb %>% 
  mutate(x_bins = cut(x, breaks = n_x, right = FALSE, labels = lab_x),
         y_bins = cut(y, breaks = n_y, right = FALSE, labels = lab_y)) %>% 
  count(x_bins, y_bins) %>% 
  pivot_wider(names_from = y_bins, values_from = n, values_fill = 0, names_sort = TRUE)
print(ii)
# #Cross Tab with Total Column and Total Row
jj <- ii %>% 
  bind_rows(ii %>% summarise(across(where(is.numeric), sum))) %>% 
	mutate(across(1, fct_explicit_na, na_level = "Total")) %>% 
	mutate(SUM = rowSums(across(where(is.numeric))))
print(jj)
```

#### cut() {#cut-c02 .unlisted .unnumbered}

- \textcolor{pink}{cut()}
  - It slightly increases the range
  - \textcolor{pink}{ggplot2::cut_interval(), cut_width()} do not increase the range
  - dig.lab : Options to exclude scientific notation
  - ordered_result : Option for ordered factor

```{r 'C02-M-Cut'}
# #Group Continuous Data to Categorical Bins by base::cut()
bb <- ex28
#
# #NOTE cut() increases the range slightly but ggplot functions do not
bb %>% mutate(x_bins = cut(x, breaks = 8)) %>% 
  pull(x_bins) %>% levels()
# 
# #By default, it excludes the lower range, but it can be included by option
bb %>% mutate(x_bins = cut(x, breaks = 8, include.lowest = TRUE)) %>% 
  pull(x_bins) %>% levels()
#
# #ggplot::cut_interval() makes n groups with equal range. There is a cut_number() also
bb %>% mutate(x_bins = cut_interval(x, n = 8)) %>% 
  pull(x_bins) %>% levels()
#
# #Specific Bins
bb %>% mutate(x_bins = cut(x, breaks = seq(10, 90, by = 10))) %>% 
  pull(x_bins) %>% levels()
ii <- bb %>% mutate(x_bins = cut(x, breaks = seq(10, 90, by = 10), include.lowest = TRUE)) %>% 
  pull(x_bins) %>% levels()
print(ii)
#
# #ggplot::cut_width() makes groups of width
bb %>% mutate(x_bins = cut_width(x, width = 10)) %>% 
  pull(x_bins) %>% levels()
#
# #Match cut_width() and cut()
jj <- bb %>% mutate(x_bins = cut_width(x, width = 10, boundary = 0)) %>% 
  pull(x_bins) %>% levels()
print(jj)
stopifnot(identical(ii, jj))
#
# #Labelling
n_breaks <- seq(10, 90, by = 10)
n_labs <- paste0("*", n_breaks, "-", n_breaks + 10) %>% head(-1)

bb %>% mutate(x_bins = cut(x, breaks = n_breaks, include.lowest = TRUE, labels = n_labs)) %>% 
  pull(x_bins) %>% levels()
```

## Summarizing Data for Two Variables {#scatterplot-c02 .tabset .tabset-fade}

- Scatterplot and Trendline
- Side by Side and Stacked Bar Charts

### Data {.unlisted .unnumbered}

```{r 'C02-N-T214'}
xxCommercials <- tibble(Week = 1:10, 
                 Commercials = c(2, 5, 1, 3, 4, 1, 5, 3, 4, 2), 
                 Sales = c(50, 57, 41, 54, 54, 38, 63, 48, 59, 46))
f_setRDS(xxCommercials)
```

```{r 'C02-N-Trendline', include=FALSE}
bb <- xxCommercials 

# #Formula for Trendline calculation
k_gg_formula <- y ~ x
#
# #Scatterplot, Trendline alongwith its equation and R2 value
C02P05 <- bb %>% {
  ggplot(data = ., aes(x = Commercials, y = Sales)) + 
  geom_smooth(method = 'lm', formula = k_gg_formula, se = FALSE) +
  stat_poly_eq(aes(label = paste0("atop(", ..eq.label.., ", \n", ..rr.label.., ")")), 
               formula = k_gg_formula, eq.with.lhs = "italic(hat(y))~`=`~",
               eq.x.rhs = "~italic(x)", parse = TRUE) +
  geom_point() + 
  labs(x = "Commercials", y = "Sales ($100s)", 
       subtitle = paste0("Trendline equation and R", '\u00b2', " value"), 
       caption = "C02P05", title = "Scatter Plot")
}
```

```{r 'C02P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C02P05", "-ScatterPlot", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C02P05, device = "png", dpi = 144) 
}
```

```{r 'C02P05', echo=FALSE, fig.cap="(C02P05) geom_point(), geom_smooth(), & stat_poly_eq()"}
knitr::include_graphics(paste0(.z$PX, "C02P05", "-ScatterPlot", ".png")) #iiii
```

### Trendline {.unlisted .unnumbered}

```{r 'C02-N-Trendline-A', ref.label=c('C02-N-Trendline'), eval=FALSE}
#
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C02-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, agg_aa, cnt_aa, df_aa, grp_aa, ii, jj, kk, kk_both, kk_Ken, kk_Abel, tab, unq, 
          xxJudges, xxKen, xxAbel, ex27, ex28, lab_x, lab_y, n_breaks, n_labs, n_x, n_y, nn,
          xxCommercials, C02P01, C02P02, C02P03, C02P04, C02P05, ll, mm)
```

```{r 'C02-Validation', include=FALSE, cache=FALSE}
# #Summarised Packages and Objects
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/302-Descriptive.Rmd-->

# Numerical Measures {#c03}

```{r 'C03', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
invisible(lapply(f_getPathR(A09isPrime, A10getUtil), knitr::read_chunk))
```

## Overview

- This chapter covers 
  - Side Topic: [Number Theory](#numbers-c03 "c03"), [Primes](#primes-c03 "c03"), [Benchmark](#benchmark-c03 "c03"), Vectorising & Compiling & Profiling Functions,
  - Measures of Location: [Mean](mean-c03 "c03"), Weighted Mean, [Median](#median-c03 "c03"), Geometric Mean, [Mode](#mode-c03 "c03"), [Percentiles](#percentiles-c03 "c03"), Quantiles
  - Measures of Variability: Range, IQR, [Variance](#variance-c03 "c03"), [Standard Deviation](#sd-c03 "c03")
  - Shape [Skewness](#skewness-c03 "c03"), [Kurtosis]({#kurtosis-c03 "c03")
  - Relative Location: [z-scores](#z-scores-c03 "c03"), Chebyshev Theorem, [Empirical Rule](#empirical-c03 "c03"), [Outliers: C03](#outliers-c03 "c03")
  - Two Variables: [Covariance](#covariance-c03 "c03"), [Correlation Coefficient](#correlation-c03 "c03")
  - Graphs: [BoxPlot](#boxplot-c03 "c03"), [Scatterplot](#covariance-c03 "c03")
  - "ForLater" - Sum Mean Formulae, Skewness

## Definitions (Ref)

```{r 'C03D01', comment="", echo=FALSE, results='asis'}
f_getDef("Parameter-vs-Statistic") #dddd
```

## Number Theory {#numbers-c03 .tabset .tabset-fade}

```{definition 'Number'}
A \textcolor{pink}{number} is a mathematical object used to count, measure, and label. Their study or usage is called \textcolor{pink}{arithmetic}, a term which may also refer to \textcolor{pink}{number theory}, the study of the properties of numbers. 
```

Individual numbers can be represented by symbols, called \textcolor{pink}{numerals}; for example, "5" is a numeral that represents the 'number five'. 

As only a relatively small number of symbols can be memorized, basic numerals are commonly organized in a \textcolor{pink}{numeral system}, which is an organized way to represent any number. The most common numeral system is the \textcolor{pink}{Hindu-Arabic numeral system}, which allows for the representation of any number using a combination of ten fundamental numeric symbols, called \textcolor{pink}{digits}.

\textcolor{pink}{Counting} is the process of determining the number of elements of a finite set of objects, i.e., determining the size of a set. \textcolor{pink}{Enumeration} refers to uniquely identifying the elements of a set by assigning a number to each element. 

\textcolor{pink}{Measurement} is the quantification of attributes of an object or event, which can be used to compare with other objects or events.

```{r 'C03-Sets-A', include=FALSE}
# #NOT Working in Bookdown
# # For Infinity '\u221e'
cat(paste0(" \u2115 (0, 1, 2, ...) \u2282 \u2124 (-1, 0, 1) \u2282 \u211a (-0.5, +0.5) \u2282 \u211d (\u221a2, e(\u2091), \u03c0) \u2282 \u2102 (a +i b) "))
```

### Sets {.unlisted .unnumbered}

> Formally, $\mathbb{N} \to \mathbb{Z} \to \mathbb{Q} \to \mathbb{R} \to \mathbb{C}$  
> Practically, $\mathbb{N} \subset \mathbb{Z} \subset \mathbb{Q} \subset \mathbb{R} \subset \mathbb{C}$

The \textcolor{pink}{natural numbers $\mathbb{N}$} are those numbers used for counting and ordering. ISO standard begin the natural numbers with 0, corresponding to the \textcolor{pink}{non-negative integers $\mathbb{N} = \{0, 1, 2, 3, \ldots \}$}, whereas others start with 1, corresponding to the \textcolor{pink}{positive integers $\mathbb{N^*} = \{1, 2, 3, \ldots \}$}

The set of \textcolor{pink}{integers $\mathbb{Z}$} consists of zero (${0}$), the positive natural numbers $\{1, 2, 3, \ldots \}$ and their additive inverses (the negative integers). Thus i.e., \textcolor{pink}{$\mathbb{Z} = \{\ldots, -3, -2, -1, 0, 1, 2, 3, \ldots \}$}. An integer is colloquially defined as a number that can be written without a fractional component. 

\textcolor{pink}{Rational numbers $\mathbb{Q}$} are those which can be expressed as the quotient or fraction p/q of two integers, a numerator p and a non-zero denominator q. Thus, \textcolor{pink}{Rational Numbers $\mathbb{Q} = \{1, 2, 3, \ldots \}$}

A \textcolor{pink}{real number} is a value of a continuous quantity that can represent a distance along a line. The real numbers include all the rational numbers $\mathbb{Q}$, and all the irrational numbers. Thus, \textcolor{pink}{Real Numbers $\mathbb{R} = \mathbb{Q} \cup \{\sqrt{2}, \sqrt{3}, \ldots\} \cup \{ \pi, e, \phi, \ldots \}$}

The \textcolor{pink}{complex numbers $\mathbb{C}$} contain numbers which are expressed in the form $a + ib$, where ${a}$ and ${b}$ are real numbers. These have two components the real numbers and a specific element denoted by ${i}$ (imaginary unit) which satisfies the equation \textcolor{pink}{$i^2 = −1$}.

### Pi {.unlisted .unnumbered}

The number \textcolor{pink}{Pi $\pi = 3.14159\ldots$} is defined as the ratio of circumference of a circle to its diameter.

\begin{equation} 
  \pi = \int _{-1}^{1} \frac{dx}{\sqrt{1- x^2}}
  (\#eq:pi)
\end{equation} 

\begin{equation} 
  e^{i\varphi}=\cos \varphi + i\sin \varphi 
  (\#eq:euler-formula)
\end{equation} 

\begin{equation} 
  e^{i\pi} + 1 = 0
  (\#eq:euler-identity)
\end{equation} 

```{r 'C03-PI'}
# #Read OIS File for 20000 PI digits including integral (3) and fractional (14159...)
# #md5sum = "daf0b33a67fd842a905bb577957a9c7f"
tbl <- read_delim(file = paste0(.z$XL, "PI-OIS-b000796.txt"), 
  delim = " ", col_names = c("POS", "VAL"), col_types = list(POS = "i", VAL = "i"))
attr(tbl, "spec") <- NULL
attr(tbl, "problems") <- NULL
xxPI <- tbl
f_setRDS(xxPI)
```

### e {.unlisted .unnumbered}

Euler Number \textcolor{pink}{$e = 2.71828\ldots$}, is the base of the natural logarithm. 

\begin{equation} 
  e = \lim_{n \to \infty} \left(1 + \frac{1}{n} \right)^{n} = \sum \limits_{n=0}^{\infty} \frac{1}{n!}
  (\#eq:euler-number-e)
\end{equation} 

### Phi {.unlisted .unnumbered}

Two quantities are in the \textcolor{pink}{golden ratio $\varphi = 1.618\ldots$} if their ratio is the same as the ratio of their sum to the larger of the two quantities.

\begin{equation} 
  \varphi^2 - \varphi - 1 = 0 \\
  \varphi = \frac{1 + \sqrt{5}}{2}
  (\#eq:golden-ratio-phi)
\end{equation} 

### Groups {.unlisted .unnumbered}

```{definition 'Prime'}
A \textcolor{pink}{prime number} is a natural number greater than 1 that is not a product of two smaller natural numbers. A natural number greater than 1 that is not prime is called a 'composite number'. 1 is neither a Prime nor a composite, it is a 'Unit'. Thus, by definition, Negative Integers and Zero cannot be Prime.
```

```{definition 'Parity-Odd-Even'}
\textcolor{pink}{Parity} is the property of an integer $\mathbb{Z}$ of whether it is even or odd. It is \textcolor{pink}{even} if the integer is divisible by 2 with no remainders left and it is \textcolor{pink}{odd} otherwise. Thus, -2, 0, +2 are even but -1, 1 are odd. Numbers ending with 0, 2, 4, 6, 8 are even. Numbers ending with 1, 3, 5, 7, 9 are odd.
```

```{definition 'Positive-Negative'}
An integer $\mathbb{Z}$ is \textcolor{pink}{positive} if it is greater than zero, and \textcolor{pink}{negative} if it is less than zero. Zero is defined as neither negative nor positive. 
```

```{definition 'Mersenne-Primes'}
\textcolor{pink}{Mersenne primes} are those prime number that are of the form \textcolor{pink}{$(2^n -1)$}; that is, $\{3, 7, 31, 127, \ldots \}$
```


Mersenne primes:

- $\{3, 7, 31, 127, 8191, 131071, 524287, 2147483647, 2305843009213693951, \ldots \}$
- $\{3 (2^{nd}), 7(4^{th}), 31(11^{th}), 127(31^{st}), 8191 (1028^{th}), 131071 (12,251^{th}), 524287 (43,390^{th}), \ldots \}$  
  - Mersenne primes with their position in List of Primes
- \textcolor{pink}{$2147483647 = (2^{231} − 1)$}
  - It is 105,097,565$^{th}$ Prime, $8^{th}$ Mersenne prime and is one of only four known double Mersenne primes.
  - It represents the largest value that a signed 32-bit integer field can hold. 

## Primes {#primes-c03 .tabset .tabset-fade}

### Empty Vector {.unlisted .unnumbered}

```{r 'C03-EmptyVector'}
# #Create empty Vector with NA
aa <- rep(NA_integer_, 10)
print(aa)
```

### f_isPrime() {.unlisted .unnumbered}

```{r 'C03F-isPrime', ref.label=c('A09A-isPrime', 'A09B-isPrimeV', 'A09C-isPrimeC'), eval=FALSE }
#
```

### Primes {.unlisted .unnumbered}

```{r 'C03-ListPrime', eval=FALSE}
# #There are 4 Primes in First 10, 25 in 100, 168 in 1000, 1229 in 10000.
# # Using Vectorise Version, get all the Primes
aa <- 1:10
bb <- aa[f_isPrimeV(aa)]
ii <- f_getPrimeUpto(10)
stopifnot(identical(bb, ii))
# #
xxPrime10 <- c(2, 3, 5, 7) |> as.integer()
# #
xxPrime100 <- c(2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 
               53, 59, 61, 67, 71, 73, 79, 83, 89, 97)  |> as.integer()
#
# #Generate List of ALL Primes till 524287 (i.e. Total 43,390 Primes)
xxPrimes <- f_getPrimeUpto(524287L)
# #Save as RDS
f_setRDS(xxPrimes)
```

### Large Integers {.unlisted .unnumbered}

```{r 'C03-LargeNumbers'}
# #NOTE: Assigning 2147483647L causes the Chunk to throw Warnings even with 'eval=FALSE'.
if(FALSE){
# #Assignment of 2305843009213693951L is NOT possible without Warning
# #Even within non-executing Block or with 'eval=FALSE' or suppressWarnings() or tryCatch()
# #It cannot be stored as integer, thus it is automatically converted to double
  #bb <- 2305843009213693951L
# #Warning: non-integer value 2305843009213693951L qualified with L; using numeric value 
# #NOTE that the value changed. It is explicitly NOT a prime anymore.
  #print(aa, digits = 20)
# #[1] 2305843009213693952
#
# #Assignment of 2147483647L is possible and direct printing in console works BUT
# #Its printing will also throw Warnings that are difficult to handle
# #Avoid Printing. Even within non-executing Block, it is affecting R Bookdown.
  aa <- 2147483647L
  #print(aa)
}
```

### f_getPrime() {.unlisted .unnumbered}

```{r 'C03F-getPrimeUpto', ref.label=c('A09D-getPrimeUpto'), eval=FALSE}
#
```

### Benchmark {#benchmark-c03 .unlisted .unnumbered}

```{r 'C03-Benchmark', eval=FALSE}
# #Compare any number of functions
result <- microbenchmark(
  sum(1:100)/length(1:100), 
  mean(1:100),
  #times = 1000,
  check = 'identical'
)
# #Print Table
print(result)
##Unit: microseconds
##                     expr min    lq    mean median     uq    max neval cld
## sum(1:100)/length(1:100) 1.2 1.301 1.54795 1.5005 1.6005  7.501   100  a 
##              mean(1:100) 5.9 6.001 6.56989 6.1010 6.2010 28.001   100   b
#
# #Boxplot of Benchmarking Result
#autoplot(result)
# #Above testcase showed a surprising result of sum()/length() being much faster than mean()
#
# #Or Compare Plot Rendering
if(FALSE) microbenchmark(print(jj), print(kk), print(ll), times = 2)
```

### Sum-Mean {.unlisted .unnumbered}

"ForLater" - Include `rowsum(), rowSums(), colSums(), rowMeans(), colMeans()` in this also.

```{r 'C03-SumMean'}
# #Conclusion: use mean() because precision is difficult to achieve compared to speed
#
# #sum()/length() is faster than mean()
# #However, mean() does double pass, so it would be more accurate
# #mean.default() and var() compute means with an additional pass and so are more accurate
# #e.g. the variance of a constant vector is (almost) always zero 
# #and the mean of such a vector will be equal to the constant value to machine precision.
aa <- 1:100
#
microbenchmark(
  sum(aa)/length(aa), 
  mean(aa),
  mean.default(aa),
  .Internal(mean(aa)),
  #times = 1000,
  check = 'identical'
)
# #rnorm() generates random deviates of given length
set.seed(3)
aa <- rnorm(1e7)
str(aa)
#
# #NOTE manual calculation and mean() is NOT matching
identical(sum(aa)/length(aa), mean(aa))
#
# #There is a slight difference
sum(aa)/length(aa) - mean(aa)
```

### Remove Objects  {.unlisted .unnumbered}

```{r 'C03-rmPattern', eval=FALSE}
if(FALSE) {
  # #Remove all objects matching a pattern
  rm(list = ls(pattern = "f_"))
}
```

### Options Memory {.unlisted .unnumbered}

```{r 'C03-Memory'}
# #Check the Current Options Value
getOption("expressions")
if(FALSE) {
  # #Change Value
  # #NOTE it did not help when recursive function failed
  # #Error: node stack overflow
  # #Error during wrapup: node stack overflow
  # #Error: no more error handlers available ...
  options(expressions=10000)
}
```

### Vectorize() {.unlisted .unnumbered}

```{r 'C03-Vectorize', eval=FALSE}
# #To Vectorise a Function
f_isPrimeV <- Vectorize(f_isPrime)
```

### Compiling {.unlisted .unnumbered}

```{r 'C03-Compile', eval=FALSE}
# #To Pre-Compile a Function for faster performance
f_isPrimeC <- cmpfun(f_isPrime)
```

### Profiling {.unlisted .unnumbered}

```{r 'C03-Profile', eval=FALSE}
# #To Profile a Function Calls for improvements
Rprof("file.out")
f_isPrime(2147483647L)
#f_getPrimesUpto(131071L)
Rprof(NULL)
summaryRprof("file.out")
```

### Legacy A {.unlisted .unnumbered}

```{r 'C03-isPrime-NOT', eval=FALSE}
# #Functions to check for PRIME - All of them have various problems
# #"-3L -2L -1L 0L 1L 8L" FALSE "2L 3L ... 524287L 2147483647L" TRUE
isPrime_a <- function(x) {
  # #Fails for "2147483647L" Error: cannot allocate vector of size 8.0 Gb
  if (x == 2L) {
    return(TRUE)
  } else if (any(x %% 2:(x-1) == 0)) {
    return(FALSE)
  } else return(TRUE)
}

isPrime_b <- function(x){
  # #Comparison of Division and Integer Division by 1, 2, ..., x
  # #Fails for "2147483647L" Error: cannot allocate vector of size 16.0 Gb
  # #Fails for "-ve and zero" Error: missing value where TRUE/FALSE needed
  # vapply(x, function(y) sum(y / 1:y == y %/% 1:y), integer(1L)) == 2L
  if(sum(x / 1:x == x %/% 1:x) == 2) {
    return(TRUE) 
  } else return(FALSE)
}

isPrime_c <- function(x) {
  # #RegEx Slowest: Iit converts -ve values and coerce non-integers which may result in bugs
  x <- abs(as.integer(x))
  if(x > 8191L) {
    print("Do not run this with large values. RegEx is really slow.")
    stop()
  }
  !grepl('^1?$|^(11+?)\\1+$', strrep('1', x))
}

isPrime_d <- function(x) {
  # #Fails for "1" & returns TRUE
  # #Fails for "-ve and zero" Error: NA/NaN argument
  if(x == 2L || all(x %% 2L:max(2, floor(sqrt(x))) != 0)) {
    return(TRUE)
  } else return(FALSE)
}

isPrime_e <- function(x) {
  # #Fails for "-ve and zero" Error: NA/NaN argument
  # #This is the most robust which can be improved by conditional check for positive integers
  # #However, this checks the number against ALL Smaller values including non-primes
  if(x == 2L || all(x %% 2L:ceiling(sqrt(x)) != 0)) {
    # # "seq.int(3, ceiling(sqrt(x)), 2)" is slower
    return(TRUE)
  } else {
    ## (any(x %% 2L:ceiling(sqrt(x)) == 0))
    ## (any(x %% seq.int(3, ceiling(sqrt(x)), 2) == 0))
    ## NOTE Further, if sequence starts from 3, add 2 also as a Prime Number
    return(FALSE)
  }
}
```

### Legacy B {.unlisted .unnumbered}

```{r 'C03-getPrime-NOT', eval=FALSE}
# #131071 (12,251th), 524287 (43,390th), 2147483647 (105,097,565th)
aa <- 1:131071
# #Following works but only till 524287L, Memory Overflow ERROR for 2147483647L
bb <- aa[f_isPrimeV(aa)]

getPrimeUpto_a <- function(x){
  # #Extremely slow, cannot go beyond 8191L in benchmark testing
  if(x < 2) return("ERROR")
  y <- 2:x
  primes <- rep(2L, x)
  j <- 1L
  for (i in y) {
    if (!any(i %% primes == 0)) {
      j <- j + 1L
      primes[j] <- i
	  #cat(paste0("i=", i, ", j=", j, ", Primes= ", paste0(head(primes, j), collapse = ", ")))
    }
	#cat("\n")
  }
  result <- head(primes, j)
  #str(result)
  #cat(paste0("Head: ", paste0(head(result), collapse = ", "), "\n"))
  #cat(paste0("Tail: ", paste0(tail(result), collapse = ", "), "\n"))
  return(result)
}

getPrimeUpto_b <- function(x){
# #https://stackoverflow.com/questions/3789968/
  # #This is much faster even from the "aa[f_isPrimeV(aa)]"
    if(x < 2) return("ERROR")
    y <- 2:x
    i <- 1
    while (y[i] <= sqrt(x)) {
        y <-  y[y %% y[i] != 0 | y == y[i]]
        i <- i+1
    }
	result <- y
    #str(result)
    #cat(paste0("Head: ", paste0(head(result), collapse = ", "), "\n"))
    #cat(paste0("Tail: ", paste0(tail(result), collapse = ", "), "\n"))
    return(result)
}

getPrimeUpto_c <- function(x) {
  # #Problems and Slow
  # #Returns a Vetor of Primes till the Number i.e. f_getPrimesUpto(7) = (2, 3, 5, 7)
  # #NOTE: f_getPrimesUpto(1) and f_getPrimesUpto(2) both return "2"
  if(!is.integer(x)) {
    cat("Error! Integer required. \n")
    stop()
  } else if(!identical(1L, length(x))) {
    cat("Error! Unit length vector required. \n")
    stop()
  } else if(x <= 0L) {
    cat("Error! Positive Integer required. \n")
    stop()
  } else if(x > 2147483647) {
    cat(paste0("Doubles are stored as approximation. Prime will not be calculated for value higher than '2147483647' \n"))
    stop()
  }
  
  # #Cannot create vector of length 2147483647L and also not needed that many
  # #ceiling(sqrt(7L)) return 3, however we need length 4 (2, 3, 5, 7)
  # #So, added extra 10
  #primes <- rep(NA_integer_, 10L + sqrt(2L))
  primes <- rep(2L, 10L + sqrt(2L))
  j <- 1L
  primes[j] <- 2L
  #
  i <- 2L
  while(i <= x) {
    # #na.omit() was the slowest step, so changed all NA to 2L in the primes
    #k <- na.omit(primes[primes <= ceiling(sqrt(i))])
    k <- primes[primes <= ceiling(sqrt(i))]
    if(all(as.logical(i %% k))) {
      j <- j + 1
      primes[j] <- i
    }  
    # #Increment with INTEGER Addition
    i = i + 1L
  }
  result <- primes[complete.cases(primes)]
  str(result)
  cat(paste0("Head: ", paste0(head(result), collapse = ", "), "\n"))
  cat(paste0("Tail: ", paste0(tail(result), collapse = ", "), "\n"))
  return(result)
}

getPrimeUpto_d <- function(n = 10L, i = 2L, primes = c(2L), bypass = TRUE){
  # #Using Recursion is NOT a good solution
  # #Function to return N Primes upto 1000 Primes (7919) or Max Value reaching 10000.
  if(i > 10000){
    cat("Reached 10000 \n")
    return(primes)
  }
  if(bypass) {
    maxN <- 1000L
    if(!is.integer(n)) {
      cat("Error! Integer required. \n")
      stop()
    } else if(!identical(1L, length(n))) {
      cat("Error! Unit length vector required. \n")
      stop()
    } else if(n <= 0L) {
      cat("Error! Positive Integer required. \n")
      stop()
    } else if(n > maxN) {
      cat(paste0("Error! This will calculate only upto ", maxN, " prime Numebers. \n"))
      stop()
    }
  }
  if(length(primes) < n) {
    if(all(as.logical(i %% primes[primes <= ceiling(sqrt(i))]))) {
      # #Coercing 0 to FALSE, Non-zero Values to TRUE
      # # "i %% 2L:ceiling(sqrt(i))" checks i agains all integers till sqrt(i)
      # # "primes[primes <= ceiling(sqrt(i))]" checks i against only the primes till sqrt(i)
      # #However, the above needs hardcoded 2L as prime so the vector is never empty
      # #Current Number is Prime, so include it in the vector and check the successive one
      f_getPrime(n, i = i+1, primes = c(primes, i), bypass = FALSE)
    } else {
      # #Current Number is NOT Prime, so check the successive one
      f_getPrime(n, i = i+1, primes = primes, bypass = FALSE)
    }
  } else {
    # #Return the vector when it reaches the count
    return(primes)
  }
}
```

## Measures of Location

```{definition 'Measures-of-Location'}
\textcolor{pink}{Measures of location} are numerical summaries that indicate where on a number line a certain characteristic of the variable lies. Examples of the measures of location are percentiles and quantiles.
```

```{definition 'Measures-of-Center'}
The \textcolor{pink}{measures of center} are a special case of measures of location. These estimate where the center of a particular variable lies. Most common are Mean, Median, and Mode.
```


### Mean {#mean-c03 .tabset .tabset-fade}

```{definition 'Mean'}
Given a data set ${X = \{{x}_1, {x}_2, \ldots, {x}_n\}}$, the \textcolor{pink}{mean ${\overline{x}}$} is the sum of all of the values ${{x}_1, {x}_2, \ldots, {x}_n}$ divided by the count ${n}$.
```

- Refer equation \@ref(eq:mean)
  - Sample mean is denoted by \textcolor{pink}{${\overline{x}}$} (x bar) and Population mean is denoted by \textcolor{pink}{${\mu}$}. 
  - Mean is the most commonly used measure of central location, even though it is influenced by extreme values.

\begin{equation} 
  \overline{x} = \frac{1}{n}\left (\sum_{i = 1}^n{{x}_i}\right ) = \frac{{x}_1 + {x}_2 + \cdots + {x}_n}{n}
  (\#eq:mean)
\end{equation} 

In the mean calculation, normally each ${{x}_i}$ is given equal importance or weightage of ${1/n}$. However, in some instances the mean is computed by giving each observation a weight that reflects its relative importance. A mean computed in this manner is referred to as the \textcolor{pink}{weighted mean}, as given in equation \@ref(eq:mean-weighted)

\begin{equation} 
  \overline{x} = \frac{\sum_{i=1}^n{w_ix_i}}{\sum_{i=1}^n{w_i}}
  (\#eq:mean-weighted)
\end{equation} 

\textcolor{orange}{Caution:} Unit of mean is same as unit of the variable e.g. cost_per_kg thus 'w' would be 'kg'.

#### Mean {.unlisted .unnumbered}

```{r 'C03-Mean'}
aa <- 1:10
# #Mean of First 10 Numbers
mean(aa)
```

#### More {.unlisted .unnumbered}

```{r 'C03-MeanMore'}
aa <- 1:10
# #Mean of First 10 Numbers
ii <- mean(aa)
print(ii)
jj <- sum(aa)/length(aa)
stopifnot(identical(ii, jj))
#
# #Mean of First 10 Prime Numbers (is neither Prime nor Integer)
mean(f_getRDS(xxPrimes)[1:10])
#
# #Mean of First 100 Digits of PI
f_getRDS(xxPI)[1:100, ] %>% pull(VAL) %>% mean()
```

#### Weighted Mean {.unlisted .unnumbered}

```{r 'C03-WeightedMean'}
aa <- tibble(Purchase = 1:5, cost_per_kg = c(3, 3.4, 2.8, 2.9, 3.25), 
             kg = c(1200, 500, 2750, 1000, 800))
# #NOTE that unit of mean is same as unit of the variable e.g. cost_per_kg thus 'w' would be 'kg'
(ii <- sum(aa$cost_per_kg * aa$kg)/sum(aa$kg))
jj <- with(aa, sum(cost_per_kg * kg)/sum(kg))
kk <- weighted.mean(x = aa$cost_per_kg, w = aa$kg)
stopifnot(all(identical(ii, jj), identical(ii, kk)))
```

### Median {#median-c03 .tabset .tabset-fade}

```{definition 'Median'}
\textcolor{pink}{Median} of a population is any value such that at most half of the population is less than the proposed median and at most half is greater than the proposed median.  
```

- Refer equation \@ref(eq:median)
  - The median is the value in the middle when the data is sorted
  - For an odd number of observations, the median is the middle value.
  - For an even number of observations, the median is the average of the two middle values.
  - Although the mean is the more commonly used measure of central location, whenever a data set contains extreme values, the median is preferred.
    - \textcolor{pink}{The mean and median are different concepts and answer different questions.} 
      - Ex: Income - nearly always reported as median, but if we are looking the the 'spending power of whole community' it may no not be right.
  - The median is well-defined for any ordered data, and is independent of any distance metric. 
    - The median can thus be applied to classes which are ranked but not numerical (ordinal), although the result might be halfway between classes if there is an even number of cases. 

\begin{equation} 
  \begin{align} 
    \text{if n is odd, } median(x) & = x_{(n + 1)/2} \\
    \text{if n is even, } median(x) & = \frac{x_{(n/2)} + x_{(n/2) + 1}}{2}
  \end{align}
  (\#eq:median)
\end{equation} 

#### Median {.unlisted .unnumbered}

```{r 'C03-Median'}
aa <- 1:10 
# #Median of First 10 Numbers
median(aa)
```

#### More {.unlisted .unnumbered}

```{r 'C03-MedianMore'}
aa <- 1:10 
# #Median of First 10 Numbers
median(aa)
#
# #Median of First 10 Prime Numbers (is NOT prime)
median(f_getRDS(xxPrimes)[1:10])
#
# #Median of First 100 Digits of PI
f_getRDS(xxPI)[1:100, ] %>% pull(VAL) %>% median()
```

### Geometric Mean {.tabset .tabset-fade}

```{definition 'Geometric-Mean'}
The \textcolor{pink}{geometric mean $\overline{x}_g$} is a measure of location that is calculated by finding the n^{th} root of the product of ${n}$ values. 
```

- Refer equation \@ref(eq:geometric-mean)
  - The geometric mean applies only to positive numbers
  - The geometric mean is often used for a set of numbers whose values are meant to be multiplied together or are exponential in nature
  - For all positive data sets containing at least one pair of unequal values, the harmonic mean is always the least of the three means, while the arithmetic mean is always the greatest of the three and the geometric mean is always in between.

\begin{equation} 
  \overline{x}_g = \left(\prod _{i=1}^{n} x_i\right)^{\frac{1}{n}} = \sqrt[{n}]{x_1 x_2 \ldots x_n}
  (\#eq:geometric-mean)
\end{equation} 

#### Geometric Mean {.unlisted .unnumbered }

```{r 'C03-GeometricMean'}
aa <- 1:10
# #Geometric Mean of of First 10 Numbers
exp(mean(log(aa)))
```

#### More {.unlisted .unnumbered }

```{r 'C03-GMmore'}
aa <- 1:10
# #Geometric Mean of of First 10 Numbers
ii <- exp(mean(log(aa)))
jj <- prod(aa)^(1/length(aa))
stopifnot(identical(ii, jj))
#
# #Geometric Mean of First 10 Prime Numbers 
exp(mean(log(f_getRDS(xxPrimes)[1:10])))
```

### Mode {#mode-c03 .tabset .tabset-fade}

```{definition 'Mode'}
The \textcolor{pink}{mode} is the value that occurs with greatest frequency.
```

- The median makes sense when there is a linear order on the possible values. Unlike median, the concept of mode makes sense for any random variable assuming values from a vector space.

#### Mode {.unlisted .unnumbered }

```{r 'C03-Mode'}
# #Mode of First 100 Digits of PI
bb <- f_getRDS(xxPI)[1:100, ] %>% pull(VAL)
f_getMode(bb)
```

#### More {.unlisted .unnumbered }

```{r 'C03-ModeMore'}
# #Mode of First 100 Digits of PI
bb <- f_getRDS(xxPI)[1:100, ]
#
# #Get Frequency
bb %>% count(VAL)
#
# #Get Mode
bb %>% pull(VAL) %>% f_getMode()
```

#### f_getMode() {.unlisted .unnumbered }

```{r 'C03F-getMode', ref.label=c('A10A-getMode')}
#
```

### Percentiles {#percentiles-c03 .tabset .tabset-fade}

```{definition 'Percentile'}
A \textcolor{pink}{percentile} provides information about how the data are spread over the interval from the smallest value to the largest value. For a data set containing ${n}$ observations, the $p^{th}$ percentile divides the data into two parts: approximately p% of the observations are less than the $p^{th}$ percentile, and approximately (100 - p)% of the observations are greater than the $p^{th}$ percentile. 
```

- Refer equation \@ref(eq:percentiles)
  - Percentile is the value which divides the data into two groups when it is sorted
  - \textcolor{pink}{Quartiles} are specific percentiles of 25%, 50% and 75%
  - \textcolor{pink}{Median} is 50% percentile
  - \textcolor{orange}{Caution:} Excel "PERCENTILE.EXC" calculations match with `type =6` option of `quantile()`, default is `type =7`
  
\begin{equation} 
  L_p = \frac{p}{100}(n + 1)
  (\#eq:percentiles)
\end{equation}

#### Percentiles {.unlisted .unnumbered}

```{r 'C03-Percentile'}
# #First 100 Digits of PI
bb <- f_getRDS(xxPI)[1:100, ]
#
# #50% Percentile of Digits i.e. Median
quantile(bb$VAL, 0.5)
```

#### More {.unlisted .unnumbered}

```{r 'C03-PercentileMore'}
# #First 100 Digits of PI
bb <- f_getRDS(xxPI)[1:100, ]
#
# #50% Percentile of Digits i.e. Median
ii <- quantile(bb$VAL, 0.5)
print(ii)
jj <- median(bb$VAL)
stopifnot(identical(unname(ii), jj))
# 
# #All Quartiles
quantile(bb$VAL, seq(0, 1, 0.25))
# #summary()
summary(bb$VAL)
#
# #To Match with Excel "PERCENTILE.EXC" use type=6 in place of default type=7
quantile(bb$VAL, seq(0, 1, 0.25), type = 6)
```

## Measures of Variability

```{definition 'Measures-of-Spread'}
\textcolor{pink}{Measures of spread} (or the measures of variability) describe how spread out the data values are. Examples are Range, SD, mean absolute deviation, and IQR
```


In addition to measures of location, it is often desirable to consider measures of variability, or dispersion.

- Range `range()`
  - (Largest value - Smallest value) i.e. `max() - min()`
  - Range is based on only two of the observations and thus is highly influenced by extreme values.
- Interquartile Range (IQR) `IQR()`
  - The difference between the third quartile $(Q3, 75\%)$, and the first quartile $(Q1, 25\%)$
  - IQR is a measure of variability, much more robust than the SD. IQR is less sensitive to the presence of the outliers.
  - It overcomes the dependency on extreme values
  - $x_i \notin [Q_1 - 1.5 * \text{IQR}, Q_3 + 1.5 * \text{IQR}] \to x_i \in \text{Outlier}$
  - It is assumed that any data point not in [Q1 - 1.5 * IQR, Q3 + 1.5 * IQR] is an outlier
    - 1.5 multiplication gives us a window of ${\mu} \pm 2.7 {\sigma} \approx {\mu} \pm 3 {\sigma}$ which is quite closer to Normal Plot limit.

- Mean Absolute Error (MAE)
  - $\text{MAE} = \frac{\sum |x_i - \overline{x}|}{n}$

### Variance {#variance-c03 .tabset .tabset-fade}

```{definition 'Variance'}
The \textcolor{pink}{variance $({\sigma}^2)$} is based on the difference between the value of each observation ${x_i}$ and the mean ${\overline{x}}$. The average of the squared deviations is called the variance. 
```

- Refer equation \@ref(eq:variance)
  - Sample Variance is denoted by \textcolor{pink}{$s^2$} and Population Variance is denoted by \textcolor{pink}{$\sigma^2$}
  - The variance is a measure of variability that utilizes all the data.
  - The difference between each ${x_i}$ and the mean ($\overline{x}, \mu$) is called a \textcolor{pink}{deviation about the mean} i.e. ($x_i - \overline{x}$). Sum of deviation about the mean is always zero i.e. $\sum (x_i - \overline{x}) =0$
  - In the computation of the variance, the deviations about the mean are squared.
    - Because of the squaring involved, it is sensitive to the presence of outliers, leading analysts to prefer other measures of spread, such as the mean absolute deviation, in situations involving extreme values.

\begin{equation} 
  \begin{align} 
    \sigma^2 &= \frac{1}{n} \sum _{i=1}^{n} \left(x_i - \mu \right)^2 \\
    s^2 &= \frac{1}{n-1} \sum _{i=1}^{n} \left(x_i - \overline{x} \right)^2
  \end{align}
  (\#eq:variance)
\end{equation} 

### Standard Deviation {#sd-c03 .tabset .tabset-fade}

```{definition 'Standard-Deviation'}
The \textcolor{pink}{standard deviation ($s, \sigma$)} is defined to be the positive square root of the variance. It is a measure of the amount of variation or dispersion of a set of values.
```

- Refer equation \@ref(eq:sd)
  - Standard deviation for sample is denoted by \textcolor{pink}{${s}$} and for Population by \textcolor{pink}{${\sigma}$}
  - It can be interpreted as the "typical" distance between a field value and the mean
  - The \textcolor{pink}{coefficient of variation} is a relative measure of variability. It measures the standard deviation relative to the mean. It is given in percentage as $100 \times \sigma / \mu$

\begin{equation} 
  \begin{align} 
     \sigma &= \sqrt{\frac{1}{N} \sum_{i=1}^N \left(x_i - \mu\right)^2} \\
    {s} &= \sqrt{\frac{1}{N-1} \sum_{i=1}^N \left(x_i - \overline{x}\right)^2}
  \end{align}
  (\#eq:sd)
\end{equation} 

## Measures of Distribution Shape 

### Skewness {#skewness-c03 .tabset .tabset-fade} 

```{definition 'Skewness'}
\textcolor{pink}{Skewness $(\tilde{\mu}_{3})$} is a measure of the shape of a data distribution. It is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean.
```

```{definition 'Tails'}
A \textcolor{pink}{tail} refers to the tapering sides at either end of a distribution curve.
```


- Data skewed to the left result in negative skewness; a symmetric data distribution results in zero skewness; and data skewed to the right result in positive skewness. 
- $\tilde{\mu}_{3}$ is the $3^{rd}$ standardized moment
  - Side topic: A \textcolor{pink}{standardized moment} of a probability distribution is a moment (normally a higher degree central moment) that is normalized. The normalization is typically a division by an expression of the standard deviation which renders the moment scale invariant. 
    - $\tilde{\mu}_{1} = 0$, because the first moment about the mean is always zero. 
    - $\tilde{\mu}_{2} = 1$, because the second moment about the mean is equal to the variance ${\sigma}^2$. 
    - $\tilde{\mu}_{3}$ is a measure of skewness
    - $\tilde{\mu}_{4}$ refers to the Kurtosis 


Refer figure \@ref(fig:C03P060405) 

- No skew: (symmetric)
  - A unimodal distribution with zero value of skewness does not imply that this distribution is symmetric necessarily. However, a symmetric unimodal or multimodal distribution always has zero skewness.   - The normal distribution has a skewness of zero. But reverse may not be true.
- Negative skew: (left-skewed, left-tailed, or skewed to the left)
  - The left tail is longer, thus the 'left' refers to the left tail being drawn out
  - The curve itself appears to be leaning to the right i.e. the mass of the distribution is concentrated on the right of the figure
- Positive skew: (right-skewed, right-tailed, or skewed to the right)
  - The right tail is longer, thus the 'right' refers to the right tail being drawn out 
  - The curve itself appears to be leaning to the left i.e. the mass of the distribution is concentrated on the left of the figure
- Relationship of mean and median
  - The skewness is not directly related to the relationship between the mean and median: a distribution with negative skew can have its mean greater than or less than the median, and likewise for positive skew
  - However, generally the skew can be calculated as $({\mu} -{\nu})/\sigma$, where ${\nu}$ is median
- Application:
  - Skewness indicates the direction and relative magnitude of deviation from the normal distribution.
  - It indicates the direction of outliers
  - With pronounced skewness, standard statistical inference procedures such as a confidence interval for a mean will be not only incorrect, in the sense that the true coverage level will differ from the nominal (e.g., 95%) level, but they will also result in unequal error probabilities on each side. 

Skewness is given by the equation \@ref(eq:skewness), which is being shown here because it ~~looked cool~~ has deep meaning

\begin{equation} 
 Skew = \frac{\tfrac {1}{n}\sum_{i=1}^{n}(x_{i}-{\overline{x}})^{3}}{\left[\tfrac {1}{n-1}\sum_{i=1}^{n}(x_{i}-{\overline{x}})^{2} \right]^{3/2}}
  (\#eq:skewness)
\end{equation} 


#### Charts {.unlisted .unnumbered}

```{r 'C03-LoadDistributions-A', include=FALSE, ref.label=c('C03-LoadDistributions')}
#
```

```{r 'C03-DensityNormal', include=FALSE, eval=FALSE}
hh <- tibble(ee = xxNormal)
ttl_hh <- "Normal Distribution (Symmetrical)"
cap_hh <- "C03P04"
```

```{r 'C03-DensityNormal-A', ref.label=c('C03-DensityNormal', 'C03-Density'), include=FALSE}
#
```

```{r 'C03P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C03P04", "-Normal-Symmetric", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = eval(parse(text = cap_hh)), device = "png", dpi = 144) 
}
```

```{r 'C03P04', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C03P04", "-Normal-Symmetric", ".png"))
```

```{r 'C03-DensityExp', include=FALSE, eval=FALSE}
hh <- tibble(ee = xxExp)
ttl_hh <- "Exponential Distribution (Positive Skew, Right Tail)"
cap_hh <- "C03P05" 
```

```{r 'C03-DensityExp-A', ref.label=c('C03-DensityExp', 'C03-Density'), include=FALSE}
#
```

```{r 'C03P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C03P05", "-Exponential-Positive", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = eval(parse(text = cap_hh)), device = "png", dpi = 144) 
}
```

```{r 'C03P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C03P05", "-Exponential-Positive", ".png"))
```

```{r 'C03-DensityBeta', include=FALSE, eval=FALSE}
hh <- tibble(ee = xxBeta)
ttl_hh <- "Beta Distribution (Negative Skew, Left Tail)"
cap_hh <- "C03P06"
```

```{r 'C03-DensityBeta-A', ref.label=c('C03-DensityBeta', 'C03-Density'), include=FALSE}
#
```

```{r 'C03P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C03P06", "-Beta-Negative", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = eval(parse(text = cap_hh)), device = "png", dpi = 144) 
}
```

```{r 'C03P06', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C03P06", "-Beta-Negative", ".png"))
```

```{r 'C03P060405', echo=FALSE, out.width='33%', ref.label=c('C03P06', 'C03P04', 'C03P05'), fig.cap="(C03P06 C03P04 C03P05) (Left Tail, Negative) Beta, Normal Distribution, Exponential (Positive, Right Tail)"}
#
```

#### skewness() {.unlisted .unnumbered}

```{r 'C03-Skewness-Gen'}
# #Skewness Calculation: Package "e1071" (Package "moments" deprecated)
even_skew <- c(49, 50, 51)
pos_skew <- c(even_skew, 60)
neg_skew <- c(even_skew, 40)
skew_lst <- list(even_skew, pos_skew, neg_skew)
# #Mean, Median, SD
cat(paste0("Mean (neg, even, pos): ", 
           paste0(vapply(skew_lst, mean, numeric(1)), collapse = ", "), "\n"))
cat(paste0("Median (neg, even, pos): ", 
           paste0(vapply(skew_lst, median, numeric(1)), collapse = ", "), "\n"))
cat(paste0("SD (neg, even, pos): ", paste0(
           round(vapply(skew_lst, sd, numeric(1)), 1), collapse = ", "), "\n"))
#
cat(paste0("Skewness (neg, even, pos): ", paste0(
           round(vapply(skew_lst, e1071::skewness, numeric(1)), 1), collapse = ", "), "\n"))
cat(paste0("Kurtosis (neg, even, pos): ", paste0(
           round(vapply(skew_lst, e1071::kurtosis, numeric(1)), 1), collapse = ", "), "\n"))
```

#### Normal Exp Beta {.unlisted .unnumbered}

```{r 'C03-SkewnessNormal'}
# #Skewness Calculation: Package "e1071" (Package "moments" deprecated)
dis_lst <- list(xxNormal, xxExp, xxBeta)
#
# #Skewness: Normal has value close to 3 Kurtosis (=0 excess Kurtosis)
# #Skewness "e1071" has Type = 3 as default. Its Type = 1 matches "moments"
# #Practically, Normal has (small) NON-Zero Positive Skewness
skew_e_t3 <- vapply(dis_lst, e1071::skewness, numeric(1))
skew_e_t2 <- vapply(dis_lst, e1071::skewness, type = 2, numeric(1))
skew_e_t1 <- vapply(dis_lst, e1071::skewness, type = 1, numeric(1))
skew_mmt <-  vapply(dis_lst, moments::skewness, numeric(1))
stopifnot(identical(round(skew_e_t1, 10), round(skew_mmt, 10)))
cat(paste0("e1071: Type = 1 Skewness (Normal, Exp, Beta): ", 
           paste0(round(skew_e_t1, 4), collapse = ", "), "\n"))
cat(paste0("e1071: Type = 2 Skewness (Normal, Exp, Beta): ", 
           paste0(round(skew_e_t2, 4), collapse = ", "), "\n"))
cat(paste0("e1071: Type = 3 Skewness (Normal, Exp, Beta): ", 
           paste0(round(skew_e_t3, 4), collapse = ", "), "\n"))
#
# #Formula: (sigma_ (x_i - mu)^3) /(n * sd^3)
bb <- xxNormal
skew_man <- sum({bb - mean(bb)}^3) / {length(bb) * sd(bb)^3}
cat(paste0("(Manual) Skewness of Normal: ", round(skew_man, 4), 
           " (vs. e1071 Type 3 = ", round(skew_e_t3[1], 4), ") \n"))
```

#### Distributions {.unlisted .unnumbered}

```{r 'C03-Distributions', eval=FALSE}
set.seed(3)
nn <- 10000L
# #Normal distribution is symmetrical
xxNormal <- rnorm(n = nn, mean = 0, sd = 1)
# #The exponential distribution is positive skew
xxExp <- rexp(n = nn, rate = 1)
# #The beta distribution with hyper-parameters α=5 and β=2 is negative skew
xxBeta <- rbeta(n = nn, shape1 = 5, shape2 = 2)
#
# #Save
f_setRDS(xxNormal)
f_setRDS(xxExp)
f_setRDS(xxBeta)
#f_getRDS(xxNormal)
```

```{r 'C03-LoadDistributions', eval=FALSE}
# #Get the Distributions
xxNormal <- f_getRDS(xxNormal)
xxExp <- f_getRDS(xxExp)
xxBeta <- f_getRDS(xxBeta)
```

#### Density {.unlisted .unnumbered}

```{r 'C03-Density', eval=FALSE}
# #Density Curve
# #Assumes 'hh' has data in 'ee'. In: cap_hh
#Basics
mean_hh <- mean(hh$ee)
sd_hh <- sd(hh$ee)
#
skew_hh <- skewness(hh$ee)
kurt_hh <- kurtosis(hh$ee)
# #Get Quantiles and Ranges of mean +/- sigma 
q05_hh <- quantile(hh[[1]], .05)
q95_hh <- quantile(hh[[1]], .95)
density_hh <- density(hh[[1]])
density_hh_tbl <- tibble(x = density_hh$x, y = density_hh$y)
sig3r_hh <- density_hh_tbl %>% filter(x >= {mean_hh + 3 * sd_hh})
sig3l_hh <- density_hh_tbl %>% filter(x <= {mean_hh - 3 * sd_hh})
sig2r_hh <- density_hh_tbl %>% filter(x >= {mean_hh + 2 * sd_hh}, {x < mean_hh + 3 * sd_hh})
sig2l_hh <- density_hh_tbl %>% filter(x <= {mean_hh - 2 * sd_hh}, {x > mean_hh - 3 * sd_hh})
sig1r_hh <- density_hh_tbl %>% filter(x >= {mean_hh + sd_hh}, {x < mean_hh + 2 * sd_hh})
sig1l_hh <- density_hh_tbl %>% filter(x <= {mean_hh - sd_hh}, {x > mean_hh - 2 * sd_hh})
sig0r_hh <- density_hh_tbl %>% filter(x > mean_hh, {x < mean_hh + 1 * sd_hh})
sig0l_hh <- density_hh_tbl %>% filter(x < mean_hh, {x > mean_hh - 1 * sd_hh})
#
# #Change x-Axis Ticks interval
xbreaks_hh <- seq(-3, 3)
xpoints_hh <- mean_hh + xbreaks_hh * sd_hh
#
# # Latex Labels 
xlabels_hh <- c(TeX(r'($\,\,\mu - 3 \sigma$)'), TeX(r'($\,\,\mu - 2 \sigma$)'), 
                TeX(r'($\,\,\mu - 1 \sigma$)'), TeX(r'($\mu$)'), TeX(r'($\,\,\mu + 1 \sigma$)'), 
                TeX(r'($\,\,\mu + 2 \sigma$)'), TeX(r'($\,\,\mu + 3\sigma$)'))
#
C03 <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) + 
  geom_density(alpha = 0.2, colour = "#21908CFF") + 
  geom_area(data = sig3l_hh, aes(x = x, y = y), fill = '#440154FF') + 
  geom_area(data = sig3r_hh, aes(x = x, y = y), fill = '#440154FF') + 
  geom_area(data = sig2l_hh, aes(x = x, y = y), fill = '#3B528BFF') + 
  geom_area(data = sig2r_hh, aes(x = x, y = y), fill = '#3B528BFF') + 
  geom_area(data = sig1l_hh, aes(x = x, y = y), fill = '#21908CFF') + 
  geom_area(data = sig1r_hh, aes(x = x, y = y), fill = '#21908CFF') + 
  geom_area(data = sig0l_hh, aes(x = x, y = y), fill = '#5DC863FF') + 
  geom_area(data = sig0r_hh, aes(x = x, y = y), fill = '#5DC863FF') + 
  scale_x_continuous(breaks = xpoints_hh, labels = xlabels_hh) + 
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), 
        axis.ticks = element_blank(), 
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
        axis.line.y = element_blank(), axis.title.y = element_blank(), axis.text.y = element_blank()) + 
  labs(x = "x", y = "Density", 
       subtitle = paste0("Mean = ", round(mean_hh, 3), "; SD = ", round(sd_hh, 3), "; Skewness = ", round(skew_hh, 3), "; Kurtosis = ", round(kurt_hh, 3)), 
        caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C03)
rm(C03)
```

### Kurtosis {#kurtosis-c03 .tabset .tabset-fade} 

```{definition 'Kurtosis'}
\textcolor{pink}{Kurtosis $(\tilde{\mu}_{4})$} is a measure of the "tailedness" of the probability distribution of a real-valued random variable. Like skewness, kurtosis describes the shape of a probability distribution. For ${\mathcal{N}}_{(\mu, \, \sigma)}$, kurtosis is 3 and \textcolor{pink}{excess kurtosis} is 0 (i.e. subtract 3).
```

Distributions with zero excess kurtosis are called \textcolor{pink}{mesokurtic}. The most prominent example of a mesokurtic distribution is the normal distribution. The kurtosis of any univariate normal distribution is 3. 

Distributions with kurtosis less than 3 are said to be \textcolor{pink}{platykurtic}. It means the distribution produces fewer and less extreme outliers than does the normal distribution. An example of a platykurtic distribution is the uniform distribution, which does not produce outliers.

Distributions with kurtosis greater than 3 are said to be \textcolor{pink}{leptokurtic}. An example of a leptokurtic distribution is the Laplace distribution, which has tails that asymptotically approach zero more slowly than a Gaussian, and therefore produces more outliers than the normal distribution. 

Kurtosis is the average (or expected value) of the standardized data raised to the fourth power. Any standardized values that are less than 1 (i.e., data within one standard deviation of the mean, where the "peak" would be), contribute virtually nothing to kurtosis, since raising a number that is less than 1 to the fourth power makes it closer to zero. The only data values that contribute to kurtosis in any meaningful way are those outside the region of the peak; i.e., the outliers. Therefore, kurtosis measures outliers only; it measures nothing about the "peak". 

The sample kurtosis is a useful measure of whether there is a problem with outliers in a data set. Larger kurtosis indicates a more serious outlier problem. 

```{r 'C03-Kurtosis'}
# #Kurtosis Calculation: Package "e1071" (Package "moments" deprecated)
dis_lst <- list(xxNormal, xxExp, xxBeta)
#
# #Kurtosis: Normal has value close to 3 Kurtosis (=0 excess Kurtosis)
# #Kurtosis "e1071" has Type = 3 as default. Its Type = 1 matches "moments" with difference of 3
kurt_e_t3 <- vapply(dis_lst, e1071::kurtosis, numeric(1))
kurt_e_t2 <- vapply(dis_lst, e1071::kurtosis, type = 2, numeric(1))
kurt_e_t1 <- vapply(dis_lst, e1071::kurtosis, type = 1, numeric(1))
kurt_mmt <-  vapply(dis_lst, moments::kurtosis, numeric(1))
stopifnot(identical(round(kurt_e_t1, 10), round(kurt_mmt - 3, 10)))
cat(paste0("e1071: Type = 1 Kurtosis (Normal, Exp, Beta): ", 
           paste0(round(kurt_e_t1, 4), collapse = ", "), "\n"))
cat(paste0("e1071: Type = 2 Kurtosis (Normal, Exp, Beta): ", 
           paste0(round(kurt_e_t2, 4), collapse = ", "), "\n"))
cat(paste0("e1071: Type = 3 Kurtosis (Normal, Exp, Beta): ", 
           paste0(round(kurt_e_t3, 4), collapse = ", "), "\n"))
#
# #Formula: (sigma_ (x_i - mu)^4) /(n * sd^4)
bb <- xxNormal
kurt_man <- {sum({bb - mean(bb)}^4) / {length(bb) * sd(bb)^4}} - 3
cat(paste0("(Manual) Kurtosis of Normal: ", round(kurt_man, 4), 
           " (vs. e1071 Type 3 = ", round(kurt_e_t3[1], 4), ") \n"))
```

## Relative Location

### z-Scores {#z-scores-c03 .tabset .tabset-fade}

Measures of relative location help us determine how far a particular value is from the mean. By using both the mean and standard deviation, we can determine the relative location of any observation. 

```{definition 'TheSample'}
A sample of \textcolor{pink}{${n}$} observations given by ${X = \{{x}_1, {x}_2, \ldots, {x}_n\}}$ have a sample mean \textcolor{pink}{${\overline{x}}$} and the sample standard deviation, \textcolor{pink}{${s}$}.
```

```{definition 'z-Scores'}
The \textcolor{pink}{z-score, ${z_i}$}, can be interpreted as the number of standard deviations ${x_i}$ is from the mean ${\overline{x}}$. It is associated with each ${x_i}$. The z-score is often called the \textcolor{pink}{standardized value} or \textcolor{pink}{standard score}. 
```

- Refer equation \@ref(eq:z-scores) (Similar to equation \@ref(eq:z-val))
  -  For example, $z_1 = 1.2$  would indicate that ${x_1}$ is 1.2 standard deviations greater than the sample mean. Similarly, $z_2 = -0.5$ would indicate that ${x_2}$ is 0.5 standard deviation less than the sample mean. 
  - A z-score greater than zero occurs for observations with a value greater than the mean, and a z-scoreless than zero occurs for observations with a value less than the mean. 
  - A z-score of zero indicates that the value of the observation is equal to the mean.
  - The z-score for any observation can be interpreted as a measure of the relative location of the observation in a data set.
  - The process of converting a value for a variable to a z-score is often referred to as a \textcolor{pink}{z transformation} or \textcolor{pink}{scaling}.

\begin{equation} 
  z_i = \frac{{x}_i - {\overline{x}}}{{s}}
  (\#eq:z-scores)
\end{equation} 

NOTE:  "Z statistic" is a special case of "Z critical" because $\sigma/\sqrt{n}$ is the 'standard error of the sample mean' which means that it is a standard deviation. Rather than (eg) a known population standard deviation or even just sample standard deviation, per CLT, it is the standard deviation of the sample mean. The 'critical Z' (i.e. standard score) is something than can always be computed ("a general case") whenever there is a mean and standard deviation; it translates X into a Z variable with zero mean and unit variance. (it "imposes normality" when the data may not be normal!). The "Z statistic" similarly standardizes as a special case where it is standardizing the sample mean.

```{definition 't-statistic'}
Computing a z-score requires knowing the mean ${\mu}$ and standard deviation ${\sigma}$ of the complete population to which a data point belongs. If one only has a sample of observations from the population, then the analogous computation with sample mean ${\overline{x}}$ and sample standard deviation ${s}$ yields the \textcolor{pink}{t-statistic}. 
```

\textcolor{orange}{Caution:} 

- Scaling does influence the interpretation of the parameters when doing many statistical analyses (regression, PCA etc) so the decision to scale should be based on how you want to interpret your parameters. 
  - Although the shapes of distributions are unchanged by scaling, the distributions themselves are definitely changed. 
  - Ex: After scaling a Poisson distribution, it would no longer be a Poisson distribution
  - However, scaling will not change the underlying distribution of the variable nor will it influence (positively or negatively) the violations of model assumptions.


```{r 'C03-Original', include=FALSE}
xxflights <- f_getRDS(xxflights)
bb <- na.omit(xxflights$air_time)
hh <- tibble(ee = bb)
ttl_hh <- "Flights: Air Time (Original)"
cap_hh <- "C03P07" 
```

```{r 'C03-Original-A', ref.label=c('C03-Original', 'C03-Histogram'), include=FALSE}
#
```

```{r 'C03P07-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C03P07", "-Flights-Original", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C03P07, device = "png", dpi = 144) 
}
```

```{r 'C03P07', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C03P07", "-Flights-Original", ".png"))
```

```{r 'C03-Scaled'}
xxflights <- f_getRDS(xxflights)
bb <- na.omit(xxflights$air_time)
# Scaling
ii <- {bb - mean(bb)} / sd(bb)
str(ii)
# #scale() gives a Matrix with original mean and sd as its attribute
jj <- scale(bb)
str(jj)
stopifnot(identical(as.vector(ii), as.vector(jj)))
#
hh <- tibble(ee = as.vector(jj))
ttl_hh <- "Flights: Air Time (Scaled)"
cap_hh <- "C03P08" #iiii
```

```{r 'C03-Scaled-A', ref.label=c('C03-Scaled', 'C03-Histogram'), include=FALSE}
#
```

```{r 'C03P08-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C03P08", "-Flights-Scaled", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C03P08, device = "png", dpi = 144) 
}
```

```{r 'C03P08', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C03P08", "-Flights-Scaled", ".png")) #iiii
```

#### Image {.unlisted .unnumbered}

```{r 'C03P0708', echo=FALSE, ref.label=c('C03P07', 'C03P08'), fig.cap="(C03P07 C03P08) Before and After Scaling"}
#
```

#### Histogram {.unlisted .unnumbered}

```{r 'C03-Histogram', eval=FALSE}
# #hh$ee ttl_hh cap_hh
#
C03 <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) +
  geom_histogram(bins = 50, alpha = 0.4, fill = '#FDE725FF') + 
  geom_vline(aes(xintercept = mean(.data[["ee"]])), color = '#440154FF') +
  annotate(geom = "text", x = mean(.[[1]]), y = -Inf, 
           label = TeX(r'($\bar{x}$)', output = "character"), 
           color = '#440154FF', hjust = -2, vjust = -2.5, parse = TRUE) +
  coord_cartesian(ylim = c(0, 35000)) +
  theme(plot.title.position = "panel") + 
  labs(x = "x", y = "Frequency", 
       subtitle = paste0("(Mean= ", round(mean(.[[1]]), 3), 
                         "; SD= ", round(sd(.[[1]]), 3),
                         ")"), 
      caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C03)
rm(C03)
```

#### Annotate  {.unlisted .unnumbered}

```{r 'C03-Annotate', eval=FALSE}
if(FALSE){
# #check_overlap = TRUE works for de-blurring. However, it still checks each point thus slow
geom_text(aes(label = TeX(r'($\bar{x}$)', output = "character"), 
              x = mean(.data[["ee"]]), y = -Inf),
          color = '#440154FF', hjust = -2, vjust = -2.5, parse = TRUE, check_overlap = TRUE) 
# #Create your own dataset
geom_text(data = tibble(x = mean(.[[1]]), y = -Inf, 
                        label = TeX(r'($\bar{x}$)', output = "character")), 
          aes(x = x, y = y, label = label), 
          color = '#440154FF', hjust = -2, vjust = -2.5, parse = TRUE ) 
# #Or Equivalent
ggplot2::annotate(geom = "text", x = mean(.[[1]]), y = -Inf, 
                  label = TeX(r'($\bar{x}$)', output = "character"), 
                  color = '#440154FF', hjust = -2, vjust = -2.5, parse = TRUE) 
#
ggpp::annotate(geom = "text", x = mean(.[[1]]), y = -Inf, 
               label = TeX(r'($\bar{x}$)', output = "character"), 
               color = '#440154FF', hjust = -2, vjust = -2.5, parse = TRUE) 
}
```

#### Colours {.unlisted .unnumbered}

```{r 'C03-Colours'}
# #List All Colour Names in R
str(colors())
# #Packages: viridis, scales, viridisLite
# #Show N Colours with Max. Contrast
q_colors <- 5
# #Display Colours
if(FALSE) show_col(viridis_pal()(q_colors))
# #Get the Viridis i.e. "D" palette Hex Values for N Colours
v_colors <-  viridis(q_colors, option = "D")
v_colors
#
# #Diverging Colour Palette from 'RColorBrewer'
# #Hex Values
brewer.pal(3, "BrBG")
if(FALSE) display.brewer.pal(3, "BrBG")
```

### Chebyshev Theorem {.tabset .tabset-fade}

```{definition 'Chebyshev-Theorem'}
\textcolor{pink}{Chebyshev Theorem} can be used to make statements about the proportion of data values that must be within a specified number of standard deviations ${\sigma}$, of the mean ${\mu}$.
```

- Refer to \@ref(def:Chebyshev-Theorem)
  - \textcolor{pink}{Chebyshev Theorem:} At least $(1-1/z^2)$ of the data values must be within z standard deviations of the mean, where z is any value greater than 1. 
    - Thus, at least 75% of the data values must be within $\overline{x} \pm 2s$, 89% within $\overline{x} \pm 3s$, and 94% $\overline{x} \pm 4s$.
  - Chebyshev theorem can be applied to any data set regardless of the shape of the distribution of the data. 
  - Ex: Test scores of 100 students have $(\mu = 70, \sigma = 5)$
  - How many students had test scores between 60 and 80
    - From equation \@ref(eq:z-scores), $z_{60} = \frac{60 - 70}{5} = -2$
    - Similarly, $z_{80} = \frac{80 - 70}{5} = +2$
    - According to theorem \@ref(def:Chebyshev-Theorem), values that must be within ${z}$ standard deviation are
      - ${(1-1/z^2) = (1 - 1/2^2) = 0.75 = 75\%}$
      - i.e. 75 students must have test scores between 60 and 80
  - How many students had test scores between 58 and 82
    - $z_{58} = -2.4, z_{82} = +2.4$
    - ${(1 - 1/2.4^2) \approx 0.826 \approx 83\%}$
      - i.e. 83 students must have test scores between 58 and 82

### Empirical Rule {#empirical-c03 .tabset .tabset-fade}

```{definition 'Empirical-Rule'}
\textcolor{pink}{Empirical rule} is used to compute the percentage of data values that must be within one, two, and three standard deviations ${\sigma}$ of the mean ${\mu}$ for a normal distribution. These probabilities are Pr(x) 68.27%, 95.45%, and 99.73%.
```

- According to the \textcolor{pink}{empirical rule}, for a Normal distribution
  - $Pr({\mu} - 1{\sigma} \leq {X} \leq {\mu} + 1{\sigma}) \approx 68.27\%$
  - $Pr({\mu} - 2{\sigma} \leq {X} \leq {\mu} + 2{\sigma}) \approx 95.45\%$ i.e. mostly
  - $Pr({\mu} - 3{\sigma} \leq {X} \leq {\mu} + 3{\sigma}) \approx 99.73\%$ i.e. almost all data values


## Outliers {#outliers-c03}

```{definition 'Outliers'}
\textcolor{pink}{Outliers} are data points or observations that does not fit the trend shown by the remaining data. These differ significantly from other observations. Unusually large or small values are commonly found to be outliers.
```

- Reasons
  - Outliers can occur by chance in any distribution, but they often indicate either measurement error or that the population has a heavy-tailed distribution. A frequent cause of outliers is a mixture of two distributions, which may be two distinct sub-populations.
  - In most larger samplings of data, some data points will be further away from the sample mean than what is deemed reasonable. However, a small number of outliers is to be expected (and not due to any anomalous condition). 
  - Estimators capable of coping with outliers are said to be \textcolor{pink}{robust}: the median is a robust statistic of central tendency, while the mean is not. However, the mean is generally a more precise estimator.
- Outliers represent observations that are suspect and warrant careful examination. 
  - They may represent erroneous data; if so, the data should be corrected. 
  - They may signal a violation of model assumptions; if so, another model should be considered. 
  - Finally, they may simply be unusual values that occurred by chance. In this case, they should be retained.
- Keeping vs. Removing Outliers
  - data value that has been incorrectly recorded /included - should be removed
  - unusual data value that has been recorded correctly and belongs in the data set - should be kept
- Standardized values (z-scores) can be used to identify outliers. 
  - [Empirical Rule](#empirical-c03 "c03") allows us to conclude that for normal distribution, almost all the data values will be within three standard deviations of the mean $(\overline{x} \pm 3s)$. 
  - Hence, in using z-scores to identify outliers, we recommend treating any data value with a z-score less than −3 or greater than +3 as an outlier. 
  - Such data values can then be reviewed for accuracy and to determine whether they belong in the data set.
  - In the case of normally distributed data, the three sigma rule can be used to identify outliers. 
    - In a sample of 1000 observations, the presence of up to five observations deviating from the mean by more than three times the standard deviation is within the range of what can be expected. If the sample size is only 100, however, just three such outliers are already reason for concern. 
- Another approach to identifying outliers is based upon IQR 
  - $\text{Lower Limit} = Q_1 - 1.5 \space \text{IQR}$ and $\text{Upper Limit} = Q_3 + 1.5 \space \text{IQR}$ 
  - An observation is classified as an outlier if its value is less than the lower limit or greater than the upper limit. 

## Summary {#boxplot-c03 .tabset .tabset-fade}

Five-Number Summary is used to quickly summarise a dataset. i.e. Min, Q1, Median, Q3, Max

- A boxplot is a graphical display of data based on a five-number summary. 
  - By using the interquartile range, IQR = Q3 − Q1, limits are located at 1.5(IQR) below Q1 and 1.5(IQR) above Q3
  - The \textcolor{pink}{whiskers} are drawn from the ends of the box to the smallest and largest values inside the limits 
  - Boxplots can also be used to provide a graphical summary of two or more groups and facilitate visual comparisons among the groups. 

### BoxPlot {.unlisted .unnumbered}

```{r 'C03-BoxPlot', include=FALSE}
# #nycflights13::weather
bb <- nycflights13::weather
# #NA are present in the data
summary(bb$temp)
#
# #BoxPlot
C03P01 <- bb %>% drop_na(temp) %>% mutate(month = factor(month, ordered = TRUE)) %>% {
    ggplot(data = ., mapping = aes(x = month, y = temp)) +
    #geom_violin() +
    geom_boxplot(aes(fill = month), outlier.colour = 'red', notch = TRUE) +
    stat_summary(fun = mean, geom = "point", size = 2, color = "steelblue") + 
    scale_y_continuous(breaks = seq(0, 110, 10), limits = c(0, 110)) +
    #geom_point() +
    #geom_jitter(position=position_jitter(0.2)) +
    k_gglayer_box +
    theme(legend.position = 'none') +
    labs(x = "Months", y = "Temperature", subtitle = "With Mean & Notch", 
         caption = "C03P01", title = "BoxPlot")
}
```

```{r 'C03P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C03P01", "-BoxPlot", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C03P01, device = "png", dpi = 144) 
}
```

```{r 'C03P01', echo=FALSE, fig.cap="(C03P01) geom_boxplot()"}
knitr::include_graphics(paste0(.z$PX, "C03P01", "-BoxPlot", ".png"))
```

### Code {.unlisted .unnumbered}
```{r 'C03-BoxPlot-A', ref.label=c('C03-BoxPlot'), eval=FALSE}
#
```

## Relationship between Two Variables

### Covariance {#covariance-c03 .tabset .tabset-fade}

```{definition 'Covariance'}
\textcolor{pink}{Covariance} is a measure of linear association between two variables. Positive values indicate a positive relationship; negative values indicate a negative relationship.
```

- Refer equation \@ref(eq:covariance)
  - For a sample of size ${n}$ with the observations $(x_1, y_1), (x_2, y_2)$, and so on, the covariance is given by equation \@ref(eq:covariance)
  - A positive value for $s_{xy}$ indicates a positive linear association between x and y; that is, as the value of x increases, the value of y increases. Similarly a negative value shows a negative linear association.
    - In the example, $s_{xy} = 11$
  - If the points are evenly distributed in the scatterplot, the value of $s_{xy}$ will be close to zero, indicating no linear association between x and y.
  - \textcolor{orange}{Caution:} Problem with using covariance as a measure of the strength of the linear relationship is that the value of the covariance depends on the units of measurement for x and y.

\begin{equation} 
  \begin{align} 
    \sigma_{xy} &= \frac{\sum (x_i - \mu_x)(y_i - \mu_y)}{n} \\
    s_{xy} &= \frac{\sum (x_i - \overline{x})(y_i - \overline{y})}{n-1} 
  \end{align}
  (\#eq:covariance)
\end{equation} 

```{r 'C03-Trendline', include=FALSE}
bb <- f_getRDS(xxCommercials) 

# #Formula for Trendline calculation
k_gg_formula <- y ~ x
#
# #Scatterplot, Trendline Equation, R2, mean x & y
C03P02 <- bb %>% {
  ggplot(data = ., aes(x = Commercials, y = Sales)) + 
  geom_smooth(method = 'lm', formula = k_gg_formula, se = FALSE) +
  stat_poly_eq(aes(label = paste0("atop(", ..eq.label.., ", \n", ..rr.label.., ")")), 
               formula = k_gg_formula, eq.with.lhs = "italic(hat(y))~`=`~",
               eq.x.rhs = "~italic(x)", parse = TRUE) +
  geom_vline(aes(xintercept = round(mean(Commercials), 3)), color = 'red', linetype = "dashed") +
  geom_hline(aes(yintercept = round(mean(Sales), 3)), color = 'red', linetype = "dashed") +
  geom_text(aes(label = TeX(r"($\bar{x} = 3$)", output = "character"), 
                x = round(mean(Commercials), 3), y = -Inf), 
            color = 'red', , hjust = -0.2, vjust = -0.5, parse = TRUE, check_overlap = TRUE) + 
  geom_text(aes(label = TeX(r"($\bar{y} = 51$)", output = "character"), 
                x = Inf, y = round(mean(Sales), 3)), 
            color = 'red', , hjust = 1.5, vjust = -0.5, parse = TRUE, check_overlap = TRUE) + 
  geom_point() +
  k_gglayer_scatter +
  labs(x = "Commercials", y = "Sales ($100s)",
       subtitle = TeX(r"(Trendline Equation, $R^{2}$, $\bar{x}$ and $\bar{y}$)"), 
       caption = "C03P02", title = "Scatter Plot")
}
```

```{r 'C03P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C03P02", "-Scatter-Mean", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C03P02, device = "png", dpi = 144) 
}
```

```{r 'C03P02', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C03P02", "-Scatter-Mean", ".png"))
```

```{r 'C03P03-A', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
# #Ref another file chunk
knitr::include_graphics(paste0(.z$PX, "C02P05", "-ScatterPlot", ".png"))
```

```{r 'C03P02-A', echo=FALSE, ref.label=c('C03P03-A', 'C03P02'), fig.cap="(C02P05 C03P02) Scatter Plot Quadrants for Covariance"}
#
```

#### Covariance {.unlisted .unnumbered}

```{r 'C03-Covariance'}
# #Get 'Deviation about the mean' i.e. devX and devY and their Product devXY
ii <- bb %>% 
  mutate(devX = Commercials - mean(Commercials), devY = Sales - mean(Sales), devXY = devX * devY) 
#
# #Sample Covariance
sxy <- sum(ii$devXY) / {length(ii$devXY) -1}
print(sxy)
```

#### Code {.unlisted .unnumbered}

```{r 'C03-Trendline-A', ref.label=c('C03-Trendline'), eval=FALSE}
#
```

#### More Text  {.unlisted .unnumbered}

- [This](https://stats.stackexchange.com/questions/229667 "https://stats.stackexchange.com")
  - Unlike Pearson correlation, covariance itself is not a measure of the magnitude of linear relationship. It is a measure of co-variation (which could be just monotonic). This is because covariance depends not only on the strength of linear association but also on the magnitude of the variances. 
- More details are in following links
  -  [This](https://stats.stackexchange.com/questions/17537   "https://stats.stackexchange.com/questions/17537/understanding-variance-intuitively")
  -  [This](https://stats.stackexchange.com/questions/26/what-is-a-standard-deviation   "https://stats.stackexchange.com/questions/26/what-is-a-standard-deviation")
  -  [This](https://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who  -understands-only-the-mean "https://stats.stackexchange.com/questions/18058/how-would-you-explain-covaria  nce-to-someone-who-understands-only-the-mean")
  -  [This](https://stats.stackexchange.com/questions/12842/covariance-and-independence   "https://stats.stackexchange.com/questions/12842/covariance-and-independence")
  -  [This](https://stats.stackexchange.com/questions/18082/how-would-you-explain-the-difference-between-co  rrelation-and-covariance "https://stats.stackexchange.com/questions/18082/how-would-you-explain-the-diffe  rence-between-correlation-and-covariance")
  -  [This](https://stats.stackexchange.com/questions/29713/what-is-covariance-in-plain-language   "https://stats.stackexchange.com/questions/29713/what-is-covariance-in-plain-language")
  -  [This](https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance   "https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance")
  -  [This](https://stats.stackexchange.com/questions/229667/difference-between-correlation-and-covariance-is-covariance-only-useful-if-the "https://stats.stackexchange.com/questions/229667/difference-between-correlation-and-covariance-is-covariance-only-useful-if-the")
  -  [This](https://stats.stackexchange.com/questions/483964/how-come-covariance-can-pick-up-non-linear-relationships-but-correlation-cant "https://stats.stackexchange.com/questions/483964/how-come-covariance-can-pick-up-non-linear-relationships-but-correlation-cant")
  -  [This](https://stats.stackexchange.com/questions/9415/measuring-non-linear-dependence "https://stats.stackexchange.com/questions/9415/measuring-non-linear-dependence")

### Correlation Coefficient {#correlation-c03 .tabset .tabset-fade}

```{definition 'Correlation-Coefficient'}
\textcolor{pink}{Correlation coefficient} is a measure of linear association between two variables that takes on values between −1 and +1. Values near +1 indicate a strong positive linear relationship; values near −1 indicate a strong negative linear relationship; and values near zero indicate the lack of a linear relationship.
```

- Refer equation \@ref(eq:correlation) & Table \@ref(tab:C03T01)
  - The 'Pearson Product Moment Correlation Coefficient' or \textcolor{pink}{sample correlation coefficient} is computed by dividing the sample covariance $s_{xy}$ by the product of the sample standard deviation of x ($s_{x}$) and the sample standard deviation of y ($s_{y}$).
    - Values close to −1 (negative) or +1 (positive) indicate a strong linear relationship. The closer the correlation is to zero, the weaker the relationship.
  - In the example, $s_{xy} = 11$ (Equation \@ref(eq:covariance)) and $s_{x} = 1.49$, $s_{y} = 7.93$ (Equation \@ref(eq:sd))
  - Thus, $r_{xy} = 0.93$
  - \textcolor{orange}{Caution:} Correlation provides a measure of linear association and not necessarily causation. A high correlation between two variables does not mean that changes in one variable will cause changes in the other variable.
  - \textcolor{orange}{Caution:} Because the correlation coefficient measures only the strength of the linear relationship between two quantitative variables, it is possible for the correlation coefficient to be near zero, suggesting no linear relationship, when the relationship between the two variables is nonlinear. 

\begin{equation} 
  \begin{align} 
    \rho_{xy} &= \frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}} \\
    r_{xy} &= \frac{s_{xy}}{s_{x}s_{y}}
  \end{align}
  (\#eq:correlation)
\end{equation} 

#### Correlation  {.unlisted .unnumbered}

```{r 'C03-Correlation', include=FALSE}
jj <- ii %>% mutate(devXsq = devX * devX, devYsq = devY * devY)
# #Sample Covariance Sx, Sample Standard Deviations Sx Sy
sxy <- sum(ii$devXY) / {nrow(ii) -1}
sx <- round(sqrt(sum(jj$devXsq) / {nrow(jj) -1}), 2)
sy <- round(sqrt(sum(jj$devYsq) / {nrow(jj) -1}), 2)
cat(paste0("Sxy =", sxy, ", Sx =", sx, ", Sy =", sy, "\n"))
#
# #Correlation Coefficient Rxy
rxy <- round(sxy / {sx * sy}, 2)
cat(paste0("Correlation Coefficient Rxy =", rxy, "\n"))
```

```{r 'C03-Correlation-A', ref.label=c('C03-Covariance', 'C03-Correlation'), eval=TRUE}
#
```

#### Data  {.unlisted .unnumbered}

```{r 'C03T01', echo=FALSE}
bb <- jj
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kbl(bb,
  caption = "(C03T01) Correlation Calculation",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#C03030)
	row_spec(0, color = "white", background = "#C03030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C03-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, ii, jj, kk, tbl, xxPI, bb, rxy, sx, sxy, sy, C03P01, C03P02, C03P04, C03P05, C03P06,
          C03P07, C03P08, cap_hh, dis_lst, ee, even_skew, kurt_e_t1, kurt_e_t2, kurt_e_t3, 
          kurt_hh, kurt_man, kurt_mmt, neg_skew, pos_skew, q_colors, sig0l_hh, sig0r_hh, 
          skew_e_t1, skew_e_t2, skew_e_t3, skew_hh, skew_lst, skew_man, skew_mmt, ttl_hh, 
          v_colors, xxalpha, xxBeta, xxExp, xxflights, density_hh, density_hh_tbl, hh, mean_hh, 
          q05_hh, q95_hh, sd_hh, sig1l_hh, sig1r_hh, sig2l_hh, sig2r_hh, sig3l_hh, sig3r_hh, 
          xbreaks_hh, xlabels_hh, xpoints_hh, xxNormal, loc_png)
```

```{r 'C03-Validation', include=FALSE, cache=FALSE}
# #Summarised Packages and Objects
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/303-Numerical.Rmd-->

# Probability {#c04}

```{r 'C04', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- This chapter covers Probability, Factorial, Combinations, Permutations, Bayes Theorem.
  - [Probability Basics](#assign-probability-c04 "c04")
  - "ForLater" - Exercises

## Probability

```{definition 'Probability'}
\textcolor{pink}{Probability} is a numerical measure of the likelihood that an event will occur. Probability values are always assigned on a scale from 0 to 1. A probability near zero indicates an event is unlikely to occur; a probability near 1 indicates an event is almost certain to occur.
```

```{definition 'Random-Experiment'}
A \textcolor{pink}{random experiment} is a process that generates well-defined experimental outcomes. On any single repetition or trial, the outcome that occurs is determined completely by chance. 
```

```{definition 'Sample-Space'}
The \textcolor{pink}{sample space} for a random experiment is the set of all experimental outcomes.
```

- Random experiment of tossing a coin has a Sample Space $S = \{\text{Head}, \text{Tail}\}$
- Random experiment of rolling a die has a Sample Space $S = \{1, 2, 3, 4, 5, 6\}$
- Random experiment of tossing Two coins has a Sample Space $S = \{\text{HH}, \text{HT}, \text{TH}, \text{TT}\}$

## Counting Rule 

```{definition 'Counting-Rule'}
\textcolor{pink}{Counting Rule for Multiple-Step Experiments:} If an experiment can be described as a sequence of ${k}$ steps with ${n_1}$ possible outcomes on the first step, ${n_2}$ possible outcomes on the second step, and so on, then the total number of experimental outcomes is given by $\{(n_1)(n_2) \cdots (n_k) \}$
```

```{definition 'Tree-Diagram'}
A \textcolor{pink}{tree diagram} is a graphical representation that helps in visualizing a multiple-step experiment. 
```

## Factorial 

```{definition 'Factorial'}
The \textcolor{pink}{factorial} of a non-negative integer ${n}$, denoted by $n!$, is the product of all positive integers less than or equal to n. The value of 0! is 1 i.e. \textcolor{pink}{$0!=1$}
```

\begin{equation} 
  \begin{align} 
    n! &= \prod _{i=1}^n i = n \cdot (n-1) \\
       &= n \cdot(n-1)\cdot(n-2)\cdot(n-3)\cdot\cdots \cdot 3 \cdot 2 \cdot 1 
  \end{align}
  (\#eq:factorial)
\end{equation} 

## Combinations

```{definition 'Combinations'}
\textcolor{pink}{Combination} allows one to count the number of experimental outcomes when the experiment involves selecting ${k}$ objects from a set of ${N}$ objects. The number of \textcolor{pink}{combinations} of ${N}$ objects taken ${k}$ at a time is equal to the \textcolor{pink}{binomial coefficient $C_k^N$}
```

\begin{equation} 
    C_k^N = \binom{N}{k} = \frac{N!}{k!(N-k)!}
  (\#eq:binom)
\end{equation} 

## Permutations

```{definition 'Permutations'}
\textcolor{pink}{Permutation} allows one to compute the number of experimental outcomes when ${k}$ objects are to be selected from a set of ${N}$ objects where the order of selection is important. The same ${k}$ objects selected in a different order are considered a different experimental outcome. The number of \textcolor{pink}{permutations} of ${N}$ objects taken ${k}$ at a time is given by \textcolor{pink}{$P_k^N$}
```

\begin{equation} 
    P_k^N = k! \binom{N}{k} = \frac{N!}{(N-k)!}
  (\#eq:permutation)
\end{equation} 

- \textcolor{pink}{The number of permutations of ${k}$ distinct objects is $k!$}
  - An experiment results in more permutations than combinations for the same number of objects because every selection of ${k}$ objects can be ordered in $k!$ different ways.

## Assigning Probabilities {#assign-probability-c04}

- Basic Requirements (Similar to the [Discrete Probability](#discrete-prob-be05 "be05") & [Continuos Probability](#continuous-prob-be06 "be06"))
  1. The probability assigned to each experimental outcome must be between 0 and 1, inclusively. If we let ${E_i}$ denote the $i^{th}$ experimental outcome and $P(E_i)$ its probability, then \textcolor{pink}{$P(E_i) \in [0, 1]$}
  1. The sum of the probabilities for all the experimental outcomes must equal 1. Thus for ${k}$ experimental outcomes \textcolor{pink}{$\sum _{i=1}^k P(E_i) =1$}

```{definition 'Event'}
An \textcolor{pink}{event} is a collection of sample points. The probability of any event is equal to the sum of the probabilities of the sample points in the event. The sample space, ${s}$, is an event. Because it contains all the experimental outcomes, it has a probability of 1; that is, \textcolor{pink}{$P(S) = 1$}
```

```{definition 'Complement'}
Given an event ${A}$, the \textcolor{pink}{complement of A ($A^c$)} is defined to be the event consisting of all sample points that are not in A. Thus, \textcolor{pink}{$P(A) + P(A^{c}) =1$}
```

```{definition 'Union'}
Given two events A and B, the \textcolor{pink}{union of A and B} is the event containing all sample points belonging to A or B or both. The union is denoted by \textcolor{pink}{$A \cup B$}
```

```{definition 'Intersection'}
Given two events A and B, the \textcolor{pink}{intersection of A and B} is the event containing the sample points belonging to both A and B. The intersection is denoted by \textcolor{pink}{$A \cap B$}
```

- Refer to the \textcolor{pink}{Addition Law} in the equation \@ref(eq:addition)

\begin{equation} 
    P(A \cup B) = P(A) + P(B) - P(A \cap B)
  (\#eq:addition)
\end{equation} 

```{definition 'Mutually-Exclusive'}
Two events are said to be \textcolor{pink}{mutually exclusive} if the events have no sample points in common. Thus, \textcolor{pink}{$A \cap B = 0$}
```

## Exercises

- How many ways can three items be selected from a group of six items
  - Solution: \textcolor{black}{$C_{3}^{6} = 6!/3!3! = 120$}
- In a experiment of tossing a coin three times, how many experimental outcomes can be
  - Solution: \textcolor{black}{$2^{3} = 8$}
- Simple random sampling uses a sample of size k from a population of size N to obtain data that can be used to make inferences about the characteristics of a population. Suppose that, from a population of 50 bank accounts, we want to take a random sample of four accounts in order to learn about the population. How many different random samples of four accounts are possible
  - Solution: \textcolor{black}{$C_{4}^{50} = 50!/4!46!$}
- To play Powerball, a participant must select five numbers from the digits 1 through 59, and then select a Powerball number from the digits 1 through 35. To determine the winning numbers for each game, lottery officials draw 5 white balls out a drum of 59 white balls numbered 1 through 59 and 1 red ball out of a 
drum of 35 red balls numbered 1 through 35. To win the Powerball jackpot, numbers on the lottery must match the numbers on the 5 white balls in any order and must also match the number on the red Powerball. How many Powerball lottery outcomes are possible
  - Solution: \textcolor{black}{$C_{5}^{59} \times C_{1}^{35}$}
- An experiment has four equally likely outcomes: E1, E2, E3, and E4
  - What is the probability that E2 occurs
    - Solution: \textcolor{black}{${1/4}$}
  - What is the probability that any two of the outcomes occur (e.g., E1 or E3)
    - Solution: \textcolor{black}{$2/4 = 1/2$}  
  - What is the probability that any three of the outcomes occur (e.g., E1 or E2 or E4)
    - Solution: \textcolor{black}{${3/4}$}  
- Consider the experiment of selecting a playing card from a deck of 52 playing cards. Each card corresponds to a sample point with a 1/52 probability.
  - Probability of the event that an ace is selected
    - Solution: \textcolor{black}{$4/52 = 1/13$}
  - Probability of the event that a club is selected
    - Solution: \textcolor{black}{$13/52 = 1/4$}
  - Probability of the event that  a face card (jack, queen, or king) is selected
    - Solution: \textcolor{black}{$4\times3/52$}
- Consider the experiment of rolling a pair of dice. Suppose that we are interested in the sum of the face values showing on the dice.
  - How many sample points are possible
    - Solution: \textcolor{black}{$6 \times 6 = 36$}
  - What is the probability of obtaining a value of 7
    - Solution: \textcolor{black}{$E_{7} = \{(1,6), (6,1), (2,5), (5,2), (3,4), (4,3)\} \Rightarrow P(E_{7}) = 6/36 = 1/6 $}
  - What is the probability of obtaining a value of 9 or greater
    - Solution: \textcolor{black}{$P(E_{\geq9}) = P(E_{9}, E_{10}, E_{11}, E_{12}) = \frac{4 + 3 + 2 + 1}{36} = \frac{5}{18}$}
  - Because each roll has six possible even values (2, 4, 6, 8, 10, and 12) and only five possible odd values (3, 5, 7, 9, and 11), the dice should show even values more often than odd values. Do you agree with this statement
    - Solution: \textcolor{black}{$\text{NO: } P(E_{\text{odd}}) = P(E_{\text{even}}) = 1/2 \iff E_{\text{odd}} = E_{\text{even}} = 18$}
- A survey of magazine subscribers showed that 45.8% rented a car during the past 12 months for business reasons, 54% rented a car during the past 12 months for personal reasons, and 30% rented a car during the past 12 months for both business and personal reasons.
  - Let B denote Business, P denote Personal
  - What is the probability that a subscriber rented a car during the past 12 months for business or personal reasons
    - Solution: \textcolor{black}{$P(B \cup P) = P(B) + P(P) - P(B \cap P) = 0.458 + 0.540 - 0.3 = 0.698$} 
  - What is the probability that a subscriber did not rent a car during the past 12 months for either business or personal reasons
  - Solution: \textcolor{black}{$P(B \cup P)^{c} = 1 - 0.698 = 0.302$} 

## Conditional Probability

```{definition 'Conditional-Probability'}
\textcolor{pink}{Conditional probability} is the probability of an event given that another event already 
occurred. The conditional probability of 'A given B' is \textcolor{pink}{$P(A|B) = \frac{P(A \cup B)}{P(B)}$}
```

```{r 'C04-Police', include=FALSE}
# #Police
xxPolice <- tibble(Promo_Gender = c('Promoted', 'NotPromoted'), Men = c(288, 672), Women = c(36, 204))
```

```{r 'C04T01', echo=FALSE}
aa <- xxPolice
bb <- aa %>% 
  bind_rows(aa %>% summarise(across(where(is.numeric), sum))) %>%
  mutate(across(1, ~replace(., . %in% NA, "Total"))) %>%
  mutate(SUM = rowSums(across(where(is.numeric)))) 
# #Probabilities
ii <- bb %>% mutate(across(where(is.numeric), ~./1200))
#
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kk_bb <- kbl(bb,
  caption = "(C04T01) Police: Promotion and Gender",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
                position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
#
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kk_ii <- kbl(ii,
  caption = "(C04T01A) Joint and Marginal Probabilities",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
                position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)

# #Multiple Kable Tables
knitr::kables(list(kk_bb, kk_ii))
```

- Refer to the Police Promotion Table \@ref(tab:C04T01) 
  - Let, M (Man), W (Woman), A (Promoted), $A^{c}$ (Not Promoted)
  - Probability that a randomly selected officer ...
    - is man and is promoted: $P(A \cap M) = 288/1200 = 0.24$
    - is woman and is promoted: $P(A \cap W) = 36/1200 = 0.03$
    - is man and is not promoted: $P(A^{c} \cap M) = 672/1200 = 0.56$
    - is woman and is not promoted: $P(A^{c} \cap W) = 204/1200 = 0.17$
    - NOTE: Each of these are \textcolor{pink}{Joint Probabilities} because these provide intersection of two events.
  - \textcolor{pink}{Marginal probabilities} are the values in the margins of the joint probability table and indicate the probabilities of each event separately. 
    - $P(M) = 0.80, P(W) = 0.20, P(A) = 0.27, P(A^{c}) = 0.73$
    - Ex: the marginal probability of being promoted is $P(A) = P(A \cap M) + P(A \cap W)$
  - Conditional Probability Analysis
    - "the probability that an officer is promoted given that the officer is a man" $P(A|M)$
      - $P(A|M) = 288/960 = 0.30$
      - OR $P(A|M) = P(A \cap M) / P(M) = 0.24/0.80 = 0.30$
      - "Given that an officer is a man, that officer had a 30% chance of receiving a promotion"
    - "the probability that an officer is promoted given that the officer is a woman" $P(A|W)$
      - $P(A|W) = P(A \cap W) / P(W) = 0.03/0.20 = 0.15$
      - "Given that an officer is a woman, that officer had a 15% chance of receiving a promotion"
    - Conclusion
      - The probability of a promotion given that the officer is a man is .30, twice the .15 probability of a promotion given that the officer is a woman. 
      - Although the use of conditional probability does not in itself prove that discrimination exists in this case, the conditional probability values do support this argument.

```{definition 'Events-Independent'}
Two events A and B are independent if \textcolor{pink}{$P(A|B) = P(A) \quad \text{OR} \quad P(B|A) = P(B) \Rightarrow P(A \cap B) = P(A) \cdot P(B)$}
```

- Refer to the \textcolor{pink}{Multiplication Law} in the equation \@ref(eq:multiplication)
  - Example: 84% of the households in a neighborhood subscribe to the daily edition of a newspaper; that is, $P(D) =0.84$. In addition, it is known that the probability that a household that already holds a daily subscription also subscribes to the Sunday edition is .75; that is, $P(S|D) =0.75$ 
    - What is the probability that a household subscribes to both the Sunday and daily editions of the 
newspaper
      - $P(S \cap D) = P(D) \cdot P(S|D) = 0.84 \times 0.75 = 0.63$
      - "63% of the households subscribe to both the Sunday and daily editions"

\begin{equation} 
  \begin{align} 
    P(A \cap B) &= P(B) \cdot P(A | B) \\
       &= P(A) \cdot P(B | A) 
  \end{align}
  (\#eq:multiplication)
\end{equation} 

- Mutually Exclusive vs. Independent Events
  - Two events with nonzero probabilities cannot be both mutually exclusive and independent. 
  - If one mutually exclusive event is known to occur, the other cannot occur; thus, the probability of the other event occurring is reduced to zero. They are therefore dependent.

## Bayes Theorem

Often, we begin the analysis with initial or \textcolor{pink}{prior probability} estimates for specific events of interest. Then, from sources such as a sample, a special report, or a product test, we obtain additional information about the events. Given this new information, we update the prior probability 
values by calculating revised probabilities, referred to as \textcolor{pink}{posterior probabilities}. Bayes theorem provides a means for making these probability calculations. 

- Refer to the equation \@ref(eq:bayes)
  - \textcolor{pink}{Bayes theorem} is applicable when the events for which we want to compute posterior 
probabilities are mutually exclusive and their union is the entire sample space.
    - An event, $P(A)$, and its complement, $P(A^{c})$, are mutually exclusive, and their union is the entire sample space. Thus, Bayes theorem is always applicable for computing posterior probabilities of an event and its complement.
  - Example: A firm has two suppliers, currently 65% parts are supplied by one and remaining by other; that is, $P(A_{1}) = 0.65, P(A_{2}) = 0.35$. Quality of products supplied is 98% Good for supplier one and 95% Good for supplier 2.
    - $P(G|A_{1}) = 0.98, P(B|A_{1}) = 0.02$
    - $P(G|A_{2}) = 0.95, P(B|A_{2}) = 0.05$
    - Given that we received a Bad Part, what is the probability that it came from supplier 2
      - $P(A_{2}|B) = \frac{P(A_{2})P(B|A_{2})}{P(A_{1}) P(B|A_{1})+ P(A_{2}) P(B|A_{2})} = \frac{0.35 \times 0.05}{0.65 \times 0.02 + 0.35 \times 0.05} = 0.5738 \approx 57\%$
      - Similarly, $P(A_{1}|B) = 0.4262 \approx 43\%$
    - NOTE: While the Probability of a random part being from supplier 1 is $P(A_{1}) = 0.65$, it is reduced to $P(A_{1}|B) = 0.4262 \approx 43\%$ as we have received new information that the part is Bad.

\begin{equation} 
  \begin{align} 
    P(A_{1}|B) &= \frac{P(A_{1})P(B|A_{1})}{P(A_{1}) P(B|A_{1})+ P(A_{2}) P(B|A_{2})} \\
    P(A_{2}|B) &= \frac{P(A_{2})P(B|A_{2})}{P(A_{1}) P(B|A_{1})+ P(A_{2}) P(B|A_{2})}
  \end{align}
  (\#eq:bayes)
\end{equation} 

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C04-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, kk_bb, kk_ii, xxPolice)
```

```{r 'C04-Validation', include=FALSE, cache=FALSE}
# #Summarised Packages and Objects
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/304-Probability.Rmd-->

# Discrete Probability Distributions {#c05}

```{r 'C05', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- This chapter covers 
  - [Discrete Probability](#discrete-prob-c05 "c05")
  - [Bivariate Distributions](#bivariate-c05 "c05")
  - "ForLater" - Financial Applications, Exercises, Binomial, Poisson

## Definitions (Ref)

```{r 'C05D01', comment="", echo=FALSE, results='asis'}
f_getDef("Discrete")
```

```{r 'C05D02', comment="", echo=FALSE, results='asis'}
f_getDef("Continuous") #dddd
```

## Random Variable

```{definition 'Random-Variable'}
A \textcolor{pink}{random variable} is a numerical description of the outcome of an experiment. Random variables must assume numerical values. It can be either 'discrete' or 'continuous'.
```

```{definition 'Discrete-Random-Variable'}
A random variable that may assume either a finite number of values or an infinite sequence of values such as $0, 1, 2, \dots$ is referred to as a \textcolor{pink}{discrete random variable}. It includes factor type i.e. Male as 0, Female as 1 etc.
```

```{definition 'Continuous-Random-Variable'}
A random variable that may assume any numerical value in an interval or collection of intervals is called a \textcolor{pink}{continuous random variable}. It is given by \textcolor{pink}{$x \in [n, m]$}. If the entire line segment between the two points also represents possible values for the random variable, then the random variable is continuous.
```

## Discrete Probability Distributions {#discrete-prob-c05}

```{definition 'Probability-Distribution'}
The \textcolor{pink}{probability distribution} for a random variable describes how probabilities are distributed over the values of the random variable.
```

```{definition 'Probability-Function'}
For a discrete random variable x, a \textcolor{pink}{probability function $f(x)$}, provides the probability for each value of the random variable.
```

- The use of the relative frequency method to develop discrete probability distributions leads to what is called an \textcolor{pink}{empirical discrete distribution}.
  - We treat the data as if they were the population and use the relative frequency method to assign probabilities to the experimental outcomes.
  - The \textcolor{pink}{distribution} of data is how often each observation occurs, and can be described by its central tendency and variation around that central tendency.

- Basic Requirements (Similar to the [Probability Basics](#assign-probability-be04 "be04") & [Continuos Probability](#continuous-prob-be06 "be06"))
  1. \textcolor{pink}{$f(x) \geq 0$}
  1. \textcolor{pink}{$\sum {f(x)} = 1$}
- The simplest example of a discrete probability distribution given by a formula is the \textcolor{pink}{discrete uniform probability distribution}; $f(x) = 1/n$, where ${n}$ is the number of values the random variable may assume
  - Each possible value of the random variable has the same probability

### Expected Value

```{definition 'Expected-Value-Discrete'}
The \textcolor{pink}{expected value, or mean}, of a random variable is a measure of the central location for the random variable. i.e. \textcolor{pink}{$E(x) = \mu = \sum xf(x)$}
```

- NOTE
  - The expected value is a weighted average of the values of the random variable where the weights are 
the probabilities.
  - The expected value does not have to be a value the random variable can assume. i.e. average need not to be integer
  
### Variance 

```{definition 'Variance-Discrete'}
The \textcolor{pink}{variance} is a weighted average of the squared deviations of a random variable from its mean. The weights are the probabilities. i.e. \textcolor{pink}{$\text{Var}(x) = \sigma^2 = \sum \{(x- \mu)^2 \cdot f(x)\}$}
```

## Bivariate Distributions {#bivariate-c05}

```{definition 'Bivariate'}
A probability distribution involving two random variables is called a \textcolor{pink}{bivariate probability distribution}. A discrete bivariate probability distribution provides a probability for each pair of values that may occur for the two random variables.
```

- NOTE:
  - Each outcome for a bivariate experiment consists of two values, one for each random variable. Example: Rolling a pair of dice
  - Bivariate probabilities are often called \textcolor{pink}{joint probabilities}

## Ex Dicarlo {.tabset .tabset-fade}

```{r 'C05-Dicarlo', include=FALSE}
# #Dicarlo: Days with Number of Cars Sold per day for last 300 days
xxdicarlo <- tibble(Cars = 0:5, Days = c(54, 117, 72, 42, 12, 3))
#
bb <- xxdicarlo
bb <- bb %>% rename(x = Cars, Fx = Days) %>% mutate(across(Fx, ~./sum(Fx))) %>% 
  mutate(xFx = x * Fx, x_mu = x - sum(xFx), 
		     x_mu_sq = x_mu * x_mu, x_mu_sq_Fx = x_mu_sq * Fx) 
R_dicarlo_var_y_C05 <- sum(bb$x_mu_sq_Fx)
# #Total Row
bb <- bb %>% 
  mutate(across(1, as.character)) %>% 
  add_row(summarise(., across(1, ~"Total")), summarise(., across(where(is.double), sum))) %>% 
  mutate(xFx = ifelse(x == "Total", paste0("mu = ", xFx), xFx),
         x_mu_sq_Fx = ifelse(x == "Total", paste0("sigma^2 = ", x_mu_sq_Fx), x_mu_sq_Fx)) %>% 
  mutate(across(4:5, ~ replace(., x == "Total", NA)))
```

### Table {.unlisted .unnumbered}

```{r 'C05T04', echo=FALSE}
displ_names <- c("${x}$", "$f(x)$", "$\\sum xf(x)$", "$(x - \\mu)$", "$(x - \\mu)^2$", 
                "$\\sum {(x - \\mu)^{2}f(x)}$") 
stopifnot(identical(ncol(bb), length(displ_names)))
#
kbl(bb,
  caption = "(C05T04) Variance Calculation",
  col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```

### Data {.unlisted .unnumbered}

```{r 'C05-Dicarlo-A', ref.label=c('C05-Dicarlo'), eval=FALSE}
#
```

### Change Class {.unlisted .unnumbered}

```{r 'C05-ColClass', eval=FALSE}
# #Change Column Classes as required
bb %>% mutate(across(1, as.character))
bb %>% mutate(across(everything(), as.character))
```

### Modify Value {.unlisted .unnumbered}

```{r 'C05-ModifyValue', eval=FALSE}
bb <- xxdicarlo
ii <- bb %>% rename(x = Cars, Fx = Days) %>% mutate(across(Fx, ~./sum(Fx))) %>% 
  mutate(xFx = x * Fx, x_mu = x - sum(xFx), 
		     x_mu_sq = x_mu * x_mu, x_mu_sq_Fx = x_mu_sq * Fx) 
# #Add Total Row
ii <- ii %>% 
  mutate(across(1, as.character)) %>% 
  add_row(summarise(., across(1, ~"Total")), summarise(., across(where(is.double), sum))) 
#
# #Modify Specific Row Values without using filter() 
# #filter() does not have 'un-filter()' function like group()-ungroup() combination
# #Selecting Row where x = "Total" and changing Column Values for Two Columns
ii <- ii %>% 
  mutate(xFx = ifelse(x == "Total", paste0("mu = ", xFx), xFx),
       x_mu_sq_Fx = ifelse(x == "Total", paste0("sigma^2 = ", x_mu_sq_Fx), x_mu_sq_Fx)) 
#
# #Selecting Row where x = "Total" and doing same replacement on Two Columns
ii %>% mutate(across(4:5, function(y) replace(y, x == "Total", NA)))
ii %>% mutate(across(4:5, ~ replace(., x == "Total", NA)))
```

## Ex Dicarlo GS {.tabset .tabset-fade}

```{r 'C05-DicarloGS', include=FALSE}
xxdicarlo_gs <- tibble(Geneva_Saratoga = c("x0", "x1", "x2", "x3"), 
             y0 = c(21, 21, 9, 3), y1 = c(30, 36, 42, 9), y2 = c(24, 33, 9, 6), 
             y3 = c(9, 18, 12, 3), y4 = c(2, 2, 3, 5), y5 = c(0, 1, 2, 0))
bb <- xxdicarlo_gs
#
# #Tibble Total SUM 
sum_bb <- bb %>% summarise(across(-1, sum)) %>% summarise(sum(.)) %>% pull(.)
#
# #Add Total Row and SUM Column
ii <- bb %>% 
  mutate(across(1, as.character)) %>% 
  add_row(summarise(., across(1, ~"Total")), summarise(., across(where(is.numeric), sum))) %>% 
  mutate(SUM = rowSums(across(where(is.numeric))))
#
# #Convert to Bivirate Probability Distribution and then add Total Row and SUM Column
jj <- bb %>% 
  mutate(across(where(is.numeric), ~./sum_bb)) %>% 
  mutate(across(1, as.character)) %>% 
  add_row(summarise(., across(1, ~"Total")), summarise(., across(where(is.numeric), sum))) %>% 
  mutate(SUM = rowSums(across(where(is.numeric)))) %>% 
  mutate(across(where(is.numeric), format, digits =1))
```

### Table {.unlisted .unnumbered}

```{r 'C05T01', echo=FALSE}
bb <- ii
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kk_ii <- kbl(bb,
  caption = "(C05T01) Bivariate Table",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
#
bb <- jj
#displ_names <- c("") 
#stopifnot(identical(ncol(bb), length(displ_names)))
#
kk_jj <- kbl(bb,
  caption = "(C05T02) Probability Distribution",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
# #Multiple Kable Tables
knitr::kables(list(kk_ii, kk_jj))
```

### DataGS {.unlisted .unnumbered}

```{r 'C05-DicarloGS-A', ref.label=c('C05-DicarloGS'), eval=FALSE}
#
```

### Tibble Total SUM {.unlisted .unnumbered}

```{r 'C05-TibbleSum'}
bb <- xxdicarlo_gs
# #Assuming there is NO Total Column NOR Total Row and First Column is character
kk <- bb %>% summarise(across(where(is.numeric), sum)) %>% summarise(sum(.)) %>% pull(.)
ll <- bb %>% summarise(across(-1, sum)) %>% summarise(sum(.)) %>% pull(.)
stopifnot(identical(kk, ll))
print(kk)
```

### format() {.unlisted .unnumbered}

```{r 'C05-Digits'}
bb <- xxdicarlo_gs
# #Round off values to 1 significant digits i.e. 0.003 or 0.02
# #NOTE: This changes the column to "character"
bb %>% mutate(across(where(is.numeric), ~./sum_bb)) %>% 
  mutate(across(where(is.numeric), format, digits =1))
```

## Bivariate ... {.tabset .tabset-fade}

- Suppose we would like to know the probability distribution for total sales at both DiCarlo dealerships and the expected value and variance of total sales.
  - We can define $s = x + y$ as Total Sales.
  - Refer to the Tables \@ref(tab:C05T01) and \@ref(tab:C05T03)
    - $f(s_0) = f(x_0, y_0) = 0.07$ 
    - $f(s_1) = f(x_0, y_1) + f(x_1, y_0) = 0.10 + 0.07 = 0.17$ 

```{r 'C05-BivariateVariance', include=FALSE}
bb <- xxdicarlo_gs
sum_bb <- bb %>% summarise(across(-1, sum)) %>% summarise(sum(.)) %>% pull(.)
# #Convert to Bivariate Probability Distribution
ii <- bb %>% mutate(across(where(is.numeric), ~./sum_bb)) %>% select(-1)
# #Using tapply(), sum the Matrix
jj <- tapply(X= as.matrix(ii), INDEX = LETTERS[row(ii) + col(ii)-1], FUN = sum)
# #Create Tibble
kk <- tibble(Fs = jj, ID = LETTERS[1:length(Fs)], s = 1:length(Fs) - 1) %>% 
  relocate(Fs, .after = last_col()) %>% 
  mutate(sFs = s * Fs, s_Es = s - sum(sFs), 
		     s_Es_sq = s_Es * s_Es, s_Es_sq_Fs = s_Es_sq * Fs) 
# #Save for Notebook
R_dicarlo_var_s_C05 <- sum(kk$s_Es_sq_Fs)
# #For Printing
ll <- kk %>% 
  add_row(summarise(., across(1, ~"Total")), summarise(., across(where(is.double), sum))) %>% 
  mutate(across(where(is.numeric), format, digits =2)) %>% 
  mutate(sFs = ifelse(ID == "Total", paste0("E(s) = ", sFs), sFs),
         s_Es_sq_Fs = ifelse(ID == "Total", paste0("Var(s) = ", s_Es_sq_Fs), s_Es_sq_Fs)) %>% 
  mutate(across(c(2, 5, 6), ~ replace(., ID == "Total", NA)))
```

### Table {.unlisted .unnumbered}

```{r 'C05T03', echo=FALSE}
bb <- ll
displ_names <- c("$ID$", "${s}$", "$f(s)$", "$\\sum sf(s)$", "$(s - E(s))$", "$(s - E(s))^2$", 
                "$\\sum {(s - E(s))^{2}f(s)}$") 
stopifnot(identical(ncol(bb), length(displ_names)))
#
kbl(bb,
  caption = "(C05T03) Bivariate Expected Value and Variance",
  col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
				        #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	)
```

### Code {.unlisted .unnumbered}

```{r 'C05-BivariateVariance-A', ref.label=c('C05-BivariateVariance'), eval=FALSE}
#
```

### Bivariate to Original {.unlisted .unnumbered}

```{r 'C05-DicarloG'}
bb <- xxdicarlo_gs
# #From the Bivariate get the original data
ii <- bb %>% 
  mutate(Fx = rowSums(across(where(is.numeric)))) %>% 
  select(1, 8) %>% 
  separate(col = Geneva_Saratoga, into = c(NA, "x"), sep = 1) %>% 
  mutate(across(1, as.integer))
# #Variance Calculation
jj <- ii %>% mutate(across(Fx, ~./sum(Fx))) %>% 
  mutate(xFx = x * Fx, x_mu = x - sum(xFx), 
		     x_mu_sq = x_mu * x_mu, x_mu_sq_Fx = x_mu_sq * Fx) 
# #Save for Notebook
R_dicarlo_var_x_C05 <- sum(jj$x_mu_sq_Fx)
print(jj)
```

### Sum Diagonals {.unlisted .unnumbered}

```{r 'C05-SumDiagonal'}
bb <- xxdicarlo_gs
#
# #Tibble Total SUM 
sum_bb <- bb %>% summarise(across(-1, sum)) %>% summarise(sum(.)) %>% pull(.)
#
# #Convert to Bivirate Probability Distribution and Exclude First Character Column
ii <- bb %>% mutate(across(where(is.numeric), ~./sum_bb)) %>% select(-1)
#
# #(1A, 2B, 3C, 4D, 4E, 4F, 3G, 2H, 1I) 9 Unique Combinations = 24 (4x6) Experimental Outcomes 
matrix(data = LETTERS[row(ii) + col(ii)-1], nrow = 4)
# 
# #Using tapply(), sum the Matrix
jj <- tapply(X= as.matrix(ii), INDEX = LETTERS[row(ii) + col(ii)-1], FUN = sum)
print(jj)
# #In place of LETTERS, Numerical Index can also be used but Letters are more clear for grouping
#tapply(X= as.matrix(ii), INDEX = c(0:8)[row(ii) + col(ii)-1], FUN = sum)
#
# #Create Tibble
kk <- tibble(Fs = jj, ID = LETTERS[1:length(Fs)], s = 1:length(Fs) - 1) %>% 
  relocate(Fs, .after = last_col())
print(kk)
```

### String Split {.unlisted .unnumbered}

```{r 'C05-SplitString'}
bb <- xxdicarlo_gs
# #Separate String based on Position 
bb %>% separate(col = Geneva_Saratoga, into = c("A", "B"), sep = 1) 
```

## Covariance

- Covariance of random variables x and y is given by \textcolor{pink}{$\sigma_{xy}$}, Refer equation \@ref(eq:covariance-c05)
  - NOTE: It does not look like \@ref(eq:covariance), but for now, I am assuming it is Equivalent
  - Calculated: $\text{Var}(s) = \text{Var}(x + y) =$ `r round(R_dicarlo_var_s_C05, 3)`; $\text{Var}(y) =$ `r R_dicarlo_var_y_C05`; $\text{Var}(x) =$ `r round(R_dicarlo_var_x_C05, 3)`
  - Variance $\sigma_{xy} = \frac{2.3895 - 0.8696 - 1.25}{2} = 0.1350$
  - A covariance of .1350 indicates that daily sales at the two dealerships have a positive relationship. 

\begin{equation} 
     \sigma_{xy} = \frac{\text{Var}(x + y) - \text{Var}(x) - \text{Var}(y)}{2}
  (\#eq:covariance-c05)
\end{equation} 

- Correlation of random variables x and y is given by, Refer equation \@ref(eq:correlation), \textcolor{pink}{$\rho_{xy}  = \frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}}$} 
  - Where $\sigma_{x} = \sqrt{\text{Var}(x)} = \sqrt{0.8696} = 0.9325$; and $\sigma_{y} = \sqrt{\text{Var}(y)} = \sqrt{1.25} = 1.1180$
  - Correlation Coefficient $\rho_{xy}  = \frac{0.1350}{0.9325 \times 1.1180} = 0.1295$
  - The correlation coefficient of .1295 indicates there is a weak positive relationship between the random variables representing daily sales at the two dealerships. 

## Distributions

- "ForLater" 
  - \textcolor{pink}{Binomial Probability Distribution} - `dbinom(), pbinom(), qbinom(), rbinom()` 
    - It can be used to determine the probability of obtaining ${x}$ successes in ${n}$ trials.
    - 4 Assumptions must be TRUE
      1. The experiment consists of a sequence of ${n}$ identical trials.
      1. Two outcomes are possible on each trial, one called success and the other failure.
      1. The probability of a success ${p}$ does not change from trial to trial. Consequently, the probability of failure, $1 − p$, does not change from trial to trial.
      1. The trials are independent.
  - \textcolor{pink}{Poisson Probability Distribution} - `dpois(), ppois(), qpois(), rpois()` 
    - To determine the probability of obtaining ${x}$ occurrences over an interval of time or space.
    - 2 Assumptions must be TRUE
      1. The probability of an occurrence of the event is the same for any two intervals of equal length.
      1. The occurrence or nonoccurrence of the event in any interval is independent of the occurrence or nonoccurrence of the event in any other interval.
  - \textcolor{pink}{Hypergeometric Probability Distribution} 
    - Like the binomial, it is used to compute the probability of ${x}$ successes in ${n}$ trials. 
    - But, in contrast to the binomial, the probability of success changes from trial to trial.

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C05-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(bb, ii, jj, kk, ll, displ_names, kk_ii, kk_jj, sum_bb, xxdicarlo, xxdicarlo_gs, R_dicarlo_var_s_C05, R_dicarlo_var_x_C05, R_dicarlo_var_y_C05)
```

```{r 'C05-Validation', include=FALSE, cache=FALSE}
# #Summarised Packages and Objects
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/305-DiscreteProbability.Rmd-->

# Continuous Probability Distributions {#c06}

```{r 'C06', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- This chapter covers 
  - [Continuos Probability](#continuous-prob-c06 "c06"), [Normal Distribution](#normal-d-c06 "c06"), [Standard Normal](#standard-normal-c06 "c06")
  - [Get P(z) by pnorm() or z by qnorm()](#get-pz-c06 "c06")
  - "ForLater" - Exercises, Normal Approximation of Binomial Probabilities, Exponential Probability Distribution, Relationship Between the Poisson and Exponential Distributions

## Definitions (Ref)

```{r 'C06D01', comment="", echo=FALSE, results='asis'}
f_getDef("Discrete-Random-Variable")
```

```{r 'C06D02', comment="", echo=FALSE, results='asis'}
f_getDef("Continuous-Random-Variable")
```


## Uniform Probability Distribution {#continuous-prob-c06}

```{definition 'Uniform-Probability-Distribution'}
\textcolor{pink}{Uniform probability distribution} is a continuous probability distribution for which the probability that the random variable will assume a value in any interval is the same for each interval of equal length. Whenever the probability is proportional to the length of the interval, the random variable is uniformly distributed.
```

```{definition 'Probability-Density-Function'}
The probability that the continuous random variable ${x}$ takes a value between $[a, b]$ is given by the area under the graph of \textcolor{pink}{probability density function $f(x)$}; that is, \textcolor{pink}{$A = \int _{a}^{b}f(x)\ dx$}. Note that $f(x)$ can be greater than 1, however its integral must be equal to 1.
```

- Basic Requirements (Similar to the [Probability Basics](#assign-probability-be04 "be04") & [Discrete Probability](#discrete-prob-be05 "be05") )
  1. \textcolor{pink}{$f(x) \geq 0$} 
  1. \textcolor{pink}{$A = \int _{-\infty}^{\infty}f(x)\ dx = 1$}

- NOTE:
  - For a discrete random variable, the probability function $f(x)$ provides the probability that the random variable assumes a particular value. With continuous random variables, the counterpart of the probability function is the \textcolor{pink}{probability density function $f(x)$}. 
    - The difference is that the probability density function does not directly provide probabilities.         - However, the area under the graph of $f(x)$ corresponding to a given interval does provide the probability that the continuous random variable ${x}$ assumes a value in that interval. 
    - So when we compute probabilities for continuous random variables we are computing the probability that the random variable assumes any value in an interval (NOT at any particular point).
    - Because the area under the graph of $f(x)$ at any particular point is zero, the probability of any particular value of the random variable is zero. 
    - It also means that the probability of a continuous random variable assuming a value in any interval is the same whether or not the endpoints are included.
  - Expected Value and Variance are given by \@ref(eq:continuous-var)

\begin{equation}
  \begin{align}
    E(x) &= \frac{a + b}{2} \\
    \text{Var}(x) &= \frac{(b - a)^2}{12}
  \end{align}
  (\#eq:continuous-var)
\end{equation}

## Normal Probability Distribution {#normal-d-c06 .tabset .tabset-fade}

```{definition 'Normal-Distribution'}
A \textcolor{pink}{normal distribution (${\mathcal{N}}_{({\mu}, \, {\sigma}^2)}$)} is a type of continuous probability distribution for a real-valued random variable.
```

- The general form of its probability density function is given by equation \@ref(eq:distribution-normal)
  - Normal distribution ${\mathcal{N}}_{({\mu}, \, {\sigma})}$ is also known as Gaussian or Gauss or Laplace-Gauss distribution
  - It is symmetrical
  - The entire family of normal distributions is differentiated by two parameters: the mean ${\mu}$ and the standard deviation ${\sigma}$. They determine the location and shape of the normal distribution.
  - The highest point on the normal curve is at the mean, which is also the median and mode of the distribution. 
  - The normal distribution is symmetric around its mean. Its skewness measure is zero. 
  - The tails of the normal curve extend to infinity in both directions and theoretically never touch the horizontal axis. 
  - Larger values of the standard deviation result in wider, flatter curves, showing more variability in the data.
  - Probabilities for the normal random variable are given by areas under the normal curve. The total area under the curve for the normal distribution is 1. 
  - Values of a normal random variable are given as: $68.27\% ({\mu} \pm {\sigma}), 95.45\% ({\mu} \pm 2{\sigma}), 99.73\% ({\mu} \pm 3{\sigma})$. This is the basis of [Empirical Rule](#empirical-be03 "be03")

\begin{equation}
  f(x) = {\frac {1}{{\sigma}{\sqrt {2 \pi}}}} e^{-{\frac {1}{2}}\left( {\frac {x-{\mu} }{\sigma}}\right) ^{2}}
  (\#eq:distribution-normal)
\end{equation}

```{r 'C06-NormalPlot', include=FALSE}
# #Normal Distribution
xxNormal <- f_getRDS(xxNormal)
ee <- xxNormal
hh <- tibble(ee)
ee <- NULL
# #Basics
median_hh <- round(median(hh[[1]]), 3)
mean_hh <- round(mean(hh[[1]]), 3)
sd_hh <- round(sd(hh[[1]]), 3)
len_hh <- nrow(hh)
#
# #Get Quantiles and Ranges of mean +/- sigma 
q05_hh <- quantile(hh[[1]], .05)
q95_hh <- quantile(hh[[1]], .95)
density_hh <- density(hh[[1]])
density_hh_tbl <- tibble(x = density_hh$x, y = density_hh$y)
sig3l_hh <- density_hh_tbl %>% filter(x <= mean_hh - 3 * sd_hh)
sig3r_hh <- density_hh_tbl %>% filter(x >= mean_hh + 3 * sd_hh)
sig2r_hh <- density_hh_tbl %>% filter(x >= mean_hh + 2 * sd_hh, x < mean_hh + 3 * sd_hh)
sig2l_hh <- density_hh_tbl %>% filter(x <= mean_hh - 2 * sd_hh, x > mean_hh - 3 * sd_hh)
sig1r_hh <- density_hh_tbl %>% filter(x >= mean_hh + sd_hh, x < mean_hh + 2 * sd_hh)
sig1l_hh <- density_hh_tbl %>% filter(x <= mean_hh - sd_hh, x > mean_hh - 2 * sd_hh)
#
# #Change x-Axis Ticks interval
xbreaks_hh <- seq(-3, 3)
xpoints_hh <- mean_hh + xbreaks_hh * sd_hh
# # Latex Labels 
xlabels_hh <- c(TeX(r'($\,\,\mu - 3 \sigma$)'), TeX(r'($\,\,\mu - 2 \sigma$)'), 
                TeX(r'($\,\,\mu - 1 \sigma$)'), TeX(r'($\mu$)'), TeX(r'($\,\,\mu + 1 \sigma$)'), 
                TeX(r'($\,\,\mu + 2 \sigma$)'), TeX(r'($\,\,\mu + 3\sigma$)'))
#
# #Base Plot: Creates Only Density Function Line 
ii <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) + geom_density() } 
#
# #Mean
ii_mean <- ggplot_build(ii)$data[[1]] %>% filter(x <= mean_hh)	
# #Median
ii_median <- ggplot_build(ii)$data[[1]] %>% filter(x <= median_hh)
#$
C06P01 <- ii + geom_density(alpha = 0.2, colour = "#21908CFF") + 
  geom_histogram(aes(y = ..density..), bins = 50, alpha = 0.4, fill = '#FDE725FF') + 
  geom_segment(data = ii_mean, 
               aes(x = mean_hh, y = 0, xend = mean_hh, yend = density), color = "#440154FF") + 
  geom_text(aes(label = paste0("Mean= ", mean_hh), x = mean_hh, y = -Inf),
            color = '#440154FF', hjust = -0.5, vjust = -1, angle = 90, check_overlap = TRUE) + 
  geom_segment(data = ii_median, 
               aes(x = median_hh, y = 0, xend = median_hh, yend = density), color = "#3B528BFF") + 
  geom_text(aes(label = paste0("Median= ", median_hh), x = median_hh, y = -Inf),
            color = '#3B528BFF', hjust = -0.4, vjust = 1.2, angle = 90, check_overlap = TRUE) +  
  coord_cartesian(xlim = c(-5, 5), ylim = c(0, 0.5)) + 
  scale_x_continuous(breaks = xpoints_hh, labels = xlabels_hh) + 
  geom_area(data = sig3l_hh, aes(x = x, y = y), fill = 'red') + 
  geom_area(data = sig3r_hh, aes(x = x, y = y), fill = 'red') + 
  ggplot2::annotate("segment", x = xpoints_hh[4] -0.5 , xend = xpoints_hh[1], y = 0.48, 
                    yend = 0.48, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) + 
  ggplot2::annotate("segment", x = xpoints_hh[4] +0.5 , xend = xpoints_hh[7], y = 0.48, 
                    yend = 0.48, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) + 
  ggplot2::annotate(geom = "text", x = xpoints_hh[4], y = 0.48, label = "99.7%") + 
  theme(plot.title.position = "panel") + 
  labs(x = "x", y = "Density", 
       subtitle = paste0("(N=", len_hh, "; ", "Mean= ", mean_hh, 
                         "; Median= ", median_hh, "; SD= ", sd_hh), 
        caption = "C06P01", title = "Normal Distribution (Symmetrical)")
```

```{r 'C06P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C06P01", "-Distribution-Normal", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C06P01, device = "png", dpi = 144) 
}
```

```{r 'C06P01', echo=FALSE, fig.cap="(C06P01) Normal Distribution"}
knitr::include_graphics(paste0(.z$PX, "C06P01", "-Distribution-Normal", ".png")) #iiii
```

### Histogram  {#histogram-c06 .unlisted .unnumbered}

```{r 'C06-BuildPlot-A', eval=FALSE}
# #Histogram with Density Curve, Mean and Median: Normal Distribution
ee <- f_getRDS(xxNormal)
hh <- tibble(ee)
ee <- NULL
# #Basics
median_hh <- round(median(hh[[1]]), 3)
mean_hh <- round(mean(hh[[1]]), 3)
sd_hh <- round(sd(hh[[1]]), 3)
len_hh <- nrow(hh)
#
# #Base Plot: Creates Only Density Function Line
ii <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) + geom_density() }
#
# #Change the line colour and alpha
ii <- ii + geom_density(alpha = 0.2, colour = "#21908CFF") 
#
# #Add Histogram with 50 bins, alpha and fill
ii <- ii + geom_histogram(aes(y = ..density..), bins = 50, alpha = 0.4, fill = '#FDE725FF')
#
# #Full Vertical Line at Mean. Goes across Function Boundary on Y-Axis
#ii <- ii + geom_vline(aes(xintercept = mean_hh), color = '#440154FF')
#
# #Shaded Area Object for line /Area upto the the Function Boundary on Y-Axis
# #Mean
ii_mean <- ggplot_build(ii)$data[[1]] %>% filter(x <= mean_hh)	
# #Median
ii_median <- ggplot_build(ii)$data[[1]] %>% filter(x <= median_hh)
#
# #To show values which are less than Mean in colour
#ii <- ii + geom_area(data = ii_mean, aes(x = x, y = y), fill = 'blue', alpha = 0.5) 
#
# #Line upto the Density Curve at Mean 
ii <- ii + geom_segment(data = ii_mean, 
             aes(x = mean_hh, y = 0, xend = mean_hh, yend = density), color = "#440154FF")
#
# #Label 'Mean' 
ii <- ii + geom_text(aes(label = paste0("Mean= ", mean_hh), x = mean_hh, y = -Inf),
            color = '#440154FF', hjust = -0.5, vjust = -1, angle = 90, check_overlap = TRUE)
#
# #Similarly, Median Line and Label
ii <- ii + geom_segment(data = ii_median, 
             aes(x = median_hh, y = 0, xend = median_hh, yend = density), color = "#3B528BFF") +
  geom_text(aes(label = paste0("Median= ", median_hh), x = median_hh, y = -Inf), 
            color = '#3B528BFF', hjust = -0.4, vjust = 1.2, angle = 90, check_overlap = TRUE) 
#
# #Change Axis Limits
ii <- ii + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 0.5))
#
# #Change x-Axis Ticks interval
xbreaks_hh <- seq(-3, 3)
xpoints_hh <- mean_hh + xbreaks_hh * sd_hh
# # Latex Labels 
xlabels_hh <- c(TeX(r'($\,\,\mu - 3 \sigma$)'), TeX(r'($\,\,\mu - 2 \sigma$)'), 
                TeX(r'($\,\,\mu - 1 \sigma$)'), TeX(r'($\mu$)'), TeX(r'($\,\,\mu + 1 \sigma$)'), 
                TeX(r'($\,\,\mu + 2 \sigma$)'), TeX(r'($\,\,\mu + 3\sigma$)'))
#
ii <- ii + scale_x_continuous(breaks = xpoints_hh, labels = xlabels_hh)
#
# #Get Quantiles and Ranges of mean +/- sigma 
q05_hh <- quantile(hh[[1]], .05)
q95_hh <- quantile(hh[[1]], .95)
density_hh <- density(hh[[1]])
density_hh_tbl <- tibble(x = density_hh$x, y = density_hh$y)
sig3l_hh <- density_hh_tbl %>% filter(x <= mean_hh - 3 * sd_hh)
sig3r_hh <- density_hh_tbl %>% filter(x >= mean_hh + 3 * sd_hh)
sig2r_hh <- density_hh_tbl %>% filter(x >= mean_hh + 2 * sd_hh, x < mean_hh + 3 * sd_hh)
sig2l_hh <- density_hh_tbl %>% filter(x <= mean_hh - 2 * sd_hh, x > mean_hh - 3 * sd_hh)
sig1r_hh <- density_hh_tbl %>% filter(x >= mean_hh + sd_hh, x < mean_hh + 2 * sd_hh)
sig1l_hh <- density_hh_tbl %>% filter(x <= mean_hh - sd_hh, x > mean_hh - 2 * sd_hh)
#
# #Use (mean +/- 3 sigma) To Highlight. NOT ALL Zones have been highlighted
ii <- ii + geom_area(data = sig3l_hh, aes(x = x, y = y), fill = 'red') +
           geom_area(data = sig3r_hh, aes(x = x, y = y), fill = 'red')
#
# #Annotate Arrows 
ii <- ii + 
#  ggplot2::annotate("segment", x = xpoints_hh[4] -0.5 , xend = xpoints_hh[3], y = 0.42, 
#                    yend = 0.42, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
#  ggplot2::annotate("segment", x = xpoints_hh[4] -0.5 , xend = xpoints_hh[2], y = 0.45, 
#                    yend = 0.45, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
  ggplot2::annotate("segment", x = xpoints_hh[4] -0.5 , xend = xpoints_hh[1], y = 0.48, 
                    yend = 0.48, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
#  ggplot2::annotate("segment", x = xpoints_hh[4] +0.5 , xend = xpoints_hh[5], y = 0.42, 
#                    yend = 0.42, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
#  ggplot2::annotate("segment", x = xpoints_hh[4] +0.5 , xend = xpoints_hh[6], y = 0.45, 
#                    yend = 0.45, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
  ggplot2::annotate("segment", x = xpoints_hh[4] +0.5 , xend = xpoints_hh[7], y = 0.48, 
                    yend = 0.48, arrow = arrow(type = "closed", length = unit(0.02, "npc")))
#
# #Annotate Labels
ii <- ii + 
#  ggplot2::annotate(geom = "text", x = xpoints_hh[4], y = 0.42, label = "68.3%") +
#  ggplot2::annotate(geom = "text", x = xpoints_hh[4], y = 0.45, label = "95.4%") +
  ggplot2::annotate(geom = "text", x = xpoints_hh[4], y = 0.48, label = "99.7%")
#
# #Add a Theme and adjust Position of Title & Subtile (Both by plot.title.position) & Caption
# #"plot" or "panel"
ii <- ii + theme(#plot.tag.position = "topleft",
                 #plot.caption.position = "plot", 
                 #plot.caption = element_text(hjust = 0),
                 plot.title.position = "panel")
#
# #Title, Subtitle, Caption, Axis Labels, Tag
ii <- ii + labs(x = "x", y = "Density", 
        subtitle = paste0("(N=", len_hh, "; ", "Mean= ", mean_hh, 
                          "; Median= ", median_hh, "; SD= ", sd_hh), 
        caption = "C06AA", tag = NULL,
        title = "Normal Distribution (Symmetrical)")
#
#ii
```

### Plot LaTex {.unlisted .unnumbered}

```{r 'C06-PlotLatexEq', eval=FALSE}
# #Syntax 
#latex2exp::Tex(r('$\sigma =10$'), output = "character")
# #Test Equation
plot(TeX(r'(abc: $\frac{2hc^2}{\lambda^5} \, \frac{1}{e^{\frac{hc}{\lambda k_B T}} - 1}$)'), cex=2)
plot(TeX(r'(xyz: $f(x) =\frac{1}{\sigma \sqrt{2\pi}}\, e^{- \, \frac{1}{2} \,\left(\frac{x - \mu}{\sigma}\right)^2} $)'), cex=2)
```

### Annotate Plot  {.unlisted .unnumbered}

```{r 'C06-AnnotatePlot', eval=FALSE}
# #Syntax
ggpp::annotate("text", x = -2, y = 0.3, label=TeX(r'($\sigma =10$)', output = "character"), parse = TRUE, check_overlap = TRUE)
# #NOTE: Complex Equations like Normal Distribution are crashing the R.
ggpp::annotate("text", x = -2, y = 0.3, label=TeX(r'($f(x) =\frac{1}{\sigma \sqrt{2\pi}}\, e^{- \, \frac{1}{2} \, \left(\frac{x - \mu}{\sigma}\right)^2} $)', output = "character"), parse = TRUE, check_overlap = TRUE)
```

### ggplot_build()  {.unlisted .unnumbered}

```{r 'C06-ggplotObjects'}
# #Data
bb <- f_getRDS(xxNormal)
hh <- tibble(bb)
# #Base Plot
ii <- hh %>% { ggplot(data = ., mapping = aes(x = bb)) + geom_density() }
# #Attributes 
attributes(ggplot_build(ii))$names
#
str(ggplot_build(ii)$data[[1]])
```

### Errors {.unlisted .unnumbered}

```{conjecture 'ggplot-list'}
\textcolor{brown}{Error in is.finite(x) : default method not implemented for type 'list'}
```

- For ggplot() subsetting inside aes() is discouraged. 
- Assuming names(hh)[1] is "ee" 
  - either use (x = "ee") : Use of hh[1] or .[1] will throw error
  - or use (x = .data[["ee"]]) : Use of hh[[1]] or .[[1]] will work but would throw warning.
    - Warning "Warning: Use of `.[[1]]` is discouraged. Use `.data[[1]]` instead."
    - Using .data[[1]] will throw different error

```{conjecture 'ggplot-data'}
\textcolor{brown}{Error: Must subset the data pronoun with a string.}
```

- ggplot() | aes() | using .data[[1]] will throw this error
- use .data[["ee"]] or "ee"
  - .data is pronoun for an environment, it is for scope resolution, not dataframe like dot (.)

### UNICODE {.unlisted .unnumbered}

> \textcolor{red}{STOP! STOP! Just STOP! using UNICODE for R Console on WINDOWS (UTF-8 Issue).}


## Standard Normal  {#standard-normal-c06 .tabset .tabset-fade}

```{definition 'Standard-Normal'}
A random variable that has a normal distribution with a mean of zero \textcolor{pink}{$({\mu} = 0)$} and a standard deviation of one \textcolor{pink}{$({\sigma} = 1)$} is said to have a \textcolor{pink}{standard normal probability distribution}. The \textcolor{pink}{z-distribution} is given by \textcolor{pink}{${\mathcal{z}}_{({\mu} = 0, \, {\sigma} = 1)}$}
```

\begin{equation} 
  f(z) = \varphi(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}
  (\#eq:normal-z)
\end{equation} 

- Refer equation \@ref(eq:normal-z)
  - Here, the factor $1/{\sqrt{2\pi}}$ ensures that the total area under the curve $\varphi(x)$ is equal to one. 
  - The factor $1/2$ in the exponent ensures that the distribution has unit variance, and therefore also unit standard deviation. 
  - This function is symmetric around $x = 0$, where it attains its maximum value $1/{\sqrt{2\pi}}$ and has inflection points at $x = +1$ and $x = -1$. 
  - While individual observations from normal distributions are referred to as ${x}$, they are referred to as ${z}$ in the z-distribution. 

```{r 'C06D03', comment="", echo=FALSE, results='asis'}
f_getDef("z-Scores") #dddd
```

- NOTE (R, C) notation denotes Row x Column of a Table
  - Because the standard normal random variable is continuous, $P(z \leq 1.00) = P(z < 1.00)$
  - The cumulative probability corresponding to $z = 1.00$ is the table value located at the intersection of the row labeled \textcolor{pink}{${1.0}$} and the column labeled \textcolor{pink}{${.00}$} i.e. $P_{\left(z\leq 1.00\right)} = P_{\left(1.0, \, .00\right)} = 0.8413$
  - To compute the probability that ${z}$ is in the interval between −.50 and 1.25 
    - $P_{\left(-0.50 \leq z\leq 1.25\right)} = P_{\left(z\leq 1.25\right)} - P_{\left(z\leq -0.50 \right)} = P_{\left(1.2, \, .05\right)} - P_{\left(-0.50, \, .00\right)} = 0.8944 - 0.3085 = 0.5859$ 
  - To compute the probability of obtaining a z value of at least 1.58
    - $P_{\left(z\geq 1.58\right)} = 1 - P_{\left(z\leq 1.58\right)} = 1 - P_{\left(1.5, \, .08\right)} = 1 - 0.9429 = 0.0571$ 
  - To compute the probability that the standard normal random variable is within one standard deviation of the mean 
    - $P_{\left(-1.00 \leq z\leq 1.00\right)} = P_{\left(z\leq 1.00\right)} - P_{\left(z\leq -1.00 \right)} = P_{\left(1.0, \, .00\right)} - P_{\left(-1.0, \, .00\right)} = 0.8413 - 0.1587 = 0.6826$ 
- Reverse i.e. given the probability, find out the z-value
  - Find a z value such that the probability of obtaining a larger z value is .10
    - The standard normal probability table gives the area under the curve to the left of a particular z value, which would be $P_{\left(z\right)} = 1 - 0.10 = 0.9000 \approx P_{\left(1.2, \, .08\right)} \to z = 1.28$


\textcolor{pink}{${z} \in \mathbb{R} \iff P_{(z)} \in (0, 1)$}

### Cal P  {#get-pz-c06 .unlisted .unnumbered}

```{r 'C06-getP'}
# #Find Commulative Probability P corresponding to the given 'z' value
# #Area under the curve to the left of z-value = 1.00
pnorm(q = 1.00)
```

### pnorm()  {.unlisted .unnumbered}

```{r 'C06-getPgivenZ'}
# #Find Commulative Probability P corresponding to the given 'z' value
# #Area under the curve to the left of z-value = 1.00
# #pnorm(q = 1.00) #(Default) 'lower.tail = TRUE'
z_ii <- 1.00 
p_ii <- round(pnorm(q = z_ii, lower.tail = TRUE), 4)
cat(paste0("P(z <= ", format(z_ii, nsmall = 3), ") = ", p_ii, "\n"))
#
# #Probability that z is in the interval between −.50 and 1.25 #0.5859
z_min_ii <- -0.50
z_max_ii <- 1.25
p_ii <- round(pnorm(q = z_max_ii, lower.tail = TRUE) - pnorm(q = z_min_ii, lower.tail = TRUE), 4)
cat(paste0("P(", format(z_min_ii, nsmall = 3), " <= z <= ", 
           format(z_max_ii, nsmall = 3), ") = ", p_ii, "\n"))
#
# #Probability of obtaining a z value of at least 1.58 #0.0571
z_ii <- 1.58
p_ii <- round(pnorm(q = z_ii, lower.tail = FALSE), 4)
cat(paste0("P(z >= ", format(z_ii, nsmall = 3), ") = ", p_ii, "\n"))
#
# #Probability that the z is within one standard deviation of the mean i.e. [-1, 1] #0.6826
z_min_ii <- -1.00
z_max_ii <- 1.00
p_ii <- round(pnorm(q = z_max_ii, lower.tail = TRUE) - pnorm(q = z_min_ii, lower.tail = TRUE), 4)
cat(paste0("P(", format(z_min_ii, nsmall = 3), " <= z <= ", 
           format(z_max_ii, nsmall = 3), ") = ", p_ii, "\n"))
```


### Cal Z  {.unlisted .unnumbered}

```{r 'C06-getZ'}
# #Find a z value such that the probability of obtaining a larger z value is .10
# #z-value for which Area under the curve towards Right is 0.10
qnorm(p = 1 - 0.10)
qnorm(p = 0.10, lower.tail = FALSE)
```

### qnorm()  {.unlisted .unnumbered}

```{r 'C06-getZgivenP'}
# #Find a z value such that the probability of obtaining a larger z value is .10
# #z-value for which Area under the curve towards Right is 0.10 i.e. right >10%
#qnorm(p = 1 - 0.10)
#qnorm(p = 0.10, lower.tail = FALSE)
p_r_ii <- 0.10 
p_l_ii <- 1 - p_r_ii
z_ii <- round(qnorm(p = p_l_ii, lower.tail = TRUE), 4)
z_jj <- round(qnorm(p = p_r_ii, lower.tail = FALSE), 4)
stopifnot(identical(z_ii, z_jj))
cat(paste0("(Left) P(z) = ", format(p_l_ii, nsmall = 3), " (i.e. (Right) 1-P(z) = ", 
           format(p_r_ii, nsmall = 3), ") at z = ", z_ii, "\n"))
```


## Any Normal  {.tabset .tabset-fade}

- Any normal distribution can be standardized by converting the individual values into z-scores. 
  - z-scores tell that how many standard deviations away from the mean each value lies.
- Probabilities for all normal distributions are computed by using the standard normal distribution. 
  - A normal distribution \textcolor{pink}{${\mathcal{N}}_{({\mu}, \, {\sigma})}$} is converted to the standard normal distribution \textcolor{pink}{${\mathcal{z}}_{({\mu} = 0, \, {\sigma} = 1)}$} by equation \@ref(eq:z-val) (Similar to equation \@ref(eq:z-scores))
  - If ${x}$ is a random variable from this population, then its z-score is $Z = \frac {X - {\mu}}{\sigma}$
  - If $\overline{X}$ is the mean of a sample of size ${n}$ from this population, then the standard error is ${\sigma}/{\sqrt{n}}$ and thus the z-score is $Z = \frac{\overline{X} - {\mu}}{{\sigma}/{\sqrt{n}}}$
  - If $\sum {X}$ is the total of a sample of size ${n}$ from this population, then the expected total is $n\times{\mu}$ and the standard error is ${\sigma}{\sqrt{n}}$. Thus the z-score is $Z = {\frac{\sum{X}-n{\mu}}{{\sigma}{\sqrt{n}}}}$

- Thus
  - $x = {\mu} \to z = 0$ i.e. A value of ${x}$ equal to its mean ${\mu}$ corresponds to $z = 0$. 
  - $x = {\mu} + {\sigma} \to z = 1$ i.e. an ${x}$ value that is one standard deviation above its mean $({\mu} + {\sigma})$ corresponds to $z = 1$. 
    - Thus, we can interpret ${z}$ as the number of standard deviations $({\sigma})$ that the normal random variable ${x}$ is from its mean $({\mu})$.
  - For a normal distribution ${\mathcal{N}}_{({\mu} = 10, \, {\sigma} = 2)}$, What is the probability that the random variable x is between 10 and 14
    - At x = 10, z = 0 and at x = 14, z = 2, Thus
    - $P_{\left(0 \leq z\leq 2\right)} = P_{\left(z\leq 2\right)} - P_{\left(z\leq 0 \right)} = P_{\left(2.0, \, .00\right)} - P_{\left(0, \, .00\right)} = 0.9772 - 0.5000 = 0.4772$ 
  - Grear Tire Company Problem
    - For a new tire product the milage is a Normal Function ${\mathcal{N}}_{({\mu} = 36500, \, {\sigma} = 5000)}$. 
    - What percentage of the tires can be expected to last more than 40,000 miles, i.e., what is the probability that the tire mileage, x, will exceed 40,000
      - Solution: \textcolor{black}{24.2\%}
    - Let us now assume that Grear is considering a guarantee that will provide a discount on replacement tires if the original tires do not provide the guaranteed mileage. What should the guarantee mileage be if Grear wants no more than 10% of the tires to be eligible for the discount guarantee
      - Solution: \textcolor{black}{$30092 \approx 30100 \text{ miles}$}
    

\begin{equation}
  z = \frac{x - {\mu}}{{\sigma}} 
  (\#eq:z-val)
\end{equation}

Reasons to convert normal distributions into the standard normal distribution:

- To find the probability of observations in a distribution falling above or below a given value
- To find the probability that a sample mean significantly differs from a known population mean
- To compare scores on different distributions with different means and standard deviations

Each z-score is associated with a probability, or p-value, that gives the likelihood of values below that z-score occurring. By converting an individual value into a z-score, we can find the probability of all values up to that value occurring in a normal distribution.

The z-score is the test statistic used in a z-test. The z-test is used to compare the means of two groups, or to compare the mean of a group to a set value. Its null hypothesis typically assumes no difference between groups.

The area under the curve to the right of a z-score is the p-value, and it is the likelihood of your observation occurring if the null hypothesis is true.

Usually, a p-value of 0.05 or less means that your results are unlikely to have arisen by chance; it indicates a statistically significant effect.


### Cal P {.unlisted .unnumbered}

```{r 'C06-getPany'}
# #For N(mu =10, sd =2) Probability that X is in [10, 14]
# #Same as P(0 <= z <= 2)
mu_ii <- 10
sd_ii <- 2
x_min_ii <- 10
x_max_ii <- 14
#
z_min_ii <- (x_min_ii - mu_ii) /sd_ii #0
z_max_ii <- (x_max_ii - mu_ii) /sd_ii #2
#
pz_ii <- round(pnorm(q = z_max_ii, lower.tail = TRUE) - pnorm(q = z_min_ii, lower.tail = TRUE), 4)
# #OR
px_ii <- round(pnorm(q = x_max_ii, mean = mu_ii, sd = sd_ii, lower.tail = TRUE) - 
                  pnorm(q = x_min_ii, mean = mu_ii, sd = sd_ii, lower.tail = TRUE), 4)
stopifnot(identical(pz_ii, px_ii))
cat(paste0("P(", format(z_min_ii, nsmall = 3), " <= z <= ", 
           format(z_max_ii, nsmall = 3), ") = ", pz_ii, "\n"))
cat(paste0("P(", x_min_ii, " <= x <= ", x_max_ii, ") = ", format(px_ii, nsmall = 3), "\n"))
```

### Grear Tire {.unlisted .unnumbered}

```{r 'C06-Tire'}
# #Grear Tire N(mu = 36500, sd =5000)
# #Probability that the tire mileage, x, will exceed 40000 # 24.2% Tires
mu_ii <- 36500
sd_ii <- 5000
x_ii <- 40000
#
z_ii <- (x_ii - mu_ii)/sd_ii
#
#pnorm(q = 40000, mean = 36500, sd = 5000, lower.tail = FALSE)
pz_ii <- round(pnorm(q = z_ii, lower.tail = FALSE), 4)
px_ii <- round(pnorm(q = x_ii, mean = mu_ii, sd = sd_ii, lower.tail = FALSE), 4)
stopifnot(identical(px_ii, pz_ii))
#
cat(paste0("P(x >= ", x_ii, ") = ", format(px_ii, nsmall = 4), " (", 
           round(100* px_ii, 2), "%)\n"))
#
# #What should the guarantee mileage be if no more than 10% of the tires to be eligible 
# #for the discount guarantee i.e. left <10% # ~30100 miles
p_l_ii <- 0.10
p_r_ii <- 1 - p_l_ii
#
#qnorm(p = 0.10, mean = 36500, sd = 5000)
z_ii <- round(qnorm(p = p_l_ii, lower.tail = TRUE), 4)
xz_ii <- z_ii * sd_ii + mu_ii
#
x_ii <- round(qnorm(p = p_l_ii, mean = mu_ii, sd = sd_ii, lower.tail = TRUE), 4)
stopifnot(abs(xz_ii - x_ii) < 1)
cat(paste0("(Left) P(x) = ", p_l_ii, " (i.e. (Right) 1-P(z) = ", p_r_ii, 
           ") at x = ", round(x_ii, 1), "\n"))
```

## Exercises  {.unlisted .unnumbered}

- "ForLater" 
  - Exercises
  - Normal Approximation of Binomial Probabilities
  - Exponential Probability Distribution
  - Relationship Between the Poisson and Exponential Distributions

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C06-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ee, ii, jj, kk, ll, C06P01, density_hh, density_hh_tbl, hh, ii_mean, ii_median, 
          len_hh, mean_hh, median_hh, q05_hh, q95_hh, sd_hh, sig1l_hh, sig1r_hh, sig2l_hh, 
          sig2r_hh, sig3l_hh, sig3r_hh, xbreaks_hh, xlabels_hh, xpoints_hh, xxNormal, mu_ii, 
          p_ii, p_l_ii, p_r_ii, px_ii, pz_ii, sd_ii, x_ii, x_max_ii, x_min_ii, xz_ii, z_ii, 
          z_jj, z_max_ii, z_min_ii)
```

```{r 'C06-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/306-ContinuousProbability.Rmd-->

# Sampling Distributions {#c07}

```{r 'C07', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Sampling and Sampling Distributions"
  - [Sample](#sample-c07 "c07"), [Standard Error](#standard-error-c07 "c07"), [Central Limit Theorem](#clt-c07 "c07")
  - "ForLater" - Sampling Distribution of $\overline{p}$, Properties of Point Estimators, Other Sampling Methods

## Definitions (Ref)

```{r 'C07D01', comment="", echo=FALSE, results='asis'}
f_getDef("Elements")
```

```{r 'C07D02', comment="", echo=FALSE, results='asis'}
f_getDef("Variable")
```

```{r 'C07D03', comment="", echo=FALSE, results='asis'}
f_getDef("Population")
```

```{r 'C07D04', comment="", echo=FALSE, results='asis'}
f_getDef("Sample")
```

```{r 'C07D05', comment="", echo=FALSE, results='asis'}
f_getDef("Parameter-vs-Statistic") #dddd
```

## Sample {#sample-c07 .tabset .tabset-fade}

The sample contains only a portion of the population. Some sampling error is to be expected. So, the sample results provide only \textcolor{pink}{estimates} of the values of the corresponding population characteristics.

```{definition 'Sampled-Population'}
The \textcolor{pink}{sampled population} is the population from which the sample is drawn.
```

```{definition 'Frame'}
\textcolor{pink}{Frame} is a list of the elements that the sample will be selected from.
```


```{definition 'Target-Population'}
The \textcolor{pink}{target population} is the population we want to make inferences about. Generally (adn preferably), it will be same as 'Sampled-Population', but it may differ also.
```


```{definition 'SRS'}
A \textcolor{pink}{simple random sample (SRS)} is a set of ${k}$ objects in a population of ${N}$ objects where all possible samples are equally likely to happen. The number of such different simple random samples is $C_k^N$
```

```{definition 'Sampling-without-Replacement'}
\textcolor{pink}{Sampling without replacement:} Once an element has been included in the sample, it is removed from the population and cannot be selected a second time.
```

```{definition 'Sampling-with-Replacement'}
\textcolor{pink}{Sampling with replacement:} Once an element has been included in the sample, it is returned to the population. A previously selected element can be selected again and therefore may appear in the sample more than once.
```

- Infinite Population
  - Sometimes the population is infinitely large or the elements of the population are being generated by an ongoing process for which there is no limit on the number of elements that can be generated. 
  - Thus, it is not possible to develop a list of all the elements in the population. This is considered the \textcolor{pink}{infinite population} case. 
  - With an infinite population, we cannot select a 'simple random sample' because we cannot construct a frame consisting of all the elements. 
  - In the infinite population case, statisticians recommend selecting what is called a 'random sample'.
  
```{definition 'Random-Sample'}
A \textcolor{pink}{random sample} of size ${n}$ from an infinite population is a sample selected such that the following two conditions are satisfied. Each element selected comes from the same population. Each element is selected independently. The second condition prevents selection bias.
```

Random sample vs. SRS

- Random sample: every element of the population has a (nonzero) probability of being drawn.
  - each element does not necessarily have an equal chance of being chosen.
- SRS: every element of the population has the same (nonzero) probability of being drawn. 
  - SRS is thus a special case of a random sample.
  - SRS is a subset of a statistical population in which each member of the subset has an equal probability of being chosen.

- Elaboration of the Two conditions for Random Sample
  - Example: Consider a production line designed to fill boxes of a breakfast cereal.
    - Each element selected comes from the same population. 
      - To ensure this, the boxes must be selected at approximately the same point in time. 
      - This way the inspector avoids the possibility of selecting some boxes when the process is operating properly and other boxes when the process is not operating properly.
    - Each element is selected independently.
      - It is satisfied by designing the production process so that each box of cereal is filled independently.
  - Example: Consider the population of customers arriving at a fast-food restaurant. 
    - McDonald, implemented a random sampling procedure for this situation. 
    - The sampling procedure was based on the fact that some customers presented discount coupons.
    - Whenever a customer presented a discount coupon, the next customer served was asked to complete a customer profile questionnaire. Because arriving customers presented discount coupons randomly and independently of other customers, this sampling procedure ensured that customers were selected independently.



## Point Estimation  {.tabset .tabset-fade}

```{definition 'Proportion'}
A \textcolor{pink}{population proportion ${P}$}, is a parameter that describes a percentage value associated with a population. It is given by $P = \frac{X}{N}$, where ${x}$ is the count of successes in the population, and ${N}$ is the size of the population. It is estimated through \textcolor{pink}{sample proportion $\overline{p} = \frac{x}{n}$}, where ${x}$ is the count of successes in the sample, and ${N}$ is the size of the sample obtained from the population.
```

```{definition 'Point-Estimation'}
To estimate the value of a \textcolor{pink}{population parameter}, we compute a corresponding characteristic of the sample, referred to as a \textcolor{pink}{sample statistic}. This process is called \textcolor{pink}{point estimation}.
```

```{definition 'Point-Estimator'}
A sample statistic is the \textcolor{pink}{point estimator} of the corresponding population parameter. For example, $\overline{x}, s, s^2, s_{xy}, r_{xy}$ sample statics are point estimators for corresponding population parameters of ${\mu}$ (mean), ${\sigma}$ (standard deviation), $\sigma^2$ (variance), $\sigma_{xy}$ (covariance), $\rho_{xy}$ (correlation)
```

```{definition 'Point-Estimate'}
The numerical value obtained for the sample statistic is called the \textcolor{pink}{point estimate}. Estimate is used for sample value only, for population value it would be parameter. Estimate is a value while Estimator is a function.
```

## Sampling Distributions

```{definition 'Sampling-Distribution'}
The \textcolor{pink}{sampling distribution of ${\overline{x}}$} is the probability distribution of all possible values of the sample mean ${\overline{x}}$.
```

Suppose, from a Population, we take a sample of size ${n}$ and calculate point estimate mean $\overline{x}_{1}$. Further, we can select another random sample from the Population and get another point estimate mean  $\overline{x}_{2}$. If we repeat this process for 500 times, we will have a frame of $\{\overline{x}_{1}, \overline{x}_{2}, \ldots, \overline{x}_{500}\}$.

If we consider the process of selecting a simple random sample as an experiment, the sample mean ${\overline{x}}$ is the numerical description of the outcome of the experiment. Thus, the sample mean ${\overline{x}}$ is a random variable. As a result, just like other random variables, ${\overline{x}}$ has a mean or expected value, a standard deviation, and a probability distribution. Because the various possible values of ${\overline{x}}$ are the result of different simple random samples, the probability distribution of ${\overline{x}}$ is called the \textcolor{pink}{sampling distribution of ${\overline{x}}$}. Knowledge of this sampling distribution and its properties will enable us to make probability statements about how close the sample mean ${\overline{x}}$ is to the population mean ${\mu}$.

Just as with other probability distributions, the sampling distribution of ${\overline{x}}$ has an expected value or mean, a standard deviation, and a characteristic shape or form.

### Mean

- Expected Value of ${\overline{x}}$
  - The mean of the ${\overline{x}}$ random variable is the expected value of ${\overline{x}}$. 
  - Let $E(\overline{x})$ represent the expected value of ${\overline{x}}$ and ${\mu}$ represent the mean of the population from which we are selecting a simple random sample. Then, \textcolor{pink}{$E(\overline{x}) = \mu$} 
  - When the expected value of a point estimator equals the population parameter, we say the point estimator is \textcolor{pink}{unbiased}. Thus, \textcolor{pink}{${\overline{x}}$ is an unbiased estimator of the population mean  ${\mu}$}.

### Standard Deviation {#standard-error-c07}

```{definition 'Standard-Error'}
In general, \textcolor{pink}{standard error $\sigma_{\overline{x}}$} refers to the standard deviation of a point estimator. The standard error of ${\overline{x}}$ is the standard deviation of the sampling distribution of ${\overline{x}}$. It is the indicator of 'Sampling Fluctuation'.
```

- Standard Deviation of ${\overline{x}}$, \textcolor{pink}{$\sigma_{\overline{x}}$} is given by \@ref(eq:sigma-x-bar)
  - $\sqrt{\frac{N - n}{N-1}}$ is commonly referred to as the \textcolor{pink}{finite population correction factor}. With large population, it approaches 1
  - Thus, $\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}}$ becomes good approximation when the sample size is less than or equal to 5% of the population size; that is, \textcolor{pink}{$n/N \leq 0.05$}.
  - To further emphasize the difference between $\sigma_{\overline{x}}$ and ${\sigma}$, we refer to the standard deviation of ${\overline{x}}$, $\sigma_{\overline{x}}$, as the \textcolor{pink}{standard error of the mean}.  
  - (Sampling Fluctuation) The standard error of the mean is helpful in determining how far the sample mean may be from the population mean.

\begin{equation}
  \begin{align}
    \text{Finite Population:} \sigma_{\overline{x}} &= \sqrt{\frac{N - n}{N-1}}\left(\frac{\sigma}{\sqrt{n}} \right) \\
    \text{Infinite Population:} \sigma_{\overline{x}} &= \frac{\sigma}{\sqrt{n}}
  \end{align}
  (\#eq:sigma-x-bar)
\end{equation}


```{definition 'Sampling-Error'}
A \textcolor{pink}{sampling error} is the difference between a population parameter and a sample statistic.
```

- Standard error is a measure of sampling error. There are others, but standard error is, by far, the most commonly used. 
  - However, sampling error is NOT the only reason for a difference between the survey estimate and the true value in the population. 
  - Another, and arguably more important, reason for this difference is bias. 
    - Bias can be introduced when designing the sampling scheme. 
	- Most forms of bias cannot be calculated nor measured after the data are collected, and are, therefore, often invisible. 
	- Bias must be avoided by using correct procedures at each step of the survey process. 
	- Bias has NOTHING to do with sample size which affects only sampling error and standard error. 
	- As a result, large sample sizes do NOT eliminate bias. In fact, the larger sample size may increase the likelihood of bias in the data collection.



Refer [Effect of Sample Size and Repeat Sampling](#sample-sampling-b12 "b12")

```{r 'C07P01-A', echo=FALSE, fig.cap="(B12A B12B) Effect of Sample Size vs Repeat Sampling"}
# #Reading image of another file #iiii
knitr::include_graphics(paste0(.z$PX, "B12A", ".gif"))
knitr::include_graphics(paste0(.z$PX, "B12B", ".gif"))
```

## Synopsis 

"ForLater"

If a statistically independent sample of ${n}$ observations ${{x}_1, {x}_2, \ldots, {x}_n}$ is taken from a statistical population with a standard deviation of $\sigma$, then the mean value calculated from the sample $\overline{x}$ will have an associated \textcolor{pink}{standard error of the mean $\sigma_\overline{x}$} given by

\begin{equation} 
  \sigma_\overline{x} = \frac{\sigma}{\sqrt{n}}
  (\#eq:std-dev-mean)
\end{equation} 

The standard deviation $\sigma$ of the population being sampled is seldom known. Therefore, $\sigma_\overline{x}$ is usually estimated by replacing $\sigma$ with the sample standard deviation $\sigma_{x}$ instead: 

\begin{equation} 
  \sigma_\overline{x} \approx \frac{\sigma_{x}}{\sqrt{n}}
  (\#eq:std-dev-mean-apx)
\end{equation} 

As this is only an 'estimator' for the true "standard error", other notations are used, such as:

\begin{equation} 
  \widehat{\sigma}_\overline{x} = \frac{\sigma_{x}}{\sqrt{n}}
  (\#eq:std-dev-mean-cap)
\end{equation} 

OR:

\begin{equation} 
  {s}_\overline{x} = \frac{s}{\sqrt{n}}
  (\#eq:std-dev-mean-sx)
\end{equation} 

Key:

- \textcolor{pink}{$\sigma$} : Standard deviation of the population
- \textcolor{pink}{$\sigma_{x}$} : Standard deviation of the sample
- \textcolor{pink}{$\sigma_\overline{x}$} : Standard deviation of the mean 
  - the standard error
- \textcolor{pink}{$\widehat{\sigma}_\overline{x}$} : Estimator of the standard deviation of the mean 
  - the most often calculated quantity
  - also often colloquially called the ~~standard error~~

Non-mathematical view:

- The SD (standard deviation) quantifies scatter — how much the values vary from one another.
- The SEM (standard error of the mean) quantifies how precisely you know the true mean of the population. 
  - It takes into account both the value of the SD and the sample size.
- Both SD and SEM are in the same units i.e. the units of the data (in contrast, variance has squared units).
- The SEM, by definition, is always smaller than the SD. (divided by $\sqrt{n}$)
  - The SEM gets smaller as your samples get larger. 
  - So, the mean of a large sample is likely to be closer to the true population mean than is the mean of a small sample. 
  - With a huge sample, you will know the value of the mean with a lot of precision even if the data is scattered.
- The SD does not change predictably as you acquire more data. 
  - The SD you compute from a sample is the best possible estimate of the SD of the overall population. 
  - As you collect more data, you will assess the SD of the population with more precision. But you cannot predict whether the SD from a larger sample will be bigger or smaller than the SD from a small sample. 
  - Technically, variance does not change predictably. Above is a simplification. For details, see [Difference between SE and SD](https://stats.stackexchange.com/questions/32318 "https://stats.stackexchange.com/questions/32318")

### Form {#clt-c07}

Form of the Sampling Distribution of ${\overline{x}}$

- When the population has a normal distribution, the sampling distribution of ${\overline{x}}$ is normally distributed for any sample size. 

- When the population from which we are selecting a random sample does not have a normal distribution, \textcolor{pink}{the central limit theorem} is helpful in identifying the shape of the sampling distribution of ${\overline{x}}$.

```{definition 'Central-Limit-Theorem'}
\textcolor{pink}{Central Limit Theorem:} In selecting random samples of size ${n}$ from a population, the sampling distribution of the sample mean ${\overline{x}}$ can be approximated by a normal distribution as the sample size becomes large.
```

How large the sample size needs to be before the central limit theorem applies and we can assume that the shape of the sampling distribution is approximately normal

- For most applications, the sampling distribution of ${\overline{x}}$ can be approximated by a normal distribution whenever the sample is size 30 or more. 
- In cases where the population is highly skewed or outliers are present, samples of size 50 may be needed. 
- Finally, if the population is discrete, the sample size needed for a normal approximation often depends on the population proportion.


## Ex: EAI {.unlisted .unnumbered .tabset .tabset-fade}

Task of developing a profile of 2500 managers. The characteristics to be identified include the mean annual salary for the managers and the proportion of managers having completed a training.

- Population 
  - Population Size N = 2500 managers
  - Training: 1500/2500 managers have completed Training 
  - Salary: ${\mathcal{N}}_{(\mu = 51800, \, \sigma = 4000)}$
  - Proportion of the population that completed the training program $p = \frac{1500}{2500} = 0.60$

- Suppose that a sample of 30 managers will be used. i.e. ${n=30}$ and 19 Yes for Training
  - Suppose, sample have ${\mathcal{N}}_{(\overline{x} = 51814, \, s = 3348)}$
  - Also, $\overline{p} = \frac{x}{n} = \frac{19}{30} = 0.63$

- If 500 such samples are taken, where each have their own ${\overline{x}}$
  - Then their expected value $E(\overline{x}) = \mu = 51800$
  - Standard Error $\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}} = \frac{4000}{\sqrt{30}} = 730.3$

- Suppose the director believes the sample mean ${\overline{x}}$ will be an acceptable estimate of the population mean ${\mu}$ if the sample mean is within 500 dollars of the population mean. 
  - However, it is not possible to guarantee that the sample mean will be within 500 dollars of the population
  - We can reframe the request in probability terms i.e.
    - What is the probability that the sample mean computed using a simple random sample of 30 EAI managers will be within 500 dollars of the population mean
    - i.e. Probability that $\overline{x} \in [51300, 52300]$
      - For $z = \frac{\overline{x} - \mu}{\sigma_{\overline{x}}}$
      - For $\overline{x} = 52300 \Rightarrow z = \frac{52300 - 51800}{730.30} = 0.68$
      - For $\overline{x} = 51300 \Rightarrow z = \frac{51300 - 51800}{730.30} = -0.68$
  - $P_{(51300 \leq \overline{x} \leq 52300)} = P_{(\overline{x} \leq 52300)} - P_{(\overline{x} \leq 51300)} = P_{(z \leq 0.68)} - P_{(z \leq -0.68)} = 0.7517 - 0.2483 = 0.5034$
    - A simple random sample of 30 EAI managers has a 0.5034 probability of providing a sample mean ${\overline{x}}$ that is within 500 dollars of the population mean. 
      - Thus, there is a $1 − 0.5034 = 0.4966$ probability that the difference between ${\overline{x}}$ and ${\mu}$ will be more than 500 dollars. 
      - In other words, a simple random sample of 30 EAI managers has roughly a 50-50 chance of providing a sample mean within the allowable 500 dollars. Perhaps a larger sample size should be considered. 
      - Let us explore this possibility by considering the relationship between the sample size and the sampling distribution of ${\overline{x}}$.

- Impact of $n = 100$ in place of $n =30$
  - First note that $E(\overline{x}) = \mu$ regardless of the sample size. Thus, the mean of all possible values of ${\overline{x}}$ is equal to the population mean ${\mu}$ regardless of the sample size ${n}$. 
  - However, standard error is reduced to $\sigma_{\overline{x}} = \frac{4000}{\sqrt{100}} = 400$
  - For $\overline{x} = 52300 \Rightarrow z = \frac{52300 - 51800}{400} = 1.25$
  - For $\overline{x} = 51300 \Rightarrow z = \frac{51300 - 51800}{400} = -1.25$
  - Thus $P_{(51300 \leq \overline{x} \leq 52300)} = P_{(\overline{x} \leq 52300)} - P_{(\overline{x} \leq 51300)} = P_{(z \leq 1.25)} - P_{(z \leq -1.25)} = 0.8944 - 0.1056 = 0.7888$
  - Thus, by increasing the sample size from 30 to 100 EAI managers, we increase the probability of obtaining a sample mean within 500 dollars of the population mean from 0.5034 to 0.7888.

\textcolor{orange}{Caution:} Here, we took advantage of the fact that the population mean ${\mu}$ and the population standard deviation ${\sigma}$ were known. However, usually these values will be unknown. 

## "ForLater" {.unlisted .unnumbered}


Properties of Point Estimators  

Three properties of good point estimators: unbiased, efficiency, and consistency.

$\theta = \text{the population parameter of interest}$
$\hat{\theta} = \text{the sample statistic or point estimator of } \theta$

- Unbiased
  - If the expected value of the sample statistic is equal to the population parameter being estimated, the sample statistic is said to be an \textcolor{pink}{unbiased estimator} of the population parameter
- Efficiency
  - When sampling from a normal population, the standard error of the sample mean is less than the standard error of the sample median. Thus, the sample mean is more efficient than the sample median.
- Consistency
  - A point estimator is consistent if the values of the point estimator tend to become closer to the population parameter as the sample size becomes larger. 

Other Sampling Methods

- Stratified Random Sampling
- Cluster Sampling
- Systematic Sampling
- Convenience Sampling
- Judgment Sampling

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C07-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C07-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/307-Sampling.Rmd-->

# Interval Estimation {#c08}

```{r 'C08', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- This chapter covers 
  - [Interval Estimation](#interval-c08 "c08") and [For P(t), find t by qt()](#get-t-c08 "c08")

## Interval Estimate {#interval-c08}

```{definition 'Interval-Estimate'}
Because a point estimator cannot be expected to provide the exact value of the population parameter, an \textcolor{pink}{interval estimate} is often computed by adding and subtracting a value, called the \textcolor{pink}{margin of error (MOE)}, to the point estimate. $\text{Interval Estimate} = \text{Point Estimate} \pm \text{MOE}_{\gamma}$
```

```{definition 'Confidence-Interval'}
\textcolor{pink}{Confidence interval} is another name for an interval estimate. Normally it is given as \textcolor{pink}{$({\gamma} = 1 - {\alpha})$}. Ex: 95% confidence interval
```

```{definition 'Confidence-Coefficient'}
The confidence level expressed as a decimal value is the \textcolor{pink}{confidence coefficient $({\gamma} = 1 - {\alpha})$}. i.e. 0.95 is the confidence coefficient for a 95% confidence level.
```


## Known SD {.unlisted .unnumbered .tabset .tabset-fade}

In order to develop an interval estimate of a population mean, either the population standard deviation ${\sigma}$ or the sample standard deviation ${s}$ must be used to compute the margin of error. In most applications ${\sigma}$ is not known, and ${s}$ is used to compute the margin of error. 

In some applications, large amounts of relevant historical data are available and can be used to estimate the population standard deviation prior to sampling. Also, in quality control applications where a process is assumed to be operating correctly, or 'in control', it is appropriate to treat the population standard deviation as known. 

\textcolor{pink}{Sampling distribution of ${\overline{x}}$ can be used to compute the probability that ${\overline{x}}$ will be within a given distance of ${\mu}$.}

Example: Lloyd Department Store

- Each week Lloyd Department Store selects a simple random sample of 100 customers in order to learn about the amount spent per shopping trip. 
  - With ${x}$ representing the amount spent per shopping trip, the sample mean ${\overline{x}}$ provides a point estimate of ${\mu}$, the mean amount spent per shopping trip for the population of all Lloyd customers. Based on the historical data, Lloyd now assumes a known value of $\sigma = 20$ for the population standard deviation. 
  - During the most recent week, Lloyd surveyed 100 customers $(n = 100)$ and obtained a sample mean of $\overline{x} = 82$.
  - we can conclude that the sampling distribution of ${\overline{x}}$ follows a normal distribution with a standard error of $\sigma_{\overline{x}} = \frac{\sigma}{\sqrt{n}} = \frac{20}{\sqrt{100}} =2$. 
  - Because the sampling distribution shows how values of ${\overline{x}}$ are distributed around the population mean ${\mu}$, the sampling distribution of ${\overline{x}}$ provides information about the possible differences between ${\overline{x}}$ and ${\mu}$.
  - Using the standard normal probability table, we find that \textcolor{pink}{95\% of the values of any normally distributed random variable are within $\pm 1.96$ standard deviations of the mean i.e. $[\mu - 1.96 \sigma, \mu + 1.96\sigma]$}. 
    - Thus, 95% of the ${\overline{x}}$ values must be within $\pm 1.96 \sigma_{\overline{x}}$ of the mean ${\mu}$. 
    - In the Lloyd example we know that the sampling distribution of ${\overline{x}}$ is normally distributed with a standard error of $\sigma_{\overline{x}} =2$. 
    - we can conclude that 95% of all ${\overline{x}}$ values obtained using a sample size of $n = 100$ will be within $(\pm 1.96 \times 2 = \pm 3.92)$ of the population mean ${\mu}$.
  - As given above, sample mean was $\overline{x} = 82$
    - Interval estimate of $\overline{x} = 82 \pm 3.92 = [78.08, 85.92]$
    - Because 95% of all the intervals constructed using $\overline{x} = 82 \pm 3.92$ will contain the population mean, we say that we are 95% confident that the interval 78.08 to 85.92 includes the population mean ${\mu}$. 
    - We say that this interval has been established at the 95% confidence level. 
    - The value 0.95 is referred to as the \textcolor{pink}{confidence coefficient}, and the interval 78.08 to 85.92 is called the 95% \textcolor{pink}{confidence interval}.

Interval Estimate of a Population Mean: ${\sigma}$ known is given by equation \@ref(eq:interval-with-sigma)

\begin{equation}
  \begin{align}
    \overline{x} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}
  \end{align}
  (\#eq:interval-with-sigma)
\end{equation}

where $(1 − \alpha)$ is the confidence coefficient and $z_{\alpha/2}$ is the z-value providing an area of $\alpha/2$ in the upper tail of the standard normal probability distribution.

For a 95% confidence interval, the confidence coefficient is $(1 − \alpha) = 0.95$ and thus, $\alpha = 0.05$. Using the standard normal probability table, an area of $\alpha/2 = 0.05/2 = 0.025$ in the upper tail provides $z_{.025} = 1.96$. 

```{r 'C08-getZgivenAlpha'}
# #Find z-value for confidence interval 95% i.e. (1-alpha) = 0.95 i.e. alpha = 0.05
# #To look for Area under the curve towards Right only i.e. alpha/2 = 0.025
p_r_ii <- 0.025
p_l_ii <- 1 - p_r_ii
z_ii <- round(qnorm(p = p_l_ii, lower.tail = TRUE), 4)
cat(paste0("(Left) P(z) = ", format(p_l_ii, nsmall = 3), " (i.e. (Right) 1-P(z) = ", 
           format(p_r_ii, nsmall = 3), ") at z = ", z_ii, "\n"))
#
# #Critical Value (z) for Common Significance level Alpha (α) or Confidence level (1-α)
xxalpha <- c("10%" = 0.1, "5%" = 0.05, "5/2%" = 0.025, "1%" = 0.01, "1/2%" = 0.005)
#
# #Left Tail Test
round(qnorm(p = xxalpha, lower.tail = TRUE), 4)
#
# #Right Tail Test
round(qnorm(p = xxalpha, lower.tail = FALSE), 4)
```


## Unknown SD {.tabset .tabset-fade}


```{definition 't-distribution'}
When ${s}$ is used to estimate ${\sigma}$, the margin of error and the interval estimate for the population mean are based on a probability distribution known as the \textcolor{pink}{t distribution}.
```

The t distribution is a family of similar probability distributions, with a specific t distribution depending on a parameter known as the\textcolor{pink}{degrees of freedom}. As the number of degrees of freedom increases, the difference between the t distribution and the standard normal distribution becomes smaller and smaller. 




Just as $z_{0.025}$ was used to indicate the z value providing a 0.025 area in the upper tail of a standard normal distribution, $t_{0.025}$ indicates a 0.025 area in the upper tail of a t distribution. In general, the notation $t_{\alpha/2}$ represents a t value with an area of $\alpha/2$ in the upper tail of the t distribution.

As the degrees of freedom increase, the t distribution approaches the standard normal distribution. Ex:  $t_{0.025} = 2.262 \, (\text{DOF} = 9)$, $t_{0.025} = 2.200 \, (\text{DOF} = 60)$, and $t_{0.025} = 1.96 \, (\text{DOF} = \infty) = z_{0.025}$

Interval Estimate of a Population Mean: ${\sigma}$ Unknown is given by equation \@ref(eq:interval-nsd)

\begin{equation}
  \begin{align}
    \overline{x} \pm t_{\alpha/2} \frac{s}{\sqrt{n}}
  \end{align}
  (\#eq:interval-nsd)
\end{equation}

where ${s}$ is the sample standard deviation, $(1 − \alpha)$ is the confidence coefficient and $t_{\alpha/2}$ is the t-value providing an area of $\alpha/2$ in the upper tail of the t distribution with ${n-1}$ degrees of freedom.

Refer equation \@ref(eq:sd), the expression for the sample standard deviation is 

\begin{equation*} 
    {s} = \sqrt{\frac{\sum \left(x_i - \overline{x}\right)^2}{n-1}}
\end{equation*} 

```{definition 'Degrees-of-Freedom'}
The number of \textcolor{pink}{degrees of freedom} is the number of values in the final calculation of a statistic that are free to vary. In general, the degrees of freedom of an estimate of a parameter are $(n - 1)$.
```

Why $(n-1)$ are the degrees of freedom

- Degrees of freedom refer to the number of independent pieces of information that go into the computation. i.e. $\{(x_{1}-\overline{x}), (x_{2}-\overline{x}), \ldots, (x_{n}-\overline{x})\}$
- However, $\sum (x_{i}-\overline{x}) = 0$ for any data set. 
- Thus, only $(n − 1)$ of the $(x_{i}-\overline{x})$ values are independent.
  - if we know $(n − 1)$ of the values, the remaining value can be determined exactly by using the condition.

\textcolor{pink}{Larger sample sizes are needed if the distribution of the population is highly skewed or includes outliers.}

### Cal T  {#get-t-c08 .unlisted .unnumbered}

```{r 'C08-getT'}
# #Like pnorm() is for P(z) and qnorm() is for z, pt() is for P(t) and qt() is for t.
# #Find t-value for confidence interval 95% i.e. (1-alpha) = 0.95 i.e. alpha = 0.05
# #To look for Area under the curve towards Right only i.e. alpha/2 = 0.025
p_r_ii <- 0.025
p_l_ii <- 1 - p_r_ii
#
# #t-tables are unique for different degrees of freedom i.e. for DOF = 9 
dof_ii <- 9
t_ii <- round(qt(p = p_l_ii, df = dof_ii, lower.tail = TRUE), 4)
cat(paste0("(Left) P(t) = ", format(p_l_ii, nsmall = 3), " (i.e. (Right) 1-P(t) = ", 
           format(p_r_ii, nsmall = 3), ") at t = ", t_ii, " (dof = ", dof_ii, ")\n"))
```
 
### qt() {.unlisted .unnumbered}

```{r 'C08-getTgivenAlpha'}
# #Like pnorm() is for P(z) and qnorm() is for z, pt() is for P(t) and qt() is for t.
# #Find t-value for confidence interval 95% i.e. (1-alpha) = 0.95 i.e. alpha = 0.05
# #To look for Area under the curve towards Right only i.e. alpha/2 = 0.025
p_r_ii <- 0.025
p_l_ii <- 1 - p_r_ii
#
# #t-tables are unique for different degrees of freedom i.e. for DOF = 9 
dof_ii <- 9
t_ii <- round(qt(p = p_l_ii, df = dof_ii, lower.tail = TRUE), 4)
cat(paste0("(Left) P(t) = ", format(p_l_ii, nsmall = 3), " (i.e. (Right) 1-P(t) = ", 
           format(p_r_ii, nsmall = 3), ") at t = ", t_ii, " (dof = ", dof_ii, ")\n"))
#
dof_ii <- 60
t_ii <- round(qt(p = p_l_ii, df = dof_ii, lower.tail = TRUE), 4)
cat(paste0("(Left) P(t) = ", format(p_l_ii, nsmall = 3), " (i.e. (Right) 1-P(t) = ", 
           format(p_r_ii, nsmall = 3), ") at t = ", t_ii, " (dof = ", dof_ii, ")\n"))
#
dof_ii <- 600
t_ii <- round(qt(p = p_l_ii, df = dof_ii, lower.tail = TRUE), 4)
cat(paste0("(Left) P(t) = ", format(p_l_ii, nsmall = 3), " (i.e. (Right) 1-P(t) = ", 
           format(p_r_ii, nsmall = 3), ") at t = ", t_ii, " (dof = ", dof_ii, ")\n"))
#
# #t-table have Infinity Row which is same as z-table. For DOF >100, it can be used.
dof_ii <- Inf
t_ii <- round(qt(p = p_l_ii, df = dof_ii, lower.tail = TRUE), 4)
cat(paste0("(Left) P(t) = ", format(p_l_ii, nsmall = 3), " (i.e. (Right) 1-P(t) = ", 
           format(p_r_ii, nsmall = 3), ") at t = ", t_ii, " (dof = ", dof_ii, ")\n"))

#
z_ii <- round(qnorm(p = p_l_ii, lower.tail = TRUE), 4)
cat(paste0("(Left) P(z) = ", format(p_l_ii, nsmall = 3), " (i.e. (Right) 1-P(z) = ", 
           format(p_r_ii, nsmall = 3), ") at z = ", z_ii, "\n"))
```

### Ex: Credit Card {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C08-CreditCards'}
# #A sample of n = 70 households provided the credit card balances.
xxCreditCards <- c(9430, 7535, 4078, 5604, 5179, 4416, 10676, 1627, 10112, 6567, 13627, 18719, 14661, 12195, 10544, 13659, 7061, 6245, 13021, 9719, 2200, 10746, 12744, 5742, 7159, 8137, 9467, 12595, 7917, 11346, 12806, 4972, 11356, 7117, 9465, 19263, 9071, 3603, 16804, 13479, 14044, 6817, 6845, 10493, 615, 13627, 12557, 6232, 9691, 11448, 8279, 5649, 11298, 4353, 3467, 6191, 12851, 5337, 8372, 7445, 11032, 6525, 5239, 6195, 12584, 15415, 15917, 12591, 9743, 10324)
f_setRDS(xxCreditCards)
```

```{r 'C08-Credit-Estimate-Interval'}
bb <- f_getRDS(xxCreditCards)
mean_bb <- mean(bb)
sd_bb <- sd(bb)
dof_bb <- length(bb) - 1L
# #t-value for confidence interval 95% | (1-alpha) = 0.95 | alpha = 0.05 | alpha/2 = 0.025
p_r_ii <- 0.025
p_l_ii <- 1 - p_r_ii
#
dof_ii <- dof_bb
t_ii <- round(qt(p = p_l_ii, df = dof_ii, lower.tail = TRUE), 4)
cat(paste0("(Left) P(t) = ", format(p_l_ii, nsmall = 3), " (i.e. (Right) 1-P(t) = ", 
           format(p_r_ii, nsmall = 3), ") at t = ", t_ii, " (dof = ", dof_ii, ")\n"))
#
# #Interval Estimate
err_margin_bb <- t_ii * sd_bb / sqrt(length(bb))
est_l <- mean_bb - err_margin_bb
est_r <- mean_bb + err_margin_bb
#
cat(paste0("Normal Sample (n=", length(bb), ", mean=", mean_bb, ", sd=", round(sd_bb, 1),
           "):\n Point Estimate = ", mean_bb, ", Margin of error = ", round(err_margin_bb, 1), 
           ", ", (1-2*p_r_ii) * 100, "% confidence interval is [", 
           round(est_l, 1), ", ", round(est_r, 1), "]"))
```

## "ForLater" {.unlisted .unnumbered}

- Determining the Sample Size
- Population Proportion

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C08-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, dof_bb, dof_ii, err_margin_bb, est_l, est_r, mean_bb, p_l_ii, 
          p_r_ii, sd_bb, t_ii, xxCreditCards, z_ii, xxalpha)
```

```{r 'C08-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/308-Interval.Rmd-->

# Hypothesis Tests {#c09}

```{r 'C09', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- This chapter covers 
  - [Hypothesis Testing](#hypothesis-c09 "c09"), [Type I and Type II Errors](#errors-ab-c09, "c09"), [Test Statistic ](##test-stat-c09 "c09"), [One Tail Test](#one-tail-c09 "c09"), [Two Tail Test](#two-tail-c09 "c09"), [Tests with unknown sigma](#get-pt-c09 "c09")
  - [Calculate P(t) by pt()](#get-pt-c09 "c09")
  - "ForLater" - Population Proportion, Calculating the Probability of Type II Errors, Determining the Sample Size

## Hypothesis Testing {#hypothesis-c09}

```{definition 'Hypothesis-Testing'}
\textcolor{pink}{Hypothesis testing} is a process in which, using data from a sample, an inference is made about a population parameter or a population probability distribution. 
```

Note:

- Hypothesis testing is used to determine whether a statement about the value of a population parameter should or should not be rejected.
- It is the process to check whether the sample information is matching with population information.
- The hypothesis testing procedure uses data from a sample to test the two competing statements indicated by ${H_0}$ and ${H_a}$

```{definition 'Hypothesis-Null'}
\textcolor{pink}{Null Hypothesis $(H_0)$} is a tentative assumption about a population parameter. It is assumed True, by default, in the hypothesis testing procedure.
```

```{definition 'Hypothesis-Alternative'}
\textcolor{pink}{Alternative Hypothesis $(H_a)$} is the complement of the Null Hypothesis. It is concluded to be True, if the Null Hypothesis is rejected.
```

Note:

- The conclusion that the alternative hypothesis $(H_a)$ is true is made if the sample data provide sufficient evidence to show that the null hypothesis $(H_0)$ can be rejected.
- The null and alternative hypotheses are competing statements about the population. Either the null hypothesis ${H_0}$ is true or the alternative hypothesis ${H_a}$ is true, but not both. 


## Developing Null and Alternative Hypotheses 

All hypothesis testing applications involve collecting a sample and using the sample results to provide evidence for drawing a conclusion.

In some situations it is easier to identify the alternative hypothesis first and then develop the null hypothesis.

- The Alternative Hypothesis as a Research Hypothesis 
  - A new fuel injection system designed to increase the miles-per-gallon rating from the current value 24 miles per gallon.
    - $H_a : \mu > 24 \iff H_0: \mu \leq 24$
  - A new teaching method is developed that is believed to be better than the current method. 
    - $H_a : \text{\{New method is better}\} \iff H_0: \text{\{New method is NOT better}\}$
  - A new sales force bonus plan is developed in an attempt to increase sales. 
    - $H_a : \text{\{New plan increases sales}\} \iff H_0: \text{\{New plan does not increase sales}\}$
  - A new drug is developed with the goal of lowering blood pressure more than an existing drug. 
    - ${H_a}$ : New drug lowers blood pressure more than the existing drug
    - ${H_0}$ : New drug does not provide lower blood pressure than the existing drug
  - In each case, rejection of the null hypothesis ${H_0}$ provides statistical support for the research hypothesis ${H_a}$.

- The Null Hypothesis as an Assumption to Be Challenged
  - The null hypothesis ${H_0}$ expresses the belief or assumption about the value of the population parameter. The alternative hypothesis ${H_a}$ is that the belief or assumption is incorrect.
  - Ex: The label on a soft drink bottle states that it contains 67.6 fluid ounces. 
    - We consider the label correct provided the population mean filling weight for the bottles is \textcolor{pink}{at least} 67.6 fluid ounces. 
    - Without any reason to believe otherwise, we would give the manufacturer the benefit of the doubt and assume that the statement provided on the label is correct. 
    - $H_0 : \mu \geq 67.6 \iff H_a: \mu < 67.6$
    - If the sample results lead to the conclusion to reject ${H_0}$, the inference that $H_a: \mu < 67.6$ is true can be made. With this statistical support, the agency is justified in concluding that the label is incorrect and underfilling of the bottles is occurring. Appropriate action to force the manufacturer to comply with labeling standards would be considered. 
    - However, if the sample results indicate ${H_0}$ cannot be rejected, the assumption that the labeling is correct cannot be rejected. With this conclusion, no action would be taken.
    - A product information is usually assumed to be true and stated as the null hypothesis. The conclusion that the information is incorrect can be made if the null hypothesis is rejected.
  - Same situation, from the point of view of the manufacturer
    - The company does not want to underfill the containers (legal requirement). However, the company does not want to overfill containers either because it would be an unnecessary cost. 
    - $H_0 : \mu = 67.6 \iff H_a: \mu \neq 67.6$
    - If the sample results lead to the conclusion to reject ${H_0}$, the inference is made that $H_a: \mu \neq 67.6$ is true. We conclude that the bottles are not being filled properly and the production process should be adjusted. 
    - However, if the sample results indicate ${H_0}$ cannot be rejected, the assumption that the process is functioning properly cannot be rejected. In this case, no further action would be taken.


## Three forms of hypotheses  {.tabset .tabset-fade}

For hypothesis tests involving a population mean, we let ${\mu}_0$ denote the hypothesized value and we must choose one of the following three forms for the hypothesis test.

Alternative is One-Sided, if it states that a parameter is larger or smaller than the null value. Alternative is Two-sided, if it states that the parameter is different from the null value. 

```{definition 'Hypothesis-1T-Lower-Tail'}
\textcolor{pink}{$\text{\{Left or Lower \} }\space\thinspace {H_0} : {\mu} \geq {\mu}_0 \iff {H_a}: {\mu} < {\mu}_0$}
```

```{definition 'Hypothesis-1T-Upper-Tail'}
\textcolor{pink}{$\text{\{Right or Upper\} } {H_0} : {\mu} \leq {\mu}_0 \iff {H_a}: {\mu} > {\mu}_0$}
```

```{definition 'Hypothesis-2T-Two-Tail'}
\textcolor{pink}{$\text{\{Two Tail Test \} } \thinspace {H_0} :{\mu} = {\mu}_0 \iff {H_a}: {\mu} \neq {\mu}_0$}
```

Refer [Equality in Hypothesis](#equality-b14 "b14")

### Exercises  {.unlisted .unnumbered}

- The manager of an automobile dealership is considering a new bonus plan designed to increase sales volume. Currently, the mean sales volume is 14 automobiles per month. The manager wants to conduct a research study to see whether the new bonus plan increases sales volume. 
  - Solution: \textcolor{black}{$H_0 : \mu \leq 14 \iff H_a: \mu > 14$}
- A director of manufacturing must convince management that a proposed manufacturing method reduces costs before the new method can be implemented. The current production method operates with a mean cost of 220 dollars per hour. 
  - Solution: \textcolor{black}{$H_0 : \mu \geq 220 \iff H_a: \mu < 220$}


## Type I and Type II Errors  {#errors-ab-c09}

Refer [Type I and Type II Errors (B12)](#errors-ab-b12, "b12")

Ideally the hypothesis testing procedure should lead to the acceptance of ${H_0}$ when ${H_0}$ is true and the rejection of ${H_0}$ when ${H_a}$ is true. Unfortunately, the correct conclusions are not always possible. Because hypothesis tests are based on sample information, we must allow for the possibility of errors. 

```{r 'C09P01', echo=FALSE, fig.cap="(C09P01) Type-I $(\\alpha)$ and Type-II $(\\beta)$ Errors"}
knitr::include_graphics(paste0(.z$PX, "C09P01", "-Hypothesis-Errors", ".jpg")) #iiii
```

```{definition 'Error-Type-I'}
The error of rejecting ${H_0}$ when it is true, is \textcolor{pink}{Type I error $({\alpha})$}.
```

```{definition 'Error-Type-II'}
The error of accepting ${H_0}$ when it is false, is \textcolor{pink}{Type II error $({\beta})$}.
```

```{definition 'Level-of-Significance'}
The \textcolor{pink}{level of significance $(\alpha)$} is the probability of making a Type I error when the null hypothesis is true as an equality.
```

```{r 'C09D05', comment="", echo=FALSE, results='asis'}
f_getDef("Confidence-Coefficient")
```


In practice, the person responsible for the hypothesis test specifies the level of significance. By selecting ${\alpha}$, that person is controlling the probability of making a Type I error.

- Most common value are ${\alpha} = 0.05, 0.01$. 
  - For example, a significance level of ${\alpha} = 0.05$ indicates a 5% risk of concluding that a difference exists when there is no actual difference. 
  - Lower significance levels indicate that you require stronger evidence before you will reject the null hypothesis.
  - If the cost of making a Type I error is high, small values of ${\alpha}$ are preferred. Ex: ${\alpha} = 0.01$ 
  - If the cost of making a Type I error is not too high, larger values of ${\alpha}$ are typically used. Ex: $\alpha = 0.05$ 


```{definition 'Significance-Tests'}
Applications of hypothesis testing that only control for the Type I error $(\alpha)$ are called \textcolor{pink}{significance tests}. 
```

Although most applications of hypothesis testing control for the probability of making a Type I error, they do not always control for the probability of making a Type II error. Hence, if we decide to accept ${H_0}$, we cannot determine how confident we can be with that decision. Because of the uncertainty associated with making a Type II error when conducting significance tests, statisticians usually recommend that we use the statement \textcolor{pink}{"do not reject ${H_0}$"} instead of "accept ${H_0}$." Using the statement "do not reject ${H_0}$" carries the recommendation to withhold both judgment and action. In effect, by not directly accepting ${H_0}$, the statistician avoids the risk of making a Type II error.

### Additional

Refer figure \@ref(fig:C09P01)

1. Type I (\textcolor{pink}{${\alpha}$}): 
    - False Positive: Rejecting a True ${H_0}$ thus claiming False ${H_a}$
    - An alpha error is when you mistakenly reject the Null and believe that something significant happened
      - i.e. you believe that the means of the two populations are different when they are not
      - i.e. you report that your findings are significant when in fact they have occurred by chance
    - The probability of making a type I error is represented by alpha level ${\alpha}$, which is the p-value below which you reject the null hypothesis
      - The \textcolor{pink}{p-value} is the actual risk you have in being wrong if you reject the null
        - You would like that to be low
        - This p-value is compared with and should be lower than the alpha
        - A p-value of 0.05 indicates that you are willing to accept a 5% chance that you are wrong when you reject the null hypothesis. You can reduce your risk of committing a type I error by using a lower value for p. For example, a p-value of 0.01 would mean there is a 1% chance of committing a Type I error.
        - However, using a lower value for alpha means that you will be less likely to detect a true difference if one really exists (thus risking a type II error).
    - ${\alpha}$ is \textcolor{pink}{Significance Level} (for $(1-{\alpha})$ confidence of not committing Type 1 error)
      - It is the boundary for specifying a statistically significant finding when interpreting the p-value
    - NOTE: Fail to reject True ${H_0}$ ($\approx$ accept) is the correct decision shown in Top Left Quadrant
1. Type II (\textcolor{pink}{${\beta}$}): 
    - False Negative: Failing to reject ($\approx$ accept) a False ${H_0}$ 
    - A beta error is when you fail to reject the null when you should have
      - i.e. you missed something significant and failed to take action
      - i.e. you conclude that there is not a significant effect, when actually there really is
      - You can decrease your risk of committing a type II error by ensuring your test has enough \textcolor{pink}{power.}
      - You can do this by ensuring your sample size is large enough to detect a practical difference when one truly exists.

```{r 'C09D06', comment="", echo=FALSE, results='asis'}
f_getDef("Power")
```


- The consequences of making a type I error mean that changes or interventions are made which are unnecessary, and thus waste time, resources, etc.
- Type II errors typically lead to the preservation of the status quo (i.e. interventions remain the same) when change is needed.
- Generally max 5% ${\alpha}$ and max 20% ${\beta}$ errors are recommended
    

## Known SD


### Test Statistic {#test-stat-c09}

```{definition 'Test-Statistic'}
\textcolor{pink}{Test statistic} is a number calculated from a statistical test of a hypothesis. It shows how closely the observed data match the distribution expected under the null hypothesis of that statistical test. It helps determine whether a null hypothesis should be rejected.
```

```{r 'C09D04', comment="", echo=FALSE, results='asis'}
f_getDef("Probability-Distribution")
```

The test statistic summarizes the observed data into a single number using the central tendency, variation, sample size, and number of predictor variables in the statistical model. Refer Table \@ref(tab:C09V01)

Table: (\#tab:C09V01) (C09V01) Test Statistic

| Test statistic | ${H_0}$ and ${H_a}$ | Statistical tests that use it |
| :--- | :--- | :--- | 
| t-value | Null: The means of two groups are equal | T-test, Regression tests | 
| | Alternative: The means of two groups are not equal | |
| z-value | Null: The means of two groups are equal | Z-test |
| | Alternative:The means of two groups are not equal | |
| F-value | Null: The variation among two or more groups is greater than or equal to the variation between the groups | ANOVA, ANCOVA, MANOVA |
| | Alternative: The variation among two or more groups is smaller than the variation between the groups  | |
| ${\chi}^2\text{-value}$ | Null: Two samples are independent | Chi-squared test, Non-parametric correlation tests |
| | Alternative: Two samples are not independent (i.e. they are correlated) | |

### Tails

```{r 'C09D08', comment="", echo=FALSE, results='asis'}
f_getDef("Tails")
```

```{definition 'Tailed-Test'}
A \textcolor{pink}{one-tailed test} and a \textcolor{pink}{two-tailed test} are alternative ways of computing the statistical significance of a parameter inferred from a data set, in terms of a test statistic.
```

One tailed-tests are concerned with one side of a statistic. Thus, one-tailed tests deal with only one tail of the distribution, and the z-score is on only one side of the statistic. Whereas, Two-tailed tests deal with both tails of the distribution, and the z-score is on both sides of the statistic. 

In a one-tailed test, the area under the rejection region is equal to the level of significance, ${\alpha}$. When the rejection region is below the acceptance region, we say that it is a \textcolor{pink}{left-tail test}. Similarly, when the rejection region is above the acceptance region, we say that it is a \textcolor{pink}{right-tail test}.

In the two-tailed test, there are two critical regions, and the area under each region is $\frac{\alpha}{2}$. 


One-Tail vs. Two-Tail

- One-tailed tests have more statistical power to detect an effect in one direction than a two-tailed test with the same design and significance level. 
  - One-tailed tests occur most frequently for studies where one of the following is true:
    - Effects can exist in only one direction.
    - Effects can exist in both directions but the researchers only care about an effect in one direction.
- The disadvantage of one-tailed tests is that they have no statistical power to detect an effect in the other direction.
  - Whereas, A two-tailed hypothesis test is designed to show whether the sample mean is significantly greater than OR significantly less than the mean of a population.
    - A two-tailed test is designed to examine both sides of a specified data range as designated by the probability distribution involved. 
- Thumb rule
  - Consider both directions when deciding if you should run a one tailed test or two. If you can skip one tail and it is not irresponsible or unethical to do so, then you can run a one-tailed test.
  - Two-tail test is done when you do not know about direction, so you test for both sides.


### One-tailed Test {#one-tail-c09}

One-tailed tests about a population mean take one of the following two forms:

```{r 'C09D01', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-1T-Lower-Tail")
```

```{r 'C09D02', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-1T-Upper-Tail")
```

```{definition 'One-Tailed-Test'}
\textcolor{pink}{One-tailed test} is a hypothesis test in which rejection of the null hypothesis occurs for values of the test statistic in one tail of its sampling distribution.
```

Example: The label on a can of Hilltop Coffee states that the can contains 3 pounds of coffee. As long as the population mean filling weight is at least 3 pounds per can, the rights of consumers will be protected. Thus, the government (FTC) interprets the label information on a large can of coffee as a claim by Hilltop that the population mean filling weight is at least 3 pounds per can. 
  
- Develop the null and alternative hypotheses for the test
  - $H_0 : \mu \geq 3 \iff H_a: \mu < 3$
- Take a Sample
  - Suppose a sample of 36 cans of coffee is selected and the sample mean ${\overline{x}}$ is computed as an estimate of the population mean ${\mu}$. If the value of the sample mean ${\overline{x}}$ is less than 3 pounds, the sample results will cast doubt on the null hypothesis. 
  - What we want to know is how much less than 3 pounds must ${\overline{x}}$ be before we would be willing to declare the difference significant and risk making a Type I error by falsely accusing Hilltop of a label violation. A key factor in addressing this issue is the value the decision maker selects for the level of significance. 
- Specify the level of significance ${\alpha}$ 
  - FTC is willing to risk a 1% chance of making such an error i.e. $\alpha 0.01$
- Compute the value of test statistic 
  - Assume, known ${\sigma} = 0.18$ and Normal distribution
  - Refer equation \@ref(eq:sigma-x-bar), standard error of ${\overline{x}}$ is ${\sigma}_{\overline{x}} = \frac{{\sigma}}{\sqrt{n}} = \frac{0.18}{\sqrt{36}} = 0.03$
  - Because the sampling distribution of ${\overline{x}}$ is normally distributed, $z = \frac{\overline{x} - {\mu}_0}{{\sigma}_{\overline{x}}} = \frac{\overline{x} - 3}{0.03}$
  - Because the sampling distribution of x is normally distributed, the sampling distribution of ${z}$ is a standard normal distribution. 
  - A value of $z = −1$ means that the value of ${\overline{x}}$ is one standard error below the hypothesized value of the mean. For a value of $z = −2$, it would be two standard errors below the mean, and so on. 
  - We can use the standard normal probability table to find the lower tail probability ${P_{\left(z\right)}}$ corresponding to any ${z}$ value. Refer [Get P(z) by pnorm() or z by qnorm()](#get-pz-c06 "c06")
    - Ex: $P_{\left(z = -3\right)} = 0.0013$
    - As a result, the probability of obtaining a value of ${\overline{x}}$ that is 3 or more standard errors below the hypothesized population mean ${\mu}_0 = 3$ is also 0.0013. i.e. Such a result is unlikely if the null hypothesis is true. 


```{definition '1s-known-sd'}
If ${\sigma}$ is known, the standard normal random variable \textcolor{pink}{${z}$ is used as test statistic} to determine whether ${\overline{x}}$ deviates from the hypothesized value of ${\mu}$ enough to justify rejecting the null hypothesis. Refer equation \@ref(eq:z-sd) $\to z = \frac{\overline{x} - {\mu}_0}{{\sigma}_{\overline{x}}} =  \frac{\overline{x} - {\mu}_0}{{\sigma}/\sqrt{n}}$
```

\begin{equation}
  z = \frac{\overline{x} - {\mu}_0}{{\sigma}_{\overline{x}}} = \frac{\overline{x} - {\mu}_0}{{\sigma}/\sqrt{n}}
  (\#eq:z-sd)
\end{equation}

The key question for a lower tail test is, \textcolor{pink}{How small must the test statistic ${z}$ be before we choose to reject the null hypothesis} 

Two approaches can be used to answer this: the p-value approach and the critical value approach.

#### p-value approach

```{definition 'Approach-p-value'}
The \textcolor{pink}{p-value approach} uses the value of the test statistic ${z}$ to compute a probability called a \textcolor{pink}{p-value}.
```

```{definition 'p-value'}
A \textcolor{pink}{p-value} is a probability that provides a measure of the evidence against the null hypothesis provided by the sample. The p-value is used to determine whether the null hypothesis should be rejected. Smaller p-values indicate more evidence against ${H_0}$.
```

\textcolor{pink}{p-value (p)} is the probability of obtaining a result equal to or more extreme than was observed in the data. It is the probability of observing the result given that the null hypothesis is true. A small p-value indicates the value of the test statistic is unusual given the assumption that ${H_0}$ is true. 

For a \textcolor{pink}{lower tail test}, the p-value is the probability of obtaining a value for the test statistic as small as or smaller than that provided by the sample. 
- we use the standard normal distribution to find the probability that ${z}$ is less than or equal to the value of the test statistic. 
- After computing the p-value, we must then decide whether it is small enough to reject the null hypothesis; this decision involves comparing the p-value to the level of significance.

For the Hilltop Coffee Example

- Suppose the sample of 36 Hilltop coffee cans provides a sample mean of ${\overline{x}}$ = 2.92 pounds. 
  - Is $\overline{x} = 2.92$ small enough to cause us to reject ${H_0}$
- Because this is a lower tail test, the p-value is the area under the standard normal curve for values of ${z}$ less than or equal to the value of the test statistic. 
  - Refer equation \@ref(eq:z-sd), $z = \frac{\overline{x} - {\mu}_0}{{\sigma}/\sqrt{n}} = \frac{2.92 - 3}{0.18/\sqrt{36}} = -2.67$
  - Thus, the p-value is the probability that ${z}$ is less than or equal to −2.67 (the lower tail area corresponding to the value of the test statistic).
- Refer [Get P(z) by pnorm() or z by qnorm()](#get-pz-c06 "c06"), to get the p-value
  - $P_{\left(\overline{x} = 2.92\right)} = P_{\left(z = -2.67\right)} = 0.0038$
  - This p-value does not provide much support for the null hypothesis, but is it small enough to cause us to reject ${H_0}$
- Compare p-value with Level of significance $\alpha = 0.01$
  - Because .0038 is less than or equal to $\alpha = 0.01$, we reject ${H_0}$. Therefore, we find sufficient statistical evidence to reject the null hypothesis at the .01 level of significance.
  - We can conclude that Hilltop is underfilling the cans.
  
\textcolor{pink}{Rejection Rule: Reject ${H_0}$ if p-value $\leq {\alpha}$}

Further, in this case, we would reject ${H_0}$ for any value of ${\alpha} \geq (p = 0.0038)$. For this reason, the p-value is also called the \textcolor{pink}{observed level of significance}.

- (Aside)
  - For the p-value approach, the likelihood (p-value) of the numerical value of the test statistic is compared to the specified significance level (${\alpha}$) of the hypothesis test.
  - The p-value corresponds to the probability of observing sample data at least as extreme as the actually obtained test statistic. Small p-values provide evidence against the null hypothesis. The smaller (closer to 0) the p-value, the stronger is the evidence against the null hypothesis.
  - "If the null hypothesis is true, what is the probability that we would observe a more extreme test statistic in the direction of the alternative hypothesis than we did" 
  - Ex: (criminal trials) "If the defendant is innocent, what is the chance that we would observe such extreme criminal evidence"
  - pnorm() returns the cumulative probability up to q (i.e. ${\overline{x}}$) for a normal distribution with a given mean ${\mu}$ and standard deviation ${\sigma}$.

#### Critical value approach

```{definition 'Approach-Critical-Value'}
The \textcolor{pink}{critical value approach} requires that we first determine a value for the test statistic called the \textcolor{pink}{critical value}. 
```


```{definition 'Critical-Value'}
\textcolor{pink}{Critical value} is the value that is compared with the test statistic to determine whether ${H_0}$ should be rejected. Significance level ${\alpha}$, or confidence level ($1 - {\alpha}$), dictates the \textcolor{pink}{critical value ($Z$)}, or critical limit. Ex: For Upper Tail Test, $Z_{{\alpha} = 0.05} = 1.645$. 
```

For a \textcolor{pink}{lower tail test}, the critical value serves as a benchmark for determining whether the value of the test statistic is small enough to reject the null hypothesis. 
  - \textcolor{pink}{Critical value} is the value of the test statistic that corresponds to an area of ${\alpha}$ (the level of significance) in the lower tail of the sampling distribution of the test statistic.
  - In other words, the critical value is the largest value of the test statistic that will result in the rejection of the null hypothesis. 

Hilltop Coffee Example

- The sampling distribution for the test statistic ${z}$ is a standard normal distribution. 
  - Therefore, the critical value is the value of the test statistic that corresponds to an area of $\alpha = 0.01$ in the lower tail of a standard normal distribution. 
  - Using the standard normal probability table, we find that $P_{\left(z\right)} = 0.01$ for $z_{\alpha = 0.01} = −2.33$ 
  - Refer [Get P(z) by pnorm() or z by qnorm()](#get-pz-c06 "c06")
  - Thus, if the sample results in a value of the test statistic that is less than or equal to −2.33, the corresponding p-value will be less than or equal to .01; in this case, we should reject the null hypothesis. 
- Compare test statistic with z-value
  - Because $(z = -2.67) < (z_{\alpha = 0.01} = −2.33)$, we can reject ${H_0}$
  - We can conclude that Hilltop is underfilling the cans.

\textcolor{pink}{Rejection Rule: Reject ${H_0}$ if $z \leq z_{\alpha}$}


#### Summary

The p-value approach to hypothesis testing and the critical value approach will always lead to the same rejection decision; that is, whenever the p-value is less than or equal to ${\alpha}$, the value of the test statistic will be less than or equal to the critical value. 

- The advantage of the p-value approach is that the p-value tells us how significant the results are (the observed level of significance). 
  - If we use the critical value approach, we only know that the results are significant at the stated level of significance.

For \textcolor{pink}{upper tail test} The test statistic ${z}$ is still computed as earlier. But, for an upper tail test, the p-value is the probability of obtaining a value for the test statistic as large as or larger than that provided by the sample. 
Thus, to compute the p-value for the upper tail test in the ${\sigma}$ known case, we must use the standard normal distribution to find the probability that ${z}$ is greater than or equal to the value of the test statistic. Using the critical value approach causes us to reject the null hypothesis if the value of the test statistic is greater than or equal to the critical value $z_{\alpha}$; in other words, we reject ${H_0}$ if $z \geq z_{\alpha}$.


#### Acceptance and Rejection Region

```{r 'C09D07', comment="", echo=FALSE, results='asis'}
f_getDef("Interval-Estimate") 
```


```{definition 'Acceptance-Region'}
A \textcolor{pink}{acceptance region} (confidence interval), is a set of values for the test statistic for which the null hypothesis is accepted. i.e. if the observed test statistic is in the confidence interval then we accept the null hypothesis and reject the alternative hypothesis.
```

\begin{equation} 
  Z = \frac {{\overline{x}} - {\mu}}{{\sigma}/{\sqrt{n}}} \quad \iff {\mu} = {\overline{x}} - Z \frac{{\sigma}}{\sqrt{n}} \quad \to {\mu} = {\overline{x}} \pm Z \frac{{\sigma}}{\sqrt{n}} \quad \to {\mu} \approx {\overline{x}} \pm Z \frac{{s}}{\sqrt{n}}
  (\#eq:z-mu)
\end{equation} 

```{definition 'Margin-Error'}
The \textcolor{pink}{margin of error} tells how far the original population means might be from the sample mean. It is given by $Z\frac{{\sigma}}{\sqrt{n}}$ 
```

```{definition 'Rejection-Region'}
A \textcolor{pink}{rejection region} (critical region), is a set of values for the test statistic for which the null hypothesis is rejected. i.e. if the observed test statistic is in the critical region then we reject the null hypothesis and accept the alternative hypothesis.
```


### Two-tailed Test {#two-tail-c09}

```{r 'C09D03', comment="", echo=FALSE, results='asis'}
f_getDef("Hypothesis-2T-Two-Tail")
```


```{definition 'Two-Tailed-Test'}
\textcolor{pink}{Two-tailed test} is a hypothesis test in which rejection of the null hypothesis occurs for values of the test statistic in either tail of its sampling distribution.
```


Ex: Golf Company, mean driving distance is 295 yards i.e. $({\mu}_0 = 295)$

- $H_0 : \mu = 295 \iff H_a: \mu \neq 295$
- The quality control team selected $\alpha = 0.05$ as the level of significance for the test.
- From previous tests, assume known ${\sigma} = 12$
- For a sample size $n = 50$
  - Standard Error of ${\overline{x}}$ is ${\sigma}_{\overline{x}} = \frac{{\sigma}}{\sqrt{n}} = \frac{12}{\sqrt{50}} = 1.7$
  - [Central Limit Theorem](#clt-c07 "c07"), allows us to conclude that the sampling distribution of ${\overline{x}}$ can be approximated by a normal distribution.
- Suppose for the sample, $\overline{x} = 297.6$


"ForLater" - This part needs to be moved to the Next Chapter.

- p-value approach
  - For a two-tailed test, the p-value is the probability of obtaining a value for the test statistic as unlikely as or more unlikely than that provided by the sample. 
  - Refer equation \@ref(eq:z-sd), $z = \frac{\overline{x} - {\mu}_0}{{\sigma}/\sqrt{n}} = \frac{297.6 - 295}{12/\sqrt{50}} = 1.53$
  - Now to compute the p-value we must find the probability of obtaining a value for the test statistic at least as unlikely as $z = 1.53$. 
    - Clearly values of $z \geq 1.53$ are at least as unlikely. 
    - But, because this is a two-tailed test, values of $z \leq −1.53$ are also at least as unlikely as the value of the test statistic provided by the sample. 
  - Refer [Get P(z) by pnorm() or z by qnorm()](#get-pz-c06 "c06"), to get the p-value
    - $P_{\left(z\right)} = P_{\left(z \leq -1.53\right)} + P_{\left(z \geq 1.53\right)}$
    - $P_{\left(z\right)} = 2 \times P_{\left(z \geq 1.53\right)}$, Because the normal curve is symmetric
    - $P_{\left(z\right)} = 2 \times 0.0630 = 0.1260$
  - Compare p-value with Level of significance $\alpha = 0.05$
    - We do not reject ${H_0}$ because the $(\text{p-value}= 0.1260) > (\alpha = 0.05)$
    - Because the null hypothesis is not rejected, no action will be taken.
- critical value approach
  - The critical values for the test will occur in both the lower and upper tails of the standard normal distribution. 
  - With a level of significance of $\alpha = 0.05$, the area in each tail corresponding to the critical values is $\alpha/2 = 0.025$. 
  - Refer [Get P(z) by pnorm() or z by qnorm()](#get-pz-c06 "c06")
    - Using the standard normal probability table, we find that $P_{\left(z\right)} = 0.025$ for $-z_{\alpha/2 = 0.025} = −1.96$ and $z_{\alpha/2 = 0.025} = 1.96$ 
  - Compare test statistic with z-value
    - Because $(z = 1.53)$ is NOT greater than  $(z_{\alpha/2 = 0.025} = 1.96)$, we cannot reject ${H_0}$

\textcolor{pink}{Rejection Rule: Reject ${H_0}$ if $z \leq -z_{\alpha/2}$ or $z \geq z_{\alpha/2}$}


(Online, might be wrong) Ex: Assume that for a Population with mean ${\mu}$ unknown and standard deviation ${\sigma} = 15$, if we take a sample ${n = 100}$ its sample mean is ${\overline{x}} = 42$.

Assume ${\alpha} = 0.05$ and if we are conducting a Two Tail Test, $Z_{\alpha/2 = 0.05/2} = 1.960$

- If we take a different sample of same size or a sample of different size, the sample mean calculated for those would be different.
- So, our sample mean ${\overline{x}}$ might not be the true population mean ${\mu}$
- Thus, a range is inferred using the sample size, the sample mean, and the population standard deviation, and it is assumed that the true population means falls under this interval. This interval is called a \textcolor{pink}{confidence interval}.
- Confidence interval is calculated using critical limit ${z}$, and thus are calculated for specific significance level ${\alpha}$
- Margin of Error $= Z\frac{{\sigma}}{\sqrt{n}} = 1.96 \times 15 /\sqrt{100} = 2.94$

As shown in the equation \@ref(eq:z-mu), our interval range is $\mu = \overline{X} \pm 2.94 = 42 \pm 2.94 \rightarrow \mu  \in (39.06, 44.94)$

> We are 95% confident that the population mean will be between 39.04 and 44.94

Note that a 95% confidence interval does not mean there is a 95% chance that the true value being estimated is in the calculated interval. Rather, given a population, there is a 95% chance that choosing a random sample from this population results in a confidence interval which contains the true value being estimated.

## Steps of Hypothesis Testing 

Common Steps

1. Develop the null and alternative hypotheses.
1. Specify the level of significance. 
1. Collect the sample data and compute the value of the test statistic. 

p-Value Approach Step 

4. Use the value of the test statistic to compute the p-value. 
4. Reject ${H_0}$ if the p-value $\leq {\alpha}$. 
4. Interpret the statistical conclusion in the context of the application. 


```{definition 'Approach-p-value-Steps'}
\textcolor{pink}{p-value Approach:} Form Hypothesis | Specify ${\alpha}$ | Calculate test statistic | Calculate p-value | Compare p-value with ${\alpha}$ | Interpret
```

Critical Value Approach 

4. Use the level of significance to determine the critical value and the rejection rule. 
4. Use the value of the test statistic and the rejection rule to determine whether to reject ${H_0}$.
4. Interpret the statistical conclusion in the context of the application.


## Relationship Between Interval Estimation and Hypothesis Testing 

Refer equation \@ref(eq:interval-with-sigma), For the ${\sigma}$ known case, the ${(1 - \alpha)}\%$ confidence interval estimate of a population mean is given by 

\begin{equation*}
  \overline{x} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}
\end{equation*}

We know that $100 {(1 - \alpha)}\%$ of the confidence intervals generated will contain the population mean and $100 {\alpha}\%$ of the confidence intervals generated will not contain the population mean. 

Thus, if we reject ${H_0}$ whenever the confidence interval does not contain ${\mu}_0$, we will be rejecting the null hypothesis when it is true $(\mu = {\mu}_0)$ with probability ${\alpha}$. 

The level of significance is the probability of rejecting the null hypothesis when it is true. So constructing a $100 {(1 - \alpha)}\%$ confidence interval and rejecting ${H_0}$ whenever the interval does not contain ${\mu}_0$ is equivalent to conducting a two-tailed hypothesis test with ${\alpha}$ as the level of significance.

Ex: Golf company

- For ${\alpha} = 0.05$, 95% confidence interval estimate of the population mean is 
  - ${\overline{x}} \pm z_{0.025} \frac{{\sigma}}{\sqrt{n}} = 297.6 \pm 1.96 \frac{12}{\sqrt{50}} = 297.6 \pm 3.3$
  - Interval: $[294.3, 300.9]$
  - We can conclude with 95% confidence that the mean distance for the population of golf balls is between 294.3 and 300.9 yards. 
  - Because the hypothesized value for the population mean, ${\mu}_0 = 295$, is in this interval, the hypothesis testing conclusion is that the null hypothesis, ${H_0: {\mu} = 295}$, cannot be rejected.

"ForLater" - Exercises


## Unknown SD  {#get-pt-c09}

```{definition '1s-unknown-sd'}
If ${\sigma}$ is unknown, the sampling distribution of the test statistic follows the \textcolor{pink}{t distribution} with $(n − 1)$ degrees of freedom. Refer equation \@ref(eq:t-nsd) $\to t = \frac{{\overline{x}} - {\mu}_0}{{s}/\sqrt{n}}$
```

\begin{equation}
  t = \frac{{\overline{x}} - {\mu}_0}{{s}/\sqrt{n}}
  (\#eq:t-nsd)
\end{equation}

One-Tailed Test

- Ex: Heathrow Airport, testing for mean rating 7 i.e. ${\mu}_0 = 7$
  - ${H_0}: {\mu} \leq 7 \iff {H_a} \geq 7$
  - Sample: ${\overline{x}} = 7.25, s = 1.052, n = 60$
  - ${\alpha} = 0.05$
  - Refer equation \@ref(eq:t-nsd), $t = \frac{\overline{x} - {\mu}_0}{s/\sqrt{n}} = \frac{7.25 - 7}{1.052/\sqrt{60}} = 1.84$
  - $\text{DOF} = n-1 = 60 -1 = 59$
  - Refer [For P(t), find t by qt()](#get-t-c08 "c08") and This is a Right Tail Test
    - ${P_{\left(t \geq 1.84\right)}} = 0.0354$ i.e. between 0.05 and 0.025
  - Comparison
    - ${(P_{\left(t \geq 1.84\right)}} = 0.035) < ({\alpha} = 0.05)$
    - Thus, we can reject the ${H_0}$ and can accept the ${H_a}$

Critical Value Approach
  - $(\text{DOF = 59}), \, t_{{\alpha} = 0.05} = 1.671$
  - Because $(t = 1.84) > (t_{{\alpha} = 0.05} = 1.671)$, Reject ${H_0}$

```{r 'C09-PtQt'}
# #Like pnorm() is for P(z) and qnorm() is for z, pt() is for P(t) and qt() is for t.
#
# #p-value approach: Find Commulative Probability P corresponding to the given t-value & DOF=59
pt(q = 1.84, df = 59, lower.tail = FALSE)
#
# #Critical Value: t-value for which Area under the curve towards Right is alpha=0.05 & DOF=59
qt(p = 0.05, df = 59, lower.tail = FALSE)
```

Two Tailed Test

- Ex: Holiday Toys, testing for sale of 40 units, i.e. ${\mu}_0 = 40$
  - ${H_0}: {\mu} = 40 \iff {H_a} \neq 40$
  - Sample: ${\overline{x}} = 37.4, s = 11.79, n = 25$
  - ${\alpha} = 0.05$
  - Refer equation \@ref(eq:t-nsd), $t = \frac{\overline{x} - {\mu}_0}{s/\sqrt{n}} = \frac{37.4 - 40}{11.79/\sqrt{25}} = -1.10$
  - $\text{DOF} = n-1 = 25 -1 = 24$
  - Because we have a two-tailed test, the p-value is two times the area under the curve of the t distribution for $t \leq -1.10$
    - $P_{\left(t\right)} = P_{\left(t \leq -1.10\right)} + P_{\left(z \geq 1.10\right)}$
    - $P_{\left(t\right)} = 2 \times P_{\left(t \leq -1.10\right)}$, Because the normal curve is symmetric
    - $P_{\left(t\right)} = 2 \times 0.1411 = 0.2822$ i.e. between 2 * (0.20 and 0.10) or (0.40, 0.20)
  - Comparison
    - $(P_{\left(t\right)}  = 0.282)2 > ({\alpha} = 0.05)$
    - Thus, we cannot reject the ${H_0}$
  
Critical Value Approach
  - $(\text{DOF = 24})$
  - We find that $P_{\left(t\right)} = 0.025$ for $-t_{\alpha/2 = 0.025} = -2.064$ and $t_{\alpha/2 = 0.025} = 2.064$ 
  - Compare test statistic with z-value
    - Because $(t = -1.10)$ is NOT lower than  $(-z_{\alpha/2 = 0.025} = -2.064)$, we cannot reject ${H_0}$


## Population Proportions

### Hypothesis

Using ${p}_0$ to denote the hypothesized value for the population proportion, the three forms for a hypothesis test about a population proportion ${p}$ are : 

```{definition 'H-1s-p-Lower'}
\textcolor{pink}{$\text{\{Left or Lower \} }\space\thinspace {H_0} : {p} \geq {p}_0 \iff {H_a}: {p} < {p}_0$}
```

```{definition 'H-1s-p-Upper'}
\textcolor{pink}{$\text{\{Right or Upper\} } {H_0} : {p} \leq {p}_0 \iff {H_a}: {p} > {p}_0$}
```

```{definition 'H-1s-p-Two'}
\textcolor{pink}{$\text{\{Two Tail Test \} } \thinspace {H_0} : {p} = {p}_0 \iff {H_a}: {p} \neq {p}_0$}
```

Hypothesis tests about a population proportion are based on the difference between the sample proportion ${\overline{p}}$ and the hypothesized population proportion ${p}_0$

The sampling distribution of ${\overline{p}}$, the point estimator of the population parameter ${p}$, is the basis for developing the test statistic.

When the null hypothesis is true as an equality, the expected value of ${\overline{p}}$ equals the hypothesized value ${p}_0$ i.e. $E_{(\overline{p})} = {p}_0$

The standard error of ${\overline{p}}$ is given in equation \@ref(eq:se-1s-p)

\begin{equation}
  {\sigma}_{\overline{p}} = \sqrt{\frac{{p}_0 (1 - {p}_0)}{n}}
  (\#eq:se-1s-p)
\end{equation}

If $np \geq 5$ and $n(1 − p) \geq 5$, the sampling distribution of ${p}$ can be approximated by a normal distribution. Under these conditions, which usually apply in practice, the quantity ${z}$ as given in equation \@ref(eq:z-1s-p) has a standard normal probability distribution.

\textcolor{pink}{Test Statistic for Hypothesis Tests about a Population Proportion :}

\begin{equation}
  z = \frac{{\overline{p}} - {p}_0}{{\sigma}_{\overline{p}}} = \frac{{\overline{p}} - {p}_0}{\sqrt{\frac{{p}_0 (1 - {p}_0)}{n}}}
  (\#eq:z-1s-p)
\end{equation}


Example: Pine Creek: Determine whether the proportion of women golfers increased from $p_0 = 0.20$

```{r 'C09D09', comment="", echo=FALSE, results='asis'}
f_getDef("H-1s-p-Upper") #dddd
```

- Count of Success $({x})$ is Number of Women
- $\{n = 400, x = 100\} \to {\overline{p}} = {n}/{x} = 0.25$ 
- \@ref(eq:z-1s-p) $z = \frac{{\overline{p}} - {p}_0}{{\sigma}_{\overline{p}}} = \frac{{\overline{p}} - {p}_0}{\sqrt{\frac{{p}_0 (1 - {p}_0)}{n}}}$
  - $z = \frac{0.25 - 0.20}{\sqrt{\frac{0.20 (1 - 0.20)}{400}}} = 2.50$
    - `{0.25 - 0.20}/{sqrt(0.20 * {1 - 0.20} / 400)}` \textcolor{pink}{$\#\mathcal{R}$} 
- ${}^U\!P_{(z = 2.50)} = 0.0062$
  - `pnorm(q = 2.50, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^U\!P_{(z)} < {\alpha} \to {H_0}$ is rejected i.e. the proportions are different
  - We can conclude that the proportion of women players has increased.

## Hypothesis Testing and Decision Making

If the purpose of a hypothesis test is to make a decision when ${H_0}$ is true and a different decision when ${H_a}$ is true, the decision maker may want to, and in some cases be forced to, take action with both the conclusion do not reject ${H_0}$ and the conclusion reject ${H_0}$. 

If this situation occurs, statisticians generally recommend controlling the probability of making a Type II error. With the probabilities of both the Type I and Type II error controlled, the conclusion from the hypothesis test is either to accept ${H_0}$ or reject ${H_0}$. In the first case, ${H_0}$ is concluded to be true, while in the second case, ${H_a}$ is concluded true. Thus, a decision and appropriate action can be taken when either conclusion is reached.

"ForLater" - Calculate ${\beta}$

When the true population mean ${\mu}$ is close to the null hypothesis value of ${\mu} = 120$, the probability is high that we will make a Type II error. However, when the true population mean ${\mu}$ is far below the null hypothesis value of ${\mu} = 120$, the probability is low that we will make a Type II error.


```{definition 'Power'}
The probability of correctly rejecting ${H_0}$ when it is false is called the \textcolor{pink}{power} of the test. For any particular value of ${\mu}$, the power is \textcolor{pink}{$1 − \beta$}.
```


```{definition 'Power-Curve'}
\textcolor{pink}{Power Curve} is a graph of the probability of rejecting ${H_0}$ for all possible values of the population parameter ${\mu}$ not satisfying the null hypothesis. It provides the probability of correctly rejecting the null hypothesis.
```

Note that the power curve extends over the values of ${\mu}$ for which the null hypothesis is false. The height of the power curve at any value of ${\mu}$ indicates the probability of correctly rejecting ${H_0}$ when ${H_0}$ is false.

## Summary

We can make 3 observations about the relationship among ${\alpha}, \beta, n (\text{sample size})$. 

1. Once two of the three values are known, the other can be computed. 
1. For a given level of significance ${\alpha}$, increasing the sample size will reduce ${\beta}$.
1. For a given sample size, decreasing ${\alpha}$ will increase ${\beta}$, whereas increasing ${\alpha}$ will decrease ${\beta}$.

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C09-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C09-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/309-Hypothesis.Rmd-->

# Two Populations {#c10}

```{r 'C10', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Inference About Means and Proportions with Two Populations"
  - [Mean - Two Sample - Independent](#mean-2s-sd "c10")
  - [Mean - Two Sample - Paired](#mean-paired-c10 "c10")
  - [Shapiro-Wilk Test for Normality](#shapiro-c10 "c10")


## Introduction

How interval estimates and hypothesis tests can be developed for situations involving two populations when the difference between the two population means or the two population proportions is of prime importance.

Example 

- To develop an interval estimate of the difference between the mean starting salary for a population of men and the mean starting salary for a population of women.
- To conduct a hypothesis test to determine whether any difference is present between the proportion of defective parts in a population of parts produced by supplier A and the proportion of defective parts in a population of parts produced by supplier B. 

## Known SD: Two Population Means {#mean-2s-sd}

\textcolor{pink}{Inferences About the Difference Between Two Population Means}

```{definition 'Independent-Simple-Random-Samples'}
Let ${\mathcal{N}}_{({\mu}_1, \, {\sigma}_1)}$ and ${\mathcal{N}}_{({\mu}_2, \, {\sigma}_2)}$ be the two populations. To make an inference about the difference between the means $({\mu}_1 - {\mu}_2)$, we select a simple random sample of ${n}_1$ units from population 1 and a second simple random sample of ${n}_2$ units from population 2. The two samples, taken separately and independently, are referred to as \textcolor{pink}{independent simple random samples}. 
```

### Interval Estimation

\textcolor{pink}{Interval Estimation of $({\mu}_1 - {\mu}_2)$}

The point estimator of the difference between the two population means $({\mu}_1 - {\mu}_2)$ is the difference between the two sample means \textcolor{pink}{$({\overline{x}}_1 - {\overline{x}}_2)$}. Thus, $E_{( {\overline{x}}_1 - {\overline{x}}_2 )}$ represents the difference of population means. It is given by equation \@ref(eq:point-estm-2s-sd)

\textcolor{pink}{Point Estimate :}

\begin{equation}
  E_{( {\overline{x}}_1 - {\overline{x}}_2 )} = {\overline{x}}_1 - {\overline{x}}_2 
  (\#eq:point-estm-2s-sd)
\end{equation}

As with other point estimators, the point estimator $E_{( {\overline{x}}_1 - {\overline{x}}_2 )}$ has a standard error \textcolor{pink}{${\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)}$}, that describes the variation in the sampling distribution of the estimator. It is the standard deviation of the sampling distribution of $({\overline{x}}_1 - {\overline{x}}_2)$. Refer equation \@ref(eq:se-2s-sd)

\textcolor{pink}{Standard Error of $({\overline{x}}_1 - {\overline{x}}_2)$ :}

\begin{equation}
  {\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = \sqrt{\frac{{\sigma}_1^2}{{n}_1} + \frac{{\sigma}_2^2}{{n}_2}}
  (\#eq:se-2s-sd)
\end{equation}

```{r 'C10D01', comment="", echo=FALSE, results='asis'}
f_getDef("Interval-Estimate")
```


In the case of estimation of the difference between two population means, an interval estimate will take the following form: $E_{( {\overline{x}}_1 - {\overline{x}}_2 )} \, \pm \text{MOE}_{{\gamma}}$. Refer equation \@ref(eq:moe-2s-sd) and \@ref(eq:interval-estm-2s-sd)

\textcolor{pink}{Margin of Error ($\text{MOE}_{{\gamma}}$) :}

\begin{equation}
  \text{MOE}_{{\gamma}} = {z}_{\frac{{\alpha}}{2}}{\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = {z}_{\frac{{\alpha}}{2}}\sqrt{\frac{{\sigma}_1^2}{{n}_1} + \frac{{\sigma}_2^2}{{n}_2}}
  (\#eq:moe-2s-sd)
\end{equation}

\textcolor{pink}{$\text{Interval Estimate}_{\gamma}$ :}

\begin{equation}
  \text{Interval Estimate}_{\gamma} = ({\overline{x}}_1 - {\overline{x}}_2) \pm {z}_{\frac{{\alpha}}{2}} \sqrt{\frac{{\sigma}_1^2}{{n}_1} + \frac{{\sigma}_2^2}{{n}_2}}
  (\#eq:interval-estm-2s-sd)
\end{equation}


Example: Greystone: Difference between the mean 

```{r 'C10D11', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Two")
```

- (1: Inner ) ${n}_1 = 36, {\overline{x}}_1 = 40, {\sigma}_1 = 9$
- (2: Suburb) ${n}_2 = 49, {\overline{x}}_2 = 35, {\sigma}_2 = 10$
- For ${\gamma = 0.95} \iff{\alpha} = 0.05 \to {z_{{\alpha}/2}} = {z_{0.025}} = 1.96$
  - `qnorm(p = 0.025, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- \@ref(eq:se-2s-sd) ${\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = \sqrt{\frac{{\sigma}_1^2}{{n}_1} + \frac{{\sigma}_2^2}{{n}_2}} = \sqrt{\frac{{9}^2}{36} + \frac{{10}^2}{49}} = 2.0714$
  - `sqrt(9^2/36 + 10^2/49)` \textcolor{pink}{$\#\mathcal{R}$}
- \@ref(eq:moe-2s-sd) $\text{MOE}_{\gamma} = {z}_{\frac{{\alpha}}{2}} {\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = 1.96 * 2.071 = 4.06$
- \@ref(eq:interval-estm-2s-sd) $\text{Interval Estimate}_{\gamma} = ({\overline{x}}_1 - {\overline{x}}_2) \pm \text{MOE}_{\gamma} = (40 - 35) \pm 4.06 = 5 \pm 4.06$

### Hypothesis Tests {.tabset .tabset-fade}

Using ${D_0}$ to denote the hypothesized difference between ${\mu}_1$ and ${\mu}_2$, the three forms for a hypothesis test are as follows: 

```{definition 'H-2s-Lower'}
\textcolor{pink}{$\text{\{Left or Lower \} }\space\thinspace {H_0} : {\mu}_1 - {\mu}_2 \geq {D_0} \iff {H_a}: {\mu}_1 - {\mu}_2 < {D_0}$}
```

```{definition 'H-2s-Upper'}
\textcolor{pink}{$\text{\{Right or Upper\} } {H_0} : {\mu}_1 - {\mu}_2 \leq {D_0} \iff {H_a}: {\mu}_1 - {\mu}_2 > {D_0}$}
```

```{definition 'H-2s-Two'}
\textcolor{pink}{$\text{\{Two Tail Test \} } \thinspace {H_0} : {\mu}_1 - {\mu}_2 = {D_0} \iff {H_a}: {\mu}_1 - {\mu}_2 \neq {D_0}$}
```

The test statistic for the difference between two population means when ${\sigma}_1$ and ${\sigma}_2$ are known is given in equation \@ref(eq:z-2s-sd) 

\textcolor{pink}{Test Statistic for Hypothesis Tests :}

\begin{equation}
  z = \frac{({\overline{x}}_1 - {\overline{x}}_2) - {D}_0}{\sqrt{\frac{{\sigma}_1^2}{{n}_1} + \frac{{\sigma}_2^2}{{n}_2}}} 
  (\#eq:z-2s-sd)
\end{equation}

#### Example {.unlisted .unnumbered}

Example: Evaluate differences in education quality between two training centers

```{r 'C10D02', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Two") 
```

- (1: A) ${n}_1 = 30, {\overline{x}}_1 = 82, {\sigma}_1 = 10$
- (2: B) ${n}_2 = 40, {\overline{x}}_2 = 78, {\sigma}_2 = 10$
- \@ref(eq:se-2s-sd) ${\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = \sqrt{\frac{{10}^2}{30} + \frac{{10}^2}{40}} = 2.4152$
  - `sqrt(10^2/30 + 10^2/40)` \textcolor{pink}{$\#\mathcal{R}$}
- \@ref(eq:z-2s-sd) $z = \frac{({\overline{x}}_1 - {\overline{x}}_2) - {D}_0}{{\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)}} = \frac{(82 - 78) - 0}{2.415} = 1.66$
- Calculate ${}^2\!P_{(z)}$
  - Because z is in upper tail, we get upper tail area and because it is Two-Tail Test, we double it
  - ${}^2\!P_{(z = 1.66)} = 2 * {}^U\!P_{(z = 1.66)} = 2 * 0.0485 = 0.0970$
    - `2 * pnorm(q = 1.66, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^2\!P_{(z)} > {\alpha} \to {H_0}$ cannot be rejected
  - The sample results do not provide sufficient evidence to conclude the training centers differ in quality.

#### Comparison {.unlisted .unnumbered}

```{r 'C10-GetPz'}
# #Get P(z) for z = 1.66 (Two-Tail)
#
# #Get the default (lower), subtract from 1, Double if Two-Tail
ii <- 2 * {1 - pnorm(q = 1.66)}
jj <- 2 * {1 - pnorm(q = 1.66, lower.tail = TRUE)}
#
# #Use the symmetry i.e. 'minus z' value, Double if Two-Tail
kk <- 2 * pnorm(q = -1.66)
#
# #Use the actual Upper Tail Option, Double if Two-Tail
ll <- 2 * pnorm(q = 1.66, lower.tail = FALSE)
#
stopifnot(all(identical(round(ii, 7), round(jj, 7)), identical(round(ii, 7), round(kk, 7)),
              identical(round(ii, 7), round(ll, 7))))
ll
```

#### Shapiro-Wilk test {#shapiro-c10 .unlisted .unnumbered}

```{definition 'Shapiro-Wilk-Test'}
The \textcolor{pink}{Shapiro-Wilk test} is a test of normality. It tests the null hypothesis that a sample came from a normally distributed population. \textcolor{pink}{$P_{\text{shapiro}} > ({\alpha} = 0.05) \to \text{Data is Normal}$}. Avoid using sample with more than 5000 observations.
```

The null-hypothesis of this test is that the population is normally distributed. Thus, if the p value is less than the chosen alpha level, then the null hypothesis is rejected and there is evidence that the data being tested is not distributed normally. 

On the other hand, if the p value is greater than the chosen alpha level, then the null hypothesis (that the data came from a normally distributed population) cannot be rejected (e.g., for an alpha level of .05, a data set with a p value of less than .05 rejects the null hypothesis that the data are from a normally distributed population).

Shapiro-Wilk test is known not to work well in samples with many identical values. 

```{conjecture 'shapiro-limit'}
\textcolor{brown}{Error in shapiro.test(...) : sample size must be between 3 and 5000}
```

- Ideally we should not test for normality of a sample with more than 5000 observations.
  - However, we can randomly select 5000 observations and test them by Shapiro.
  - We can use Anderson-Darling test or Kolmogorov-Smirnov test (Nonparametric) (KS is weaker than AD)
  - Anderson-Darling test is not quite as good as the Shapiro-Wilk test, but is better than other tests.
- [(SO) Normality Testing](https://stackoverflow.com/questions/7781798 "https://stackoverflow.com")
  - Normality Tests like Shapiro or Anderson Darling are NULL Hypothesis Tests \textcolor{pink}{against} the assumption of normality.
    - When the sample size is small, even big departures from normality are not detected
    - When your sample size is large, even the smallest deviation from normality will lead to a rejected null.
      - In Shapiro-Wilk test, with more data, the chances of the null hypothesis being rejected becomes larger even though for practical purposes the data is more than normal enough.
  - Do not worry much about normality. The CLT takes over quickly and if you have all but the smallest sample sizes and an even remotely reasonable looking histogram you are fine.
  - Worry about unequal variances (heteroskedasticity). Heteroscedasticity-consistent standard errors are checked by HCCM tests. A scale location plot will give some idea of whether this is broken, but not always. 
  - Also, there is no a priori reason to assume equal variances in most cases.
  - Further Outliers: A cooks distance of > 1 is reasonable cause for concern.


## Exercises {.tabset .tabset-fade}

### 01 {.unlisted .unnumbered}

```{r 'C10D03', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Two")
```

- (1:) ${n}_1 = 50, {\overline{x}}_1 = 13.6, {\sigma}_1 = 2.2$
- (2:) ${n}_2 = 35, {\overline{x}}_2 = 11.6, {\sigma}_2 = 3.0$
- What is the point estimate of the difference between the two population means
  - \@ref(eq:point-estm-2s-sd) $E_{( {\overline{x}}_1 - {\overline{x}}_2 )} = 13.6 - 11.6 = 2$
- \@ref(eq:se-2s-sd) ${\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = \sqrt{\frac{{2.2}^2}{50} + \frac{{3}^2}{35}} = 0.5949$
  - `sqrt(2.2^2/50 + 3^2/35)` \textcolor{pink}{$\#\mathcal{R}$}  
- Provide a 90% confidence interval for the difference between the two population means
  - ${\gamma = 0.90} \iff{\alpha} = 0.10 \to {z_{{\alpha}/2}} = {z_{0.05}} = 1.6448$
    - `qnorm(p = 0.05, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$}
  - \@ref(eq:moe-2s-sd) $\text{MOE}_{\gamma =0.90} = 1.6448 * 0.5949 = 0.9785$ 
  - \@ref(eq:interval-estm-2s-sd) $\text{Interval Estimate}_{\gamma} = 2 \pm 0.9785$
- Provide a 95% confidence interval for the difference between the two population means
  - ${\gamma = 0.95} \iff{\alpha} = 0.05 \to {z_{{\alpha}/2}} = {z_{0.025}} = 1.96$
  - $\text{MOE}_{\gamma =0.95} = 1.96 * 0.5949 = 1.166$ 
  - $\text{Interval Estimate}_{\gamma} = 2 \pm 1.166$

### 02 {.unlisted .unnumbered}

```{r 'C10D04', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Upper")
```

- (1:) ${n}_1 = 40, {\overline{x}}_1 = 25.2, {\sigma}_1 = 5.2$
- (2:) ${n}_2 = 50, {\overline{x}}_2 = 22.8, {\sigma}_2 = 6.0$
- \@ref(eq:se-2s-sd) ${\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = \sqrt{\frac{{5.2}^2}{40} + \frac{{6}^2}{50}} = 1.1815$
  - `sqrt(5.2^2/40 + 6^2/50)` \textcolor{pink}{$\#\mathcal{R}$}
- What is the value of the test statistic
  - \@ref(eq:z-2s-sd) $z = \frac{(25.2 - 22.8) - 0}{1.18} = 2.03$
- What is the p-value 
  - ${}^U\!P_{z = 2.03} = 0.0212$
    - `pnorm(q = 2.03, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$}
- With ${\alpha} = 0.05$, what is your hypothesis testing conclusion
  - ${}^U\!P_{z} < {\alpha} \to {H_0}$ Rejected


### 04 Conde {.unlisted .unnumbered}

```{r 'C10D05', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Two")
```

- (1: small) ${n}_1 = 37, {\overline{x}}_1 = 85.36, {\sigma}_1 = 4.55$
- (2: large) ${n}_2 = 44, {\overline{x}}_2 = 81.40, {\sigma}_2 = 3.97$
- \@ref(eq:point-estm-2s-sd) $E_{( {\overline{x}}_1 - {\overline{x}}_2 )} = 85.36 - 81.40 = 3.96$ 
- \@ref(eq:se-2s-sd) ${\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = \sqrt{\frac{{4.55}^2}{37} + \frac{{3.97}^2}{44}} = 0.958$
    - `sqrt(4.55^2/37 + 3.97^2/44)` \textcolor{pink}{$\#\mathcal{R}$}
- ${\gamma = 0.95} \iff{\alpha} = 0.05 \to {z_{{\alpha}/2}} = {z_{0.025}} = 1.96$
    - `qnorm(p = 0.025, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$}
- \@ref(eq:moe-2s-sd), $\text{MOE}_{\gamma =0.95} = 1.96 * 0.958 = 1.87768$ 
- \@ref(eq:interval-estm-2s-sd), $\text{Interval Estimate}_{\gamma} = 3.96 \pm 1.88$
- Test Statistic z
  - \@ref(eq:z-2s-sd) $z = \frac{(85.36 - 81.40 ) - 0}{0.958} = 4.13369$
- p-value
  - ${}^2\!P_{(z = 4.13369)} = 2 * {}^U\!P_{(z = 4.13369)} = 2 * 0.0000178 \approx 0$
    - `pnorm(q = 4.13369, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$}
  - ${}^2\!P_{z} < {\alpha} \to {H_0}$ Rejected


### 08 Rite Aid {.unlisted .unnumbered}

```{r 'C10D06', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Lower")
```

- Will improving customer service result in higher stock prices 
- For each case: ${n} = 60, {\sigma} = 6, {\alpha} = 0.05$
  - \@ref(eq:se-2s-sd) ${\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = \sqrt{\frac{{6}^2}{60} + \frac{{6}^2}{60}} = 1.0954$
    - `sqrt(6^2/60 + 6^2/60)` \textcolor{pink}{$\#\mathcal{R}$}

- Rite $\{{\overline{x}}_1 = 73, {\overline{x}}_2 = 76\}$, Expedia $\{{\overline{x}}_1 = 75,  {\overline{x}}_2 = 77\}$, JC $\{{\overline{x}}_1 = 77, {\overline{x}}_2 = 78\}$
- For Rite Aid, is the increase in the satisfaction score from year 1 to year 2 statistically significant
  - \@ref(eq:z-2s-sd) $z = \frac{(73 - 76) - 0}{1.0954} = -2.738613$
  - ${}^L\!P_{(z = -2.738613)} = 0.003$
    - `pnorm(q = -2.738613, lower.tail = TRUE)` \textcolor{pink}{$\#\mathcal{R}$}
    - ${}^L\!P_{z} < {\alpha} \to {H_0}$ is rejected i.e. the increase is significant
    - \textcolor{orange}{Caution:} Using Two-Tail Test will result in different result. The type of test should be considered carefully.

```{r 'C10D07', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Upper") 
```

- Can you conclude that the year 2 score for Rite Aid is above the national average of 75.7
  - 1 is 76, 2 is national 75.7
  - \@ref(eq:z-2s-sd) $z = \frac{(76 - 75.7) - 0}{1.0954} = 0.2739$
  - ${}^U\!P_{(z = 0.2739)} = 0.392$
    - `pnorm(q = 0.2739, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$}
    - ${}^U\!P_{z} > {\alpha} \to {H_0}$ cannot be rejected. Difference is NOT significant.
    - Cannot conclude that the increase is above the national average

```{r 'C10D08', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Lower")
```

- For Expedia, is the increase from year 1 to year 2 statistically significant
  - \@ref(eq:z-2s-sd) $z = \frac{(75 - 77) - 0}{1.0954} = -1.826$
  - ${}^L\!P_{(z = -1.826)} = 0.0339$
    - `pnorm(q = -1.826, lower.tail = TRUE)` \textcolor{pink}{$\#\mathcal{R}$}
    - ${}^L\!P_{z} < {\alpha} \to {H_0}$ is rejected i.e. the increase is significant

- When conducting a hypothesis test with the values given for the standard deviation, sample size, and alpha, how large must the increase from year 1 to year 2 be for it to be statistically significant
  - ${\alpha} = 0.05 \iff {}^L\!P_{(z)} = 0.05 \to {z_{0.05}} = -1.6448$
    - `qnorm(p = 0.05, lower.tail = TRUE)` \textcolor{pink}{$\#\mathcal{R}$}
  - \@ref(eq:moe-2s-sd), $\text{MOE}_{\gamma =0.95} = -1.6448 * 1.0954 = -1.8$ 
  - At least 1.8 increase should be there for result to be significant
  - For JC, because the increase is only 1 i.e. less than 1.8, it will not be significant


## Unknown SD: Two Population Means {.tabset .tabset-fade}

Use the sample standard deviations, ${s}_1$ and ${s}_2$, to estimate the unknown population standard deviations $({\sigma}_1, {\sigma}_2)$.

When $({\sigma}_1, {\sigma}_2)$ are estimated by $({s}_1, {s}_2)$, the t distribution is used to make inferences about the difference between two population means.

\textcolor{pink}{Interval Estimation of $({\mu}_1 - {\mu}_2)$}

\textcolor{pink}{Margin of Error ($\text{MOE}_{{\gamma}}$) :} Refer \@ref(eq:moe-2s-nsd) like \@ref(eq:moe-2s-sd) 

\begin{equation}
  \text{MOE}_{{\gamma}} = {t}_{\frac{{\alpha}}{2}}{\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = {t}_{\frac{{\alpha}}{2}}\sqrt{\frac{{\sigma}_1^2}{{n}_1} + \frac{{\sigma}_2^2}{{n}_2}}
  (\#eq:moe-2s-nsd)
\end{equation}

\textcolor{pink}{$\text{Interval Estimate}_{\gamma}$ :} Refer \@ref(eq:interval-estm-2s-nsd) like \@ref(eq:interval-estm-2s-sd) using \@ref(eq:se-2s-sd)

\begin{equation}
  \text{Interval Estimate}_{\gamma} = ({\overline{x}}_1 - {\overline{x}}_2) \pm {t}_{\frac{{\alpha}}{2}}{\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = ({\overline{x}}_1 - {\overline{x}}_2) \pm {t}_{\frac{{\alpha}}{2}}\sqrt{\frac{{s}_1^2}{{n}_1} + \frac{{s}_2^2}{{n}_2}}
  (\#eq:interval-estm-2s-nsd)
\end{equation}

\textcolor{pink}{Degrees of Freedom (DOF) :}  Refer \@ref(eq:dof-2s)

\begin{equation}
  \text{DOF} = \frac{ { \left( \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right) }^2} {\frac{1}{n_1 - 1}{ \left( \frac{s_1^2}{n_1} \right) }^2 + \frac{1}{n_2 - 1}{ \left( \frac{s_2^2}{n_2} \right) }^2}
  (\#eq:dof-2s)
\end{equation}

Example: Clearwater - To estimate the difference between the mean

```{r 'C10D09', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Two")
```

- (1: Cherry) ${n}_1 = 28, {\overline{x}}_1 = 1025, {s}_1 = 150$
- (2: Beech) ${n}_2 = 22, {\overline{x}}_2 = 910, {s}_2 = 125$
- \@ref(eq:se-2s-sd) ${\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = \sqrt{\frac{{150}^2}{28} + \frac{{125}^2}{22}} = 38.9076$
  - `sqrt(150^2/28 + 125^2/22)` \textcolor{pink}{$\#\mathcal{R}$}
- \@ref(eq:dof-2s) $\text{DOF} = 47$
  - `floor({150^2 / 28 + 125^2 / 22 }^2 / {{150^2 / 28}^2/{28-1} + {125^2 / 22}^2/{22-1}})` \textcolor{pink}{$\#\mathcal{R}$}
- ${\gamma = 0.95} \iff{\alpha} = 0.05 \to {{}^2\!t_{{\alpha}/2}} = {{}^2\!t_{0.025}} = 2.012$
  - `qt(p = 0.025, df = 47, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- \@ref(eq:moe-2s-nsd) $\text{MOE}_{\gamma =0.95} = 2.012 * 38.9076 = 78.3$
- \@ref(eq:interval-estm-2s-nsd) $\text{Interval Estimate}_{\gamma} = (1025-910) \pm 78$

\textcolor{pink}{Hypothesis Tests}

\textcolor{pink}{Test Statistic for Hypothesis Tests :} Refer \@ref(eq:t-2s-nsd) like \@ref(eq:z-2s-sd) using \@ref(eq:se-2s-sd)

\begin{equation}
  t = \frac{({\overline{x}}_1 - {\overline{x}}_2) - {D}_0}{{\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)}} = \frac{({\overline{x}}_1 - {\overline{x}}_2) - {D}_0}{\sqrt{\frac{{s}_1^2}{{n}_1} + \frac{{s}_2^2}{{n}_2}}}
  (\#eq:t-2s-nsd)
\end{equation}

\textcolor{orange}{Caution:} Use of ${\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)}$ symbol is probably wrong here because it should not represent formula containing ${s}_1, {s}_2$. "ForLater"

```{conjecture 't-test-grouping'}
\textcolor{brown}{Error in t.test.formula() : grouping factor must have exactly 2 levels}
```

- For t.test(): Formula needs to be 'value ~ key', not ~~'key ~ value'~~

### Example {.unlisted .unnumbered}

Software: To show that the new software will provide a shorter mean time

```{r 'C10D10', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Upper")
```

- (1: Old) ${n}_1 = 12, {\overline{x}}_1 = 325, {s}_1 = 40$
- (2: New) ${n}_2 = 12, {\overline{x}}_2 = 286, {s}_2 = 44$
- \@ref(eq:se-2s-sd) ${\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = \sqrt{\frac{{40}^2}{12} + \frac{{44}^2}{12}} = 17.1659$
  - `sqrt(40^2/12 + 44^2/12)` \textcolor{pink}{$\#\mathcal{R}$}
- \@ref(eq:dof-2s) $\text{DOF} = 21$
  - `floor({40^2 / 12 + 44^2 / 12 }^2 / {{40^2 / 12}^2/{12-1} + {44^2 / 12}^2/{12-1}})` \textcolor{pink}{$\#\mathcal{R}$}
- \@ref(eq:t-2s-nsd) $t = \frac{(325 - 286) - 0}{17.1659} = 2.272$
- Calculate ${}^U\!P_{(t)}$
  - ${}^U\!P_{(t = 2.272)} = 0.0168$
    - `pt(q = 2.272, df = 21, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^U\!P_{(t)} < {\alpha} \to {H_0}$ is rejected i.e. the decrease is significant
  - It supports the conclusion that the new software provides a smaller population mean.

### Code {.unlisted .unnumbered}

CONVERT HERE to Tilde Based Option, to take advantage of Column Headers

```{r 'C10-xxSoftware'}
# #Software 
xxSoftware <- tibble(Old = c(300, 280, 344, 385, 372, 360, 288, 321, 376, 290, 301, 283), 
                     New = c(274, 220, 308, 336, 198, 300, 315, 258, 318, 310, 332, 263))
aa <- xxSoftware
# Summary
aa %>% 
  pivot_longer(everything(), names_to = "key", values_to = "value") %>% 
  group_by(key) %>% 
  summarise(across(value, list(Count = length, Mean = mean, SD = sd), .names = "{.fn}"))
bb <- aa %>% pivot_longer(everything(), names_to = "key", values_to = "value")
#
# #Welch Two Sample t-test
# #Alternative must be: "two.sided" (Default), "less", "greater"
bb_ha <- "greater"
#bb_testT <- t.test(x = bb$Old, y = bb$New, alternative = bb_ha)
bb_testT <- t.test(formula = value ~ key, data = bb, alternative = bb_ha)
bb_testT
#
cat(paste0("t is the t-test statistic value (t = ", round(bb_testT$statistic, 6), ")\n"))
cat(paste0("df is the degrees of freedom (df = ", round(bb_testT$parameter, 1), ")\n"))
cat(paste0("p-value is the significance level of the t-test (p-value = ", 
           round(bb_testT$p.value, 6), ")\n"))
cat(paste0("conf.int is the confidence interval of the mean at 95% (conf.int = [",
           paste0(round(bb_testT$conf.int, 3), collapse = ", "), "])\n"))
cat(paste0("sample estimates is the mean value of the samples. Mean: ", 
           paste0(round(bb_testT$estimate, 2), collapse = ", "), "\n"))
#
# #Compare p-value with alpha = 0.05
alpha <- 0.05
if(any(all(bb_ha == "two.sided", bb_testT$p.value >= alpha / 2), 
       all(bb_ha != "two.sided", bb_testT$p.value >= alpha))) {
  cat(paste0("p-value (", round(bb_testT$p.value, 6), ") is greater than alpha (", alpha, 
      "). We failed to reject H0. We cannot conclude that the populations are different.\n")) 
} else {
    cat(paste0("p-value (", round(bb_testT$p.value, 6), ") is less than alpha (", alpha, 
      ").\nWe can reject the H0 with 95% confidence. The populations are different.\n"))
}
```


### Pooled {.unlisted .unnumbered}

Another approach used to make inferences about the difference between two population means when ${\sigma}_1$ and ${\sigma}_2$ are unknown is based on the assumption that the two population standard deviations are equal $({\sigma}_1 = {\sigma}_2 = {\sigma})$. Under this assumption, the two sample standard deviations are combined to provide the \textcolor{pink}{pooled sample variance} as given in equation \@ref(eq:var-pool-2s-nsd). 

\begin{equation}
  {s}_p^2 = \frac{({n}_1 - 1){s}_1^2 + ({n}_2 - 1){s}_2^2}{{n}_1 + {n}_2 - 2}
  (\#eq:var-pool-2s-nsd)
\end{equation}


The t test statistic becomes \@ref(eq:t-pool-2s-nsd) with \textcolor{pink}{$({n}_1 + {n}_2 - 2)$} degrees of freedom.

\begin{equation}
  t = \frac{({\overline{x}}_1 - {\overline{x}}_2) - {D}_0}{{s}_p\sqrt{\frac{1}{{n}_1} + \frac{1}{{n}_2}}}
  (\#eq:t-pool-2s-nsd)
\end{equation}

Then the computation of the p-value and the interpretation of the sample results are same as earlier. 

- \textcolor{orange}{Caution:} A difficulty with this procedure is that the assumption that the two population standard deviations are equal is usually difficult to verify. 
  - Unequal population standard deviations are frequently encountered. 
  - Using the pooled procedure may not provide satisfactory results, especially if the sample sizes ${n}_1$ and ${n}_2$ are quite different. 
  - The original t procedure does not require this assumption. It is a more general procedure and is recommended for most applications.

### Pooled Code {.unlisted .unnumbered}

[(External) Unpaired Two-Samples T-test](http://www.sthda.com/english/wiki/unpaired-two-samples-t-test-in-r "http://www.sthda.com")

```{r 'C10-PooledSampleVariance'}
# #Pooled Sample Variance
# #Evaluate differences in education quality between two training centers
# #Generate Data for Two Centers
set.seed(3)
setA <- rnorm(n = 30, mean = 82, sd = 10)
setB <- rnorm(n = 40, mean = 78, sd = 10)
bb <- tibble(sets = c(rep("setA", length(setA)), rep("setB", length(setB))), values = c(setA, setB))
# Summary
bb %>% group_by(sets) %>% summarise(Count = n(), Mean = mean(values), SD = sd(values))
#
# #Assumption 1: Are the two samples independents
# #YES
#
# #Assumtion 2: Are the data from each of the 2 groups follow a normal distribution
# #Shapiro-Wilk normality test
isNormal_A <- with(bb, shapiro.test(values[sets == "setA"]))
isNormal_B <- with(bb, shapiro.test(values[sets == "setB"]))
#isNormal_A
#isNormal_A$statistic
# #p-value > 0.05 is needed for Normality
isNormal_A$p.value
isNormal_B$p.value
#
# #Both have p-values greater than the significance level alpha = 0.05.
# #implying that the distribution of the data are not significantly different from the normal
# #In other words, we can assume the normality.
#
bb %>% 
  group_by(sets) %>% 
  summarise(p = shapiro.test(values)$p.value)
#
# #Assumption 3. Do the two populations have the same variances
# #We will use F-test to test for homogeneity in variances. (p-value > 0.05 is needed)
#
bb_testF <- var.test(values ~ sets, data = bb)
# bb_testF
bb_testF$p.value
#
# #The p-value of F-test is greater than the significance level alpha = 0.05. 
# #In conclusion, there is no significant difference between the variances of the two sets of data.
# #Therefore, we can use the classic t-test witch assume equality of the two variances.
#
# #Compute unpaired two-samples t-test
#
# #Question : Is there any significant difference between the mean of two populations
#
bb_testT <- t.test(formula = values ~ sets, data = bb, var.equal = TRUE)
bb_testT
#
cat(paste0("t is the t-test statistic value (t = ", round(bb_testT$statistic, 6), ")\n"))
cat(paste0("df (n1 + n2 - 2) is the degrees of freedom (df = ", bb_testT$parameter, ")\n"))
cat(paste0("p-value is the significance level of the t-test (p-value = ", 
           round(bb_testT$p.value, 6), ")\n"))
cat(paste0("conf.int is the confidence interval of the mean at 95% (conf.int = [",
           paste0(round(bb_testT$conf.int, 3), collapse = ", "), "])\n"))
cat(paste0("sample estimates is the mean value of the samples. Mean: ", 
           paste0(round(bb_testT$estimate, 2), collapse = ", "), "\n"))
#
# #Compare p-value with alpha = 0.05
alpha <- 0.05
if(bb_testT$p.value >= alpha) {
  cat(paste0("p-value (", round(bb_testT$p.value, 6), ") is greater than alpha (", alpha, 
      "). We failed to reject H0. We cannot conclude that the populations are different.\n")) 
} else {
    cat(paste0("p-value (", round(bb_testT$p.value, 6), ") is less than alpha (", alpha, 
      ")\n. We can reject the H0 with 95% confidence. The populations are different.\n"))
}
```


### Columns Summary {.unlisted .unnumbered}

```{r 'C10-ColumnSummary'}
bb <- xxSoftware
str(bb)
#
# #Applying Multiple Functions with Summarise but Output as Cross-Table
# #(Original) Columns as Rows, Functions as Columns
# #NOTE: n() can be applied as lambda function 
bb %>% 
  pivot_longer(everything(), names_to = "key", values_to = "value") %>% 
  group_by(key) %>% 
  summarise(across(value, list(N = ~n(), Count = length, Mean = mean, SD = sd), .names = "{.fn}"))
```

### pivot_longer() {.unlisted .unnumbered}

```{r 'C10-PivotLonger'}
str(bb)
#
# #gather() is deprecated. Here is for reference.
# #Longer Tibble is filled with All Values of Col A, then All Values fo Col B and so on
ii <- gather(bb)
jj <- bb %>% gather("key", "value")
kk <- bb %>% gather("key", "value", everything()) 
#
# #pivot_longer()
# #Longer Tibble is filled with First Row of All Columns, then 2nd Row of All Columns and so on
ll <- bb %>% pivot_longer(everything(), names_to = "key", values_to = "value") %>% arrange(desc(key))
stopifnot(all(identical(ii, jj), identical(ii, kk), identical(ii, ll)))
```

### Multiple Functions {.unlisted .unnumbered}

```{r 'C10-SummariseMultipleFunctions'}
str(bb)
# #Store a Grouped Tibble
ii <- bb %>% 
  pivot_longer(everything(), names_to = "key", values_to = "value") %>% 
  group_by(key) 
str(ii)
ii %>% summarise(across(value, list(N = ~n(), Count = length, Mean = mean, SD = sd), 
                        .names = "{.fn}"))
# 
# #Equivalent (except Column Headers)
ii %>% summarise(N = n(), Count = across(value, length), 
                 Mean = across(value, mean), SD = across(value, sd))
```


## Exercises {.tabset .tabset-fade}

"ForLater"

### 09 {.unlisted .unnumbered}

- (1:) ${n}_1 = 20, {\overline{x}}_1 = 22.5, {\sigma}_1 = 2.5$
- (2:) ${n}_2 = 30, {\overline{x}}_2 = 20.1, {\sigma}_2 = 4.8$
- \@ref(eq:se-2s-sd) ${\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = \sqrt{\frac{{2.5}^2}{20} + \frac{{4.8}^2}{30}} = 1.0395$
  - `sqrt(2.5^2/20 + 4.8^2/30)` \textcolor{pink}{$\#\mathcal{R}$}
- point estimate of the difference between the two population means
  - \@ref(eq:point-estm-2s-sd) $E_{( {\overline{x}}_1 - {\overline{x}}_2 )} = 22.5 - 20.1 = 2.4$
- \@ref(eq:dof-2s) $\text{DOF} = 45$
  - `floor({2.5^2 / 20 + 4.8^2 / 30 }^2 / {{2.5^2 / 20}^2/{20-1} + {4.8^2 / 30}^2/{30-1}})` \textcolor{pink}{$\#\mathcal{R}$}
- ${\gamma = 0.95} \iff{\alpha} = 0.05 \to {{}^2\!t_{{\alpha}/2}} = {{}^2\!t_{0.025}} = 2.014$
  - `qt(p = 0.025, df = 45, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- \@ref(eq:moe-2s-nsd) $\text{MOE}_{\gamma =0.95} = 2.014 * 1.0395 = 2.094 \approx 2.1$
- \@ref(eq:interval-estm-2s-nsd) $\text{Interval Estimate}_{\gamma} = 2.4 \pm 2.1$

### 10 {.unlisted .unnumbered}

```{r 'C10D12', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Two")
```

- (1:) ${n}_1 = 35, {\overline{x}}_1 = 13.6, {\sigma}_1 = 5.2$
- (2:) ${n}_2 = 40, {\overline{x}}_2 = 10.1, {\sigma}_2 = 8.5$
- \@ref(eq:point-estm-2s-sd) $E_{( {\overline{x}}_1 - {\overline{x}}_2 )} = 13.6 - 10.1 = 3.5$
- \@ref(eq:se-2s-sd) ${\sigma}_{({\overline{x}}_1 - {\overline{x}}_2)} = \sqrt{\frac{{5.2}^2}{35} + \frac{{8.5}^2}{40}} = 1.6059$
  - `sqrt(5.2^2/35 + 8.5^2/40)` \textcolor{pink}{$\#\mathcal{R}$}
- \@ref(eq:dof-2s) $\text{DOF} = 65$
  - `floor({5.2^2 / 35 + 8.5^2 / 40 }^2 / {{5.2^2 / 35}^2/{35-1} + {8.5^2 / 40}^2/{40-1}})`  \textcolor{pink}{$\#\mathcal{R}$}
- \@ref(eq:t-2s-nsd) $t = \frac{(13.6 - 10.1) - 0}{1.6059} = 2.179$
- ${}^2\!P_{(t = 2.179)} = 2 * {}^U\!P_{(t = 2.179)} = 2 * 0.0165 = 0.033$
  - `pt(q = 2.179, df = 65, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^2\!P_{(t)} < {\alpha} \to {H_0}$ is rejected 

## Matched Samples (Paired) {#mean-paired-c10}

Suppose employees at a manufacturing company can use two different methods to perform a production task. To maximize production output, the company wants to identify the method with the smaller population mean completion time. We can use two alternative designs for the sampling procedure.


```{definition 'Independent-Sample-Design-Example'}
\textcolor{pink}{Independent sample design:} A simple random sample of workers is selected and each worker in the sample uses method 1. A second independent simple random sample of workers is selected and each worker in this sample uses method 2.
```


```{definition 'Matched-Sample-Design-Example'}
\textcolor{pink}{Matched sample design:} One simple random sample of workers is selected. Each worker first uses one method and then uses the other method. The order of the two methods is assigned randomly to the workers, with some workers performing method 1 first and others performing method 2 first. Each worker provides a pair of data values, one value for method 1 and another value for method 2.
```

In the matched sample design the two production methods are tested under similar conditions (i.e., with the same workers); hence this design often leads to a smaller sampling error than the independent sample design. 

The primary reason is that in a matched sample design, variation between workers is eliminated because the same workers are used for both production methods.

The key to the analysis of the matched sample design is to realize that we consider only the column of differences.

The matched sample design is generally preferred to the independent sample design because \textcolor{pink}{the matched-sample procedure often improves the precision of the estimate.}

Let ${\mu}_d$ = the mean of the difference in values for the population

```{definition 'Hypo-Paired-Two'}
\textcolor{pink}{$\text{\{Two Tail Test \} } \thinspace {H_0} : {\mu}_d = 0 \iff {H_a}: {\mu}_d \neq 0$}
```

Sample Mean is given by \@ref(eq:mean-d) like \@ref(eq:mean) and Sample Standard Deviation is given by \@ref(eq:sd-d) like \@ref(eq:sd) 

\begin{equation} 
  \overline{d} = \frac{\sum{{d}_i}}{n}
  (\#eq:mean-d)
\end{equation} 

\begin{equation} 
  {s}_d = \sqrt{\frac{\sum ({d}_i - \overline{d})^2}{n-1}}
  (\#eq:sd-d)
\end{equation} 

\textcolor{pink}{Test Statistic with $(n-1)$ degrees of freedom :} Refer \@ref(eq:t-pair) like \@ref(eq:t-nsd) 


\begin{equation}
  t = \frac{\overline{d} - {\mu}_d}{{s}_d/\sqrt{n}}
  (\#eq:t-pair)
\end{equation}

\textcolor{pink}{Margin of Error ($\text{MOE}_{{\gamma}}$) :} Refer \@ref(eq:moe-pair) like \@ref(eq:moe-2s-sd) 


\begin{equation}
  \text{MOE}_{{\gamma}} = {t}_{\frac{{\alpha}}{2}}\frac{{s}_d}{\sqrt{n}}
  (\#eq:moe-pair)
\end{equation}

\textcolor{pink}{$\text{Interval Estimate}_{\gamma}$ :} Refer \@ref(eq:interval-estm-pair) like \@ref(eq:interval-estm-2s-sd) 

\begin{equation}
  \text{Interval Estimate}_{\gamma} = \overline{d} \pm {t}_{\frac{{\alpha}}{2}}\frac{{s}_d}{\sqrt{n}}
  (\#eq:interval-estm-pair)
\end{equation}

"ForLater" - Exercise

### Example {.unlisted .unnumbered}

Example: Two Production Methods

- (1: Method 1) $\{6, 5, 7, 6.2, 6, 6.4\}$
- (2: Method 2) $\{5.4, 5.2, 6.5, 5.9, 6, 5.8\}$
- Difference: ${d}_i =\{0.6, -0.2, 0.5, 0.3, 0, 0.6\}$
- ${n} = 6, {\overline{d}} = 0.3, {{s}_d} = 0.335, \text{DOF} = 5, \text{SE} = {{s}_d}/\sqrt{n} = 0.1366$
- \@ref(eq:t-pair) $t = \frac{\overline{d} - {\mu}_d}{{s}_d/\sqrt{n}} = \frac{0.30 - 0}{0.335/\sqrt{6}} = 2.196$
  - `{mean(bb$d) - 0 } / {sd(bb$d) / sqrt(length(bb$d))}` \textcolor{pink}{$\#\mathcal{R}$}
- Calculate ${}^2\!P_{(t)}$
  - ${}^2\!P_{(t = 2.196)} = 2 * {}^U\!P_{(t = 2.196)} = 2 * 0.0397 = 0.07949$
    - `2 * pt(q = 2.196, df = 5, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^2\!P_{(z)} > {\alpha} \to {H_0}$ cannot be rejected
- 95% Confidence Interval can also be estimated
  - ${\gamma = 0.95} \iff{\alpha} = 0.05 \to {{}^2\!t_{{\alpha}/2}} = {{}^2\!t_{0.025}} = 2.57$
  - `qt(p = 0.025, df = 5, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- \@ref(eq:moe-pair) $\text{MOE}_{\gamma =0.95} = 2.57 * 0.1366 = 0.35$
- \@ref(eq:interval-estm-pair) $\text{Interval Estimate}_{\gamma} = 0.3 \pm 0.35$

### Code {.unlisted .unnumbered}

```{r 'C10-Paired-TwoT'}
# #Matched Samples: Same workers providing data for two methods
xxMatchedMethods <- tibble(M1 = c(6, 5, 7, 6.2, 6, 6.4), 
                           M2 = c(5.4, 5.2, 6.5, 5.9, 6, 5.8))
aa <- xxMatchedMethods
# #Get Differnce
bb <- aa %>% mutate(d = M1-M2)
str(bb)
paste0(round(bb[3], 1))
#
cat(paste0("- ${n} = ", length(bb$d), ", {\\overline{d}} = ", round(mean(bb$d), 1), 
           ", {{s}_d} = ", round(sd(bb$d), 3), "$\n"))
cat(paste0("t = ", round({mean(bb$d) - 0 } / {sd(bb$d) / sqrt(length(bb$d))}, 3), "\n"))
#
# #Paired t-test
bb <- aa %>% pivot_longer(everything(), names_to = "key", values_to = "value")
#
# #Welch Two Sample t-test
# #Alternative must be: "two.sided" (Default), "less", "greater"
bb_ha <- "two.sided"
bb_testT <- t.test(formula = value ~ key, data = bb, alternative = bb_ha, paired = TRUE)
bb_testT
```

## Population Proportions

To make an inference about the difference between the two population proportions $({p}_1 - {p}_2)$, we select a simple random sample of ${n}_1$ units from population 1 and a second simple random sample of ${n}_2$ units from population 2. Let $\overline{{p}}_1, \overline{{p}}_2$ denote the sample proportions for simple random sample from populations 1 and 2.

\textcolor{pink}{$({x})$ denotes Count of Success}

\textcolor{pink}{Interval Estimation of $({p}_1 - {p}_2)$}

The point estimator of the difference between two population proportions is the difference between the sample proportions of two independent simple random samples.

\textcolor{pink}{Point Estimate :} Refer \@ref(eq:point-estm-p) like \@ref(eq:point-estm-2s-sd)

\begin{equation}
  E_{( {\overline{p}}_1 - {\overline{p}}_2 )} = {\overline{p}}_1 - {\overline{p}}_2
  (\#eq:point-estm-p)
\end{equation}

As with other point estimators, the point estimator $E_{( {\overline{p}}_1 - {\overline{p}}_2 )}$ has a standard error \textcolor{pink}{${\sigma}_{({\overline{p}}_1 - {\overline{p}}_2)}$}, that describes the variation in the sampling distribution of the estimator. 

The two population proportions, $({p}_1, {p}_2)$, are unknown. Thus, sample proportions $({\overline{p}}_1, {\overline{p}}_2)$ are being used to estimate them. 

\textcolor{pink}{Standard Error of $({\overline{p}}_1 - {\overline{p}}_2)$ :} Refer equation \@ref(eq:se-p) like \@ref(eq:se-2s-sd)

\begin{equation}
  \begin{align}
  {\sigma}_{({\overline{p}}_1 - {\overline{p}}_2)} &= \sqrt{\frac{{p}_1 (1-{p}_1)}{{n}_1} + \frac{{p}_2 (1-{p}_2)}{{n}_2}} \\
&= \sqrt{\frac{{\overline{p}}_1 (1-{\overline{p}}_1)}{{n}_1} + \frac{{\overline{p}}_2 (1-{\overline{p}}_2)}{{n}_2}} 
  \end{align}
  (\#eq:se-p)
\end{equation}

\textcolor{pink}{Margin of Error ($\text{MOE}_{{\gamma}}$) :} Refer equation \@ref(eq:moe-p) like \@ref(eq:moe-2s-sd)

\begin{equation}
  \text{MOE}_{{\gamma}} = {z}_{\frac{{\alpha}}{2}}{\sigma}_{({\overline{p}}_1 - {\overline{p}}_2)} = {z}_{\frac{{\alpha}}{2}}\sqrt{\frac{{\overline{p}}_1 (1-{\overline{p}}_1)}{{n}_1} + \frac{{\overline{p}}_2 (1-{\overline{p}}_2)}{{n}_2}}
  (\#eq:moe-p)
\end{equation}

\textcolor{pink}{$\text{Interval Estimate}_{\gamma}$ :} Refer equation \@ref(eq:interval-estm-p) like \@ref(eq:interval-estm-2s-sd)

\begin{equation}
  \text{Interval Estimate}_{\gamma} = ({\overline{p}}_1 - {\overline{p}}_2) \pm {z}_{\frac{{\alpha}}{2}}\sqrt{\frac{{\overline{p}}_1 (1-{\overline{p}}_1)}{{n}_1} + \frac{{\overline{p}}_2 (1-{\overline{p}}_2)}{{n}_2}}
  (\#eq:interval-estm-p)
\end{equation}

Example: Tax Preparation: (Count of Success $({x})$ is Number of Returns with Errors)

- (1: Office 1) $\{{n}_1 = 250, {x}_1 = 35\} \to {\overline{p}}_1 = {n}_1/{x}_1 = 0.14$
- (2: Office 2) $\{{n}_2 = 300, {x}_2 = 27\} \to {\overline{p}}_2 = {n}_2/{x}_2 = 0.09$
- point estimate
  - \@ref(eq:point-estm-p) $E_{( {\overline{p}}_1 - {\overline{p}}_2 )} = 0.14 - 0.09 = 0.05$
    - Thus, we estimate that Office 1 has a .05, or 5%, greater error rate than Office 2.
- For ${\gamma = 0.90} \iff{\alpha} = 0.10 \to {z_{{\alpha}/2}} = {z_{0.05}} = 1.645$
  - `qnorm(p = 0.05, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- \@ref(eq:se-p) ${\sigma}_{({\overline{p}}_1 - {\overline{p}}_2)} = 0.0275$
  - `sqrt(0.14 * (1 - 0.14) / 250 + 0.09 * (1 - 0.09) / 300)` \textcolor{pink}{$\#\mathcal{R}$}
- \@ref(eq:moe-p) $\text{MOE}_{\gamma} = {z}_{\frac{{\alpha}}{2}} {\sigma}_{({\overline{p}}_1 - {\overline{p}}_2)} = 1.645 * 0.0275 = 0.045$
- \@ref(eq:interval-estm-p) $\text{Interval Estimate}_{\gamma} = ({\overline{p}}_1 - {\overline{p}}_2) \pm \text{MOE}_{\gamma} = 0.05 \pm 0.045$

### Hypothesis

```{definition 'H-2s-p-Lower'}
\textcolor{pink}{$\text{\{Left or Lower \} }\space\thinspace {H_0} : {p}_1 - {p}_2 \geq 0 \iff {H_a}: {p}_1 - {p}_2 < 0$}
```

```{definition 'H-2s-p-Upper'}
\textcolor{pink}{$\text{\{Right or Upper\} } {H_0} : {p}_1 - {p}_2 \leq 0 \iff {H_a}: {p}_1 - {p}_2 > 0$}
```

```{definition 'H-2s-p-Two'}
\textcolor{pink}{$\text{\{Two Tail Test \} } \thinspace {H_0} : {p}_1 - {p}_2 = 0 \iff {H_a}: {p}_1 - {p}_2 \neq 0$}
```

When we assume ${H_0}$ is true as an equality, we have ${p}_1 - {p}_2 = 0$, which is the same as saying that the population proportions are equal, ${p}_1 = {p}_2 = {p}$. The equation \@ref(eq:se-p) becomes \@ref(eq:se-p-same) 

\textcolor{pink}{Standard Error of $({\overline{p}}_1 - {\overline{p}}_2)$ :} When ${p}_1 = {p}_2 = {p}$

\begin{equation}
  \begin{align}
  {\sigma}_{({\overline{p}}_1 - {\overline{p}}_2)} &= \sqrt{{p} (1-{p})\left(\frac{1}{{n}_1} + \frac{1}{{n}_2}\right)} \\
&= \sqrt{{\overline{p}} (1-{\overline{p}})\left(\frac{1}{{n}_1} + \frac{1}{{n}_2}\right)} 
  \end{align}
  (\#eq:se-p-same)
\end{equation}


With ${p}$ unknown, we pool, or combine, the point estimators from the two samples $({p}_1, {p}_2)$ to obtain a single point estimator of ${p}$ is given by \@ref(eq:pooled-estm-p)

\textcolor{pink}{Pooled Estimator of ${p}$ :} When ${p}_1 = {p}_2 = {p}$

\begin{equation}
  {\overline{p}} = \frac{{n}_1 {\overline{p}}_1 + {n}_2 {\overline{p}}_2}{{n}_1 + {n}_2}
  (\#eq:pooled-estm-p)
\end{equation}

\textcolor{pink}{Test Statistic for Hypothesis Tests :}

\begin{equation}
  z = \frac{({\overline{p}}_1 - {\overline{p}}_2)}{{\sigma}_{({\overline{p}}_1 - {\overline{p}}_2)}} = \frac{({\overline{p}}_1 - {\overline{p}}_2)}{\sqrt{{\overline{p}} (1-{\overline{p}})\left(\frac{1}{{n}_1} + \frac{1}{{n}_2}\right)}}
  (\#eq:z-p)
\end{equation}

Example: Tax Preparation: Continuation

```{r 'C10D13', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-p-Two") #dddd
```

- \@ref(eq:pooled-estm-p) ${\overline{p}} = \frac{{n}_1 {\overline{p}}_1+ {n}_2 {\overline{p}}_2}{{n}_1 + {n}_2} = \frac{250 * 0.14 + 300 * 0.09}{250 + 300} = 0.1127$
  - `{250 * 0.14 + 300 * 0.09} / {250 + 300}` \textcolor{pink}{$\#\mathcal{R}$}
- \@ref(eq:z-p) $z = \frac{({\overline{p}}_1 - {\overline{p}}_2)}{{\sigma}_{({\overline{p}}_1 - {\overline{p}}_2)}} = \frac{({\overline{p}}_1 - {\overline{p}}_2)}{\sqrt{{\overline{p}} (1-{\overline{p}})\left(\frac{1}{{n}_1} + \frac{1}{{n}_2}\right)}}$
  - $z = \frac{(0.14 - 0.09)}{\sqrt{0.1127 (1-0.1127)\left(\frac{1}{250} + \frac{1}{300}\right)}}  = \frac{(0.14 - 0.09)}{0.0271} = 1.845$
  - NOTE: The denominator calculated in this manner $(0.0271)$ is very close to the originally calculated ${\sigma}_{({\overline{p}}_1 - {\overline{p}}_2)} = 0.0275$
- Calculate ${}^2\!P_{(z)}$
  - ${}^2\!P_{(z = 1.845)} = 2 * {}^U\!P_{(z = 1.845)} = 2 * 0.0325 = 0.065$
    - `2 * pnorm(q = 1.845, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.01$
  - ${}^2\!P_{(z)} < {\alpha} \to {H_0}$ is rejected i.e. the proportions are different
  - The firm can conclude that the error rates differ between the two offices.

### Code {.unlisted .unnumbered}

[(External) Two-Proportions Z-Test](http://www.sthda.com/english/wiki/two-proportions-z-test-in-r "http://www.sthda.com")

```{r 'C10-TwoProp'}
bb_ha <- "two.sided"
bb_gamma <- 0.90
bb_propT <- prop.test(x = c(35, 27), n = c(250, 300), alternative = bb_ha, 
                      conf.level = bb_gamma, correct = FALSE)
bb_propT
names(bb_propT)
#
# #X-squared might be the square of calculated z-value
cat(paste0("X-squared is the number of Successes (X-squared = ", round(bb_propT$statistic, 6), ")\n"))
#
# #By default, the function prop.test() uses the Yates continuity correction
# #It is important if either the expected successes or failures is < 5. 
# #If you do not want the correction, use the additional argument correct = FALSE. 
# #i.e. To make the test mathematically equivalent to the uncorrected z-test of a proportion.
cat(paste0("p-value is the significance level of the t-test (p-value = ", 
           round(bb_propT$p.value, 6), ")\n"))
cat(paste0("conf.int is the confidence interval for the probability of success at ", bb_gamma,
           " level (conf.int = [", paste0(round(bb_propT$conf.int, 3), collapse = ", "), "])\n"))
cat(paste0("sample estimates is the the estimated probability of success. p: ", 
           paste0(round(bb_propT$estimate, 2), collapse = ", "), "\n"))
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C10-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, alpha, bb_testF, bb_testT, isNormal_A, isNormal_B, setA, setB,
          xxSoftware, bb_ha, xxMatchedMethods, bb_gamma, bb_propT)
```

```{r 'C10-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/310-TwoPopulations.Rmd-->

# Variance {#c11}

```{r 'C11', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Inferences About Population Variances"
  - "ForLater" - Hypothesis Testing, Inferences About Two Population Variances 


## Inferences About a Population Variance 

In many manufacturing applications, controlling the process variance is extremely important in maintaining quality.

The sample variance ${s^2}$, given by equation \@ref(eq:var-1s), is the point estimator of the population variance ${\sigma}^2$.

\begin{equation}
  {s^2} = \frac{\sum {({x}_i - {\overline{x}})}^2}{n-1}
  (\#eq:var-1s)
\end{equation}

```{definition 'Distribution-Chi-Square'}
Whenever a simple random sample of size ${n}$ is selected from a normal population, the sampling distribution of $\frac{(n-1)s^2}{{\sigma}^2}$ is a \textcolor{pink}{chi-square distribution} with ${n − 1}$ degrees of freedom.
```

Note:

- The chi-square distribution is based on sampling from a normal population.
- It can be used to develop interval estimates and conduct hypothesis tests about a population variance.
- The notation \textcolor{pink}{${\chi_{\alpha}^2}$} denotes the value for the chi-square distribution that provides an area or probability of ${\alpha}$ to the \textcolor{pink}{right} of the ${\chi_{\alpha}^2}$ value. 

### Interval Estimation

Example: A sample of 20 containers ${n = 20}$ has the sample variance ${s^2} = 0.0025$ 

- ${\chi_{\alpha = 0.025}^2} = 32.852$
  - `qchisq(p = 0.025, df = 19, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
  - It indicates that 2.5% of the chi-square values are to the right of 32.852
  - Also, ${\chi_{\alpha = 0.975}^2} = 8.907$ indicates that 97.5% of the chi-square values are to the right of 8.907.
    - `qchisq(p = 0.975, df = 19, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
  - Thus, 95% of the chi-square values are between ${\chi_{\alpha = 0.975}^2}$ and ${\chi_{\alpha = 0.025}^2}$ 
  - There is a .95 probability of obtaining a ${\chi^2}$ value such that ${\chi_{0.975}^2} \leq {\chi^2} \leq{\chi_{0.025}^2}$

```{r 'C11-ChiSq'}
# #pnorm() qnorm() | pt() qt() | pchisq() qchisq() | pf() qf() 
#
# #p-value approach: Find Commulative Probability P corresponding to the given ChiSq & DOF=59
pchisq(q = 32.852, df = 19, lower.tail = FALSE)
#
# #ChiSq value for which Area under the curve towards Right is alpha=0.025 & DOF=19 #32.852
qchisq(p = 0.025, df = 19, lower.tail = FALSE)
```

- Using equation \@ref(eq:var-1s), we can get \@ref(eq:chi-95), which provides a 95% confidence interval estimate for the population variance ${\sigma}^2$.

\begin{equation}
  \frac{(n-1)s^2}{{\chi_{0.025}^2}} \leq {\sigma}^2 \leq \frac{(n-1)s^2}{{\chi_{0.975}^2}}
  (\#eq:chi-95)
\end{equation}

- In the example, $(n-1)s^2 = 19 * 0.0025 = 0.0475$
- \@ref(eq:chi-95) $\frac{0.0475}{32.852} \leq {\sigma}^2 \leq \frac{0.0475}{8.907} \to 0.0014 \leq {\sigma}^2 \leq 0.0053 \to 0.0380 \leq {\sigma} \leq 0.0730$
  - which gives the 95% confidence interval for the population standard deviation

Generalising the equation \@ref(eq:chi-95), the equation \@ref(eq:interval-estm-1s-var) is the interval estimate of a population variance.

\begin{equation}
  \frac{(n-1)s^2}{{\chi_{{\alpha}/2}^2}} \leq {\sigma}^2 \leq \frac{(n-1)s^2}{{\chi_{{1-\alpha}/2}^2}}
  (\#eq:interval-estm-1s-var)
\end{equation}

where the ${\chi^2}$ values are based on a chi-square distribution with ${n-1}$ degrees of freedom and where $(1 − {\alpha})$ is the confidence coefficient.

### Hypothesis Tests {.tabset .tabset-fade}


Using ${{\sigma}_0^2}$ to denote the hypothesized value for the population variance, the three forms for a hypothesis test are as follows: 

```{definition 'H-1s-Var-Lower'}
\textcolor{pink}{$\text{\{Left or Lower \} }\space\thinspace {H_0} : {\sigma}^2 \geq {{\sigma}_0^2} \iff {H_a}: {\sigma}^2 < {{\sigma}_0^2}$}
```

```{definition 'H-1s-Var-Upper'}
\textcolor{pink}{$\text{\{Right or Upper\} } {H_0} : {\sigma}^2 \leq {{\sigma}_0^2} \iff {H_a}: {\sigma}^2 > {{\sigma}_0^2}$}
```

```{definition 'H-1s-Var-Two'}
\textcolor{pink}{$\text{\{Two Tail Test \} } \thinspace {H_0} : {\sigma}^2 = {{\sigma}_0^2} \iff {H_a}: {\sigma}^2 \neq {{\sigma}_0^2}$}
```

Note: In general, Upper Tail test is the most frequently observed test because low variances are generally desirable. With a statement about the maximum allowable population variance, we can test the null hypothesis that the population variance is less than or equal to the maximum allowable value against the alternative hypothesis that the population variance is greater than the maximum allowable value. With this test structure, corrective action will be taken whenever rejection of the null hypothesis indicates the presence of an excessive population variance. 

\textcolor{pink}{Test Statistic for Hypothesis Tests About a Population Variance: } Refer \@ref(eq:chi-1s), where ${\chi^2}$ has a chi-square distribution with ${n - 1}$ degrees of freedom.

\begin{equation}
  {\chi^2} = \frac{(n - 1){s}^2}{{\sigma}_0^2}
  (\#eq:chi-1s)
\end{equation}

Example: Louis: the company standard specifies an arrival time variance of 4 or less

```{r 'C11D01', comment="", echo=FALSE, results='asis'}
f_getDef("H-1s-Var-Upper")
```

- (Sample) ${n} = 24, {s}^2 = 4.9$
- \@ref(eq:chi-1s) ${\chi^2} = \frac{(n - 1){s}^2}{{\sigma}_0^2} = \frac{(24 - 1) * 4.9}{4} = 28.18$
- ${}^U\!P_{(\chi^2)} = 0.209$
  - `pchisq(q = 28.18, df = 23, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^U\!P_{(\chi^2)} > {\alpha} \to {H_0}$ cannot be rejected
  - The sample results do not provide sufficient evidence to conclude that the variance is high.

Example: bureau of motor vehicles:  Evaluate the variance in the new examination test scores with the historical value ${\sigma}_0^2 = 100$

```{r 'C11D02', comment="", echo=FALSE, results='asis'}
f_getDef("H-1s-Var-Two")
```

- (Sample) ${n} = 30, {s}^2 = 162$
- \@ref(eq:chi-1s) ${\chi^2} = \frac{(n - 1){s}^2}{{\sigma}_0^2} = \frac{(30 - 1) * 162}{100} = 46.98$
- ${}^2\!P_{(\chi^2)} = 2 * {}^U\!P_{(\chi^2)} = 2 * 0.0187 = 0.0374$
  - `2 * pchisq(q = 46.98, df = 29, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^2\!P_{(\chi^2)} < {\alpha} \to {H_0}$ is rejected i.e. the change is significant
  - The new examination test scores have a population variance different from the historical variance

## Inferences About Two Population Variances

The two sample variances ${s}_1^2$ and ${s}_2^2$ will be the basis for making inferences about the two population variances ${\sigma}_1^2$ and ${\sigma}_2^2$. 

```{definition 'Distribution-F'}
Whenever independent simple random samples of sizes ${n}_1$ and ${n}_2$ are selected from two normal populations with equal variances $({\sigma}_1^2 = {\sigma}_2^2)$, the sampling distribution of $\frac{{s}_1^2}{{s}_2^2}$ is an \textcolor{pink}{F distribution} with $({n}_1 - 1)$ degrees of freedom for the numerator and $({n}_2 - 1)$ degrees of freedom for the denominator. 
```

Note:

- The \textcolor{pink}{F distribution} is based on sampling from two normal populations. 
- The F distribution is not symmetric, and the F values can never be negative.
  - The shape of any particular F distribution depends on its numerator and denominator degrees of freedom.
- We refer to the population providing the larger sample variance as population 1.
  - Because the F test statistic is constructed with the larger sample variance ${s}_1^2$ in the numerator, the value of the test statistic will be in the upper tail of the F distribution. 

\textcolor{pink}{Test Statistic for Hypothesis Tests About Population Variances with $({\sigma}_1^2 = {\sigma}_2^2)$ :} Refer equation \@ref(eq:f-2s)


\begin{equation}
  F = \frac{{s}_1^2}{{s}_2^2}
  (\#eq:f-2s)
\end{equation}

\textcolor{pink}{Hypothesis Tests :}

```{definition 'H-2s-Var-Lower'}
\textcolor{pink}{$\text{\{Left or Lower \} }\space\thinspace \text{Do not do this.}$}
```

```{definition 'H-2s-Var-Upper'}
\textcolor{pink}{$\text{\{Right or Upper\} } {H_0} : {\sigma}_1^2 \leq {\sigma}_2^2 \iff {H_a}: {\sigma}_1^2 > {\sigma}_2^2$}
```

```{definition 'H-2s-Var-Two'}
\textcolor{pink}{$\text{\{Two Tail Test \} } \thinspace {H_0} : {\sigma}_1^2 = {\sigma}_2^2 \iff {H_a}: {\sigma}_1^2 \neq {\sigma}_2^2$}
```


Example: Dullus County Schools: 

```{r 'C11D03', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Var-Two")
```

- (1: Milbank) ${n} = 26, {s}_1^2 = 48$
- (2: Gulf ) ${n} = 16, {s}_1^2 = 20$
- \@ref(eq:f-2s) $F = \frac{{s}_1^2}{{s}_2^2} = \frac{48}{20} = 2.4$
- ${}^2\!P_{(F)} =  2 * {}^U\!P_{(F)} = 2 * 0.0406 = 0.0812$
  - `2 * pf(q = 2.4, df1 = 25, df2 = 15, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.10$
  - ${}^2\!P_{(F)} < {\alpha} \to {H_0}$ is rejected i.e. The two populations have difference variances
  - The sample results provide sufficient evidence to conclude that the variances are different.

Example:  public opinion survey: do women show a greater variation in attitude on political issues than men

```{r 'C11D04', comment="", echo=FALSE, results='asis'}
f_getDef("H-2s-Var-Upper") #dddd
```

- (1: Women) ${n} = 41, {s}_1^2 = 120$
- (2: Men) ${n} = 31, {s}_1^2 = 80$
- \@ref(eq:f-2s) $F = \frac{{s}_1^2}{{s}_2^2} = \frac{120}{80} = 1.5$
- ${}^U\!P_{(F)} = 0.1256$
  - `pf(q = 1.5, df1 = 40, df2 = 30, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^U\!P_{(F)} > {\alpha} \to {H_0}$ cannot be rejected i.e. The two populations have same variances
  - The sample results does not provide sufficient evidence to conclude that women have higher variance in political opinion compared to men

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C11-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C11-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/311-Variance.Rmd-->

# Independence {#c12}

```{r 'C12', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Comparing Multiple Proportions, Test of Independence and Goodness of Fit"
  - "ForLater" - Everything

## Introduction

Hypothesis-testing procedures that expand our capacity for making statistical inferences about populations

  - The test statistic used in conducting the hypothesis tests in this chapter is based on the chi-square ${\chi^2}$ distribution.
  - In all cases, the data are categorical. 
  - Applications
    - Testing the equality of population proportions for three or more populations 
    - Testing the independence of two categorical variables 
    - Testing whether a probability distribution for a population follows a specific historical or theoretical probability distribution

## Testing the Equality of Population Proportions for Three or More Populations

```{definition 'H-3p'}
\textcolor{pink}{$\text{\{Equality of Population Proportions \}} {H_0} : {p}_1 = {p}_2 = \dots = {p}_k \iff {H_a}: \text{Not all population proportions are equal}$}
```

where ${p}_j$ is population proportion of the $j^{\text{th}}$ population. We assume that a simple random sample of size ${n}_j$ has been selected from each of the ${k}$ populations or treatments. 

Select a random sample from each of the populations and record the observed frequencies, $f_{ij}$, in a table with 2 rows and k columns. 

\textcolor{pink}{Expected Frequencies Under the Assumption ${H_0}$ is true :} Refer equation \@ref(eq:e-freq)

\begin{equation}
  e_{ij} = \frac{(\text{Row } i \text{ Total})(\text{Column } j \text{ Total})}{\text{Total Sample Size}}
  (\#eq:e-freq)
\end{equation}


\textcolor{pink}{Chi-Square Test Statistic :} Refer equation \@ref(eq:chi-freq)


\begin{equation}
  {\chi^2} = \sum_{i}{\sum_{j}{\frac{(f_{ij} - e_{ij})^2}{e_{ij}}}}
  (\#eq:chi-freq)
\end{equation}

Where: 

\begin{equation}
  \begin{align}
  f_{ij} &= \text{observed frequency for the cell in row } i \text{ and column } j \\
  e_{ij} &= \text{expected frequency for the cell in row } i \text{ and column } j
  \end{align}
\end{equation}

Note: In a chi-square test involving the equality of ${k}$ population proportions, the above test statistic has a chi-square distribution with ${k - 1}$ degrees of freedom provided the expected frequency is 5 or more for each cell.

A chi-square test for equal population proportions will always be an upper tail test with rejection of ${H_0}$ occurring when the test statistic is in the upper tail of the chi-squre distribution.
 
In studies such as these, we often use the same sample size for each population. We have chosen different sample sizes in this example to show that the chi-square test is not restricted to equal sample sizes for each of the k populations.

"ForLater" - Creating the ChiSq Table

Example: JD Power: Compare customer loyalty for three automobiles by using the proportion of owners likely to repurchase a particular automobile

- Count of Success $({x})$ is Number of Owners likely to repurchase
- (1: Impala) $\{{n}_1 = 125, {x}_1 = 69\}$
- (1: Fusion) $\{{n}_2 = 200, {x}_2 = 120\}$
- (1: Accord) $\{{n}_2 = 175, {x}_3 = 123\}$
- \@ref(eq:chi-freq) ${\chi^2} = 7.89$
- $P_{\chi^2} = 0.0193$
  - `pchisq(q = 7.89, df = 2,lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$}

"ForLater" - Marascuilo procedure

## Test of Independence

An important application of a chi-square test involves using sample data to test for the independence of two categorical variables. The null hypothesis for this test is that the two categorical variables are independent. Thus, the test is referred to as a test of independence. 

Example: Beer: Preference vs. gender

- Since an objective of the study is to determine if there is difference between the beer preferences for male and female beer drinkers, we consider gender an explanatory variable and follow the usual practice of making the \textcolor{pink}{explanatory variable the column variable} in the data tabulation table. 
- The beer preference is the categorical response variable and is shown as the row variable.
- "ForLater" 


## Goodness of Fit Test

- "ForLater" 



## Summary

All tests apply to categorical variables and all tests use a chi-square ${\chi^2}$ test statistic that is based on the differences between observed frequencies and expected frequencies. In each case, expected frequencies are computed under the assumption that the null hypothesis is true. These chi-square tests are upper tailed tests. Large differences between observed and expected frequencies provide a large value for the chi-square test statistic and indicate that the null hypothesis should be rejected. 

The test for the equality of population proportions for three or more populations is based on independent random samples selected from each of the populations. The sample data show the counts for each of two categorical responses for each population. The null hypothesis is that the population proportions are equal. Rejection of the null hypothesis supports the conclusion that the population proportions are not all equal. 

The test of independence between two categorical variables uses one sample from a population with the data showing the counts for each combination of two categorical variables. The null hypothesis is that the two variables are independent and the test is referred to as a test of independence. If the null hypothesis is rejected, there is statistical evidence of an association or dependency between the two variables. 

The goodness of fit test is used to test the hypothesis that a population has a specific historical or theoretical probability distribution. We showed applications for populations with a multinomial probability distribution and with a normal probability distribution. Since the normal probability distribution applies to continuous data, intervals of data values were established to create the categories for the categorical variable required for the goodness of fit test.


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C12-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C12-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/312-Independence.Rmd-->

# ANOVA {#c13}

```{r 'C13', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Experimental Design and Analysis of Variance"
  - "ForLater" - Exercises, Fisher LSD Onwards 

## Introduction

Example: Chemitech: Comparision of theree methods of assembly A, B, C in terms of most assemblies per week

- In this experiment
  - assembly method is the \textcolor{pink}{independent variable or factor}. 
  - Because three assembly methods correspond to this factor, we say that three treatments are associated with this experiment; each treatment corresponds to one of the three assembly methods. 
    - The three assembly methods or \textcolor{pink}{treatments} define the three \textcolor{pink}{populations of interest}
  - This is an example of a single-factor experiment; it involves one categorical factor (method of assembly)
  - For each population the \textcolor{pink}{dependent or response variable} is the number of filtration systems assembled per week, and the primary statistical objective of the experiment is to determine whether the mean number of units produced per week is the same for all three populations (methods).
- Suppose a random sample of three employees is selected from all assembly workers. 
  - The three randomly selected workers are the \textcolor{pink}{experimental units}. 
  - The experimental design that we will use is called a \textcolor{pink}{completely randomized design}.       - This type of design requires that each of the three assembly methods or treatments be assigned randomly to one of the experimental units or workers. 

```{definition 'Randomization'}
\textcolor{pink}{Randomization} is the process of assigning the treatments to the experimental units at random.
```

- Suppose that instead of selecting just three workers at random we selected 15 workers and then randomly assigned each of the three treatments to 5 of the workers. 
  - Because each method of assembly is assigned to 5 workers, we say that five \textcolor{pink}{replicates} have been obtained.
- As given in data, sample means for A, B, C are : $\{{\overline{x}}_1 = 62, {\overline{x}}_1 = 66, {\overline{x}}_1 = 52\}$
  - From these data, method B appears to result in higher production rates than either of the other methods.

```{r 'C13-Chemitech'}
xxChemitech <- tibble(A = c(58, 64, 55, 66, 67), 
                      B = c(58, 69, 71, 64, 68), 
                      C = c(48, 57, 59, 47, 49))
aa <- xxChemitech
# #Summary
aa %>% 
    pivot_longer(everything(), names_to = "key", values_to = "value") %>% 
    group_by(key) %>% 
    summarise(across(value, 
                     list(Count = length, Mean = mean, SD = sd, Variance = var), 
                     .names = "{.fn}"))

```


## Hypothesis

- The real issue is \textcolor{pink}{whether the three sample means observed are different enough} for us to conclude that the means of the populations corresponding to the three methods of assembly are different.
  - Let $\{{\mu}_1, {\mu}_2, {\mu}_3\}$ denote mean number of units produced per week using methods A, B and C
  - we want to use the sample means to test the following hypotheses:
  
```{r 'C13D01', comment="", echo=FALSE, results='asis'}
f_getDef("H-ANOVA") #dddd
```

If ${H_0}$ is rejected, we cannot conclude that all population means are different. Rejecting ${H_0}$ means that at least two population means have different values.

## Assumptions for Analysis of Variance
  
Three assumptions are required to use analysis of variance. 

1. \textcolor{pink}{For each population, the response variable is normally distributed.}
    - Implication: In the example, the number of units produced per week (response variable) must be normally distributed for each assembly method. 
2. \textcolor{pink}{The variance of the response variable, ${\sigma}^2$, is the same for all of the populations.} 
    - Implication: In the example, the variance of the number of units produced per week must be the same for each assembly method. 
3. \textcolor{pink}{The observations must be independent.} 
    - Implication: In the example, the number of units produced per week for each employee must be independent of the number of units produced per week for any other employee.

If the sample sizes are equal, analysis of variance is not sensitive to departures from the assumption of normally distributed populations.

\textcolor{pink}{Normality:}

- In ANOVA, the entire response column is typically nonnormal because the different groups in the data have different means.

## Conceptual Overview

If the means for the three populations are equal, we would expect the three sample means to be close together. The more the sample means differ, the stronger the evidence we have for the conclusion that the population means differ. In other words, if the variability among the sample means is "small", it supports ${H_0}$; if the variability among the sample means is "large", it supports ${H_a}$.

If the null hypothesis is true, we can use the variability among the sample means to develop an estimate of ${\sigma}^2$. 

First, note that if the assumptions for analysis of variance are satisfied and the null hypothesis is true, each sample will have come from the same normal distribution with mean ${\mu}$ and variance ${\sigma}^2$. 

Recall that the sampling distribution of the sample mean ${\overline{x}}$ for a simple random sample of size ${n}$ from a normal population will be normally distributed with mean ${\mu}$ and variance ${\sigma}_{{\overline{x}}}^2 = \frac{{\sigma}^2}{n}$.

In this case, the mean and variance of the three sample mean values $\{{\overline{x}}_1 = 62, {\overline{x}}_1 = 66, {\overline{x}}_1 = 52\}$ can be used to estimate the mean and variance of the sampling distribution. 

When the sample sizes are equal, as in this example, the best estimate of the mean of the sampling distribution of ${\overline{x}}$ is the mean or average of the sample means. 

In this example, an estimate of the mean of the sampling distribution of ${\overline{x}}$ is $(62 + 66 + 52)/3 = 60$. We refer to this estimate as the \textcolor{pink}{overall sample mean}. Refer equation \@ref(eq:mean-all-n-anv)

An estimate of the variance of the sampling distribution of ${\overline{x}}$, ${\sigma}_{{\overline{x}}}^2$, is provided by the variance of the three sample means.

$${s}_{\overline{x}}^2 = \frac{(62 - 60)^2 + (66 - 60)^2 + (52 - 60)^2}{3 - 1} = 52$$
Because ${\sigma}^2 = n {\sigma}_{{\overline{x}}}^2$, the estimate can be given by 

$$E_{{\sigma}^2} = n E_{{\sigma}_{{\overline{x}}}^2} = n {s}_{\overline{x}}^2 = 5 * 52 = 260$$

The $n {s}_{\overline{x}}^2$ is referred as \textcolor{pink}{between-treatments estimate of ${\sigma}^2$.} It is based on the assumption that the null hypothesis is true. In this case, each sample comes from the same population, and there is only one sampling distribution of ${\overline{x}}$.

In contrast, when the population means are not equal, the between-treatments estimate will overestimate the population variance ${\sigma}^2$.

The variation within each of the samples also has an effect on the conclusion we reach in analysis of variance. When a simple random sample is selected from each population, each of the sample variances provides an unbiased estimate of ${\sigma}^2$. Hence, we can combine or pool the individual estimates of ${\sigma}^2$ into one overall estimate. The estimate of ${\sigma}^2$ obtained in this way is called the \textcolor{pink}{pooled or within-treatments estimate of ${\sigma}^2$}. 

Because each sample variance provides an estimate of ${\sigma}^2$ based only on the variation within each sample, the within-treatments estimate of ${\sigma}^2$ is not affected by whether the population means are equal. When the sample sizes are equal, the within-treatments estimate of ${\sigma}^2$ can be obtained by computing the average of the individual sample variances $\{27.5, 26.5, 31\}$. 

For this exmple we obtain:

$$\text{Within-treatments estimate of } {\sigma}^2 = \frac{27.5 + 26.5 + 31}{3} = 28.33$$
Remember, that the between-treatments approach provides a good estimate of ${\sigma}^2$ only if the null hypothesis is true; if the null hypothesis is false, the between-treatments approach overestimates ${\sigma}^2$. The within-treatments approach provides a good estimate of ${\sigma}^2$ in either case. 

Thus, if the null hypothesis is true, the two estimates will be similar and their ratio will be close to 1. 

If the null hypothesis is false, the between-treatments estimate will be larger than the within-treatments estimate, and their ratio will be large. 

## ANOVA

\textcolor{pink}{Analysis of Variance and the Completely Randomized Design}


```{definition 'H-ANOVA'}
\textcolor{pink}{$\text{\{ANOVA\}} {H_0} : {\mu}_1 = {\mu}_2 = \dots = {\mu}_k \iff {H_a}: \text{Not all population means are equal}$}
```

where ${\mu}_j$ is mean of the $j^{\text{th}}$ population. We assume that a simple random sample of size ${n}_j$ has been selected from each of the ${k}$ populations or treatments. For the resulting sample data, let

\begin{equation}
  \begin{align}
  {x}_{ij} &= \text{value of observation } i \text{ for treatment } j \\
{n}_{j} &= \text{number of observations for treatment } j \\
{\overline{x}}_{j} &= \text{sample mean for treatment } j \\
{s}_{j}^2 &= \text{sample variance for treatment } j \\
{s}_{j} &= \text{sample e standard deviation for treatment } j
  \end{align}
\end{equation}

The formulas for the sample mean and sample variance for treatment ${j}$ are given in equations \@ref(eq:mean-anv) and \@ref(eq:var-anv)


\begin{equation}
  {\overline{x}}_j =  \frac{\sum_{i=1}^{n_j}{x}_{ij}}{{n}_j}
  (\#eq:mean-anv)
\end{equation}

\begin{equation}
  {s}_j^2 = \frac{\sum_{i=1}^{n_j}{\left({x}_{ij} - {\overline{x}}_j\right)^2}}{{n}_j - 1}
  (\#eq:var-anv)
\end{equation}

The overall sample mean, denoted ${\overline{\overline{x}}}$, is the sum of all the observations divided by the total number of observations. 


\begin{equation}
  {\bar{\bar{x}}} = \frac{\sum_{j=1}^k{\sum_{i=1}^{{n}_j}{{x}_{ij}}}}{{n}_T}
  (\#eq:mean-all-anv)
\end{equation}

Where

\begin{equation}
  {n}_T = {n}_1 + {n}_2 + \cdots + {n}_k
  (\#eq:n-anv)
\end{equation}

If the size of each sample is {n}, the equation \@ref(eq:n-anv) becomes ${n}_T = kn$, and the equation \@ref(eq:mean-all-anv) reduces to \@ref(eq:mean-all-n-anv)

\begin{equation}
  {\bar{\bar{x}}} = \frac{\sum_{j=1}^k{{\overline{x}}_{j}}}{k}
  (\#eq:mean-all-n-anv)
\end{equation}

Thus, whenever the sample sizes are the same, the overall sample mean is just the average of the ${k}$ sample means.

Thus, in the example, from \@ref(eq:mean-all-n-anv), ${\bar{\bar{x}}} = \frac{62 + 66 + 52}{3} = 60$

## MSTR

\textcolor{pink}{Between-Treatments Estimate of Population Variance}

The between-treatments estimate of ${\sigma}^2$ is called the \textcolor{pink}{mean square due to treatments} and is denoted \textcolor{pink}{$\text{MSTR}$}. Refer equation \@ref(eq:mstr-anv)

\begin{equation}
  \text{MSTR} = \frac{\text{SSTR}}{k - 1} = \frac{\sum_{j=1}^{k}{n}_j\left({\overline{x}}_j - {\bar{\bar{x}}} \right)^2}{k - 1}
  (\#eq:mstr-anv)
\end{equation}

The numerator in equation \@ref(eq:mstr-anv) is called the \textcolor{pink}{sum of squares due to treatments} and is denoted \textcolor{pink}{$\text{SSTR}$}. The denominator, ${k − 1}$, represents the degrees of freedom associated with SSTR. Refer equation \@ref(eq:sstr-anv)

\begin{equation}
  \text{SSTR} = \sum_{j=1}^{k}{n}_j\left({\overline{x}}_j - {\bar{\bar{x}}} \right)^2
  (\#eq:sstr-anv)
\end{equation}

If ${H_0}$ is true, MSTR provides an unbiased estimate of ${\sigma}^2$. However, if the means of the ${k}$ populations are not equal, MSTR is not an unbiased estimate of ${\sigma}^2$; in fact, in that case, MSTR should overestimate ${\sigma}^2$.

In the example:

- \@ref(eq:sstr-anv) $\text{SSTR} = 5(62 - 60)^2 + 5(66 - 60)^2 + 5(52 - 60)^2 = 520$
- \@ref(eq:mstr-anv) $\text{MSTR} = \frac{520}{3 - 1} = 260$

## MSE

\textcolor{pink}{Within-Treatments Estimate of Population Variance}

The within-treatments estimate of ${\sigma}^2$ is called the \textcolor{pink}{mean square due to error} and is denoted \textcolor{pink}{$\text{MSE}$}. Refer equation \@ref(eq:mse-anv)

\begin{equation}
  \text{MSE} = \frac{\text{SSE}}{{n}_T - k} = \frac{\sum_{j=1}^{k}{({n}_j - 1){s}_j^2}}{{n}_T - k}
  (\#eq:mse-anv)
\end{equation}

The numerator in equation \@ref(eq:mse-anv) is called the \textcolor{pink}{sum of squares due to error} and is denoted \textcolor{pink}{$\text{SSE}$}. The denominator, ${{n}_T - k}$ is referred to as the degrees of freedom associated with SSE. Refer equation \@ref(eq:sse-anv)


\begin{equation}
  \text{SSE} = \sum_{j=1}^{k}{({n}_j - 1){s}_j^2}
  (\#eq:sse-anv)
\end{equation}

In the example:

- \@ref(eq:sse-anv) $\text{SSE} = (5 - 1)27.5 + (5 - 1)26.5 + (5 - 1)31 = 340$
- \@ref(eq:mse-anv) $\text{MSE} = \frac{340}{15 - 3} = 28.33$

## F test

\textcolor{pink}{Comparing the Variance Estimates}

If the null hypothesis is true, $\text{MSTR}$ and $\text{MSE}$ provide two independent, unbiased estimates of ${\sigma}^2$. 

Refer [Variance](#c11 "c11").

We know that for normal populations, the sampling distribution of the ratio of two independent estimates of ${\sigma}^2$ follows an F distribution. Hence, if the null hypothesis is true and the ANOVA assumptions are valid, the sampling distribution of $\frac{\text{MSTR}}{\text{MSE}}$ is an F distribution with numerator degrees of freedom equal to ${k - 1}$ and denominator degrees of freedom equal to ${{n}_T - k}$. 

In other words, if the null hypothesis is true, the value of MSTR/MSE should appear to have been selected from this F distribution. However, if the null hypothesis is false, the value of $\frac{\text{MSTR}}{\text{MSE}}$ will be inflated because MSTR overestimates ${\sigma}^2$. Hence, we will reject ${H_0}$ if the resulting value of $\frac{\text{MSTR}}{\text{MSE}}$ appears to be too large to have been selected from an F distribution with ${k - 1}$ numerator degrees of freedom and ${{n}_T - k}$ denominator degrees of freedom. 

Because the decision to reject ${H_0}$ is based on the value of $\frac{\text{MSTR}}{\text{MSE}}$, the test statistic used to test for the equality of ${k}$ population means is given by equation \@ref(eq:f-anv)

\textcolor{pink}{Test Statistic for the Equality of ${k}$ Population Means :}


\begin{equation}
  F = \frac{\text{MSTR}}{\text{MSE}}
  (\#eq:f-anv)
\end{equation}


Because we will only reject the null hypothesis for large values of the test statistic, the p-value is the upper tail area of the F distribution to the right of the test statistic ${F}$.

- \textcolor{pink}{aov()}
  - Terms
    - Df degrees of freedom 
      - for the independent variable (levels - 1)
      - and for the residuals (total observations - 1 - levels)
    - Sum Sq shows the sum of squares 
      - SSTR: total variation between the group means 
      - SSE: and the overall mean
    - Mean Sq shows the mean of the sum of squares 
      - MSTR (Between): sum of squares / degrees of freedom for each parameter
      - MSE (Within): mean square of the residuals
    - F-value is the test statistic from the F test. 
      - Mean square of each independent variable / mean square of the residuals. 
      - The larger the F value, the more likely it is that the variation caused by the independent variable is real and not due to chance.
    - Pr(>F) is the p-value of the F-statistic. 
      - likelihood that the F-value calculated from the test would have occurred if the null hypothesis of no difference among group means were true.
- In the Example
  - Total Variance = Between or MSTR + Within or MSE
  - First Line (Column): $\text{DOF}_{(k-1)} = 2, \text{SSTR} = 520, \text{MSTR} = 260$
  - Residuals (Within) : $\text{DOF}_{(n-k)} = 12, \text{SSE} = 340, \text{MSE} = 28.33$
  - \@ref(eq:f-anv) $F = \frac{\text{MSTR}}{\text{MSE}} = 9.18$
- Calculate ${}^U\!P_{(F)}$
  - ${}^U\!P_{F = 9.18} = 0.0038$
    - `pf(q = 9.18, df1 = 2, df2 = 12, lower.tail = FALSE)` \textcolor{pink}{$\#\mathcal{R}$} 
- Compare with ${\alpha} = 0.05$
  - ${}^U\!P_{(F)} < {\alpha} \to {H_0}$ is rejected i.e. the means are different
  - The test provides sufficient evidence to conclude that the means of the three populations are not equal.
  - Analysis of variance supports the conclusion that the population mean number of units produced per week for the three assembly methods are not equal

## ANOVA Table

The sum of squares associated with the source of variation referred to as "Total" is called the \textcolor{pink}{total sum of squares (SST)}. SST divided by its degrees of freedom ${n}_T - 1$ is nothing more than the overall sample variance that would be obtained if we treated the entire set of 15 observations as one data set. With the entire data set as one sample. Refer equation \@ref(eq:sst-anv)


\begin{equation}
  \text{SST} = \text{SSTR} + \text{SSE} = \sum_{j=1}^k{\sum_{i=1}^{{n}_j}{\left( {x}_{ij} - \bar{\bar{x}}\right)^2}}
  (\#eq:sst-anv)
\end{equation}

The degrees of freedom associated with this total sum of squares is the sum of the degrees of freedom associated with the sum of squares due to treatments and the sum of squares due to error i.e. ${n}_T - 1 = (k - 1) + ({n}_T - k)$.

In other words, SST can be partitioned into two sums of squares: the sum of squares due to treatments and the sum of squares due to error. Note also that the degrees of freedom corresponding to SST, ${n}_T - 1$, can be partitioned into the degrees of freedom corresponding to SSTR, $k - 1$, and the degrees of freedom corresponding to SSE, ${n}_T - k$. 

The analysis of variance can be viewed as the process of partitioning the total sum of squares and the degrees of freedom into their corresponding sources: treatments and error. Dividing the sum of squares by the appropriate degrees of freedom provides the variance estimates, the F value, and the p-value used to test the hypothesis of equal population means.

The square root of MSE provides the best estimate of the population standard deviation ${\sigma}$. This estimate of ${\sigma}$ on the computer output is \textcolor{pink}{Pooled StDev}.


## Code

```{r 'C13-ANOVA'}
str(aa)
bb <- aa %>% pivot_longer(everything(), names_to = "key", values_to = "value")
# 
# #ANOVA
ii_aov <- aov(formula = value ~ key, data = bb)
#names(ii_aov)
#ii_aov
#
# #
model.tables(ii_aov, type = "means")
#
# #Summary
summary(ii_aov)
```








## Summary

Analysis of variance (ANOVA) can be used to test for differences among means of several populations or treatments. 

The completely randomized design and the randomized block design are used to draw conclusions about differences in the means of a single factor. The primary purpose of blocking in the randomized block design is to remove extraneous sources of variation from the error term. Such blocking provides a better estimate of the true error variance and a better test to determine whether the population or treatment means of the factor differ significantly. 

The basis for the statistical tests used in analysis of variance and experimental design is the development of two independent estimates of the population variance ${\sigma}^2$. In the single-factor case, one estimator is based on the variation between the treatments; this estimator provides an unbiased estimate of ${\sigma}^2$ only if the means $\{{\mu}_1, {\mu}_2, \ldots, {\mu}_k\}$ are all equal. A second estimator of ${\sigma}^2$ is based on the variation of the observations within each sample; this estimator will always provide an unbiased estimate of ${\sigma}^2$. 

By computing the ratio of these two estimators (the F statistic), it is determined whether to reject the null hypothesis that the population or treatment means are equal.

In all the experimental designs considered, the partitioning of the sum of squares and degrees of freedom into their various sources enabled us to compute the appropriate values for the analysis of variance calculations and tests. 

Further, Fisher LSD procedure and the Bonferroni adjustment can be used to perform pairwise comparisons to determine which means are different.

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C13-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, ii_aov, xxChemitech)
```

```{r 'C13-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/313-ANOVA.Rmd-->

# Simple Linear Regression {#c14}

```{r 'C14', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- Larose Chapter 8 (338) : "Simple Linear Regression" has been merged here. 

## Simple Linear Regression Model 

```{definition 'Regression-Analysis'}
\textcolor{pink}{Regression analysis} can be used to develop an equation showing how two or more variables are related.
```

```{definition 'Variable-Dependent'}
The variable being predicted is called the \textcolor{pink}{dependent variable $({y})$}.
```

```{definition 'Variable-Independent'}
The variable or variables being used to predict the value of the dependent variable are called the \textcolor{pink}{independent variables $({x})$}.
```


```{definition 'Simple-Linear-Regression'}
The simplest type of regression analysis involving one independent variable and one dependent variable in which the relationship between the variables is approximated by a straight line, is called \textcolor{pink}{simple linear regression}.
```

```{definition 'Regression-Model'}
The equation that describes how ${y}$ is related to ${x}$ and an error term $\epsilon$ is called the \textcolor{pink}{regression model}. For example, simple linear regression model is given by equation ${y} = {\beta}_0 + {\beta}_1 {x} + {\epsilon}$
```

\begin{equation}
  {y} = {\beta}_0 + {\beta}_1 {x} + {\epsilon}
  (\#eq:simple-linear)
\end{equation}

Note

- ${\beta}_0$ and ${\beta}_1$ are referred to as the \textcolor{pink}{parameters of the model}


```{definition 'Error-Term'}
The random variable, \textcolor{pink}{error term $({\epsilon})$}, accounts for the variability in ${y}$ that cannot be explained by the linear relationship between ${x}$ and ${y}$.
```


```{definition 'Regression-Equation'}
The equation that describes how the mean or expected value of ${y}$, denoted $E(y)$, is related to ${x}$ is called the \textcolor{pink}{regression equation}. Simple Linear Regression Equation is: $E(y) = {\beta}_0 + {\beta}_1 {x}$. The graph of the simple linear regression equation is a straight line; ${\beta}_0$ is the y-intercept of the regression line, ${\beta}_1$ is the slope.
```

```{r 'C14P01', echo=FALSE, fig.cap="(C14P01) Linear Regression"}
knitr::include_graphics(paste0(.z$PX, "C14P01", "-Linear-Regression", ".png")) #iiii
```

- [(External) Image Source](http://www.sthda.com/english/articles/40-regression-analysis/167-simple-linear-regression-in-r "http://www.sthda.com")

## Least Squares Method

```{definition 'Estimated-Regression-Equation'}
Sample statistics (denoted $b_0$ and $b_1$) are computed as estimates of the population parameters ${\beta}_0$ and ${\beta}_1$. Thus \textcolor{pink}{Estimated Simple Linear Regression Equation} is: $\hat{y} = b_0 + b_1 {x}$. The value of $\hat{y}$ provides both a point estimate of $E(y)$ for a given value of 'x' and a prediction of an individual value of 'y' for a given value of 'x'. 
```


```{definition 'Least-Squares'}
The \textcolor{pink}{least squares method} is a procedure for using sample data to find the estimated regression equation. It uses the sample data to provide the values of $b_0$ and $b_1$ that minimize the \textcolor{pink}{sum of the squares of the deviations} between the observed values of the dependent variable $y_i$ and the predicted values of the dependent variable $\hat{y}_i$. i.e. min$\Sigma(y_i - \hat{y}_i)^2 or min(SSE)$
```


- Scatter diagrams for regression analysis are constructed with the independent variable 'x' on the horizontal axis and the dependent variable 'y' on the vertical axis.

"ForLater" - Equation and calculation for $b_0$ and $b_1$

## Coefficient of Determination

```{definition 'Residuals'}
The deviations of the y values about the estimated regression line are called \textcolor{pink}{residuals}. The $i^{\text{th}}$ residual represents the error in using (predicted) $\hat{y}_i$ to estimate (observed) $y_i$. i.e. $\text{Residual}_i =  y_i - \hat{y}_i$
```

- For the $i^{\text{th}}$ observation, the difference between the observed value of the dependent variable, $y_i$, and the predicted value of the dependent variable, $\hat{y}_i$, is called the \textcolor{pink}{$i^{\text{th}}$ residual}. 

```{definition 'SSE'}
The sum of squares of residuals or errors is the quantity that is minimized by the least squares method. This quantity, also known as the \textcolor{pink}{sum of squares due to error}, is denoted by \textcolor{pink}{SSE}. i.e. $\text{SSE} = \Sigma(y_i - \hat{y}_i)^2$
```

- The value of SSE is a measure of the error in using the estimated regression equation to predict the values of the dependent variable in the sample.

```{definition 'SSR'}
To measure how much the $\hat{y}$ values on the estimated regression line deviate from $\overline{y}$, another sum of squares is computed. This sum of squares, called the \textcolor{pink}{sum of squares due to regression}, is denoted \textcolor{pink}{SSR}. i.e. $\text{SSR} = \Sigma(\hat{y}_i - \overline{y})^2$
```


```{definition 'SST'}
For the $i^{\text{th}}$ observation in the sample, the difference $y_i - \overline{y}$ provides a measure of the error involved in using $\overline{y}$ for prediction. The corresponding sum of squares, called the \textcolor{pink}{total sum of squares}, is denoted \textcolor{pink}{SST}. i.e. $\text{SST} = \Sigma(y_i - \overline{y})^2 \to \text{SST} = \text{SSE} + \text{SSR}$. SST is a measure of the total variability in the values of the response variable alone, without reference to the predictor.
```

- We can think of SST as a measure of how well the observations cluster about the $\overline{y}$ line and SSE as a measure of how well the observations cluster about the $\hat{y}$ line.
- The estimated regression equation would provide a perfect fit if every value of the dependent variable $y_i$ happened to lie on the estimated regression line. 
  - In this case, $y_i - \hat{y}_i$ would be zero for each observation, resulting in SSE = 0. 
  - Thus for a perfect fit SSR must equal SST, and the ratio (SSR/SST) must equal one.

```{definition 'Coefficient-of-Determination'}
The ratio $r^2 =\frac{\text{SSR}}{\text{SST}} \in [0, 1]$, is used to evaluate the goodness of fit for the estimated regression equation. This ratio is called the \textcolor{pink}{coefficient of determination ($r^2$)}.  It can be interpreted as the proportion of the variability in the dependent variable y that is explained by the estimated regression equation.
```

- $r^2$ can be interpreted as the percentage of the total sum of squares that can be explained by using the estimated regression equation. 
- Larger values of $r^2$ imply that the least squares line provides a better fit to the data; that is, the observations are more closely grouped about the least squares line. But, using only $r^2$, we can draw no conclusion about whether the relationship between x and y is statistically significant. 

## Correlation Coefficient

- Refer [Correlation Coefficient](#correlation-c03 "c03")

```{r 'C14D01', comment="", echo=FALSE, results='asis'}
f_getDef("Correlation-Coefficient")
```

- If a regression analysis has already been performed and the coefficient of determination $r^2$ computed, the sample correlation coefficient $r_{xy} = (\text{sign of } b_1)\sqrt{r^2}$
- In the case of a linear relationship between two variables, both the coefficient of determination $(r^2)$ and the sample correlation coefficient $(r_{xy})$ provide measures of the strength of the relationship. 
  - $(r^2) \in [0, 1]$ : The coefficient of determination provides a measure between zero and one
  - $(r_{xy}) \in [-1, 1]$ : The sample correlation coefficient provides a measure between −1 and +1.
  - Although $r_{xy}$ is restricted to a linear relationship between two variables, $r^2$ can be used for nonlinear relationships and for relationships that have two or more independent variables. 
  - Thus, the coefficient of determination provides a wider range of applicability.

## Model Assumptions

- Value of the coefficient of determination $(r^2)$ is a measure of the goodness of fit of the estimated regression equation. However, even with a large value of $r^2$, the estimated regression equation should not be used until further analysis of the appropriateness of the assumed model has been conducted. 
- An important step in determining whether the assumed model is appropriate involves testing for the significance of the relationship. 
- The tests of significance in regression analysis are based on the following assumptions about the error term $\epsilon$.


```{definition 'Simple-Linear-Regression-Assumption-1'}
\textcolor{pink}{Regression Assumption 1/4 (Zero-Mean):} For Regression Model ${y} = {\beta}_0 + {\beta}_1 {x} + {\epsilon}$ : The error term $\epsilon$ is a random variable with a mean or expected value of zero; $E(\epsilon) = 0$. (Implication) $\beta_0$ and $\beta_1$ are constants, therefore $E(\beta_0) = \beta_0$ and $E(\beta_1) = \beta_1$; thus, for a given value of x, the expected value of y is given by \textcolor{pink}{Regression equation $E(y) = {\beta}_0 + {\beta}_1 {x}$}
```


```{definition 'Simple-Linear-Regression-Assumption-2'}
\textcolor{pink}{Regression Assumption 2/4 (Constant Variance):} For Regression Model ${y} = {\beta}_0 + {\beta}_1 {x} + {\epsilon}$ and Regression equation $E(y) = {\beta}_0 + {\beta}_1 {x}$ : The variance of $\epsilon$, denoted by ${\sigma}^2$, is the same for all values of x. (Implication) The variance of y about the regression line equals ${\sigma}^2$ and is the same for all values of x.
```


```{definition 'Simple-Linear-Regression-Assumption-3'}
\textcolor{pink}{Regression Assumption 3/4 (Independence):} For Regression Model ${y} = {\beta}_0 + {\beta}_1 {x} + {\epsilon}$ and Regression equation $E(y) = {\beta}_0 + {\beta}_1 {x}$ : The values of $\epsilon$ are independent. (Implication) The value of $\epsilon$ for a particular value of x is not related to the value of $\epsilon$ for any other value of x; thus, the value of y for a particular value of x is not related to the value of y for any other value of x.
```


```{definition 'Simple-Linear-Regression-Assumption-4'}
\textcolor{pink}{Regression Assumption 4/4 (Normality):} For Regression Model ${y} = {\beta}_0 + {\beta}_1 {x} + {\epsilon}$ and Regression equation $E(y) = {\beta}_0 + {\beta}_1 {x}$ : The error term $\epsilon$ is a normally distributed random variable for all values of x. (Implication) Because y is a linear function of $\epsilon$, y is also a normally distributed random variable for all values of x.
```

```{definition 'Simple-Linear-Regression-Assumption-Summary'}
\textcolor{pink}{Four Regression Assumptions}: (1) Zero-Mean: $E(\epsilon) = 0$. (2) Constant Variance: The variance of $\epsilon$ (${\sigma}^2$) is the same for all values of x. (3) Independence: The values of $\epsilon$ are independent. (4) Normality: The error term  $\epsilon$ has a normal distribution.
```

- \textcolor{orange}{Caution:} We are also making an assumption or hypothesis about the form of the relationship between x and y. That is, we assume that a straight line represented by ${\beta}_0 + {\beta}_1 {x}$ is the basis for the relationship between the variables. We must not lose sight of the fact that some other model, for instance ${y} = {\beta}_0 + {\beta}_1 {x}^2 + {\epsilon}$, may turn out to be a better model for the underlying relationship.

## Testing for Significance {#beta1-c14}

```{r 'C14D02', comment="", echo=FALSE, results='asis'}
f_getDef("Simple-Linear-Regression-Assumption-1")
```

- If ${\beta}_1 = 0 \to E(y) = {\beta}_0$ : In this case, the mean value of y does not depend on the value of x and hence we would conclude that x and y are not linearly related. 
- Alternatively, if ${\beta}_1 \neq 0$, we would conclude that the two variables are related. 
- Thus, to test for a significant regression relationship, we must conduct a hypothesis test to determine whether the value of ${\beta}_1$ is zero. i.e. \textcolor{pink}{${H_0} : {\beta}_1 = 0$}
- Two tests are commonly used. Both require an estimate of ${\sigma}^2$, the variance of $\epsilon$ in the regression model.

```{r 'C14D03', comment="", echo=FALSE, results='asis'}
f_getDef("Simple-Linear-Regression-Assumption-2")
```

- Estimate of ${\sigma}^2$
  - ${\sigma}^2$, the variance of $\epsilon$, also represents the variance of the y values about the regression line. 

```{r 'C14D04', comment="", echo=FALSE, results='asis'}
f_getDef("Residuals")
```

  - Thus, SSE, the sum of squared residuals, is a measure of the variability of the actual observations about the estimated regression line. 
    - SSE has $(n − 2)$ degrees of freedom because two parameters (${\beta}_0$ and ${\beta}_1$) must be estimated to compute SSE. 

```{definition 'MSE'}
\textcolor{pink}{Mean-Squared error (MSE)} is an evaluating measure of accuracy of model estimation for a continuous target variable. It provides the estimate of ${\sigma}^2$. It is given by SSE divided by its degrees of freedom $(n - 2)$. i.e. $s^2 = \text{MSE} = \frac{\text{SSE}}{n - 2}$. Where 's' is the \textcolor{pink}{standard error of the estimate}. Lower MSE is preferred.
```

## Inference in Regression

### t-Test

If we use a different random sample for the same regression study the resultant regression would be obviously different from the earlier. Indeed, $b_0$ and $b_1$, the least squares estimators, are sample statistics with their own sampling distributions. 

```{definition 'Standard-Error-B1'}
Standard deviation of $b_1$ is ${\sigma}_{b_1}$. Its estimate, estimated standard deviation of $b_1$, is given by $s_{b_1} = \frac{s}{\sqrt{\Sigma (x_i - {\overline{x}})^2}}$. The standard deviation of $b_1$ is also referred to as the \textcolor{pink}{standard error of $b_1$}. Thus, $s_{b_1}$ provides an estimate of the standard error of $b_1$.
```


- The t test for a significant relationship is based on the fact that the test statistic $\frac{b_1 - \beta_1}{s_{b_1}}$ follows a t distribution with $(n − 2)$ degrees of freedom. If the null hypothesis is true, then $\beta_1 = 0$ and $t = \frac{b_1}{s_{b_1}}$
  - If ${}^2\!P_{(t)} \leq {\alpha} \to {H_0}$ Rejected.
  - The form of a confidence interval for $\beta_1$ is as follows: $b_1 \pm t_{{\alpha}/2} s_{b_1}$
  - Large values of $s_{b_1}$ indicate that the estimate of the slope $b_1$ is unstable, while small values of $s_{b_1}$ indicate that the estimate of the slope $b_1$ is precise. 

```{definition 'H-SimpleRegression'}
\textcolor{pink}{$\text{\{Test for Significance in Simple Linear Regression\} } {H_0} : {\beta}_1 = 0 \iff {H_a}: {\beta}_1 \neq 0$}
```

### F-Test

An F test, based on the F probability distribution, can also be used to test for significance in regression. With only one independent variable, the F test will provide the same conclusion as the t test; that is, if the t test indicates $b_1 \neq 0$ and hence a significant relationship, the F test will also indicate a significant relationship. But with more than one independent variable, only the F test can be used to test for an overall significant relationship. 

As shown earlier, MSE provides an estimate of ${\sigma}^2$. If the null hypothesis ${H_0} : {\beta}_1 = 0$ is true, the sum of squares due to regression, \textcolor{pink}{SSR}, divided by its degrees of freedom provides another independent estimate of ${\sigma}^2$. 

```{definition 'MSR'}
The \textcolor{pink}{mean square due to regression (MSR)} provides the estimate of ${\sigma}^2$. It is given by SSR divided by its degrees of freedom. If the \textcolor{pink}{standard error of the estimate} is denoted by 's' then $s^2 = \text{MSR} = \frac{\text{SSR}}{\text{Regression degrees of freedom}} = \frac{\text{SSR}}{\text{Number of independent variables}}$
```

- If the null hypothesis ${H_0} : {\beta}_1 = 0$ is true, MSR and MSE are two independent estimates of ${\sigma}^2$ and the sampling distribution of MSR/MSE follows an F distribution with numerator degrees of freedom equal to one and denominator degrees of freedom equal to $(n − 2)$. 
  - Therefore, when ${\beta}_1 = 0$, the value of MSR/MSE should be close to one. 
    - Both MSE and MSR provide unbiased estimates of ${\sigma}^2$
  - However, if the null hypothesis is false ${\beta}_1 \neq 0$, MSR will overestimate ${\sigma}^2$ and the value of MSR/MSE will be inflated; thus, large values of MSR/MSE lead to the rejection of ${H_0}$ and the conclusion that the relationship between x and y is statistically significant.
    - MSE still provides an unbiased estimate of ${\sigma}^2$ 
  - Test Statistic $F = \frac{\text{MSR}}{\text{MSE}}$
  - If $P_{(F)} \leq {\alpha} \to {H_0}$ Rejected.

```{r 'C14D05', comment="", echo=FALSE, results='asis'}
f_getDef("H-SimpleRegression") 
```

## Cautions About the Interpretation of Significance Tests

Regression analysis, which can be used to identify how variables are associated with one another, cannot be used as evidence of a cause-and-effect relationship.

Rejecting the null hypothesis ${H_0} : {\beta}_1 = 0$ and concluding that the relationship between x and y is significant does not enable us to conclude that a cause-and-effect relationship is present between x and y. 

Concluding a cause-and-effect relationship is warranted only if the analyst can provide some type of theoretical justification that the relationship is in fact causal. 

In addition, just because we are able to reject ${H_0} : {\beta}_1 = 0$ and demonstrate statistical significance does not enable us to conclude that the relationship between x and y is linear. 

We can state only that x and y are related and that a linear relationship explains a significant portion of the variability in y over the range of values for x observed in the sample.

Given a significant relationship, we should feel confident in using the estimated regression equation for predictions corresponding to x values within the range of the x values observed in the sample. Unless other reasons indicate that the model is valid beyond this range, predictions outside the range of the independent variable should be made with caution. 

i.e. if sample has x-value 2 to 26, we can use it to estimate for a value of x = 20 but it should not be extrapolated to x = 40.

## Using the Estimated Regression Equation for Estimation and Prediction

"ForLater" - Interval Estimation, Confidence Interval, Prediction Interval

## Residual Analysis

Residual analysis is the primary tool for determining whether the assumed regression model is appropriate.

```{r 'C14D06', comment="", echo=FALSE, results='asis'}
f_getDef("Simple-Linear-Regression-Assumption-Summary")
```

- Common Plots
  - Residual Plot Against x (Scatterplot of $x_i$ and $\{y_i - \hat{y}_i\}$)
  - Residual Plot Against the fits (predicted values) $\hat{y}$ (Scatterplot of $\hat{y}_i$ and $\{y_i - \hat{y}_i\}$)
    - It is more widely used in multiple regression analysis, because of the presence of more than one independent variable.
    - There should be no deviant pattern observed for model to remain valid
      - No Curvature - violates the independence assumption
      - No Funnel - violates the constant variance assumption
      - No directional change - violates the zero-mean assumption
    - "Rorschach effect" : Do not see pattern in randomness
      - The null hypothesis when examining these plots is that the assumptions are intact; only systematic and clearly identifiable patterns in the residuals plots offer evidence to the contrary.
  - Standardized Residuals (Scaled) Plot Against x 
  - Normal Probability Plot (QQ plot) - Standardised Residuals vs. Normal Scores


```{definition 'QQ-Plot'}
A \textcolor{pink}{normal probability plot} is a quantile-quantile plot of the quantiles of a particular distribution against the quantiles of the standard normal distribution, for the purposes of determining whether the specified distribution deviates from normality.
```

- Normal Probability Plot (QQ plot)
  -  In a normality plot, the observed values of the distribution of interest are compared against the same number of values that would be expected from the normal distribution. 
  - If the distribution is normal, then the bulk of the points in the plot should fall on a straight line; systematic deviations from linearity in this plot indicate non-normality.

- Tests
  - \textcolor{pink}{Anderson-Darling Test} for Normality
    - The null hypothesis is that the normal distribution fits, so that small p-values will indicate lack of fit.
  - For assessing whether the constant variance assumption has been violated, either \textcolor{pink}{Bartlett test} or \textcolor{pink}{Levene test} may be used.
  - For determining whether the independence assumption has been violated, either the \textcolor{pink}{Durban-Watson test} or the \textcolor{pink}{runs test} may be applied.

## Outliers

```{r 'C14D07', comment="", echo=FALSE, results='asis'}
f_getDef("Outliers") #dddd
```

- Refer [Outliers: C03](#outliers-c03 "c03")
  - An outlier is an observation that has a very large standardized residual (scaled) in absolute value. 
  

```{definition 'High-Leverage-Points'}
\textcolor{pink}{High leverage points} are observations with extreme values for the independent variables. The leverage of an observation is determined by how far the values of the independent variables are from their mean values.
```


- A high leverage point is an observation that is extreme in the predictor space.
  - For leverage only the x is considered not the y.
  - Example: we have Distance Travelled (y) vs. Time Taken (x) information of 10 people with max(x) is 9 hours. If another person travel 39 km in 16 hours, it automatically becomes a high leverage point solely based on 16 hours (x).
    - If the point lies on the regression line i.e. its standardised residual is low then it is not an outlier. The decision for designating it as outlier conside the distance travelled (y) 


```{definition 'Influential-Observations'}
\textcolor{pink}{Influential observations} are those observations which have a strong influence or effect on the regression results. Influential observations can be identified from a scatter diagram when only one independent variable is present. 
```

- An observation is influential if the regression parameters alter significantly based on the presence or absence of the observation in the data set.
  - An outlier may or may not be influential. Similarly, a high leverage point may or may not be influential.
  - Usually, influential observations combine both the characteristics of large residual and high leverage
  - It is possible for an observation to be not-quite flagged as an outlier, and not-quite flagged as a high leverage point, but still be influential through the combination of the two characteristics.
    - Influential observations that are caused by an interaction of large residuals and high leverage can be difficult to detect. One of the diagnostic procedure is called 'Cook D statistic'.
  - Example: Suppose another person travels 20 km in 5 hours when the mean(x) is 5 hours. i.e. it is situated exactly at the mean of independent variable
    - Altohugh this would be an outlier because it has large standardised residual
    - It is not influential because it has very low leverage (placed at exactly the mean of x)
      - Its presence or absence are going to change parameters of regression equation only by a small value.


```{definition 'Cook-Distance'}
\textcolor{pink}{Cook distance ($D_i$)} is the most common measure of the influence of an observation. It works by taking into account both the size of the residual and the amount of leverage for that observation. Generally an observation is influential is if $D_i > 1$
```

- $D_i$ can be compared against the percentiles of the F-distribution with (m, n − m − 1) degrees of freedom. 
  - 'n' is the total number of observations
  - 'm' indicates the number of predictor variables
  - If the observed value lies within the first quartile of this distribution (lower than the $25^{\text{th}}$ percentile), then the observation has little influence on the regression; 
  - However, if $D_i$ is greater than the median of this distribution, then the observation is influential. 
  - The hiker in the earlier example (39 km in 16 hours), was that observation influential
    - This observatio has high leverage however it is not an outlier because the observation lies near the regression line
    - It has low $D_i$ and thus the observation is not influential
  - What if the hiker travelled 23 km in 10 hours
    - It lacks high leverage or high residual
    - However, it is influential because $D_i$ is beyond $50^{\text{th}}$ percentile
    - The influence of this observation stems from the combination of its moderately large residual with its moderately large leverage. 

## Transformations to achieve Linearity

- Ladder of Re-expressions - "ForLater"
  - The ladder of re-expressions consists of the following ordered set of transformations for any continuous variable t: $t^{-3}, t^{-2}, t^{-1}, t^{-1/2}, \ln(t), \sqrt{t}, t^1, t^2, t^3$

- Box-Cox Transformations
  - This method involves first choosing a set of candidate values for $\lambda$, and finding SSE for regressions performed using each value of $\lambda$. Then, plotting $\text{SSE}_\lambda$ versus $\lambda$, find the lowest point of a curve through the points in the plot. This represents the maximum-likelihood estimate of $\lambda$.


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C14-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C14-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/314-Regression.Rmd-->

# Multiple Regression {#c15}

```{r 'C15', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- Larose Chapter 9 (339) : "Multiple Regression and Model Building" has been merged here. 


```{definition 'Multiple-Regression'}
\textcolor{pink}{Multiple regression} analysis is the study of how a dependent variable $y$ is related to two or more independent variables. Multiple Regression Model is ${y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon$
```

```{definition 'Multiple-Regression-Equation'}
The equation that describes how the mean or expected value of ${y}$, denoted $E(y)$, is related to ${x}$ is called the \textcolor{pink}{regression equation}. Multiple Regression Linear Equation is: $E(y) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p$. 
```

- Note: Regression Model ${y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon$ contains error term $\epsilon$, whereas the Regression Equation $E(y) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p$ does not have that.
  - $E(y)$ represents the average of all possible values of y that might occur for the given values of $\{x_1, x_2, \ldots, x_p\}$.

- Model parameters $\{\beta_0, \beta_1, \beta_2, \ldots, \beta_p\}$ are generally unknown and thus are estimated by sample statistics $\{b_0, b_1, b_2, \ldots, b_p\}$. 


```{definition 'Estimated-Multiple-Regression-Equation'}
\textcolor{pink}{Estimated Multiple Regression Equation} is given by $\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \cdots + b_p x_p$. Where $b_i$ represents an estimate of the change in y corresponding to a one-unit change in $x_i$ when all other independent variables are held constant. 
```

## SST, SSR, SSE and MST, MSR, MSE

- Relationship between SST, SSR, SSE
  - $\text{SST} = \text{SSR} + \text{SSE}$ 
  - Total Sum of Squares $\text{SST} = \sum(y_i - \overline{y})^2$
    - Degrees of Freedom = $(n - 1)$
    - Mean Sum of Squares $\text{MST} = \frac{\text{SST}}{(n - 1)}$
  - Sum of Squares due to Regression $\text{SSR} = \sum(\hat{y}_i - \overline{y})^2$
    - Degrees of Freedom = $(p)$
    - Mean square due to regression $\text{MSR} = \frac{\text{SSR}}{(p)}$
  - Sum of Squares due to Error $\text{SSE} = \sum(y_i - \hat{y}_i)^2$
    - Degrees of Freedom = $(n - p - 1)$
    - Mean square due to error $\text{MSE} = \frac{\text{SSE}}{(n - p - 1)}$    
  - Coefficient of Determination
    - Simple $r^2 = \frac{\text{SSR}}{\text{SST}}$
    - Multiple $R^2 = \frac{\text{SSR}}{\text{SST}}$
    - In general, $R^2$ always increases as independent variables are added to the model.
  - F-statistic
    - $F = \frac{\text{MSR}}{\text{MSE}}$
    - Note that for $r^2$ the denominator was Total i.e. SST whereas for F-statistic denominator is Error (MSE)
    

```{definition 'RSq-Adj'}
If a variable is added to the model, $R^2$ becomes larger even if the variable added is not statistically significant. The \textcolor{pink}{adjusted multiple coefficient of determination $(R_a^2)$} compensates for the number of independent variables in the model. With 'n' denoting the number of observations and 'p' denoting the number of independent variables: $R_a^2 = 1 - (1 - R^2) \frac{n - 1}{n - p - 1}$
```

- Note: If the value of $R^2$ is small and the model contains a large number of independent variables, the adjusted coefficient of determination $(R_a^2)$ can take a negative value.

## Model Assumptions

```{r 'C15D01', comment="", echo=FALSE, results='asis'}
f_getDef("Simple-Linear-Regression-Assumption-Summary") #dddd
```

- All 4 assumptions of Simple Linear Regression are applicable on Multiple Linear Regression also. Only the number of independent variables and model would increase.

## Testing for Significance

- In multiple regression, the t-test and the F-test have different purposes. 
  - The F test is used to determine whether a significant relationship exists between the dependent variable and the set of all the independent variables; we will refer to the F-test as the test for \textcolor{pink}{overall significance}. 
    - F-Test Statistic $F = \frac{\text{MSR}}{\text{MSE}}$
  - If the F test shows an overall significance, the t-test is used to determine whether each of the individual independent variables is significant. 
    - A separate t-test is conducted for each of the independent variables in the model
    - we refer to each of these t tests as a test for \textcolor{pink}{individual significance}.
    - t-Test Statistic $t = \frac{b_i}{s_{b_i}}$

```{definition 'H-MultipleRegression-F'}
\textcolor{pink}{$\text{\{F-Test in Multiple Linear Regression\} } {H_0} : {\beta}_1 = {\beta}_2 = \cdots = {\beta}_p = 0 \iff {H_a}: \text{At least one parameter is not zero}$}
```

```{definition 'H-MultipleRegression-t'}
\textcolor{pink}{$\text{\{t-Test in Multiple Linear Regression\} } {H_0} : {\beta}_i = 0 \iff {H_a}: {\beta}_i \neq 0$}
```

## Multicollinearity

```{definition 'Multicollinearity-c15'}
\textcolor{pink}{Multicollinearity} refers to the correlation among the independent variables.
```

- In t-tests for the significance of individual parameters, the difficulty caused by multicollinearity is that it is possible to conclude that none of the individual parameters are significantly different from zero when an F test on the overall multiple regression equation indicates a significant relationship.
  - When the independent variables are highly correlated, it is not possible to determine the separate effect of any particular independent variable on the dependent variable.
  - Multicollinearity is a potential problem if the absolute value of the sample correlation coefficient $r_{x_1, x_2}$ exceeds 0.7 for any two of the independent variables. 

## Categorical Independent Variables {#reg-cat-c15}

- Ex: Gender (Male, Female)
  - We would need a \textcolor{pink}{dummy variable} or 'indicator variable' which will be {0 = Male, 1 = Female}
  - Let ${x_1}$ denote a numerical variable and ${x_2}$ is the dummy variable which can take 2 values {M = 0, F = 1}.
  - Multiple Regression equation would be: $E(y) = \beta_0 + \beta_1 x_1 + \beta_2 x_2$
    - Expected Value of Y given M : $E(y | \text{M}) = \beta_0 + \beta_1 x_1$
    - Expected Value of Y given F : $E(y | \text{F}) = \beta_0 + \beta_1 x_1 + \beta_2$
  - In effect, the use of a dummy variable provides two estimated regression equations that can be used to predict Y, each corresponding to either level of ${x_2}$.
    - These regression lines represent same slope but effectively different intercepts when Y is plotted against ${x_1}$ i.e. on each of the different graphs of Y vs. $x_p$ there will be two lines each corresponding to either level of ${x_2}$.
    
- Number of dummy variables
  - Above example had only 2 levels so it was modelled with a single dummy variable with 2 levels of {0, 1}.
  - A variable with 3 levels e.g. {low, medium, high} will NOT use a single dummy variable ~~with 3 levels of {0, 1, 2}~~.
  - Rather, we would need 2 dummy variables each with 2 levels of {0, 1}.


```{definition 'Dummy-Variables'}
A categorical variable with $k$ levels must be modeled using $k − 1$ \textcolor{pink}{dummy variables} (or indicator variables). It can take only the values 0 and 1. e.g. A variable with 3 levels of {low, medium, high} would need 2 dummy variables $\{x_1, x_2\}$ each being either 0 or 1 only. i.e. low $\to \{x_1 = 1, x_2 = 0\}$, medium $\to \{x_1 = 0, x_2 = 1\}$, high $\to \{x_1 = 0, x_2 = 0\}$. Thus $x_1$ is 1 when low and 0 otherwise, $x_2$ is 1 when medium and 0 otherwise. High is represented as neither $x_1$ nor $x_2$ i.e. both are zero. Note that both cannot be 1. Only one of them can be TRUE at a time.
```

- The category that is not assigned an indicator variable is denoted the \textcolor{pink}{reference category} (or the Benchmark). In the example, "high" is the reference category.



"ForLater" - Studentized Deleted Residuals and Outliers


## Logistic Regression

- Generally part of Classification
- In many regression applications the dependent variable may only assume two discrete values. 
  - For instance, a bank might want to develop an estimated regression equation for predicting whether a person will be approved for a credit card. 
  - The dependent variable can be coded as y = 1 if the bank approves the request for a credit card and y = 0 if the bank rejects the request for a credit card. 
  - Using logistic regression we can estimate the probability that the bank will approve the request for a credit card given a particular set of values for the chosen independent variables.

- The odds in favor of an event occurring is defined as the probability the event will occur divided by the probability the event will not occur. In logistic regression the event of interest is always y = 1. 



```{definition 'Odds-Ratio'}
The \textcolor{pink}{odds ratio} measures the impact on the odds of a one-unit increase in only one of the independent variables. The odds ratio is the odds that y = 1 given that one of the independent variables has been increased by one unit $(\text{odds}_1)$ divided by the odds that y = 1 given no change in the values for the independent variables $(\text{odds}_0)$. i.e. $\text{Odds Ratio} = \frac{\text{odds}_1}{\text{odds}_0}$
```


"ForLater" - "logit"
  
## VIF

- Suppose we did not check for the presence of correlation among our predictors and performed the regression anyway. 
- Is there some way that the regression results can warn us of the presence of multicollinearity
  - We may ask for the variance inflation factors (VIF) to be reported
  - $R_i^2 = 0.80 \to \text{VIF} \geq 5$ to be an indicator of moderate multicollinearity
  - $R_i^2 = 0.90 \to \text{VIF} \geq 10$ to be an indicator of severe multicollinearity
  


```{definition 'VIF'}
The \textcolor{pink}{variance inflation factors (VIF)} is given by $\text{VIF}_i = \frac{1}{1 - R_i^2} \in [1, \infty]$. That is, the minimum value for VIF is 1, and is reached when $x_i$ is completely uncorrelated with the remaining predictors.
```

- Solutions for multicollinearity
  - Eliminate one of the variable
    - However, the variable might have some information relevant to the model
  - User-defined Composite
    - Scale both variable and take mean of the values. Use this as an independent variable instead of the correlated variables
      - However if one of the variable is an excellent predictor of the dependent variable then averaging it with a weaker predictor is going to reduce the model performance.
      - Even if we change the weights from average (50:50) to something else the problem would remain
  - PCA
    - Definitely Better    

- High VIF on Dummy Variables of Categorical Variable Levels
  - We need to choose the most frequent level as the reference variable. If the reference variable has few observations, it would result in Very High VIF for more frequent levels.


## Variable Selection Methods

- These include - "ForLater"
  - Forwards Selection
  - Backward elimination
  - Stepwise selection
  - Best Subsets

- The Forward Selection Procedure
  - It starts with no variables in the model.
  1. For the first variable to enter the model, select the predictor most highly correlated with the target $(x_1)$. 
      - If the resulting model is not significant, then stop and report that no variables are important predictors
  2. For each remaining variable, compute the sequential F-statistic for that variable, given the variables already in the model. 
      - For example, in this first pass through the algorithm, these sequential F-statistics would be $\{F(x_2|x_1), F(x_3|x_1), F(x_4|x_1), \ldots \}$. 
      - On the second pass through the algorithm, these might be $\{F(x_3|x_1, x_2), F(x_4|x_1, x_2), \ldots \}$. 
      - Select the variable with the largest sequential F-statistic.
  3. For the variable selected in step 2, test for the significance of the sequential F-statistic. 
      - If the resulting model is not significant, then stop, and report the current model without adding the variable from step 2. 
      - Otherwise, add the variable from step 2 into the model and return to step 2.

- The Backward Elimination Procedure
  - It starts with all the variables in the model.
  1. Perform the regression on the full model; that is, using all available variables. 
      - For example, perhaps the full model has four variables, $\{x_1, x_2, x_3, x_4 \}$.
  2. For each variable in the current model, compute the partial F-statistic. 
      - In the first pass through the algorithm, these would be $\{F(x_1|x_2, x_3, x_4), F(x_2|x_1, x_3, x_4), F(x_3|x_1, x_2, x_4), F(x_4|x_1, x_2, x_3)\}$. 
      - Select the variable with the smallest partial F-statistic. Denote this value $F_{\text{min}}$. 
  3. Test for the significance of $F_{\text{min}}$. 
    - If $F_{\text{min}}$ is not significant, then remove the variable associated with $F_{\text{min}}$ from the model, and return to step 2. 
    - If $F_{\text{min}}$ is significant, then stop the algorithm and report the current model. 
    - If this is the first pass through the algorithm, then the current model is the full model. 
    - If this is not the first pass, then the current model has been reduced by one or more variables from the full model.

## Stepwise Regression

```{definition 'Stepwise-Regression'}
In \textcolor{pink}{stepwise regression}, the regression model begins with no predictors, then the most significant predictor is entered into the model, followed by the next most significant predictor. At each stage, each predictor is tested whether it is still significant. The procedure continues until all significant predictors have been entered into the model, and no further predictors have been dropped. The resulting model is usually a good regression model, although it is not guaranteed to be the global optimum.
```

- The stepwise procedure represents a modification of the forward selection procedure.
  - A variable that has been entered into the model early in the forward selection process may turn out to be nonsignificant, once other variables have been entered into the model. 
  - The stepwise procedure checks on this possibility, by performing at each step a partial F-test, using the partial sum of squares, for each variable currently in the model. 
  - If there is a variable in the model that is no longer significant, then the variable with the smallest partial F-statistic is removed from the model.



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C15-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C15-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/315-MultipleRegression.Rmd-->

# Regression Models {#c16}

```{r 'C16', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- 
  - "ForLater" - Everything


## Summary



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C16-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C16-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/316-RegressionModels.Rmd-->

# Time Series {#c17}

```{r 'C17', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Time Series Analysis and Forecasting"
  - "ForLater" - Everything


## Summary



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C17-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C17-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/317-TimeSeries.Rmd-->

# Nonparametric Methods {#c18}

```{r 'C18', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- 
  - "ForLater" - Everything
  - [Parametric Methods](#parametric-c18 "c18"), [Nonparametric Methods](#non-parametric-c18 "c18")

## Paramtetric Methods {#parametric-c18}

```{definition 'Parametric-Methods'}
\textcolor{pink}{Parametric methods} are the statistical methods that begin with an assumption about the probability distribution of the population which is often that the population has a normal distribution. A sampling distribution for the test statistic can then be derived and used to make an inference about one or more parameters of the population such as the population mean ${\mu}$ or the population standard deviation ${\sigma}$.
```

Parametric methods mostly require quantitative data. However these are ~~generally~~ sometimes more powerful than nonparametric methods. 

- The reason that parametric tests are sometimes more powerful than randomisation and tests based on ranks is that the parametric tests make use of some extra information about the data: the nature of the distribution from which the data are assumed to have come. 
- \textcolor{pink}{Powerful} here means, they require smaller sample size.
- However, their power advantage is not invariant
- Further, Rarely if ever a parametric test and a non-parametric test actually have the same null. 
  - The parametric t-test is testing the mean of the distribution, assuming the first two moments exist.
  - The Wilcoxon rank sum test does not assume any moments, and tests equality of distributions instead. 
  - The two tests are testing different hypotheses (comparable in a limited sense but different).
- \textcolor{pink}{At large sample sizes, either of the parametric or the nonparametric tests work adequately.}


## Nonparamtetric Methods {#non-parametric-c18}

```{definition 'Distribution-free-Methods'}
\textcolor{pink}{Distribution-free methods} are the Statistical methods that make no assumption about the probability distribution of the population.
```


```{definition 'Nonparametric-Methods'}
\textcolor{pink}{Nonparametric methods} are the statistical methods that require no assumption about the form of the probability distribution of the population and are often referred to as distribution free methods. Several of the methods can be applied with categorical as well as quantitative data.
```

Most of the statistical methods referred to as parametric methods require quantitative data, while nonparametric methods allow inferences based on either categorical or quantitative data. 

- However, the computations used in the nonparametric methods are generally done with categorical data.
  - Nominal or ordinal measures in many cases require a nonparametric test.
- Whenever the data are quantitative, we will transform the data into categorical data in order to conduct the nonparametric test. 
- Most nonparametric tests use some way of ranking the measurements.
- Nonparametric tests are used in cases where parametric tests are not appropriate.
  - Nonparametric tests are often necessary, specially when the distribution is not normal (skewness), the distribution is not known, or the sample size is too small (<30) to assume a normal distribution. 
  - Also, if there are extreme values or values that are clearly "out of range" nonparametric tests should be used.








## Summary



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C18-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C18-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/318-Nonparametric.Rmd-->

# Quality Control {#c19 .unlisted .unnumbered}

```{r 'C19', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "Statistical Methods for Quality Control"
  - "ForLater" - Everything


## Summary



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C19-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C19-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/319-QualityControl.Rmd-->

# Index Numbers {#c20 .unlisted .unnumbered}

```{r 'C20', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater" - Everything

## Summary


## Validation {.unlisted .unnumbered .tabset .tabset-fade}
-->

```{r 'C20-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C20-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/320-Indexes.Rmd-->

# Introduction to Data Mining {#c31}
> Definitions and Exercises are from the Book [@Larose]

```{r 'C31', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "An Introduction to Data Mining and Predictive Analytics"

```{definition 'Data-Mining-331'}
\textcolor{pink}{Data mining} is the process of discovering useful patterns and trends in large data sets. 
```

```{definition 'Predictive-Analytics-331'}
\textcolor{pink}{Predictive analytics} is the process of extracting information from large data sets in order to make predictions and estimates about future outcomes.
```

- The Cross-Industry Standard Process for Data Mining (CRISP-DM) (Iterative)
  - Business/Research Understanding Phase
    - Clearly enunciate the project objectives and requirements in terms of the business or research unit as a whole. 
    - Translate these goals and restrictions into the formulation of a data mining problem definition.
    - Prepare a preliminary strategy for achieving these objectives.
  - Data Understanding Phase 
    - Collect the data.
    - Use \textcolor{pink}{Exploratory Data Analysis (EDA)} to familiarize yourself with the data, and discover initial insights. 
    - Evaluate the quality of the data. 
    - if desired, select interesting subsets that may contain actionable patterns. 
  - Data Preparation Phase 
    - Raw | Select | Filter | Subset | Clean | Transform 
  - Modeling Phase
    - Select and apply appropriate modeling techniques
    - Calibrate
  - Evaluation Phase
    - Models must be evaluated for quality and effectiveness. 
    - Also, determine whether the model in fact achieves the objectives set for it
    - Establish whether some important facet of the business or research problem has not been sufficiently accounted for. 
  - Deployment Phase / Report / Publish

## Data Mining Methods {#mining-def-c31}

- [Data Mining Methods](#mining-b19 "b19") and [Definitions](#mining-def-c31 "c31")
  - Data mining methods may be categorized as either supervised or unsupervised.
  - Most data mining methods are supervised methods.
  - Unsupervised : Clustering, PCA, Factor Analysis, Association Rules, RFM
  - Supervised : 
    - Regression (Continuous Target) : Linear Regression, Regularised Regression, Decision trees, Ensemble learning 
      - Linear Regression : Ridge, Lasso and Elastic Regression
      - Ensemble learning : Bagging, Boosting (AdaBoost, XGBoost), Random forests
    - Classification (Categorical Target) : Decision trees, Ensemble learning, Logistic Regression, k-nearest neighbor (k-NN), Naive-Bayes 
    - Deep Learning : Neural Networks


```{definition 'Description'}
\textcolor{pink}{Description} of patterns and trends often suggest possible explanations for existence of theme within the data.
```

```{definition 'Estimation'}
In \textcolor{pink}{estimation}, we approximate the value of a numeric target variable using a set of numeric and/or categorical predictor variables. Methods: Point Estimation, Confidence Interval Estimation, Simple Linear Regression, Correlation, Multiple Regression etc.
```

```{definition 'Prediction'}
\textcolor{pink}{Prediction} is similar to classification and estimation, except that for prediction, the results lie in the future. Estimation methods are also used for Prediction. Additional Methods: k-nearest neighbor methods, decision trees, neural networks etc.
```

```{definition 'Classification'}
\textcolor{pink}{Classification} is similar to estimation, however, instead of approximating the value of a numeric target variable, the target variable is categorical. 
```

```{r 'C31D01', comment="", echo=FALSE, results='asis'}
f_getDef("Clustering")
```

```{r 'C31D02', comment="", echo=FALSE, results='asis'}
f_getDef("Cluster")
```

```{r 'C31D03', comment="", echo=FALSE, results='asis'}
f_getDef("Affinity-Analysis")
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C31-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C31-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/331-Introduction.Rmd-->

# Data Processing {#c32}

```{r 'C32', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Data

Refer [Lecture: Data Pre-Processing](#b16 "b16")
Refer [Numerical Measures](#c03 "c03")


\textcolor{pink}{Please import the "B18-Churn.xlsx"}
\textcolor{pink}{Please import the "B16-Cars2.csv"}

- \textcolor{orange}{Caution:} The Cars2 dataset has 263 observations, whereas the Cars dataset has 261.

```{r 'C32-ImportData', include=FALSE}
xxB16Cars <- f_getRDS(xxB16Cars)
xxB18Churn <- f_getRDS(xxB18Churn)
#
c32churn <- xxB18Churn %>% rename_with(make.names) %>% 
  rename_with(~ tolower(gsub(".", "_", .x, fixed = TRUE))) %>% 
  mutate(across(c(int_l_plan, vmail_plan), ~case_when(. == "yes" ~ TRUE, . == "no" ~ FALSE))) %>% 
  mutate(across(churn, ~case_when(. == "True." ~ TRUE, . == "False." ~ FALSE))) %>% 
  mutate(across(ends_with("_calls"), as.integer))
```

## Flag Variables

```{definition 'Variable-Flag-Dummy'}
A \textcolor{pink}{flag variable} (or dummy variable, or indicator variable) is a categorical variable taking only two values, 0 and 1. Ex: Gender (Male, Female) can be recoded into dummy Gender (Male = 0, Female = 1). 
```

- When a categorical predictor takes $k \geq 3$ possible values, then define $k - 1$ dummy variables, and use the unassigned category as the \textcolor{pink}{reference category}. 
  - Ex: Region = North, East, South, West i.e. k = 4 
    - 3 flags : flag_north, flag_east, flag_south; each will be 1 for their own region; 0 otherwise
    - The flag variable for the west is not needed, as 'region = west' is already uniquely identified by zero values for each of the three existing flag variables.
    - Instead, the unassigned category becomes the reference category, meaning that, the interpretation of the value of north_flag is 'region = north' compared to 'region = west'. 
      - For example, if we are running a regression analysis with income as the target variable, and the regression coefficient for north_flag equals 1000, then the estimated income for 'region=north' is 1000 greater than for 'region=west', when all other predictors are held constant.
    - Further, inclusion of the fourth flag variable will cause some algorithms to fail, because of the singularity of the $(X^{T}X)^{-1}$ matrix in regression, for instance.

## Transforming Categorical to Numerical

- Ex: Region = North, East, South, West can be assigned numbers 1 to 4
  - It may result in algorithm treating them as continuous and /or ordered values
- Generally, categorical varaiables should not be transformed into numerical, except when these are clearly ordered. 
  - e.g. Survey Response is an ordered categorical variable and can be assigned values 1 to 5

## Binning Continuous to Categorical

- Ex: Income into low, medium, high
  - \textcolor{pink}{Equal width binning} divides the numerical predictor into k categories of equal width
    - NOT recommended for most applications because it can be greatly affected by the outliers
  - \textcolor{pink}{Equal frequency binning} divides the numerical predictor into k categories, each having k/n records, where n is the total number of records. 
    - Simple. However, it has problem that sometimes same value can be found in two consecutive groups
    - Equal data values must belong to same category.
  - \textcolor{pink}{Binning by clustering} uses a clustering algorithm, such as k-means clustering to automatically calculate the "optimal" partitioning. 
  - \textcolor{pink}{Binning based on predictive value: } Above methods ignore the target variable; binning based on predictive value partitions the numerical predictor based on the effect each partition has on the value of the target variable. 


```{conjecture 'Insufficient-Data'}
\textcolor{brown}{Error: Insufficient data values to produce ... bins.}
```

- `summary(cut_number(diamonds$depth, n = 27))` Passed
- `summary(cut_number(diamonds$depth, n = 28))` Failed (N = 53940)
- \textcolor{pink}{ggplot2::cut_number()} has some internal logic about size of the bins. It is not as simple as total size, it also has to do with relative size. 
- It also fails when Bins have overlap. 
- Use \textcolor{pink}{dplyr::ntile()}
- OR Pick a bin size that works for your data.

```{r 'C32-Bins'}
# #n = 12
bb <- c(1, 1, 1, 1, 1, 2, 2, 11, 11, 12, 12, 44)
#
# #Fixing Number of Bins: Unequal number of Observations and also Bins with 0 Observations
summary(cut(bb, breaks = 3))
summary(cut_interval(bb, n = 3))
#
# #For reference, NOT equivalent to above. 
summary(cut_width(bb, width = 15))
#
# #Using Equal Frequency: Same Observation may belong to different consecutive Bins
if(FALSE) ggplot2::cut_number(bb, n = 3) #ERROR
if(FALSE) { #Works
  #ceiling(seq_along(bb)/4)[rank(bb, ties.method = "first")] 
  tibble(bb = bb, RANK = rank(bb, ties.method = "first"), 
         ALONG = seq_along(bb), CEIL = ceiling(seq_along(bb)/4)) 
}
#
# #dplyr::ntile() can be used in place of ggplot2::cut_number()
dplyr::ntile(bb, n = 3)
#
```

## Adding an Index Field (ID)

\textcolor{orange}{Caution:} ID fields should be filtered out from the data mining algorithms, but should not be removed from the data. These are for easy identification of records not for correlation.

```{r 'C32-ID'}
mtcars %>% mutate(ID = row_number()) %>% relocate(ID) %>% slice(1:6L)
```

## Variable that should not be removed (probably)

- Ex: An example of correlated variables may be precipitation and attendance at a state beach. 
  - As precipitation increases, attendance at the beach tends to decrease, so that the variables are negatively correlated.
- Inclusion of correlated variables may at best double-count a particular aspect of the analysis, and at worst lead to instability of the model results. 
- Thus, we may decide to simply remove one of the variables. 
- However, it should not be done, as important information may thereby be discarded. 
- Instead, it is suggested that PCA be applied, where the common variability in correlated predictors may be translated into a set of uncorrelated principal components.

```{r 'C32-EDA', include=FALSE, eval=FALSE}
bb <- aa <- c32churn
#str(bb)
# #CrossTab: There are No users who have the International Plan but did not call
bb %>% select(c(starts_with("int"))) %>% 
  mutate(isZero = ifelse(intl_mins == 0, TRUE, FALSE)) %>% 
  #filter(!int_l_plan) %>% 
  select(int_l_plan, isZero) %>% count(int_l_plan, isZero) %>% 
  pivot_wider(names_from = isZero, values_from = n, values_fill = 0)
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C32-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, c32churn, xxB16Cars, xxB18Churn)
```

```{r 'C32-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/332-dProcessing.Rmd-->

# EDA {#c33}

```{r 'C33', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Exploratory Data Analysis"
  - Refer [L: Data Pre-Processing](#b16 "b16"), [L: Data Pre-Processing](#b17 "b17"), [L: Unsupervised Learning](#b18 "b18"), and [Numerical Measures](#c03 "c03")

## Churn

\textcolor{pink}{Please import the "B18-Churn.xlsx"}

```{r 'C33-ImportData', include=FALSE}
xxB16Cars <- f_getRDS(xxB16Cars)
xxB18Churn <- f_getRDS(xxB18Churn)
#
c33churn <- xxB18Churn %>% rename_with(make.names) %>% 
  rename_with(~ tolower(gsub(".", "_", .x, fixed = TRUE))) %>% 
  mutate(across(c(int_l_plan, vmail_plan), ~case_when(. == "yes" ~ TRUE, . == "no" ~ FALSE))) %>% 
  mutate(across(churn, ~case_when(. == "True." ~ TRUE, . == "False." ~ FALSE))) %>% 
  mutate(across(ends_with("_calls"), as.integer))
#
bb <- aa <- c33churn
```

- Churn (attrition)
  - It is a term used to indicate a customer leaving the service of one company in favor of another company. 
  - The data set contains 20 predictors worth of information about 3333 customers, along with the target variable, churn, an indication of whether that customer churned (left the company) or not.
- Description [3333, 21]
  - State: Categorical, for the 50 states and the District of Columbia. 
  - Account length: Integer-valued, how long account has been active. 
  - Area code: Categorical 
  - Phone number: Essentially a surrogate for customer ID. 
  - International plan: Dichotomous categorical, yes or no. 
  - Voice mail plan: Dichotomous categorical, yes or no. 
  - Number of voice mail messages: Integer-valued. 
  - Total day minutes: Continuous, minutes customer used service during the day. 
  - Total day calls: Integer-valued. 
  - Total day charge: Continuous, perhaps based on above two variables. 
  - Total eve minutes: Continuous, minutes customer used service during the evening. 
  - Total eve calls: Integer-valued. 
  - Total eve charge: Continuous, perhaps based on above two variables. 
  - Total night minutes: Continuous, minutes customer used service during the night. 
  - Total night calls: Integer-valued. 
  - Total night charge: Continuous, perhaps based on above two variables. 
  - Total international minutes: Continuous, minutes customer used service to make international calls. 
  - Total international calls: Integer-valued. 
  - Total international charge: Continuous, perhaps based on above two variables. 
  - Number of calls to customer service: Integer-valued. 
  - Churn: Target. Indicator of whether the customer has left the company (True or False)

## Basics, Skewness & Normality

```{r 'C33-Normality', include=FALSE}
bb <- aa <- c33churn
#str(bb)
ii <- bb %>% select(2, 7:20) %>% 
  relocate(vmail_message, .after = last_col()) %>% 
  relocate(account_length, .after = last_col()) %>% 
  pivot_longer(everything(), names_to = "Key", values_to = "Values") %>% 
  mutate(across(Key, factor, levels = unique(Key)))
#
#str(ii)
#levels(ii$Key)
#
jj <- ii %>% group_by(Key) %>% 
  summarise(Min = min(Values), Max = max(Values), SD = sd(Values), 
            Mean = mean(Values), Median = median(Values), Mode = f_getMode(Values),
            Unique = length(unique(Values)), isNA = sum(is.na(Values)), 
            Skewness = skewness(Values), 
            p_Shapiro = shapiro.test(Values)$p.value, 
            isNormal = ifelse(p_Shapiro > 0.05, TRUE, FALSE))
#
kk <- jj %>% 
  mutate(across(where(is.numeric), round, digits = 4)) %>% 
  mutate(across(where(is.numeric), format, digits = 3, nsmall = 0, 
                replace.zero = TRUE, zero.print = "0", scientific = FALSE, drop0trailing = TRUE)) 
```

```{r 'C33T01', echo=FALSE}
kbl(kk,
  caption = "(C33T01) Churn: Normality",
  #col.names = displ_names,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
                #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	) %>% 
# #Conditional Row Text Highlight. Background etc. can also be done.
  row_spec(which(!kk$isNormal), color = "red") # %>% row_spec(which(kk$isNormal), color = "black")
```

## Explore Categorical Variables

### International Plan

```{r 'C33-SetIntl', include=FALSE}
# #xyw: x (independent), y (dependent), c (column =y), r (row =x)
# #g (grouped), w (wider), l (longer), 
r_xyg <- "International"
c_xyg <- "Churn"
xyg <- bb %>% select(int_l_plan, churn) %>% rename(Predictor = 1, Target = 2) %>% 
  mutate(across(1:2, ~ifelse(., 'Yes', 'No'))) %>% 
  count(Predictor, Target) 
#
str(xyg)
```

```{r 'C33-CrossIntl', echo=FALSE, ref.label=c('C33-CrossTab')}
#
```

#### Contingency Table (CrossTab) {.unlisted .unnumbered}

- 28.4% of churners belong to the International Plan, compared to 6.5% of non-churners.
- 42.4% of the International Plan holders churned, as compared to only 11.5% of those without the International Plan.
- The proportion of International Plan holders is greater among the churners.
- Summary
  - We should investigate what is it about our international plan that is inducing our customers to leave
  - We should expect that, the model (in future) will probably include whether or not the customer selected the International Plan.

```{r 'C33-SetCrossIntl', include=FALSE}
hh <- ctab %>% select(-c(5:6, 9:11)) 
#names_hh <- names(hh)
names_hh <-c(paste0(c_xyg, " ", "$\\rightarrow$", "<br/>", "$\\downarrow$", " ", r_xyg), 
                "No <br/> <br/>", "Yes <br/> <br/>", "Row <br/> SUM", 
                "Row <br/> No %", "Row <br/> Yes %", 
                "Col <br/> No %", "Col <br/> Yes %", "Col <br/> Row SUM %") 
stopifnot(identical(ncol(hh), length(names_hh)))
stopifnot(nrow(hh) < 10)
#
cap_hh <- paste0("(C33T02) ", r_xyg, " vs. ", c_xyg)
```


```{r 'C33T02', echo=FALSE, ref.label=c('C33-CrossKbl')}
#
```

#### Bar Charts {.unlisted .unnumbered .tabset .tabset-fade}

- Bar Charts
  - Grouped 
    - These are good for comparing between each element in the categories, and comparing elements across categories.
  - Stacked 
    - They are great for showing the total because they visually aggregate all of the categories in a group. 
    - The downside is that it becomes harder to compare the sizes of the individual categories. 
    - Stacking also indicates a part to whole relationship.
  - Stacked Percent
    - The total quantity is hidden by using percentages, but it iss easier to see the relative difference between quantities in each group.
  - Summary
    - If there is no part to whole relationship (maybe there is overlap in the categories), then grouped
    - If there is a part to whole relationship, then the next question to ask is what relationship is the most important to show. 
      - If the goal is to show sizes between individual categories, use a grouped column or bar chart. 
      - If the goal is to show the total sizes of groups, use a regular stacked bar chart. 
      - If the goal is to show relative differences within each group, use a stacked percentage column chart.

```{conjecture 'stat-count-xy'}
\textcolor{brown}{Error: stat_count() can only have an x or y aesthetic.}
```

```{conjecture 'stat-count-y'}
\textcolor{brown}{Error: stat_count() must not be used with a y aesthetic.}
```

- \textcolor{pink}{stat_count()} Based on the data, choose stat 
  - stat = 'identity' tells ggplot2 that you will provide the y-values (i.e. Wide Data)
  - stat = 'count' is the default, which implies that ggplot2 will count the aggregate number of rows for each x value. (i.e. Long Data)

```{r 'C33-BarGroupIplan', include=FALSE}
# #Grouped Bar Chart
hh <- xyg %>% rename(Group = 1, SubGroup = 2, N = 3) %>% 
  group_by(Group) %>% 
  mutate(Ratio = paste0(round(100 * N/sum(N), 1), "%")) %>% ungroup()
#
cap_hh <- "C33P01"
ttl_hh <- "Churn: Grouped Bar: International Plan vs. Churn"
sub_hh <- NULL 
x_hh <- "International Plan" #r_xyg
y_hh <- "Frequency"
lgd_hh  <- "Churn" #c_xyg
#
C33 <- hh %>% { ggplot(., aes(x = Group, y = N, fill = SubGroup)) + 
    geom_bar(position = "dodge", stat = "identity", alpha = 1) + 
    geom_text(position = position_dodge(width = 1), aes(label = N), vjust = 1.5, 
              colour = rep(c("black", "white"), 2)) +
    scale_fill_viridis_d(direction = -1) +	
    theme(panel.grid.major.x = element_blank(), axis.line = element_blank(),
          panel.border = element_rect(colour = "black", fill = NA, size = 1),
          legend.position = c(.5, .95), 
          legend.box = "horizontal", legend.direction = "horizontal") +
    labs(x = x_hh, y = y_hh, fill = lgd_hh, 
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P01", "-Churn-Bar-Intl", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P01, device = "png", dpi = 144) 
}
```

```{r 'C33P01', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P01", "-Churn-Bar-Intl", ".png"))
```

```{r 'C33-BarStackIplan', include=FALSE}
# #Stacked Bar Chart
hh <- xyg %>% rename(Group = 1, SubGroup = 2, N = 3) %>% 
  group_by(Group) %>% 
  mutate(Ratio = paste0(round(100 * N/sum(N), 1), "%")) %>% ungroup()
#
cap_hh <- "C33P02"
ttl_hh <- "Churn: Stacked Bar: International Plan vs. Churn"
sub_hh <- NULL 
x_hh <- "International Plan"
y_hh <- "Frequency"
lgd_hh  <- "Churn"
#
C33 <- hh %>% { ggplot(., aes(x = Group, y = N, fill = SubGroup)) + 
    geom_bar(position = "stack", stat = "identity", alpha = 1) + 
    geom_text(position = position_stack(vjust = 0.5), aes(label = N), 
              colour = rep(c("black", "white"), 2)) +
    scale_fill_viridis_d(direction = -1) +	
    theme(panel.grid.major.x = element_blank(), axis.line = element_blank(),
          panel.border = element_rect(colour = "black", fill = NA, size = 1),
          legend.position = c(.9, .9)) +
    labs(x = x_hh, y = y_hh, fill = lgd_hh, 
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P02", "-Churn-Stack-Intl", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P02, device = "png", dpi = 144) 
}
```

```{r 'C33P02', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P02", "-Churn-Stack-Intl", ".png"))
```


```{r 'C33-BarPStackIplan', include=FALSE}
# #Percent Stacked Bar Chart
hh <- xyg %>% rename(Group = 1, SubGroup = 2, N = 3) %>% 
  group_by(Group) %>% 
  mutate(Ratio = paste0(round(100 * N/sum(N), 1), "%")) %>% ungroup()
#
cap_hh <- "C33P03"
ttl_hh <- "Churn: Percent Stacked Bar: International Plan vs. Churn"
sub_hh <- NULL 
x_hh <- "International Plan"
y_hh <- "Grouped Percentage"
lgd_hh  <- "Churn"
#
C33 <- hh %>% { ggplot(., aes(x = Group, y = N, fill = SubGroup)) + 
    geom_bar(position = "fill", stat = 'identity') + 
    geom_text(position = position_fill(vjust = 0.5), aes(label = Ratio), 
              colour = rep(c("black", "white"), 2)) +
    scale_fill_viridis_d(direction = -1) +	
    scale_y_continuous(labels = percent) +
    theme(panel.grid.major.x = element_blank(), axis.line = element_blank(),
          panel.border = element_rect(colour = "black", fill = NA, size = 1)) +
    labs(x = x_hh, y = y_hh, fill = lgd_hh, 
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P03", "-Churn-pStack-Intl", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P03, device = "png", dpi = 144) 
}
```

```{r 'C33P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P03", "-Churn-pStack-Intl", ".png"))
```

##### Image {.unlisted .unnumbered}

```{r 'C33P010203', echo=FALSE, out.width='33%', ref.label=c('C33P01', 'C33P02', 'C33P03'), fig.cap="(C33P01 C33P02 C33P03) International Plan holders tend to churn more frequently"}
#
```

##### Code CrossTab {.unlisted .unnumbered}

```{r 'C33-SetIntl-A', eval=FALSE, ref.label=c('C33-SetIntl')}
#
```


```{r 'C33-CrossTab', eval=FALSE}
# #IN: xyg (Predictor, Target), r_xyg, c_xyg 
# #Generate Contingency Table (CrossTab)
ctab <- xyg %>% 
  pivot_wider(names_from = Target, values_from = n, values_fill = 0, names_sort = TRUE) %>%
  #mutate(across(1, as.character)) %>% 
  add_row(summarise(., across(1, ~ "Total")), summarise(., across(where(is.numeric), sum))) %>% 
  mutate(SUM = rowSums(across(where(is.numeric)))) %>% 
  mutate(across(where(is.numeric) & !matches("SUM"), 
                list(r = ~ round(. /SUM, 3)), .names = "xx_{.fn}{.col}")) %>% 
  mutate(across(where(is.numeric) & !starts_with("xx_") & !matches("SUM"), 
                list(rp = ~ paste0(round(100 * . /SUM, 1), "%")), .names = "{.fn}{.col}")) %>%
  mutate(across(where(is.numeric) & !starts_with("xx_"), 
                list(c = ~ 2 * ./sum(.)), .names = "yy_{.fn}{.col}")) %>% 
  mutate(across(where(is.numeric) & !starts_with("xx_") & !starts_with("yy_"), 
                list(cp = ~ paste0(round(100 * 2 * . /sum(.), 1), "%")), .names = "{.fn}{.col}"))
```

##### Code CrossTab Print {.unlisted .unnumbered}


```{r 'C33-SetCrossIntl-A', eval=FALSE, ref.label=c('C33-SetCrossIntl')}
#
```


```{r 'C33-CrossKbl', eval=FALSE}
#
kbl(hh,
  caption = cap_hh,
  col.names = names_hh,
  escape = FALSE, align = "c", booktabs = TRUE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                html_font = "Consolas",	font_size = 12,
                full_width = FALSE,
                #position = "float_left",
                fixed_thead = TRUE
  ) %>%
# #Header Row Dark & Bold: RGB (48, 48, 48) =HEX (#303030)
	row_spec(0, color = "white", background = "#303030", bold = TRUE,
	         extra_css = "border-bottom: 1px solid; border-top: 1px solid"
	) #%>% row_spec(row = 1:nrow(hh), color = "black")
```

### Voice Mail Plan

```{r 'C33-SetVmail', include=FALSE}
# #xyw: x (independent), y (dependent), c (column =y), r (row =x)
# #g (grouped), w (wider), l (longer), 
r_xyg <- "VoiceMail"
c_xyg <- "Churn"
xyg <- bb %>% select(vmail_plan, churn) %>% rename(Predictor = 1, Target = 2) %>% 
  mutate(across(1:2, ~ifelse(., 'Yes', 'No'))) %>% 
  count(Predictor, Target) 
#
str(xyg)
```

```{r 'C33-CrossVmail', echo=FALSE, ref.label=c('C33-CrossTab')}
#
```


#### Contingency Table (CrossTab) {.unlisted .unnumbered}

- 16.7% of those without the Voice Mail Plan are churners, as compared to 8.7% of customers who do have the Voice Mail Plan. 
  - Thus, customers without the Voice Mail Plan are nearly twice as likely to churn as customers with the plan.
- Summary
  - Perhaps we should enhance our Voice Mail Plan still further, or make it easier for customers to join it, as an instrument for increasing customer loyalty
  - We should expect that, the model (in future) will probably include whether or not the customer selected the Voice Mail Plan. Our confidence in this expectation is perhaps not quite as high as for the International Plan

```{r 'C33-SetCrossVmail', include=FALSE}
hh <- ctab %>% select(-c(5:6, 9:11)) 
#names_hh <- names(hh)
names_hh <-c(paste0(c_xyg, " ", "$\\rightarrow$", "<br/>", "$\\downarrow$", " ", r_xyg), 
                "No <br/> <br/>", "Yes <br/> <br/>", "Row <br/> SUM", 
                "Row <br/> No %", "Row <br/> Yes %", 
                "Col <br/> No %", "Col <br/> Yes %", "Col <br/> Row SUM %") 
stopifnot(identical(ncol(hh), length(names_hh)))
stopifnot(nrow(hh) < 10)
#
cap_hh <- paste0("(C33T03) ", r_xyg, " vs. ", c_xyg)
```


```{r 'C33T03', echo=FALSE, ref.label=c('C33-CrossKbl')}
#
```

### Three Categorical Variables {.tabset .tabset-fade}

- Two-way interactions among categorical variables with respect to churn. 
  - Multilayer clustered bar chart of churn, clustered by both International Plan and Voice Mail Plan. 
    - There are many more customers who have neither plan. 
    - There are many more customers who have the voice mail plan only than have both plans. 
    - More importantly, among customers with no voice mail plan, the proportion of churners is greater for those who do have an international plan (43.7%) than for those who do not (13.9%).
    - Again, however, among customers with the voice mail plan, the proportion of churners is much greater for those who also select the international plan (39.1%) than for those who do not (5.3%).
    - Note also that there is no interaction among the categorical variables. That is, international plan holders have greater churn regardless of whether they are Voice Mail plan adopters or not.

```{r 'C33-ThreeCat', include=FALSE}
xsyg <- bb %>% select(int_l_plan, vmail_plan, churn) %>% 
  rename(Intl = 1, Vmail =2, Churn = 3) %>% 
  mutate(across(Churn, ~ifelse(., "Yes", "No"))) %>% 
  mutate(across(Intl, ~ifelse(., "I: Yes", "I: No"))) %>% 
  mutate(across(Vmail, ~ifelse(., "V: Yes", "V: No"))) %>% 
  count(Intl, Vmail, Churn) %>% rename(N = n)
```

```{r 'C33-BarMultiCatMultiFacet', include=FALSE}
# #Multilayer Clustered Bar Chart
hh <- xsyg
#
cap_hh <- "C33P04"
ttl_hh <- "Churn: Churn vs. International & Voice Mail Plans"
sub_hh <- NULL 
x_hh <- "Churn" #r_xyg
y_hh <- "Frequency"
lgd_hh  <- "Churn" #c_xyg
#
C33 <- hh %>% { ggplot(., aes(x = Churn, y = N, fill = Churn)) + 
    geom_bar(position = "dodge", stat = "identity", alpha = 1) + 
    geom_text(position = position_stack(vjust = 0.5), aes(label = N)) +
    facet_wrap(Intl ~ Vmail, nrow = 1) +
    scale_fill_manual(values = c('#FFEA46FF', '#787877FF')) +
    theme(panel.grid.major.x = element_blank(), axis.line = element_blank(),
          panel.border = element_rect(colour = "black", fill = NA, size = 1),
          legend.position = 'top', 
          legend.box = "horizontal", legend.direction = "horizontal") +
    labs(x = x_hh, y = y_hh, fill = lgd_hh, 
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P04", "-Churn-MultiClusterBar", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P04, device = "png", dpi = 144) 
}
```

```{r 'C33P04', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P04", "-Churn-MultiClusterBar", ".png"))
```


```{r 'C33-BarMultiCatSingleFacet', include=FALSE}
# #Multilayer Clustered Bar Chart
hh <- xsyg
#
cap_hh <- "C33P05"
ttl_hh <- "Churn: Churn vs. International & Voice Mail Plans"
sub_hh <- NULL 
x_hh <- NULL #"Churn" #r_xyg
y_hh <- "Frequency"
lgd_hh  <- "Churn" #c_xyg
#
C33 <- hh %>% { ggplot(., aes(x = Vmail, y = N, fill = Churn)) + 
    geom_bar(position = "dodge", stat = "identity", alpha = 1) + 
    geom_text(position = position_dodge(width = 1), aes(label = N), vjust = 1.5) +
    facet_wrap(~Intl, nrow = 1) +
    scale_fill_manual(values = c('#FFEA46FF', '#787877FF')) +
    theme(panel.grid.major.x = element_blank(), axis.line = element_blank(),
          panel.border = element_rect(colour = "black", fill = NA, size = 1),
          legend.position = 'top', 
          legend.box = "horizontal", legend.direction = "horizontal") +
    labs(x = NULL, y = y_hh, fill = lgd_hh, 
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P05", "-Churn-MultiClusterBarSingleFacet", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P05, device = "png", dpi = 144) 
}
```

```{r 'C33P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P05", "-Churn-MultiClusterBarSingleFacet", ".png"))
```

#### Image {.unlisted .unnumbered}

```{r 'C33P0405', echo=FALSE, ref.label=c('C33P04', 'C33P05'), fig.cap="(C33P04 C33P05) Churn vs. International & Voice Mail Plans (Both are Same Graphs)"}
#
```


```{r 'C33-BarPStackIplanMulti', include=FALSE}
# #Percent Stacked Bar Chart
hh <- xsyg %>% group_by(Vmail, Intl) %>% 
    mutate(Ratio = paste0(round(100 * N/sum(N), 1), "%")) %>% ungroup()
#
cap_hh <- "C33P06"
ttl_hh <- "Churn: Churn vs. International & Voice Mail Plans"
sub_hh <- NULL 
x_hh <- "Voice Mail"
y_hh <- "Grouped Percentage"
lgd_hh  <- "Churn"
#
C33 <- hh %>% { ggplot(., aes(x = Vmail, y = N, fill = Churn)) + 
        geom_bar(position = "fill", stat = 'identity') + 
        facet_wrap(~Intl, nrow = 1) +
        geom_text(position = position_fill(vjust = 0.5), aes(label = Ratio), 
              colour = rep(c("black", "white"), 4)) +
        scale_fill_viridis_d(direction = -1) +	
        scale_y_continuous(labels = percent) +
        theme(panel.grid.major.x = element_blank(), axis.line = element_blank(),
              panel.border = element_rect(colour = "black", fill = NA, size = 1)) +
        labs(x = x_hh, y = y_hh, fill = lgd_hh, 
            subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P06", "-Churn-pStack-Multi", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P06, device = "png", dpi = 144) 
}
```

```{r 'C33P06', echo=FALSE, fig.cap="(C33P06) Churn vs. International & Voice Mail Plans (Proportional Stack)"}
knitr::include_graphics(paste0(.z$PX, "C33P06", "-Churn-pStack-Multi", ".png"))
```


#### Code {.unlisted .unnumbered}

```{r 'C33-ThreeCat-A', eval=FALSE, ref.label=c('C33-ThreeCat')}
#
```

```{r 'C33-BarMultiCatMultiFacet-A', eval=FALSE, ref.label=c('C33-BarMultiCatMultiFacet')}
#
```

```{r 'C33-BarMultiCatSingleFacet-A', eval=FALSE, ref.label=c('C33-BarMultiCatSingleFacet')}
#
```

#### Code More {.unlisted .unnumbered}

```{r 'C33-BarPStackIplanMulti-A', eval=FALSE, ref.label=c('C33-BarPStackIplanMulti')}
#
```

## Exploring Numeric Variables

- Refer figure \@ref(fig:C33P07) & Table \@ref(tab:C33T01)
  - Fields not showing evidence of symmetry include voice mail messages and customer service calls.
  - Voice Mail Messages: Median = 0 
    - Indicating that at least half of all customers had no voice mail messages. 
    - Because fewer than half of the customers select the Voice Mail Plan (27.7%). 
  - Customer Service Calls: Mean = 1.56 > Median = 1
    - It indicates some right-skewness
    - Also indicated by the maximum number of customer service calls (9).

```{r 'C33P07', echo=FALSE, ref.label=c('B18P03'), fig.cap="(B18P03) Churn: All Histograms"}
# #Ref another file chunk
```


### Service Calls {.tabset .tabset-fade}

- Overlay Histogram
  - To explore whether a predictor is useful for predicting the target variable
  - Bars are colored according to the values of the target variable. 
  - Normalised Histogram
    - Proportions stretched out to enable better contrast
      - Normalized histograms are useful for teasing out the relationship between a numerical predictor and the target. However, data analysts should always provide the companion a non-normalized histogram along with the normalized histogram, because the normalized histogram does not provide any information on the frequency distribution of the variable.
      - Ex: The churn rate for customers logging nine service calls is 100%; but there are only two customers with this number of calls.
    - Customers who have called customer service three times or less have a markedly lower churn rate  than customers who have called customer service four or more times.
- Summary
  - By the third service call, specialized incentives should be offered to retain customer loyalty, because, by the fourth call, the probability of churn increases greatly
  - We should expect that, the model (in future) will probably include the number of customer service calls made by the customer.
  
```{r 'C33-ServCall', include=FALSE}
ii <- bb %>% select(custserv_calls, churn) %>% 
  rename(Churn = 2) %>% 
  mutate(across(Churn, ~ifelse(., "Yes", "No")))
```

```{r 'C33-HistServCall', include=FALSE}
# #Histogram Default (Not useful for association of predictor and target)
hh <- ii
ttl_hh <- "Churn: Customer Service Calls"
cap_hh <- "C33P08"
sub_hh <- "Predictor Only" 
x_hh <- "x"
y_hh <- "Frequency"
lgd_hh  <- "Churn"
#
C33 <- hh %>% { ggplot(data = ., mapping = aes(x = custserv_calls, fill = '#FDE725FF')) + 
    geom_histogram(bins = length(unique(.[[1]])), alpha = 1) + 
    #stat_bin(bins = length(unique(.[[1]])), aes(y=..count.., label=..count..), 
    #         geom="text", position=position_stack(vjust=0.5)) +
    scale_x_continuous(breaks = breaks_pretty()) + 
    scale_fill_viridis_d(direction = -1) +
    theme(plot.title.position = "panel", 
          axis.title.x = element_blank(), 
          #legend.position = c(0.5, -0.08), legend.direction = 'horizontal', 
          legend.position = 'none') +
    labs(x = x_hh, y = y_hh, #fill = lgd_hh,
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P08-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P08", "-Churn-Hist-ServCall", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P08, device = "png", dpi = 144) 
}
```

```{r 'C33P08', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P08", "-Churn-Hist-ServCall", ".png"))
```

```{r 'C33-HistServCallChurn', include=FALSE}
# #Histogram (Predictor and Target Count)
hh <- ii
ttl_hh <- "Churn: Customer Service Calls & Churn"
cap_hh <- "C33P09"
sub_hh <- "Count of Predictor & Target" 
x_hh <- "x"
y_hh <- "Frequency"
lgd_hh  <- "Churn"
#
C33 <- hh %>% { ggplot(data = ., mapping = aes(x = custserv_calls, fill = Churn)) + 
    geom_histogram(bins = length(unique(.[[1]])), alpha = 1) + 
    #stat_bin(bins = length(unique(.[[1]])), aes(y=..count.., label=..count..), 
    #         geom="text", position=position_stack(vjust=0.5)) +
    scale_x_continuous(breaks = breaks_pretty()) + 
    scale_fill_viridis_d(direction = -1) +
    theme(plot.title.position = "panel", 
          axis.title.x = element_blank(), 
          legend.position = c(0.5, -0.07), legend.direction = 'horizontal') +
    labs(x = x_hh, y = y_hh, fill = lgd_hh,
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P09-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P09", "-Churn-Hist-ServCall-Churn", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P09, device = "png", dpi = 144) 
}
```

```{r 'C33P09', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P09", "-Churn-Hist-ServCall-Churn", ".png")) 
```


```{r 'C33-pHistServCallChurn', include=FALSE}
# #Histogram (Predictor and Target Proportion)
hh <- ii
ttl_hh <- "Churn: Customer Service Calls & Churn"
cap_hh <- "C33P10"
sub_hh <- "Proportion of Predictor & Target (Using Histogram)" 
x_hh <- "x"
y_hh <- "Grouped Percentage"
lgd_hh  <- "Churn"
#
# #NOTES: Group wise Proportion i.e. Yes 1, No 1 (Not Each Bin wise)
# #Adding Density automatically converts the y-axis to 'frequency density' not to 'percentage'
# #Both will match when Bin Width =1 but otherwise there will be a mismatch
# #Using y=..density.. scales the histograms so the area under each is 1, or sum(binwidth*y)=1. 
# #So use y = binwidth *..density.. to have y represent the fraction of the total in each bin. 
# #OR aes(y = stat(width*density))
# #OR aes(y = stat(count / sum(count)))
# #Clarification
# # ..count../sum(..count..) each count is divided by the total count
# # ..density.. it is applied to each group independently
#
C33 <- hh %>% { ggplot(data = ., mapping = aes(x = custserv_calls, fill = Churn)) + 
    geom_histogram(bins = length(unique(.[[1]])), alpha = 1, position = 'fill') + 
    #stat_bin(bins = length(unique(.[[1]])), 
    #         aes(y = c(..count..[..group..==1]/sum(..count..[..group..==1]),
    #                   ..count..[..group..==2]/sum(..count..[..group..==2])), 
    #             label=round(..density.., 2)), geom="text") +
    #stat_bin(bins = length(unique(.[[1]])), 
    #         aes(y=..density.., group = ..group.., 
    #             label=round(..density.., 2)), geom="text") +
    scale_x_continuous(breaks = breaks_pretty()) + 
		#Deprecated : percent_format()
		scale_y_continuous(labels = percent) +
    scale_fill_viridis_d(direction = -1) +
    theme(plot.title.position = "panel", 
          axis.title.x = element_blank(), 
          legend.position = c(0.5, -0.07), legend.direction = 'horizontal') +
    labs(x = x_hh, y = y_hh, fill = lgd_hh,
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P10-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P10", "-Churn-pHist-ServCall-Churn", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P10, device = "png", dpi = 144) 
}
```

```{r 'C33P10', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P10", "-Churn-pHist-ServCall-Churn", ".png"))
```

#### Image {.unlisted .unnumbered}

```{r 'C33P080910', echo=FALSE, out.width='33%', ref.label=c('C33P08', 'C33P09', 'C33P10'), fig.cap="(C33P08 C33P09 C33P10) Service Calls beyond 3 have signficant increase in Churn"}
#
```


```{r 'C33-pBarServCallChurn', include=FALSE}
# #complete() can be used to add the missing combination but not using it for now
# #NOTE: Bar is easier to include Labels, However Histogram is easier for large number of bins
xyg  <- ii %>% count(custserv_calls, Churn) 
     #%>% complete(custserv_calls, Churn, fill = list(n = 0)) 
hh <- xyg %>% rename(Group = 1, SubGroup = 2, N = 3) %>% 
  group_by(Group) %>% 
  mutate(Ratio = paste0(round(100 * N/sum(N), 1), "%")) %>% ungroup()
#
ttl_hh <- "Churn: Customer Service Calls & Churn"
cap_hh <- "C33P11"
sub_hh <- "Proportion of Predictor & Target (Using Bar)" 
x_hh <- "x"
y_hh <- "Grouped Percentage"
lgd_hh  <- "Churn"
#
C33 <- hh %>% { ggplot(., aes(x = Group, y = N, fill = SubGroup)) + 
    geom_bar(position = "fill", stat = 'identity') + 
    geom_text(position = position_fill(vjust = 0.5), 
              aes(label = ifelse(SubGroup == 'No', "", Ratio)), 
              colour = c(rep(c("black", "white"), 9), "white")) +
    scale_fill_viridis_d(direction = -1) +
    scale_x_continuous(breaks = breaks_pretty()) +
    scale_y_continuous(labels = percent) +
    theme(plot.title.position = "panel", axis.title.x = element_blank(), 
          panel.grid.major.x = element_blank(), axis.line = element_blank(),
          panel.border = element_rect(colour = "black", fill = NA, size = 1),
          legend.position = c(0.5, -0.07), legend.direction = 'horizontal') +
    labs(x = x_hh, y = y_hh, fill = lgd_hh, 
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P11-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P11", "-Churn-pBar-ServCall-Churn", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P11, device = "png", dpi = 144) 
}
```

```{r 'C33P11', echo=FALSE, fig.cap="(C33P11) Churn vs. Service Call Proportions (Bar)"}
knitr::include_graphics(paste0(.z$PX, "C33P11", "-Churn-pBar-ServCall-Churn", ".png")) 
```

#### Code {.unlisted .unnumbered}

```{r 'C33-ServCall-A', eval=FALSE, ref.label=c('C33-ServCall', 'C33-HistServCall', 'C33-HistServCallChurn', 'C33-pHistServCallChurn')}
#
```

#### Code More {.unlisted .unnumbered}

```{r 'C33-pBarServCallChurn-A', eval=FALSE, ref.label=c('C33-pBarServCallChurn')}
#
```

### Day Minutes 

- Summary: Day Minutes
  - High day-users tend to churn at a higher rate. 
  - As the number of day minutes passes 200, we should consider special incentives
  - We should investigate why heavy day-users are tempted to leave
  - We should expect that, the model (in future) will probably include 'day minutes'.

```{r 'C33-pHistDayMins', include=FALSE}
# #Day Minutes
ii <- bb %>% select(day_mins, churn) %>% 
  rename(Predictor = 1, Target = 2) %>% 
  mutate(across(Target, ~ifelse(., "Yes", "No")))
```

```{r 'C33-HistDayMinsChurn', include=FALSE}
# #Histogram (Predictor and Target Count)
hh <- ii
ttl_hh <- "Churn: Day Minutes & Churn"
cap_hh <- "C33P12"
sub_hh <- "Count of Predictor & Target" 
x_hh <- "x"
y_hh <- "Frequency"
lgd_hh  <- "Churn"
```

```{r 'C33-HistDayMinsChurn-A', include=FALSE, ref.label=c('C33-HistDayMinsChurn', 'C33-Hist')}
#
```

```{r 'C33P12-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P12", "-Churn-Hist-DayMins-Churn", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P12, device = "png", dpi = 144) 
}
```

```{r 'C33P12', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P12", "-Churn-Hist-DayMins-Churn", ".png")) 
```

```{r 'C33-pHistDayMinsChurn', include=FALSE, eval=FALSE}
# #Histogram (Predictor and Target Proportion)
hh <- ii
ttl_hh <- "Churn: Day Minutes & Churn"
cap_hh <- "C33P13"
sub_hh <- "Proportion of Predictor & Target" 
x_hh <- "x"
y_hh <- "Grouped Percentage"
lgd_hh  <- "Churn"
```

```{r 'C33-pHistDayMinsChurn-A', include=FALSE, ref.label=c('C33-pHistDayMinsChurn', 'C33-pHist')}
#
```

```{r 'C33P13-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P13", "-Churn-pHist-DayMins-Churn", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P13, device = "png", dpi = 144) 
}
```

```{r 'C33P13', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P13", "-Churn-pHist-DayMins-Churn", ".png"))
```

```{r 'C33P1213', echo=FALSE, ref.label=c('C33P12', 'C33P13'), fig.cap="(C33P12 C33P13) Higher Day Minutes (>200) have higher Churn"}
#
```


```{r 'C33-Hist', include=FALSE, eval=FALSE}
C33 <- hh %>% { ggplot(data = ., mapping = aes(x = Predictor, fill = Target)) + 
    geom_histogram(bins = ifelse(length(unique(.[[1]])) > 50, 50, length(unique(.[[1]]))), 
                   alpha = 1) + 
    scale_x_continuous(breaks = breaks_pretty()) + 
    scale_fill_viridis_d(direction = -1) +
    theme(plot.title.position = "panel", 
          axis.title.x = element_blank(), 
          legend.position = c(0.5, -0.07), legend.direction = 'horizontal') +
    labs(x = x_hh, y = y_hh, fill = lgd_hh,
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```


```{r 'C33-pHist', include=FALSE, eval=FALSE}
# #Histogram (Predictor and Target Proportion)
# #Caution: Warning on Removal of Missing Values has been removed
C33 <- hh %>% { ggplot(data = ., mapping = aes(x = Predictor, fill = Target)) + 
    geom_histogram(bins = ifelse(length(unique(.[[1]])) > 50, 50, length(unique(.[[1]]))), 
                   alpha = 1, position = 'fill', na.rm = TRUE) + 
    scale_x_continuous(breaks = breaks_pretty()) + 
		scale_y_continuous(labels = percent) +
    scale_fill_viridis_d(direction = -1) +
    theme(plot.title.position = "panel", 
          axis.title.x = element_blank(), 
          legend.position = c(0.5, -0.07), legend.direction = 'horizontal') +
    labs(x = x_hh, y = y_hh, fill = lgd_hh,
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

### Evening Minutes 

- Summary : Evening Minutes
  - It shows a slight tendency for customers with higher evening minutes to churn. 
  - However, it is inconclusive solely based on graph. 

```{r 'C33-pHistEveMins', include=FALSE}
# #Evening Minutes
ii <- bb %>% select(eve_mins, churn) %>% 
  rename(Predictor = 1, Target = 2) %>% 
  mutate(across(Target, ~ifelse(., "Yes", "No")))
```

```{r 'C33-HistEveMinsChurn', include=FALSE}
# #Histogram (Predictor and Target Count)
hh <- ii
ttl_hh <- "Churn: Evening Minutes & Churn"
cap_hh <- "C33P14"
sub_hh <- "Count of Predictor & Target" 
x_hh <- "x"
y_hh <- "Frequency"
lgd_hh  <- "Churn"
```

```{r 'C33-HistEveMinsChurn-A', include=FALSE, ref.label=c('C33-HistEveMinsChurn', 'C33-Hist')}
#
```

```{r 'C33P14-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P14", "-Churn-Hist-EveMins-Churn", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P14, device = "png", dpi = 144) 
}
```

```{r 'C33P14', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P14", "-Churn-Hist-EveMins-Churn", ".png")) 
```

```{r 'C33-pHistEveMinsChurn', include=FALSE, eval=FALSE}
# #Histogram (Predictor and Target Proportion)
hh <- ii
ttl_hh <- "Churn: Evening Minutes & Churn"
cap_hh <- "C33P15"
sub_hh <- "Proportion of Predictor & Target" 
x_hh <- "x"
y_hh <- "Grouped Percentage"
lgd_hh  <- "Churn"
```

```{r 'C33-pHistEveMinsChurn-A', include=FALSE, ref.label=c('C33-pHistEveMinsChurn', 'C33-pHist')}
#
```

```{r 'C33P15-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P15", "-Churn-pHist-EveMins-Churn", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P15, device = "png", dpi = 144) 
}
```

```{r 'C33P15', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P15", "-Churn-pHist-EveMins-Churn", ".png"))
```

```{r 'C33P1415', echo=FALSE, ref.label=c('C33P14', 'C33P15'), fig.cap="(C33P14 C33P15) (Inconclusive) Slight tendency to Churn with higher Evening Minutes"}
#
```

### Night Minutes 

- Summary : Night Minutes
  - There is no obvious association visible (in graphs) between churn and night minutes 
  - The lack of obvious association at the EDA stage between a predictor and a target variable is not sufficient reason to omit that predictor from the model. 
  - Unless there is a good reason for eliminating the variable before modeling, we should keep them and allow the modeling process to identify which variables are predictive and which are not.

```{r 'C33-pHistNightMins', include=FALSE}
# #Night Minutes
ii <- bb %>% select(night_mins, churn) %>% 
  rename(Predictor = 1, Target = 2) %>% 
  mutate(across(Target, ~ifelse(., "Yes", "No")))
```

```{r 'C33-HistNightMinsChurn', include=FALSE}
# #Histogram (Predictor and Target Count)
hh <- ii
ttl_hh <- "Churn: Night Minutes & Churn"
cap_hh <- "C33P16"
sub_hh <- "Count of Predictor & Target" 
x_hh <- "x"
y_hh <- "Frequency"
lgd_hh  <- "Churn"
```

```{r 'C33-HistNightMinsChurn-A', include=FALSE, ref.label=c('C33-HistNightMinsChurn', 'C33-Hist')}
#
```

```{r 'C33P16-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P16", "-Churn-Hist-NightMins-Churn", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P16, device = "png", dpi = 144) 
}
```

```{r 'C33P16', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P16", "-Churn-Hist-NightMins-Churn", ".png")) 
```

```{r 'C33-pHistNightMinsChurn', include=FALSE, eval=FALSE}
# #Histogram (Predictor and Target Proportion)
hh <- ii
ttl_hh <- "Churn: Night Minutes & Churn"
cap_hh <- "C33P17"
sub_hh <- "Proportion of Predictor & Target" 
x_hh <- "x"
y_hh <- "Grouped Percentage"
lgd_hh  <- "Churn"
```

```{r 'C33-pHistNightMinsChurn-A', include=FALSE, ref.label=c('C33-pHistNightMinsChurn', 'C33-pHist')}
#
```

```{r 'C33P17-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P17", "-Churn-pHist-NightMins-Churn", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P17, device = "png", dpi = 144) 
}
```

```{r 'C33P17', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P17", "-Churn-pHist-NightMins-Churn", ".png"))
```

```{r 'C33P1617', echo=FALSE, ref.label=c('C33P16', 'C33P17'), fig.cap="(C33P16 C33P17) No obvious association between Churn and Night Minutes"}
#
```

### International Calls {.tabset .tabset-fade}

- Summary : International Calls
  - There is no obvious association visible (in graphs) between churn and International Calls
  - However: t-test for the difference in mean number of international calls for churners and  non-churners is statistically significant. i.e. Means are Different.
  - NOTE: Hypothesis Testing is NOT part of EDA. It is mentioned here to show that statistically significant association may exist without being obviously visible.

```{r 'C33-pHistIcalls', include=FALSE}
# #International Calls
ii <- bb %>% select(intl_calls, churn) %>% 
  rename(Predictor = 1, Target = 2) %>% 
  mutate(across(Target, ~ifelse(., "Yes", "No")))
```

```{r 'C33-HistIcallsChurn', include=FALSE}
# #Histogram (Predictor and Target Count)
hh <- ii
ttl_hh <- "Churn: International Calls & Churn"
cap_hh <- "C33P20"
sub_hh <- "Count of Predictor & Target" 
x_hh <- "x"
y_hh <- "Frequency"
lgd_hh  <- "Churn"
```

```{r 'C33-HistIcallsChurn-A', include=FALSE, ref.label=c('C33-HistIcallsChurn', 'C33-Hist')}
#
```

```{r 'C33P20-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P20", "-Churn-Hist-Icalls-Churn", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P20, device = "png", dpi = 144) 
}
```

```{r 'C33P20', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P20", "-Churn-Hist-Icalls-Churn", ".png")) 
```

```{r 'C33-pHistIcallsChurn', include=FALSE, eval=FALSE}
# #Histogram (Predictor and Target Proportion)
hh <- ii
ttl_hh <- "Churn: International Calls & Churn"
cap_hh <- "C33P21"
sub_hh <- "Proportion of Predictor & Target" 
x_hh <- "x"
y_hh <- "Grouped Percentage"
lgd_hh  <- "Churn"
```

```{r 'C33-pHistIcallsChurn-A', include=FALSE, ref.label=c('C33-pHistIcallsChurn', 'C33-pHist')}
#
```

```{r 'C33P21-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P21", "-Churn-pHist-Icalls-Churn", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P21, device = "png", dpi = 144) 
}
```

```{r 'C33P21', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P21", "-Churn-pHist-Icalls-Churn", ".png"))
```

#### Image {.unlisted .unnumbered}

```{r 'C33P2021', echo=FALSE, ref.label=c('C33P20', 'C33P21'), fig.cap="(C33P20 C33P21) No obvious association between Churn and International Calls (But t-test)"}
#
```

#### t-test {.unlisted .unnumbered}

```{r 'C33-IcallTestT'}
# #t-test for difference in Mean of Target Values
str(ii)
# 
# #Variance
var_ii <- var.test(formula = Predictor ~ Target, data = ii)
var_ii
#
isVarEqual <- ifelse(var_ii$p.value > 0.05, TRUE, FALSE)
if(isVarEqual) print("Variances are Equal.") else print("Variances are Different.")
#
# #t-test: Welch 
ha_ii <- "two.sided" #"two.sided", "less", "greater"
#tt_ii <- t.test(Predictor ~ Target, data = ii, alternative = ha_ii, var.equal = isVarEqual)
tt_ii <- t.test(Predictor ~ Target, data = ii, alternative = ha_ii, var.equal = FALSE)
tt_ii
#
alpha <- 0.05
if(any(all(ha_ii == "two.sided", tt_ii$p.value >= alpha / 2), 
       all(ha_ii != "two.sided", tt_ii$p.value >= alpha))) {
    print("Failed to reject H0.")
} else { 
    print("H0 Rejected.")
}
```

### All Histograms {.tabset .tabset-fade}

```{r 'C33-AllNumeric', include=FALSE}
# #All Continuous Predictors and Target (Categorical)
xsy <- bb %>% 
  select(where(is.numeric) | "churn") %>% 
  select(!area_code) %>% 
  relocate(ends_with("_mins")) %>% 
  relocate(ends_with("_calls")) %>% 
  relocate(vmail_message, .after =  last_col()) %>% 
  relocate("churn") %>% rename("Target" = 1) %>% 
  mutate(across(Target, ~ifelse(., "Yes", "No"))) %>% 
  mutate(across(Target, factor, levels = unique(Target)))
#
xsyl <- xsy %>% pivot_longer(where(is.numeric), names_to = "Predictors", values_to = "Values") %>% 
  mutate(across(Predictors, ~ factor(., levels = unique(Predictors))))
#
#str(ii)
```

```{r 'C33-AllHistograms', include=FALSE}
# #Histogram
hh <- xsyl
ttl_hh <- "Churn: Histograms of All Predictors with Target (Count)"
cap_hh <- "C33P18"
sub_hh <- NULL #"Count of Predictor & Target" 
x_hh <- NULL # "x"
y_hh <- NULL # "Frequency" #"Grouped Percentage"
lgd_hh  <- "Churn"
#
C33 <- hh %>% { ggplot(data = ., mapping = aes(x = Values, fill = Target)) + 
    geom_histogram(alpha = 1, boundary = 0, #position = position_stack(reverse = TRUE),
        bins = ifelse(length(unique(.$Predictors)) > 50, 50, length(unique(.$Predictors)))) + 
    #geom_histogram(alpha = 1, boundary = 0,
    #    bins = ifelse(nrow(distinct(.[2])) > 50, 50, nrow(distinct(.[2])))) + 
    facet_wrap(~Predictors, nrow = 3, scales = 'free') +
    scale_x_continuous(breaks = breaks_pretty()) + 
    scale_fill_viridis_d(direction = -1) +
    theme(plot.title.position = "panel", 
          strip.text.x = element_text(size = 10, colour = "white"), 
          axis.title.x = element_blank(), 
          legend.position = c(0.5, -0.07), legend.direction = 'horizontal') +
    labs(x = x_hh, y = y_hh, fill = lgd_hh, 
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P18-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P18", "-Churn-Hist-All-Churn", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P18, device = "png", dpi = 144, width = k_width, height = k_height) 
}
```

```{r 'C33-AllpHistograms', include=FALSE}
# #Histogram
hh <- xsyl
ttl_hh <- "Churn: Histograms of All Predictors with Target (Proportion)"
cap_hh <- "C33P19"
sub_hh <- NULL #"Count of Predictor & Target" 
x_hh <- NULL # "x"
y_hh <- NULL # "Frequency" #"Grouped Percentage"
lgd_hh  <- "Churn"
#
# #Caution: Warning on Removal of Missing Values has been removed
# #NOTE: position_fill() normalizes the Bars and is same as 'fill'
# # 
C33 <- hh %>% { ggplot(data = ., mapping = aes(x = Values, fill = Target)) + 
    geom_histogram(alpha = 1, boundary = 0, position = position_fill(), na.rm = TRUE, 
        bins = ifelse(length(unique(.$Predictors)) > 50, 50, length(unique(.$Predictors)))) + 
    #geom_histogram(alpha = 1, boundary = 0,
    #    bins = ifelse(nrow(distinct(.[2])) > 50, 50, nrow(distinct(.[2])))) + 
    facet_wrap(~Predictors, nrow = 3, scales = 'free_x') +
    scale_x_continuous(breaks = breaks_pretty()) + 
    scale_fill_viridis_d(direction = -1) +
    theme(plot.title.position = "panel", 
          strip.text.x = element_text(size = 10, colour = "white"), 
          axis.title.x = element_blank(), 
          legend.position = c(0.5, -0.07), legend.direction = 'horizontal') +
    labs(x = x_hh, y = y_hh, fill = lgd_hh, 
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P19-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P19", "-Churn-pHist-All-Churn", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P19, device = "png", dpi = 144, width = k_width, height = k_height) 
}
```

#### Images {.unlisted .unnumbered}

```{r 'C33P18', echo=FALSE, out.width='100%', fig.cap="(C33P18) Histograms of All Predictors with Target (Count)"}
knitr::include_graphics(paste0(.z$PX, "C33P18", "-Churn-Hist-All-Churn", ".png"))
```

```{r 'C33P19', echo=FALSE, out.width='100%', fig.cap="(C33P19) Histograms of All Predictors with Target (Proportion)"}
knitr::include_graphics(paste0(.z$PX, "C33P19", "-Churn-pHist-All-Churn", ".png"))
```

#### Code {.unlisted .unnumbered}

```{r 'C33-AllNumeric-A', eval=FALSE, ref.label=c('C33-AllNumeric', 'C33-AllHistograms', 'C33-AllpHistograms')}
#
```

## Exploring Multivariate Variables : Scatter plots

### Day minutes & Evenings minutes  {.tabset .tabset-fade}

- Summary:
  - Customers with both high day minutes and high evening minutes, appear to have a higher proportion of churners than records below the line. 

#### Image {.unlisted .unnumbered}

```{r 'C33-MinEveDay', include=FALSE}
ii <- bb %>% select(day_mins, eve_mins, churn)  %>% 
  rename(Target = churn) %>% 
  mutate(across(Target, ~ifelse(., "Yes", "No")))
```

```{r 'C33-ScatterMinEveDay', include=FALSE}
hh <- ii
ttl_hh <- "Churn: Scatterplot of Evening Minutes and Day Minutes"
cap_hh <- "C33P22"
sub_hh <- NULL #subtitle = TeX(r"(Trendline Equation, $R^{2}$, $\bar{x}$ and $\bar{y}$)")
x_hh <- "Evening Minutes" # "x"
y_hh <- "Day Minutes" # "Frequency" #"Grouped Percentage"
lgd_hh  <- "Churn"
#
C33 <- hh %>% { ggplot(data = ., aes(x = eve_mins, y = day_mins)) + 
    #geom_smooth(method = 'lm', formula = k_gg_formula, se = FALSE) +
    #geom_point(position = "jitter", aes(colour = Target)) +
    geom_jitter(aes(colour = Target)) +
		scale_colour_viridis_d(alpha = 0.9, direction = -1) +
    theme(panel.grid.minor = element_blank(),
          panel.border = element_blank()) +
    labs(x = x_hh, y = y_hh, colour = lgd_hh,
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P22-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P22", "-Churn-Scatter-MinEveDay", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P22, device = "png", dpi = 144) 
}
```

```{r 'C33P22', echo=FALSE, fig.cap="(C33P22) Scatterplot of Evening Minutes (X) and Day Minutes (Y) shows a clear separation for Churn at High X and High Y"}
knitr::include_graphics(paste0(.z$PX, "C33P22", "-Churn-Scatter-MinEveDay", ".png"))
```

#### Code {.unlisted .unnumbered}

```{r 'C33-MinEveDay-A', eval=FALSE, ref.label=c('C33-MinEveDay', 'C33-ScatterMinEveDay')}
#
```

### Day minutes & Customer Service Calls  {.tabset .tabset-fade}

- Summary:
  - It indicates a high-churn area in the upper left section of the graph. These records represent customers who have a combination of a high number of customer service calls and a low number of day minutes used. 
  - Note that this group of customers could not have been identified had we restricted ourselves to univariate exploration (exploring variable by single variable). This is because of the interaction between the variables. 
  - In general, customers with higher numbers of customer service calls tend to churn at a higher rate, as we learned earlier in the univariate analysis. However, of these customers with high numbers of customer service calls, those who also have high day minutes are somewhat "protected" from this high churn rate. The customers in the upper right of the scatter plot exhibit a lower churn rate than those in the upper left.

#### Image {.unlisted .unnumbered}

```{r 'C33-ServCallDayMins', include=FALSE}
ii <- bb %>% select(day_mins, custserv_calls, churn)  %>% 
  rename(Target = churn) %>% 
  mutate(across(Target, ~ifelse(., "Yes", "No")))
```

```{r 'C33-ScatterServCallDayMins', include=FALSE}
hh <- ii
ttl_hh <- "Churn: Day Minutes and Customer Service Calls"
cap_hh <- "C33P27"
sub_hh <- NULL
x_hh <- "Day Minutes"
y_hh <- "Customer Service Calls"
lgd_hh  <- "Churn"
#
C33 <- hh %>% { ggplot(data = ., aes(x = day_mins, y = custserv_calls)) + 
    geom_jitter(aes(colour = Target), width = 0.1, height = 0.1) +
    scale_colour_viridis_d(alpha = 0.9, direction = -1) +
    scale_y_continuous(breaks = breaks_pretty()) + 
    theme(panel.grid.minor = element_blank(),
          panel.border = element_blank()) +
    labs(x = x_hh, y = y_hh, colour = lgd_hh,
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P27-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P27", "-Churn-Scatter-ServCallDayMins", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P27, device = "png", dpi = 144) 
}
```

```{r 'C33P27', echo=FALSE, fig.cap="(C33P27) Scatterplot of Day Minutes (X) and Customer Service Calls (Y) shows an interaction effect for Churn"}
knitr::include_graphics(paste0(.z$PX, "C33P27", "-Churn-Scatter-ServCallDayMins", ".png")) #iiii
```

#### Code {.unlisted .unnumbered}

```{r 'C33-ServCallDayMins-A', eval=FALSE, ref.label=c('C33-ServCallDayMins', 'C33-ScatterServCallDayMins')}
#
```


### package:GGally {.tabset .tabset-fade}

#### SPLOM {.unlisted .unnumbered}

- Scatter Plot of Matrices (SPLOM), with bivariate scatter plots below the diagonal, histograms on the diagonal, and the Pearson correlation above the diagonal.

```{r 'C33-GGally-A7', include=FALSE}
hh <- xsy %>% select(1:6, 11, 16) 
ttl_hh <- "GGally: Churn: SPLOM: Calls, Account Length & Voice Mail (7)"
cap_hh <- "C33P23"
sub_hh <- NULL #"Count of Predictor & Target" 
lgd_hh  <- "Churn"
#
```

```{r 'C33-GGally-A7-A', ref.label=c('C33-GGally-A7', 'C33-GGallyHelpers', 'C33-GGallyBiVariate'), include=FALSE}
#
```

```{r 'C33P23-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P23", "-Churn-SPLOM-Calls-7", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P23, device = "png", dpi = 72, width = k_width, height = k_height) 
}
```

```{r 'C33P23', echo=FALSE, out.width='100%', fig.cap="(C33P23) Bivariate Analysis of Calls showing Churn"}
knitr::include_graphics(paste0(.z$PX, "C33P23", "-Churn-SPLOM-Calls-7", ".png"))
```


```{r 'C33-GGally-B9', include=FALSE}
hh <- xsy %>% select(1, 7:10, 12:15) 
ttl_hh <- "GGally: Churn: SPLOM: Minutes and Charges (8)"
cap_hh <- "C33P24"
sub_hh <- NULL #"Count of Predictor & Target" 
lgd_hh  <- "Churn"
#
```

```{r 'C33-GGally-B9-A', ref.label=c('C33-GGally-B9', 'C33-GGallyHelpers', 'C33-GGallyBiVariate'), include=FALSE}
#
```


```{r 'C33P24-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P24", "-Churn-SPLOM-MinsCharges-8", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P24, device = "png", dpi = 72, width = k_width, height = k_height) 
}
```

```{r 'C33P24', echo=FALSE, out.width='100%', fig.cap="(C33P24) Bivariate Analysis of Minutes & Charges showing Churn"}
knitr::include_graphics(paste0(.z$PX, "C33P24", "-Churn-SPLOM-MinsCharges-8", ".png"))
```

#### GGally Bivariate {.unlisted .unnumbered}

```{r 'C33-GGallyBiVariate', eval=FALSE}
# #Assumes Column 1 has Target Variable (Factor)
C33 <- hh %>% { 
  ggpairs(data = ., mapping = aes(colour = Target, fill = Target, alpha = 0.3), 
          columns = 2:ncol(.), 
          lower = list(continuous = f_gg_scatter),
          diag = list(continuous = f_gg_density)) +
    labs(subtitle = sub_hh, caption = cap_hh, title = ttl_hh) 
}
assign(cap_hh, C33)
rm(C33)
```

#### GGally Manual {.unlisted .unnumbered}

```{r 'C33-GGallyHelpers', eval=FALSE}
# #For GGally Manual Functions
f_gg_scatter <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) +
    geom_jitter(...) +
    scale_colour_viridis_d(direction = -1)
}

f_gg_density  <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) +
    geom_density(...) +
    scale_fill_viridis_d(direction = -1) + 
	  scale_colour_viridis_d(direction = -1) 
}
```


### package:psych {.tabset .tabset-fade}

#### SPLOM {.unlisted .unnumbered}

```{r 'C33-Psych-A7', include=FALSE}
hh <- xsy %>% select(1:6, 11, 16) 
cap_hh <- "C33P25"
ttl_hh <- "Psych: Churn: SPLOM: Calls, Account Length & Voice Mail (7)"
loc_png <- paste0(.z$PX, "C33P25", "-Churn-SPLOM-Psych-A7", ".png")
```

```{r 'C33-Psych-A7-A', include=FALSE, ref.label=c('C33-PsychPair')}
#
```

```{r 'C33P25', echo=FALSE, out.width='100%', fig.cap="(C33P25) Bivariate Analysis of Calls showing Churn"}
knitr::include_graphics(paste0(.z$PX, "C33P25", "-Churn-SPLOM-Psych-A7", ".png"))
```

```{r 'C33-Psych-B8', include=FALSE}
hh <- xsy %>% select(1, 7:10, 12:15) 
cap_hh <- "C33P26"
ttl_hh <- "Psych: Churn: SPLOM: SPLOM: Minutes and Charges (8)"
loc_png <- paste0(.z$PX, "C33P26", "-Churn-SPLOM-Psych-B8", ".png")
```

```{r 'C33-Psych-B8-A', include=FALSE, ref.label=c('C33-PsychPair')}
#
```

```{r 'C33P26', echo=FALSE, out.width='100%', fig.cap="(C33P26) Bivariate Analysis of Minutes & Charges showing Churn"}
knitr::include_graphics(paste0(.z$PX, "C33P26", "-Churn-SPLOM-Psych-B8", ".png"))
```

#### pairs.panels() {.unlisted .unnumbered}

```{r 'C33-PsychPair', eval=FALSE}
# #IN: hh, cap_hh, ttl_hh, loc_png
if(!file.exists(loc_png)) {
  png(filename = loc_png, width = k_width, height = k_height, units = "in", res = 144) 
  #dev.control('enable') 
  pairs.panels(hh[2:ncol(hh)], smooth = FALSE, jiggle = TRUE, rug = FALSE, ellipses = FALSE, 
               bg = rev(viridis(2))[hh$Target], pch = 21, lwd = 1, cex.cor = 1, cex = 1, 
               gap = 0, main = ttl_hh)
  #title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  C33 <- recordPlot()
  dev.off()
  assign(cap_hh, C33)
  rm(C33)
}
```

## Subset Interesting Datapoints

- Those which were identified by EDA
- Anomalous fields like Area Code which have only 3 values yet it is distributed over all the States

## Binning

- Figure \@ref(fig:C33P11) showed that customers with less than four calls to customer service had a lower churn rate than customers who had four or more calls to customer service. 
  - We may therefore decide to bin the customer service calls variable into two classes, low (fewer than four) and high (four or more).

```{r 'C33-BinnedServCall', include=FALSE}
# #xyw: x (Predictor), y (Target), c (column =y), r (row =x)
# #g (grouped), w (wider), l (longer)
r_xyg <- "Service Calls"
c_xyg <- "Churn"
# #Select Relevant | Binning | Rename & Relocate Target | Select Target & Predictor |
# #Relabel Target | Factor Levels Each | Count | Rename N | Group | Ratio | Ungroup |
xyg <- bb %>% select(custserv_calls, churn) %>% 
  mutate(Predictor = ifelse(custserv_calls < 4, "Low", "High")) %>% 
  relocate(Target = 2) %>% select(Target, Predictor) %>% 
  mutate(across(Target, ~ifelse(., "Yes", "No"))) %>% 
  mutate(across(Target, factor, levels = c("No", "Yes"))) %>% 
  mutate(across(Predictor, factor, levels = c("Low", "High"))) %>% 
  count(Target, Predictor) %>% 
  rename(N = 3) %>% 
  group_by(Predictor) %>% 
  mutate(Ratio = paste0(round(100 * N / sum(N), 1), "%")) %>% 
  ungroup()
```

```{r 'C33-CTab', include=FALSE}
# #IN: xyg (Predictor, Target), r_xyg, c_xyg 
# #Generate Contingency Table (CrossTab)
ctab <- xyg %>% select(1:3) %>% 
  pivot_wider(names_from = Target, values_from = N, values_fill = 0, names_sort = TRUE) %>%
  #mutate(across(1, as.character)) %>% 
  add_row(summarise(., across(1, ~ "Total")), summarise(., across(where(is.numeric), sum))) %>% 
  mutate(SUM = rowSums(across(where(is.numeric)))) %>% 
  mutate(across(where(is.numeric) & !matches("SUM"), 
                list(r = ~ round(. /SUM, 3)), .names = "xx_{.fn}{.col}")) %>% 
  mutate(across(where(is.numeric) & !starts_with("xx_") & !matches("SUM"), 
                list(rp = ~ paste0(round(100 * . /SUM, 1), "%")), .names = "{.fn}{.col}")) %>%
  mutate(across(where(is.numeric) & !starts_with("xx_"), 
                list(c = ~ 2 * ./sum(.)), .names = "yy_{.fn}{.col}")) %>% 
  mutate(across(where(is.numeric) & !starts_with("xx_") & !starts_with("yy_"), 
                list(cp = ~ paste0(round(100 * 2 * . /sum(.), 1), "%")), .names = "{.fn}{.col}"))
```

```{r 'C33-BarGroupServCall', include=FALSE}
# #Grouped Bar Chart
hh <- xyg
#
cap_hh <- "C33P28"
ttl_hh <- "Churn: Customer Service Calls (Binned)"
sub_hh <- NULL 
x_hh <- "Cutomer Service Calls" #r_xyg
y_hh <- "Frequency"
lgd_hh  <- "Churn" #c_xyg
#
C33 <- hh %>% { ggplot(., aes(x = Predictor, y = N, fill = Target)) + 
    geom_bar(position = position_dodge(), stat = "identity", alpha = 1) + 
    geom_text(position = position_dodge(width = 1), aes(label = N), vjust = 1.5, 
              colour = c(rep("black", 2), rep("white", 2))) +
    scale_fill_viridis_d(direction = -1) +	
    theme(panel.grid.major.x = element_blank(), axis.line = element_blank(),
          panel.border = element_rect(colour = "black", fill = NA, size = 1),
          legend.position = c(.5, .95), 
          legend.box = "horizontal", legend.direction = "horizontal") +
    labs(x = x_hh, y = y_hh, fill = lgd_hh, 
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P28-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P28", "-Churn-Bar-ServiceCalls", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P28, device = "png", dpi = 144) 
}
```

```{r 'C33P28', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P28", "-Churn-Bar-ServiceCalls", ".png"))
```

```{r 'C33-BarStackServCall', include=FALSE}
# #Stacked Bar Chart
hh <- xyg
#
cap_hh <- "C33P29"
ttl_hh <- "Churn: Customer Service Calls (Binned)"
sub_hh <- NULL 
x_hh <- "Cutomer Service Calls"
y_hh <- "Frequency"
lgd_hh  <- "Churn"
#
C33 <- hh %>% { ggplot(., aes(x = Predictor, y = N, fill = Target)) + 
    geom_bar(position = position_stack(), stat = "identity", alpha = 1) + 
    geom_text(position = position_stack(vjust = 0.5), aes(label = N), 
              colour = c(rep("black", 2), rep("white", 2))) +
    scale_fill_viridis_d(direction = -1) +	
    theme(panel.grid.major.x = element_blank(), axis.line = element_blank(),
          panel.border = element_rect(colour = "black", fill = NA, size = 1),
          legend.position = c(.9, .9)) +
    labs(x = x_hh, y = y_hh, fill = lgd_hh, 
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P29-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P29", "-Churn-Stack-ServiceCalls", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P29, device = "png", dpi = 144) 
}
```

```{r 'C33P29', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P29", "-Churn-Stack-ServiceCalls", ".png"))
```

```{r 'C33-BarPStackServCall', include=FALSE}
# #Percent Stacked Bar Chart
hh <- xyg 
#
cap_hh <- "C33P30"
ttl_hh <- "Churn: Customer Service Calls (Binned)"
sub_hh <- NULL 
x_hh <- "Cutomer Service Calls"
y_hh <- "Grouped Percentage"
lgd_hh  <- "Churn"
#
C33 <- hh %>% { ggplot(., aes(x = Predictor, y = N, fill = Target)) + 
    geom_bar(position = position_fill(), stat = 'identity') + 
    geom_text(position = position_fill(vjust = 0.5), aes(label = Ratio), 
              colour = c(rep("black", 2), rep("white", 2))) +
    scale_fill_viridis_d(direction = -1) +	
    scale_y_continuous(labels = percent) +
    theme(panel.grid.major.x = element_blank(), axis.line = element_blank(),
          panel.border = element_rect(colour = "black", fill = NA, size = 1)) +
    labs(x = x_hh, y = y_hh, fill = lgd_hh, 
         subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C33)
rm(C33)
```

```{r 'C33P30-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C33P30", "-Churn-pStack-ServiceCalls", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C33P30, device = "png", dpi = 144) 
}
```

```{r 'C33P30', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C33P30", "-Churn-pStack-ServiceCalls", ".png"))
```

```{r 'C33P282930', echo=FALSE, out.width='33%', ref.label=c('C33P28', 'C33P29', 'C33P30'), fig.cap="(C33P28 C33P29 C33P30) High Customer Service Calls have major Churn"}
#
```

```{r 'C33-SetCrossServCall', include=FALSE}
hh <- ctab %>% select(-c(5:6, 9:11)) 
#names_hh <- names(hh)
names_hh <-c(paste0(c_xyg, " ", "$\\rightarrow$", "<br/>", "$\\downarrow$", " ", r_xyg), 
                "No <br/> <br/>", "Yes <br/> <br/>", "Row <br/> SUM", 
                "Row <br/> No %", "Row <br/> Yes %", 
                "Col <br/> No %", "Col <br/> Yes %", "Col <br/> Row SUM %") 
stopifnot(identical(ncol(hh), length(names_hh)))
stopifnot(nrow(hh) < 10)
#
cap_hh <- paste0("(C33T04) ", r_xyg, " vs. ", c_xyg)
```

```{r 'C33T04', echo=FALSE, ref.label=c('C33-CrossKbl')}
#
```


- Similarly, Evening Minutes can be given flags (Categorical)
  - Low (<= 160), Medium (160, 240), High (>240)
  - Recall that the baseline churn rate for all customers is 14.49%. The medium group comes in very close to this baseline rate, 14.1%. 
  - However, the High evening minutes group has nearly double the churn proportion (19.5%) compared to the low evening minutes group (10%). 
  - The chi-square test is significant, meaning that these results are most likely real and not due to chance alone. "ForLater"

```{r 'C33-BinnedEveMins', include=FALSE}
# #xyw: x (Predictor), y (Target), c (column =y), r (row =x)
# #g (grouped), w (wider), l (longer)
r_xyg <- "Evening Minutes"
c_xyg <- "Churn"
# #Select Relevant | Binning | Rename & Relocate Target | Select Target & Predictor |
# #Relabel Target | Factor Levels Each | Count | Rename N | Group | Ratio | Ungroup |
xyg <- bb %>% select(eve_mins, churn) %>% 
    mutate(Predictor = across(eve_mins, cut, 
        breaks = c(min(.), 160, 240, max(.)), 
        include.lowest = TRUE, labels = c("Low", "Medium", "High"))[[1]]) %>% 
  relocate(Target = 2) %>% select(Target, Predictor) %>% 
  mutate(across(Target, ~ifelse(., "Yes", "No"))) %>% 
  mutate(across(Target, factor, levels = c("No", "Yes"))) %>% 
  #mutate(across(Predictor, factor, levels = c("Low", "High"))) %>% 
  count(Target, Predictor) %>% 
  rename(N = 3) %>% 
  group_by(Predictor) %>% 
  mutate(Ratio = paste0(round(100 * N / sum(N), 1), "%")) %>% 
  ungroup()
```

```{r 'C33-CTab-EveMins', include=FALSE, ref.label=c('C33-CTab')}
#
```

```{r 'C33-SetCrossEveMins', include=FALSE}
hh <- ctab %>% select(-c(5:6, 9:11)) 
#names_hh <- names(hh)
names_hh <-c(paste0(c_xyg, " ", "$\\rightarrow$", "<br/>", "$\\downarrow$", " ", r_xyg), 
                "No <br/> <br/>", "Yes <br/> <br/>", "Row <br/> SUM", 
                "Row <br/> No %", "Row <br/> Yes %", 
                "Col <br/> No %", "Col <br/> Yes %", "Col <br/> Row SUM %") 
stopifnot(identical(ncol(hh), length(names_hh)))
stopifnot(nrow(hh) < 10)
#
cap_hh <- paste0("(C33T05) ", r_xyg, " vs. ", c_xyg)
```

```{r 'C33T05', include=FALSE, ref.label=c('C33-CrossKbl')}
#
```

## Correlation  {.tabset .tabset-fade}

- The threshold for significance of the correlation coefficient $r$ depends not only on the sample size but also on data mining, where there are a large number of records (over 1000), even small values of r, such as −0.1 ≤ r ≤ 0.1 may be statistically significant. 
  - One should take care to avoid feeding correlated variables to data mining and statistical models. 
  - At best, using correlated variables will overemphasize one data component; at worst, using correlated variables will cause the model to become unstable and deliver unreliable results. 
  - However, just because two variables are correlated does not mean that we should omit one of them.
    - Identify any variables that are perfectly correlated (i.e., r = 1.0 or r = −1.0). Do not retain both variables in the model, but rather omit one. 
    - Identify groups of variables that are correlated with each other. Then, later, during the modeling phase, apply dimension-reduction methods, such as PCA, to these variables.
  - Example: Charge is perfectly correlated to Minutes (For each of Day, Eve, Night, International). Thus it can be eliminated. Thus number of predictors has been reduced from 20 to 16.
  - Correlation coefficient 0.038 between account length and day calls has a small p-value of 0.026, telling us that account length and day calls are positively correlated. We should note this, and prepare to apply the PCA during the modeling phase.
  - "ForLater" - Get Pearson Correlation Coefficient

### ALL Continuos Variables {.unlisted .unnumbered}

```{r 'C33-AllVarNum'}
# #All Continuous Predictors and Target (Categorical)
# #Select Relevant | Binning | Rename & Relocate | Relabel & Factor Target|
xsy <- bb %>% select( (where(is.numeric) & ! "area_code") | "churn") %>% 
  relocate(ends_with("_mins")) %>% 
  relocate(ends_with("_calls")) %>% 
  relocate(vmail_message, .after =  last_col()) %>% 
  relocate(Target = "churn") %>% 
  mutate(across(Target, ~ifelse(., "Yes", "No"))) %>% 
  mutate(across(Target, factor, levels = unique(Target)))
# #
xsyl <- xsy %>% pivot_longer(where(is.numeric), names_to = "Predictors", values_to = "Values") %>% 
  mutate(across(Predictors, ~ factor(., levels = unique(Predictors))))
# #
# #Select All Minutes and All Charges (8)
hh <- xsy %>% select(1, 7:10, 12:15) 
#
ttl_hh <- "GGally: Churn: SPLOM: Minutes and Charges (8)"
cap_hh <- "C33P31"
sub_hh <- NULL #"Count of Predictor & Target" 
lgd_hh  <- "Churn"
```

### GGally {.unlisted .unnumbered}

```{r 'C33-GGallyBiVariate-A', eval=FALSE, ref.label=c('C33-GGallyBiVariate')}
# 
```

### Psych {.unlisted .unnumbered}

```{r 'C33-PsychPair-A', eval=FALSE, ref.label=c('C33-PsychPair')}
# 
```

### chart.Correlation {.unlisted .unnumbered}

```{r 'C33-PerfAnalytics', eval=FALSE}
# #For Reference Only. Only add Histogram at the Diagonal to the Base pairs()
PerformanceAnalytics::chart.Correlation(hh[2:ncol(hh)], histogram = TRUE)
```

### corrplot() vs. corPlot() {.unlisted .unnumbered}

```{r 'C33-CorrPlots', include=FALSE}
ttl_hh <- "corrplot::corrplot (with Clusters) vs. psych::corPlot"
loc_png <- paste0(.z$PX, "C33P31", "-Churn-corrplot-corPlot", ".png")
#
if(!file.exists(loc_png)) {
  # #Two Graphs Side by Side
  png(filename = loc_png, width = k_width, height = k_height, units = "in", res = 144) 
  #dev.control('enable') 
  # #Correlation along with Clusters
  par(mfrow = c(1, 2))
  corrplot::corrplot(cor(hh[2:ncol(hh)]), method = "circle", type = "full", diag = FALSE, 
                     cl.pos = 'n', tl.pos = 'l', addCoef.col = "black", 
                     order = "hclust", hclust.method = "ward.D", addrect = 2, rect.col = 3,
                     rect.lwd = 3, title = NULL, col = RColorBrewer::brewer.pal(3, "BrBG"))
  title(main = ttl_hh, line = 2, adj = 0)
  #
  #corPlot(hh[2:ncol(hh)], upper = TRUE, diag = TRUE, gr = viridis)
  #corPlot(hh[2:ncol(hh)], upper = TRUE, diag = TRUE, gr = cividis)
  psych::corPlot(hh[2:ncol(hh)], upper = TRUE, diag = FALSE, show.legend = FALSE, keep.par = FALSE, 
    gr = colorRampPalette(RColorBrewer::brewer.pal(3, "BrBG")), main = "")
  title(sub = cap_hh, line = 4, adj = 1)
  C33 <- recordPlot()
  dev.off()
  par(mfrow = c(1, 1))
  assign(cap_hh, C33)
  rm(C33)
}
```

```{r 'C33P31', echo=FALSE, out.width='100%', fig.cap="(C33P31) corrplot::corrplot vs. psych::corPlot()"}
knitr::include_graphics(paste0(.z$PX, "C33P31", "-Churn-corrplot-corPlot", ".png")) #iiii
```

## Insights

- The four 'charge' fields are linear functions of the 'minute' fields, and should be omitted. 
- The 'area code' field and/or the 'state' field are anomalous, and should be omitted.
- Customers with the International Plan tend to churn more frequently. 
- Customers with the Voice Mail Plan tend to churn less frequently. 
- Customers with four or more Customer Service Calls churn more than four times as often as the other customers. 
- Customers with both high Day Minutes and high Evening Minutes tend to churn at a higher rate (6 times) than the other customers. 
- Customers with low Day Minutes and high Customer Service Calls churn at a higher rate than the other customers. 
- Customers with lower numbers of International Calls churn at a higher rate than customers with more international calls. 
  

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C33-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, c33churn, C33P01, C33P02, C33P03, cap_hh, hh, lgd_hh, 
          loc_png, sub_hh, ttl_hh, x_hh, xxB16Cars, xxB18Churn, y_hh, c_xyg, ctab, 
          names_hh, r_xyg, xyg, C33P04, C33P05, C33P06, C33P08, C33P09, C33P10, xsyg, C33P11, 
          C33P12, C33P13, C33P14, C33P15, C33P16, C33P17, C33P18, C33P19, xsyl, alpha, C33P20, 
          C33P21, ha_ii, isVarEqual, tt_ii, var_ii, C33P22, C33P23, C33P24, xsy, C33P25, C33P26,
          C33P27, C33P28, C33P29, C33P30, C33P31)
```

```{r 'C33-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/333-EDA.Rmd-->

# Dimension Reduction {#c34}

```{r 'C34', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
invisible(lapply(f_getPathR(A12pKbl), knitr::read_chunk)) #wwww
```

## Dimensions {.tabset .tabset-fade}

> 10 or 11 Dimensions are enough for the Universe. How many are needed for your data!


### Multicollinearity {.unlisted .unnumbered}

```{r 'C34P09', echo=FALSE, fig.cap="(C34P09) Multicollinearity"}
knitr::include_graphics(paste0(.z$PX, "C34P09", "-Multicollinearity", ".png")) #iiii
```


```{definition 'Multicollinearity'}
\textcolor{pink}{Multicollinearity} is a condition where some of the predictor variables are strongly correlated with each other. 
```

- [(External) Image Source](https://datatab.net/tutorial/multicollinearity "https://datatab.net")
- Problems: Multicollinearity
  - Multicollinearity leads to instability in the solution space, leading to possible incoherent results, such as in multiple regression, where a multicollinear set of predictors can result in a regression which is significant overall, even when none of the individual variables is significant. 
  - Even if such instability is avoided, inclusion of variables which are highly correlated tends to overemphasize a particular component of the model, as the component is essentially being double counted.
- Problems: Too many variables
  - The sample size needed to fit a multivariate function grows exponentially with the number of variables.
  - The use of too many predictor variables to model a relationship with a response variable can unnecessarily complicate the interpretation of the analysis, and violates the principle of parsimony
    - i.e. keep the number of predictors to such a size that would be easily interpreted. 
  - Also, retaining too many variables may lead to overfitting
    - i.e. generality of the findings is hindered because new data do not behave the same as the training data for all the variables.

### Parsimony {.unlisted .unnumbered}

```{definition 'Principle-of-Parsimony'}
\textcolor{pink}{Principle of parsimony} is the problem-solving principle that "entities should not be multiplied beyond necessity". 
```

- It is inaccurately paraphrased as "the simplest explanation is usually the best one". 
- It advocates that when presented with competing hypotheses about the same prediction, one should select the solution with the fewest assumptions, and that this is not meant to be a way of choosing between hypotheses that make different predictions. 

### Overfitting & Underfitting {.unlisted .unnumbered}

```{definition 'Overfitting'}
\textcolor{pink}{Overfitting} is the production of an analysis that corresponds too closely to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.
```


```{definition 'Underfitting'}
\textcolor{pink}{Underfitting} occurs when a statistical model cannot adequately capture the underlying structure of the data. 
```

- An overfitted model is a statistical model that contains more parameters than can be justified by the data.
  - The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.
  - over-fitting occurs when a model begins to "memorize" training data rather than "learning" to generalize from a trend.
- Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.
- Overfitting Example
  - If the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. Such a model, though, will typically fail severely when making predictions.
  - A noisy linear dataset can be fitted by a polynomial function also which would give a perfect fit. However, the linear function (in this case) would be better in extrapolating beyond the fitted data.
- To decrease the chance or amount of overfitting
  - model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout
  - To explicitly penalize overly complex models 
  - To evaluate the model performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter. 

## Dimension-reduction methods 

- These use the correlation structure among the predictor variables to accomplish the following: 
  - To reduce the number of predictor items. 
  - To help ensure that these predictor items are independent. 
  - To provide a framework for interpretability of the results. 

- Dimension-reduction methods: 
  - PCA
  - Factor analysis 
  - User-defined composites 
  
## PCA

```{definition 'PCA'}
\textcolor{pink}{Principal components analysis (PCA)} seeks to explain the correlation structure of a set of predictor variables ${m}$, using a smaller set of linear combinations of these variables, called components ${k}$. PCA acts solely on the predictor variables, and ignores the target variable.
```


- Suppose that the original variables ${\{X_1, X_2, \ldots, X_m\}}$ form a coordinate system in m-dimensional space. 
  - Let each variable ${X_i}$ represent an ${n \times 1}$ vector, where ${n}$ is number of records.
  - The principal components represent a new coordinate system, found by rotating the original system along the directions of maximum variability.
- Analysis
  - Standardize the data, so that the mean for each variable is zero, and the standard deviation is one.
  - ${X_i \to Z_i = \frac{X_i - {\mu}_i}{{\sigma}_{ii}}}$
  - The covariance is a measure of the degree to which two variables vary together.
  - A positive covariance indicates that, when one variable increases, the other tends to increase, while a negative covariance indicates that, when one variable increases, the other tends to decrease. 
  - ${{\sigma}_{ii}^2}$ denotes the variance of ${X_i}$. 
    - If ${X_i}$ and ${X_j}$ are independent, then ${{\sigma}_{ij}^2 = 0}$; but reverse may not be TRUE i.e. ${{\sigma}_{ij}^2 = 0}$ does not imply that ${X_i}$ and ${X_j}$ are independent. 
    - Note that the covariance measure is not scaled, so that changing the units of measure would change the value of the covariance.
    - The correlation coefficient ${r_{ij}} = \frac{{\sigma}_{ij}^2}{{\sigma}_{ii}{\sigma}_{jj}}$ avoids this difficulty by scaling the covariance by each of the standard deviations.
    - Then, the correlation matrix is denoted as ${\rho}$

```{definition 'Eigenvalues'}
Let \textcolor{pink}{$\mathbf{B}$} be an $m \times m$ matrix, and let \textcolor{pink}{$\mathbf{I}$} be the $m \times m$ identity matrix. Then the scalars $\{\lambda_1, \lambda_2, \ldots, \lambda_m\}$ are said to be the \textcolor{pink}{eigenvalues of $\mathbf{B}$} if they satisfy $|\mathbf{B} - \lambda \mathbf{I}| = 0$, where $|\mathbf{Q}|$ denotes the determinant of Q.
```

```{definition 'Eigenvector'}
Let \textcolor{pink}{$\mathbf{B}$} be an $m \times m$ matrix, and let ${\lambda}$ be an eigenvalue of $\mathbf{B}$. Then nonzero $m \times 1$ vector \textcolor{pink}{$\overrightarrow{e}$} is said to be an \textcolor{pink}{eigenvector of B}, if $\mathbf{B} \overrightarrow{e} = 𝜆\overrightarrow{e}$.
```

- The total variability in the standardized set of predictors equals the sum of the variances of the Z-vectors, which equals the sum of the variances of the components, which equals the sum of the eigenvalues, which equals the numer of predictors
  - i.e. $\sum_{i=1}^m {\text{Var}({Y_i})} = \sum_{i=1}^m {\text{Var}({Z_i})} = \sum_{i=1}^m {\lambda_i} = m$
- The partial correlation between a given component and a given predictor variable is a function of an eigenvector and an eigenvalue. Specifically, $\text{Corr}(Y_i, Z_j) = e_{ij}\sqrt{\lambda_i}$, where $\{ (\lambda_1, e_1), (\lambda_2, e_2), \ldots, (\lambda_m, e_m)\}$ are the eigenvalue-eigenvector pairs for the correlation matrix $\rho$, and we note that $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_m$. In other words, the eigenvalues are ordered by size. (A partial correlation coefficient is a correlation coefficient that takes into account the effect of all the other variables.)
- The proportion of the total variability in Z that is explained by the $i^{\text{th}}$ principal component is the ratio of the $i^{\text{th}}$ eigenvalue to the number of variables, that is, the ratio $\frac{\lambda_i}{m}$.

## Data Housing

\textcolor{pink}{Please import the "C34-cadata.txt"} 

- Source: http://lib.stat.cmu.edu/datasets/houses.zip
- About: [20640, 9]
  - It provides census information from all the block groups from the 1990 California census. 
  - For this data set, a block group has an average of 1425.5 people living in an area that is geographically compact. 
  - Block groups were excluded that contained zero entries for any of the variables. 
  - Variables: median house value (Target), median income, housing median age, total rooms, total bedrooms, population, households, latitude, and longitude.


```{r 'C34-Housing', include=FALSE, eval=FALSE}
# #read_table() can handle double space delimited file.
tbl <- read_table(paste0(.z$XL, "C34-cadata.txt"), skip = 27, 
                col_names = c("median_house_value", "median_income", "housing_median_age", 
                              "total_rooms", "total_bedrooms", "population", "households", 
                              "latitude", "longitude"))
stopifnot(!"problems" %in% names(attributes(tbl)))
#problems(tbl)
#attr(tbl, "problems") <- NULL
attr(tbl, "spec") <- NULL
xxC34Housing <- tbl
f_setRDS(xxC34Housing)
```

```{r 'C34-GetHousing', include=FALSE}
bb <- aa <- xxC34Housing <- f_getRDS(xxC34Housing)  #wwww
```

## Partition Data {#train-test-c34 .tabset .tabset-fade}

### Train & Test {.unlisted .unnumbered}

```{r 'C34-Partition'}
set.seed(3)  #wwww
# #sample() and its variants can be used for Partitioning of Dataset
if(FALSE) {
  # #This approach is difficult to extend for 3 or more splits
  # #Further, Using floor() multiple times might result in loss of a row 
  # #sample() can automatically expand the sequence from 1 to x if single x is given 
  # #This is the Caution about the sample(). We are avoiding this for fewer bugs in future.
  train_idx <- sample.int(n = nrow(bb), size = floor(0.8 * nrow(bb)), replace = FALSE)
  train_idx <- sample(seq_len(nrow(bb)), size = floor(0.8 * nrow(bb)), replace = FALSE)  
  train_bb <- bb[train_idx, ]
  test_bb <- bb[-train_idx, ]
}
#
if(FALSE) {# #This approach might work for 3 or more samples but not tested
  train_idx <- sample(2, size = nrow(bb), replace = TRUE, p = c(0.8, 0.2))
  train_bb <- bb[train_idx == 1, ]
  test_bb  <- bb[train_idx == 2, ]
}
#
# #For 3 or more splits
#brk_bb = c(train = 0.8, test = 0.1, validate = 0.1)
brk_bb = c(train = 0.9, test = 0.1)
idx_bb = sample(cut(seq_len(nrow(bb)), nrow(bb) * cumsum(c(0, brk_bb)), labels = names(brk_bb)))
#
# #Splits by Fixed Numbers not Percentages
if(FALSE) {
  brk_bb = c(train = 18570, test = nrow(bb))
  idx_bb = sample(cut(seq_len(nrow(bb)), c(0, brk_bb), labels = names(brk_bb)))
}
# #List of Multiple Tibbles
part_l = split(bb, idx_bb)
#
# #nrow(), ncol(), dim() can be applied
vapply(part_l, nrow, FUN.VALUE = integer(1))
stopifnot(identical(nrow(bb), sum(vapply(part_l, nrow, FUN.VALUE = integer(1)))))
```

### sample() {.unlisted .unnumbered}


```{r 'C34-Sample'}
# #Create Test Datasets of 10 items of Numeric and Character.
# #Initial 10 letters or Numbers were not chosen to show difference between 
# #indexing numbers /position and actual item values.
ii_num <- 11:20
ii_num
jj_chr <- letters[11:20]
jj_chr
#
set.seed(3)
# #Note: length() is used on vectors & nrow() is used on dataframes
# #Choose 60% samples out of 10 items
sample(1:length(ii_num), 0.6 * length(ii_num))
sample(1:length(jj_chr), 0.6 * length(jj_chr))
#
# #Note the outout above is index of numbers for both cases and always gives numbers within [1, 10]
# #Note the output below always gives the numbers within [2, 10]. "1" index will not be available.
sample(2:length(ii_num), 0.6 * length(ii_num))
sample(2:length(jj_chr), 0.6 * length(jj_chr))
```


## Basics

```{r 'C34-Basic', include=FALSE}
xsyw <- part_l$train  #wwww
# #Separate Working Names and Display Names
if(FALSE) paste0(names(xsyw), collapse = ", ")
c_xsyw <- c("h_value", "income", "h_age", "rooms", "bedrooms", 
               "population", "households", "latitude", "longitude")
names(c_xsyw) <- c("House Value (Median)", "Income (Median)", "House Age (Median)", 
        "Rooms (Total)", "Bedrooms (Total)", "Population", "Households", "Latitude", "Longitude")
names(xsyw) <- c_xsyw
#
c_zsyw <- c_xsyw
names(c_zsyw) <- c("Target", "Income", "Age", "Rooms", "Beds", "Pop", "Houses", "Lat", "Long")
#
# #Scaling
zsyw <- xsyw %>% mutate(across(everything(), ~ as.vector(scale(.))))
# #Predictors Only
xw <- xsyw %>% select(-1)
zw <- zsyw %>% select(-1)
# #Long
f_wl(xsyw, zsyw, xw, zw)
#
# #Summary
xsyg <- xsyl %>% group_by(Keys) %>% 
  summarise(Min = min(Values), Max = max(Values), SD = sd(Values), 
            Mean = mean(Values), Median = median(Values), Mode = f_getMode(Values),
            Unique = length(unique(Values)), isNA = sum(is.na(Values)))
# #Relabel with Verification Before and After
levels(xsyg$Keys)
levels(xsyg$Keys) <- names(c_xsyw)
levels(xsyg$Keys)
#
# #Long to Wide: For Reference Only
if(FALSE) { zsyw <- zsyl %>% 
  group_by(Keys) %>% 
  mutate(ID = row_number()) %>% 
  pivot_wider(names_from = Keys, values_from = Values)
}
```

```{r 'C34T01', echo=FALSE}
# #Print Kable Table
hh <- xsyg %>% 
  #mutate(across(where(is.numeric), round, digits = 4)) %>% 
  mutate(across(where(is.numeric), format, digits = 3, 
                #nsmall = 0, replace.zero = TRUE, zero.print = "0", 
                drop0trailing = TRUE, scientific = FALSE)) 
#
cap_hh <- paste0("(C34T01)", "[", nrow(xsyw), ", ", ncol(xsyw), "] ", "Houses: Training Basics") 
f_pKbl(x = hh, caption = cap_hh, debug = FALSE)
```

## Boxplot

```{r 'C34-BoxScaledHouse', include=FALSE}
hh <- zsyl
levels(hh$Keys) <- names(c_xsyw)
#
ttl_hh <- "Houses: BoxPlots (Scaled)"
cap_hh <- "C34P01"
sub_hh <- NULL 
lgd_hh  <- NULL
```

```{r 'C34-ScaleBox', include=FALSE}
# #IN: hh(Keys, Values), 
C34 <- hh %>% { ggplot(data = ., mapping = aes(x = Keys, y = Values, fill = Keys)) +
    geom_boxplot() +
    k_gglayer_box +
    scale_y_continuous(breaks = breaks_pretty()) + 
    coord_flip() +
    theme(legend.position = 'none') +
    labs(x = NULL, y = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P01", "-Houses-Box-Z", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P01, device = "png", dpi = 144) 
}
```

```{r 'C34P01', echo=FALSE, fig.cap="(C34P01) Houses: Boxplot (Scaled)"}
knitr::include_graphics(paste0(.z$PX, "C34P01", "-Houses-Box-Z", ".png"))
```

## Normality

Note that normality of the data is not strictly required to perform non-inferential PCA but that strong departures from normality may diminish the observed correlations. As data mining applications usually do not involve inference, we will not worry about normality.

## Predictors SPLOM

- Rooms, bedrooms, population, and households all appear to be positively correlated. 
- Latitude and longitude appear to be negatively correlated. 
  - Scatter plot between them looks like the State of California
- House Median Age appears to be correlated the least with the other predictors

```{r 'C34-Predictors', include=FALSE}
hh <- zw
#hh <- hh[1:100, ]
labels_hh <- names(c_zsyw)[-1] #names(hh)
#
ttl_hh <- "Houses: SPLOM (8)"
cap_hh <- "C34P02"
sub_hh <- NULL 
lgd_hh  <- NULL 
```

```{r 'C34-SPLOM', include=FALSE, eval=FALSE}
# #IN: hh, labels_hh
C34 <- hh %>% { 
  ggpairs(data = ., mapping = aes(alpha = I(0.1)), columnLabels = labels_hh,
          upper = list(continuous = wrap("cor", size = 5, alpha = 1))) +
    labs(subtitle = sub_hh, caption = cap_hh, title = ttl_hh) 
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34-SPLOM-zw', include=FALSE, ref.label=c('C34-SPLOM')}
#
```

```{r 'C34P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P02", "-House-SPLOM-z", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P02, device = "png", dpi = 72, width = k_width, height = k_height) 
}
```

```{r 'C34P02', echo=FALSE, out.width='100%', fig.cap="(C34P02) Houses: SPLOM (8)"}
knitr::include_graphics(paste0(.z$PX, "C34P02", "-House-SPLOM-z", ".png"))
```

## Predictors Corplot

```{r 'C34-PredCor', include=FALSE, eval=FALSE}
# #Correlation Matrix | Table (Long) | Tibble
hh <- cor(zw) %>% as.table() %>% as_tibble(.name_repair = 'unique') %>% 
  #filter(...1 != ...2) %>% 
  filter(!duplicated(paste0(pmax(as.character(...1), as.character(...2)), 
                            pmin(as.character(...1), as.character(...2))))) %>% 
  mutate(across(where(is.character), factor, levels = c_zsyw[-1], labels = names(c_zsyw)[-1]))
#
ttl_hh <- "Houses: Corrplot (8)"
cap_hh <- "C34P03"
```

```{r 'C34-CorPlot', include=FALSE, eval=FALSE}
# #IN: hh (Correlation Tibble Long, Triangle with Diagonal) 
C34 <- hh %>% { ggplot(., aes(y = ...1, x = ...2, fill = n)) + 
    geom_tile(color = "white") + 
    geom_text(aes(label = round(n, 2)), color = "black", size = 4) +
    coord_fixed() +
    scale_fill_distiller(palette = "BrBG", direction = 1, limits = c(-1, 1)) +
    guides(fill = guide_colourbar(barwidth = 0.5, barheight = 15)) +
    theme(axis.title = element_blank(), 
          axis.line = element_blank(), 
          axis.ticks = element_blank(),
          panel.grid.major = element_blank(), 
          panel.border = element_blank()) +
	  labs(subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34-PredCor-A', ref.label=c('C34-PredCor', 'C34-CorPlot'), include=FALSE}
#
```

```{r 'C34P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P03", "-House-Corplot-z", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P03, device = "png", dpi = 144) 
}
```

```{r 'C34P03', echo=FALSE, fig.cap="(C34P03) Houses: Corrplot (8)"}
knitr::include_graphics(paste0(.z$PX, "C34P03", "-House-Corplot-z", ".png"))
```

## Correlation {.tabset .tabset-fade}

### Correlation Matrix {.unlisted .unnumbered}

```{r 'C34T02', echo=FALSE}
# #cor() produces a Matrix of Correlations: Redundant Triangle (Upper or Lower) and Diagonal
# #Print Kable Table
hh <- cor(zw)
cap_hh <- paste0("(C34T02) ", "Houses: Correlation Matrix") 
f_pKblM(x = hh, caption = cap_hh, negPos = c(-0.5, 0.5))
```

### Matrices {.unlisted .unnumbered}

- \textcolor{pink}{cor()} : Correlation Function produces a Matrix
  - Matirces has HUGE number of problems but unfortunately some function output is in that form
  - names() does NOT work on Matrices but colnames() works, even though names() is superior to colnames() in all other aspects
  - Symmatrical Matrix. Diagonal and one of the Triangles (Upper or Lower) are redundant
  - Too many decimal printing

```{r 'C34-Matrices'}
# #cor() produces a Matrix
ii <- cor(zw)
str(ii)
ii
#
# #We can eliminate Lower Triangle and Diagonal. However NA does not print well with format()
# #outcome of upper.tri() is easily compared to as.table(). lower.tri() will need extra step
#
# #Take advatage of Matrix Triangle and Set to 0 for later handling by format()
# #IF we remove the diagonal then dimensions gets haywire i.e. 8 to 7 columns left [28] elements
# #IF we keep the diagonal then dimensions gets haywire i.e. 8 to 9 columns left [36] elements
# #So, cannot use NA, has to use ZERO (So that, later, format can replace it.)
#
# #However, we finally went ahead with dplyr solution which handled NA separately from format()
# #Thus eliminating need of assigning 0, NA are being used for redundant triangle and diagonal
#
kk <- ii #ii is FULL 8x8 Matrix
kk[upper.tri(kk, diag = TRUE)] <- NA 
mm  <- kk %>% as.table() %>% as_tibble(.name_repair = 'unique') %>% drop_na() %>% 
  filter(n > 0.5 | n < -0.5) %>% arrange(desc(abs(n)))
#
kk <- ii #ii is FULL 8x8 Matrix
nn  <- kk %>% as.table() %>% as_tibble(.name_repair = 'unique') %>% 
    filter(...1 != ...2) %>% 
    filter(!duplicated(paste0(pmax(as.character(...1), as.character(...2)), 
                              pmin(as.character(...1), as.character(...2))))) %>%
  filter(n > 0.5 | n < -0.5) %>% arrange(desc(abs(n)))
stopifnot(identical(mm, nn))
#
# #However, above are long because of the usage of as.table(). For Wide:
ll <- kk %>% #as.table() %>% 
  as_tibble() %>% 
  mutate(ID = row_number()) 
# #Get Names
oo <- names(ll)
ll %>% mutate(across(-ID, ~ ifelse(ID <= match(cur_column(), oo), NA, .x))) %>% select(-ID)
```

### f_pKblM() {.unlisted .unnumbered}

```{r 'C34F-pKblM', eval=FALSE, ref.label=c('A12B-pKblM')}
#
```

### f_pKbl() {.unlisted .unnumbered}

```{r 'C34F-pKbl', eval=FALSE, ref.label=c('A12A-pKbl')}
#
```

## PCA {.tabset .tabset-fade}

- There are two general methods to perform PCA in R :
  - Spectral decomposition which examines the covariances / correlations between variables
    - \textcolor{pink}{princomp()}
      -  It uses divisor $N$ for the covariance matrix.
    - Names: "sdev, loadings, center, scale, n.obs, scores, call"
  - Singular value decomposition (SVD) which examines the covariances / correlations between individuals
    - SVD has slightly better numerical accuracy
    - \textcolor{pink}{prcomp()}
      - Unlike princomp, variances are computed with the usual divisor $N - 1$.
      - \textcolor{pink}{cov()} also uses $N - 1$
      - Use sdev or sqrt(Eigenvalues) to convert Eigenvectors into Loadings (For BOOK /psych Comparison)
    - Names: "sdev, rotation, center, scale, x"
- Output contains
  - SD of Principal Components
  - rotation / loadings: the matrix of variable loadings (columns are eigenvectors)
  - x / scores: The coordinates of the individuals (observations) on the principal components.
- Understanding the result
  - PCA was carried out on the eight predictors in the house data set.
  - PCA was carried out on the eight predictors in the house data set. The \textcolor{pink}{component matrix} is shown in Table \@ref(tab:C34T03). 
  - Each of the columns in Table represents one of the compnonents $Y_i = e_i^T\mathbf{Z}$.
  - The cell entries are called the \textcolor{pink}{component weights}, and represent the partial correlation between the variable and the component.
    - As the component weights are correlations, they range between one and negative one.
    - NOTE: Sign may differ from the Book. 
    - Eigenvalues are given by ${s}^2$ as shown in Table \@ref(tab:C34T04) 
    - First Eigenvalue is 3.9 and there are 8 predictor variables, thus, first component (PC1) explains $3.9/8 \approx 48\%$ of the variance
      - i.e. this single component by itself carries about half of the information in all eight predictors.
      - In general, the first principal component may be viewed as the single best summary of the correlations among the predictors. Specifically, this particular linear combination of the variables accounts for more variability than any other linear combination. 
      - The second principal component is the second-best linear combination of the variables, on the condition that it is orthogonal to the first principal component. It is derived from the variability that is left over, once the first component has been accounted for.


```{definition 'Orthogonal'}
Two vectors are \textcolor{pink}{orthogonal} if they are mathematically independent, have no correlation, and are at right angles to each other. 
```

### Component Matrix {.unlisted .unnumbered}

```{r 'C34-PCA'}
# #Perform PCA by prcomp() #wwww
#ii <- princomp(zw)
pca_zw <- prcomp(zw)
#
names(pca_zw)
#
# #Principal components have "loadings" i.e. $rotation and "scores" i.e. $x
# #Loadings specify the weight that each variable contributes to the principal component.
# #Scores show the value each sample has on each principal component.
#
dim(pca_zw$rotation)
dim(pca_zw$x)
#
# #Matrix Multiplication i.e. %*% of original variables with loadings gives scores
bb <- as.matrix(zw) %*% pca_zw$rotation
all.equal(bb, pca_zw$x)
identical(round(bb, 5), round(pca_zw$x, 5))
#
summary(pca_zw)$importance
#
pca_eigen <- summary(pca_zw)$importance %>% t() %>% as_tibble(rownames = "PCA") %>% 
  rename(SD = 2, pVar = 3, pVarCum = 4) %>% 
  mutate(EigenVal = SD^2, pVarManual = EigenVal/sum(EigenVal), 
         isOne = ifelse(EigenVal > 1, "Yes", "No"),
         isNinty = ifelse(pVarCum < 0.9, "Yes", "No"))
```

```{r 'C34T03', echo=FALSE}
# #Print Kable Table
hh <- pca_zw$rotation 
rownames(hh) <- names(c_xsyw)[-1]
#
cap_hh <- paste0("(C34T03) ", "Houses: PCA Component Matrix") 
f_pKblM(x = hh, caption = cap_hh, isTri = FALSE, debug = TRUE)
```

### Code {.unlisted .unnumbered}

```{r 'C34-PCA-A', eval=TRUE, ref.label=c('C34-PCA')}
#
```

### Loadings and Eigenvectors {.unlisted .unnumbered}

```{r 'C34-Loadings'}
#wwww
# #Component Matrix of prcomp() does not match with either BOOK or psych::principal()
# #prcomp() rotation contains eigenvectors not loadings. 
# #Loadings = Eigenvectors * sqrt(Eigenvalues) = Eigenvectors * sdev
#
# #psych::principal()
psy_zw <- principal(zw, nfactors = ncol(zw), rotate = 'none', scores = TRUE)
names(psy_zw)
#
#psy_zw$loadings
#
# #To Match them Multiply by SD = sqrt(Eigenvalues)
sd_pca <- summary(pca_zw)$sdev
eigen_pca <- sd_pca ^ 2
#
# #Multiply PC1 column with sqrt(Eigenvalue) of PC1 i.e. SD and so on
load_pca <- t(t(pca_zw$rotation) * sd_pca)
#
round(load_pca, 3)
round(psy_zw$loadings, 3)
```



## Orthogonality of PCA {.tabset .tabset-fade}

### Image {.unlisted .unnumbered}

```{r 'C34-PredCorPCA', include=FALSE}
# #Correlation Matrix | Table (Long) | Tibble
hh <- cor(pca_zw$x) %>% as.table() %>% as_tibble(.name_repair = 'unique') %>% 
  #filter(...1 != ...2) %>% 
  filter(!duplicated(paste0(pmax(as.character(...1), as.character(...2)), 
                            pmin(as.character(...1), as.character(...2))))) #%>% 
  #mutate(across(where(is.character), factor, levels = c_zsyw[-1], labels = names(c_zsyw)[-1]))
#
ttl_hh <- "Houses: PCA Corrplot - ALL are ZERO"
cap_hh <- "C34P04"
```

```{r 'C34-PredCorPCA-A', ref.label=c('C34-PredCorPCA', 'C34-CorPlot'), include=FALSE}
#
```

```{r 'C34P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P04", "-House-Corplot-PCA", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P04, device = "png", dpi = 144) 
}
```

```{r 'C34P04', echo=FALSE, fig.cap="(C34P04) Houses: PCA Corrplot - ALL are ZERO"}
knitr::include_graphics(paste0(.z$PX, "C34P04", "-House-Corplot-PCA", ".png"))
```

### Code {.unlisted .unnumbered}

```{r 'C34-PredCorPCA-B', eval=FALSE, ref.label=c('C34-PredCorPCA')}
#
```

## ScreePlot {.tabset .tabset-fade}

### Image {.unlisted .unnumbered}

```{r 'C34-Scree', include=FALSE}
hh <- pca_eigen
#
ttl_hh <- "Houses: PCA Eigenvalue ScreePlot"
cap_hh <- "C34P05"
y_hh <- "Eigenvalue"
```

```{r 'C34-ScreePlot', include=FALSE}
# #IN: hh 
C34 <- hh %>% { ggplot(., aes(x = PCA, y = EigenVal)) + 
    geom_point(aes(color = isOne), size = 3) +
    geom_line(aes(group = 1)) +
    geom_hline(aes(yintercept = 1), color = '#440154FF', linetype = "dashed") +
    annotate("segment", x = 3.5, xend = 3.1, y = 1.6, 
                    yend = 1.2, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("segment", x = 4.5, xend = 4.1, y = 1.3, 
                    yend = 0.9, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("segment", x = 5.5, xend = 5.1, y = 0.6, 
                    yend = 0.2, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    geom_text(data = tibble(x = c(3.5, 4.5, 5.5), y = c(1.7, 1.4, 0.7), 
              labels = c("Eigenvalue Criterion", "Screeplot Criterion", "Elbow Point")), 
              aes(x=x, y=y, label=labels), check_overlap = TRUE) + 
    scale_fill_distiller(palette = "BrBG") +
    #coord_fixed() +
    theme(legend.position = 'none') +
	  labs(y = y_hh, subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P05", "-House-PCA-Scree", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P05, device = "png", dpi = 144) 
}
```

```{r 'C34P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C34P05", "-House-PCA-Scree", ".png"))
```

```{r 'C34-CumVar', include=FALSE}
ttl_hh <- "Houses: PCA Proportion of Variance Explained"
cap_hh <- "C34P06"
y_hh <- NULL
```

```{r 'C34-CumVarPlot', include=FALSE}
# #IN: hh 
C34 <- hh %>% { ggplot(., aes(x = PCA, y = pVarCum)) + 
    geom_point(aes(color = isNinty), size = 3) +
    geom_line(aes(group = 1)) +
    geom_hline(aes(yintercept = 0.9), color = '#440154FF', linetype = "dashed") +
    annotate("segment", x = 4, xend = 4, y = 0.83, 
                    yend = 0.93, arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    geom_text(data = tibble(x = 5.5, y = 0.8, labels = c("Proportion of Variance Covered Criterion")), 
              aes(x=x, y=y, label=labels), check_overlap = TRUE) + 
    scale_fill_distiller(palette = "BrBG") +
    scale_y_continuous(limits = c(0, 1), labels = percent) + 
    theme(legend.position = 'none') +
    labs(y = y_hh,
         subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, C34)
rm(C34)
```

```{r 'C34P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P06", "-House-PCA-Var", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P06, device = "png", dpi = 144) 
}
```

```{r 'C34P06', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C34P06", "-House-PCA-Var", ".png"))
```

```{r 'C34P0506', echo=FALSE, ref.label=c('C34P05', 'C34P06'), fig.cap="(C34P05 C34P06) House: PCA Screeplot with Variance"}
#
```


```{r 'C34T04', echo=FALSE}
# #Print Kable Table
hh <- pca_eigen %>% select(-pVarManual) %>% 
  rename(exp_Variance = 3, cum_Var = 4, isEigenOne = 6, isVarNinty = 7) %>% 
  #mutate(across(where(is.numeric), round, digits = 4)) %>% 
  add_row(summarise(., across(1, ~"Total")), summarise(., across(EigenVal, sum))) %>% 
  mutate(across(where(is.numeric), format, digits = 3, 
                #nsmall = 0, replace.zero = TRUE, zero.print = "0", 
                drop0trailing = TRUE, scientific = FALSE)) 
#
cap_hh <- paste0("(C34T04) ", "Houses: PCA Eigenvalues & Variance") 
f_pKbl(x = hh, caption = cap_hh, debug = FALSE)
```

### Code {.unlisted .unnumbered}

```{r 'C34-Scree-A', eval=FALSE, ref.label=c('C34-Scree', 'C34-ScreePlot', 'C34-CumVar', 'C34-CumVarPlot')}
#
```

## How many Components 

- In our example Single Component (PC1) can account for approximately half of the variability. But, all 8 accounts for 100% variability. So between 1 and 8 where is the cut-off
  - Criteria
    - The Eigenvalue Criterion 
    - The Proportion of Variance Explained Criterion 
    - The Minimum Communality Criterion - Deferred by Book
    - The Scree Plot Criterion

- The Eigenvalue Criterion
  - Refer Figure \@ref(fig:C34P0506) and Table \@ref(tab:C34T04)
  - Sum of the eigenvalues represents the number of variables entered into the PCA i.e. 8
  - An eigenvalue of 1 would then mean that the component would explain about "one variable worth" of the variability. 
  - Therefore, the eigenvalue criterion states that only components with eigenvalues greater than 1 should be retained. 
  - Note that, if there are fewer than 20 variables, the eigenvalue criterion tends to recommend extracting too few components, while, if there are more than 50 variables, this criterion may recommend extracting too many.
  - Thus in example: 3 can be retained. PC4 has value around 0.8 so it may or may not be retained.

- The Proportion of Variance Explained Criterion
  - We can define how much of the total variability that we would like the principal components to account for and then selects them acccordingly
  - Thus in example: 3 can be retained. PC4 will be selected if more than 90% should be accounted
  
- The Scree Plot Criterion
  - \textcolor{pink}{Elbow Point:} The maximum number of components that should be extracted is just before where the plot first begins to straighten out into a horizontal line.
  

## Factor Scores

- Modified Table \@ref(tab:C34T03) as \@ref(tab:C34T05) 
- To investigate the relationship between PC3 and PC4, and their constituent variables, we next consider \textcolor{pink}{factor scores}. Factor scores are estimated values of the factors for each observation, and are based on factor analysis.

```{r 'C34T05', echo=FALSE}
# #Print Kable Table
hh <- pca_zw$rotation[ , 1:4] 
hh[abs(hh) < 0.15] <- NA
rownames(hh) <- names(c_zsyw)[-1]
#
cap_hh <- paste0("(C34T05) ", "Houses: PCA Eigenvectors upto PC4") 
f_pKblM(x = hh, caption = cap_hh, isTri = FALSE, dig = 3, debug = FALSE)
```

```{r 'C34-PC3', include=FALSE}
hh <- zw %>% select(income, h_age) %>% add_column(PC3 = pca_zw$x[ , 3]) %>% 
  rename(Income = 1, Age = 2)
#hh <- hh[1:100, ]
labels_hh <- names(hh) #names(c_zsyw)[-1] #names(hh)
#
ttl_hh <- "Houses: SPLOM PC3 Factor Scores"
cap_hh <- "C34P07"
sub_hh <- NULL 
lgd_hh  <- NULL 
```

```{r 'C34-SPLOM-PC3', include=FALSE, ref.label=c('C34-SPLOM')}
#
```

```{r 'C34P07-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P07", "-House-SPLOM-PC3", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P07, device = "png", dpi = 144) 
}
```

```{r 'C34P07', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C34P07", "-House-SPLOM-PC3", ".png"))
```

```{r 'C34-PC4', include=FALSE}
hh <- zw %>% select(income, h_age) %>% add_column(PC4 = pca_zw$x[ , 4]) %>% 
  rename(Income = 1, Age = 2)
#hh <- hh[1:100, ]
labels_hh <- names(hh) #names(c_zsyw)[-1] #names(hh)
#
ttl_hh <- "Houses: SPLOM PC4 Factor Scores"
cap_hh <- "C34P08"
sub_hh <- NULL 
lgd_hh  <- NULL 
```

```{r 'C34-SPLOM-PC4', include=FALSE, ref.label=c('C34-SPLOM')}
#
```

```{r 'C34P08-Save', include=FALSE}
loc_png <- paste0(.z$PX, "C34P08", "-House-SPLOM-PC4", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = C34P08, device = "png", dpi = 144) 
}
```

```{r 'C34P08', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "C34P08", "-House-SPLOM-PC4", ".png"))
```


```{r 'C34P0708', echo=FALSE, ref.label=c('C34P07', 'C34P08'), fig.cap="(C34P07 C34P08) Correlation Matrices of Factor Scores of PC3 and PC4"}
#
```

- Consider the left side of Figure \@ref(fig:C34P0708)
  - The strong negative correlation between component 3 and median income is strikingly evident
  - But the relationship between component 3 and housing median age is rather amorphous. It would be difficult to estimate the correlation between component 3 and housing median age as being 0.413, with only the scatter plot to guide us. 
- Similarly for the right side of Figure \@ref(fig:C34P0708)
  - The relationship between component 4 and housing median age is crystal clear
  - while the relationship between component 4 and median income is not entirely clear, reflecting its lower positive correlation of 0.374. 
  - We conclude, therefore, that the component weight of 0.413 for housing median age in component 3 is not of practical significance, and similarly for the component weight for median income in component 4.
- This discussion leads us to the following criterion for assessing the component weights. 
  - For a component weight to be considered of practical significance, it should exceed $\pm 0.5$ in magnitude. 
  - Note that the component weight represents the correlation between the component and the variable; thus, the squared component weight represents the amount of the total variability of the variable that is explained by the component. 
  - Thus, this threshold value of $\pm 0.5$ requires that at least 25% of the variance of the variable be explained by a particular component.
- Thus, the Table \@ref(tab:C34T03) is modified further as \@ref(tab:C34T06) 
  - NOTE: Sign differ from the Book but that is OK.
  - NOTE: Valeus should be matching when eigenvectors are converted to loadings
    - Note that the partition of the variables among the four components is \textcolor{pink}{mutually exclusive}, meaning that no variable is shared (after suppression) by any two components
    - and \textcolor{pink}{exhaustive}, meaning that all eight variables are contained in the four components.

```{r 'C34T06', echo=FALSE}
# #Print Kable Table
#hh <- pca_zw$rotation[ , 1:4] 
hh <- load_pca[ , 1:4] 
hh[abs(hh) < 0.5] <- NA
rownames(hh) <- names(c_zsyw)[-1]
#
cap_hh <- paste0("(C34T06) ", "Houses: PCA Loadings (not Eigenvectors) upto PC4") 
f_pKblM(x = hh, caption = cap_hh, isTri = FALSE, dig = 3, debug = FALSE)
```

## Matrix {.tabset .tabset-fade}

### Create {.unlisted .unnumbered}

```{r 'C34-Matrix'}
# #Create a Matrix
# #(Default) The Matrix is Filled Column by Column
matrix(1:9, nrow = 3)
#
# #Change Behavour to fill the matrix by Row
matrix(1:9, nrow = 3, byrow = TRUE)
```

### Multiply Vector {.unlisted .unnumbered}

```{r 'C34-MultiplyVec'}
# #names() does not work on Matrix
mm <- matrix(1:9, ncol = 3, byrow = TRUE)
rownames(mm) <- tail(letters, 3)
colnames(mm) <- head(letters, 3)
mm
#
vv <- 1:3
#
# #Matrix Multipley (Deprecated)
# #Multiply the Vector with Matrix Rows i.e. x * 1, y * 2, z * 3
ii <- diag(vv) %*% mm #loss of rownames because it is taken from Left Side Matrix
#
# #Multiply the Vector with Matrix Columns i.e. a * 1, b * 2, c * 3
jj <- mm %*% diag(vv) #loss of colnames because it is taken from Right Side Matrix
#
# #Check Attributes
attributes(ii)$dimnames
attributes(jj)$dimnames
#
# #Add Missing RowNames or ColNames 
rownames(ii) <- rownames(mm)
colnames(jj) <- colnames(mm)
#
# #Coercing by as.integer() will produce a vector not matrix. Use eiher mode() or class()
#ii[] <- as.integer(ii) #Even using [] is NOT coercing to integer
class(ii) <- "integer"
mode(jj) <- "integer" 
#
# #Print
ii
str(ii)
jj
str(jj)
#
# #Equivalent: sweep() (Deprecated). SLOW, However it keeps the dimnames.  
swp_ii <- sweep(mm, MARGIN = 1, vv, `*`)
swp_jj <- sweep(mm, MARGIN = 2, vv, `*`)
#
# #Recommended:
# #Equivalent: R Recycle Vector Column-wise. So double-transpose is needed if multiplying on jj.
# #Double Transpose is FASTEST & retains dimnames. Bonus: This is Commutative.
rec_ii <- mm * vv
com_ii <- vv * mm 
rec_jj <- t(t(mm) * vv)
com_jj <- t(vv * t(mm))
all(identical(rec_ii, com_ii), identical(rec_jj, com_jj)) #Commutative
#
all(identical(ii, swp_ii), identical(jj, swp_jj), identical(ii, rec_ii), identical(jj, rec_jj))
```

### Square Elements {.unlisted .unnumbered}

```{r 'C34-MatSquare'}
mm <- matrix(1:9, ncol = 3, byrow = TRUE)
rownames(mm) <- tail(letters, 3)
colnames(mm) <- head(letters, 3)
mm
#
# #Square Each Element of the Matrix
mm ** 2 # ** operator is highly obscure and is actually parsed as ^ so use that not this one
mm ^ 2
stopifnot(identical(mm ^ 2, mm ** 2))
```


## Communalities

```{definition 'Communality'}
PCA does not extract all the variance from the variables, but only that proportion of the variance that is shared by several variables. \textcolor{pink}{Communality} represents the proportion of variance of a particular variable that is shared with other variables. Communality values are calculated as the sum of squared component weights, for a given variable.
```

- The communalities represent the overall importance of each of the variables in the PCA as a whole.
- Communalities that are very low for a particular variable should be an indication that the particular variable might not participate in the PCA solution (i.e., might not be a member of any of the principal components).
- Communalities less than 0.5 can be considered to be too low, as this would mean that the variable shares less than half of its variability in common with the other variables. 
  - Now, if we want to keep the variable housing median age as an active part of the analysis, then, extracting only three components would not be adequate, as housing median age shares only 35% of its variance with the other variables. To keep this variable in the analysis, we would need to extract the fourth component, which lifts the communality for housing median age over the 50% threshold. 
- Minimum Communality Criterion
  - Enough components should be extracted so that the communalities for each of these variables exceed a certain threshold (e.g., 50%)

```{r 'C34T07', echo=FALSE}
# #Print Kable Table
hh <- load_pca[ , 1:4] 
# #Add New Columns by Squaring and Sum
hh <- cbind(hh, Comm_PC3 = rowSums(hh[ , 1:3]^2), Comm_PC4 = rowSums(hh^2))
#hh[abs(hh) < 0.4] <- NA
rownames(hh) <- names(c_zsyw)[-1]
#
cap_hh <- paste0("(C34T07) ", "Houses: PCA Loadings with Communalities") 
f_pKblM(x = hh, caption = cap_hh, negPos = c(-0.5, 0.5), isTri = FALSE, dig = 2, debug = FALSE)
```

## Decision on How many Components

- The Eigenvalue Criterion recommended 3 components, but did not absolutely reject the 4 component. Also, for small numbers of variables, this criterion can underestimate the best number of components to extract.
- The Proportion of Variance Explained Criterion stated that we needed to use 4 components if we wanted to account for >90% of the variability. 
- The Scree Plot Criterion said not to exceed 4 components. 
- The Minimum Communality Criterion stated that, if we wanted to keep housing median age in the analysis, we had to extract 4 components.
- Conclusion: PC4 is included.

- Test Dataset
  - We can perform PCA on Test dataset also and that should provide us similar pattern of PC1 to PC4 in terms of eigenvectors and loadings (not exactly same but similar). This should be taken as the confirmation that PCA of Training Data can be applied on Test Data
  - If the results differ too much then it should be taken as the indication of traning data being not a representation of test data. 

## Factor Analysis

Factor Analysis (FA) is related to PCA but the two methods have different goals.

Principal components seek to identify orthogonal linear combinations of the variables, to be used either for descriptive purposes or to substitute a smaller number of uncorrelated components for the original variables. 

In contrast, factor analysis represents a model for the data, and as such is more elaborate. 

The \textcolor{pink}{factor analysis model} hypothesizes that the response vector ${\{X_1, X_2, \ldots, X_m\}}$ can be modeled as linear combinations of a smaller set of ${k}$ unobserved, "latent" random variables ${\{F_1, F_2, \ldots, F_k\}}$ called \textcolor{pink}{common factors}, along with an error term $\mathbf{\epsilon} = {\{\epsilon_1, \epsilon_2, \ldots, \epsilon_m\}}$. Specifically, the factor analysis model is :

\begin{equation}
  \underset{m \times 1}{\mathbf{X - \mu}} = \underset{m \times k}{\mathbf{L}} \, \underset{k \times 1}{\mathbf{F}} + \underset{m \times 1}{\mathbf{\epsilon}}
  (\#eq:fa)
\end{equation}


Where $\underset{m \times 1}{\mathbf{X - \mu}}$ is the response vector, centered by the mean vector, $\underset{m \times k}{\mathbf{L}}$ is the matrix of factor loadings, with $l_{ij}$ representing the factor loading of the $i^{\text{th}}$ variable on the $j^{\text{th}}$ factor, $\underset{k \times 1}{\mathbf{F}}$ represents the vector of unobservable common factors, and $\underset{m \times 1}{\mathbf{\epsilon}}$ represents the error vector. 

The factor analysis model differs from other models, such as the linear regression model, in that the predictor variables ${\{F_1, F_2, \ldots, F_k\}}$ are unobservable. Because so many terms are unobserved, further assumptions must be made before we may uncover the factors from the observed responses alone. 

These assumptions are that $E(\mathbf{F}) = \mathbf{0}, \text{Cov}(\mathbf{F}) = \mathbf{I}, E(\mathbf{\epsilon}) = \mathbf{0}, \text{Cov}(\mathbf{\epsilon})$ is a diagonal matrix. 

Unfortunately, the factor solutions provided by factor analysis are invariant to transformations. Hence, the factors uncovered by the model are in essence nonunique, without further constraints. This indistinctness provides the motivation for factor rotation.

## UCI Data Repository

- Repository of many standard datasets
  - Archieved: https://archive.ics.uci.edu/ml/datasets.php
  - Beta: https://archive-beta.ics.uci.edu/ml/datasets
  - The data is generally in ".data" format which is a CSV File
  - Details are generally in ".names" format which is a Text File

## Data Adult {.tabset .tabset-fade}

\textcolor{pink}{Please import the "C34-adult.csv"} 

- Source: https://archive-beta.ics.uci.edu/ml/datasets/adult
- About: Train [32561, 15] & Test[16281, 15] = Total[48842, 15]
  - The intended task is to find the set of demographic characteristics that can best predict whether or not the individual has an income of over 50000 dollars per year. 
- Steps [(External) Luke Perich](https://rpubs.com/s3589539/503258 "https://rpubs.com")
  - Merged Train and Test for easy and simultaneous cleaning. Source Column attached for easy separation later.
  - 'fnlwgt' (stands for final weight) - Dropped
    - It has no predictive power since it is a feature aimed to allocate similar weights to people with similar demographic characteristics.
  - 'income' 
    - Edited "." in Text and Modified to 0 (<= 50K) and 1 (>50K) as Factor for Summary
  - 'Education' is dropped 
    - It is just a label on 'education_num' (number of years of education). (Not Tested "ForLater")
  - 'marital_status'
    - Number of Levels reduced by merging 3 types of Married. Two of them have small count.
  - 'age' - num - Min 17 to Max 90 All Numbers are Present
  - 'education_num' - num - Min 1 to Max 16 All Numbers are Present
  - 'hours_per_week' - num - 
    - 3 Hours are missing but that can happen 
    - cannot remove 99 hours because 98 hours and other nearby hours are also present in the data
  - 'capital_gain' & 'capital_loss' - Both removed
    - After 41310 dollars, There is directly 99999 dollars which cannot be correct. It should be set to Median if it is kept
    - 44807 observations have 0 capital gain
    - 46560 observations have 0 capital loss
  - 'workclass', 'occupation', 'native_country'
    - These 3 contain Question Mark which have been converted to NA but not removed from dataset to have the possibility of imputation or to keep their other column variable information for analysis
  - 'native_country'
    - With huge bias towards US there is no point in having so many countries or regions even.
    - Changed to Binary Factors of USA and Other


```{r 'C34-GetAdult', include=FALSE}
bb <- aa <- xxC34Adult <- f_getRDS(xxC34Adult)  #wwww
```

### Import {.unlisted .unnumbered}

```{r 'C34-Adult', include=FALSE, eval=FALSE}
tbl <- read_csv(paste0(.z$XL, "C34-adult-train.csv"),
                col_names = c("age", "workclass", "fnlwgt", "education", "education_num",
                              "marital_status", "occupation", "relationship", "race", "sex",
                              "capital_gain", "capital_loss", "hours_per_week", "native_country",
                              "Income"), 
                show_col_types = FALSE)
attr(tbl, "problems") <- NULL
attr(tbl, "spec") <- NULL
tbl_aa <- tbl
# # 1st row needs to be skippe in Test
tbl <- read_csv(paste0(.z$XL, "C34-adult-test.csv"), skip = 1, 
                col_names = c("age", "workclass", "fnlwgt", "education", "education_num",
                              "marital_status", "occupation", "relationship", "race", "sex",
                              "capital_gain", "capital_loss", "hours_per_week", "native_country",
                              "Income"), 
                show_col_types = FALSE)
attr(tbl, "problems") <- NULL
attr(tbl, "spec") <- NULL
tbl_bb <- tbl
```

### Processing {.unlisted .unnumbered}

```{r 'C34-Process', eval=FALSE}
# #Merge Tibbles with ID Names in Column
# #NA Introduced by changing Question Mark to NA
aa <- bind_rows(Train = tbl_aa, Test = tbl_bb, .id = 'source') 
#
bb <- aa %>% 
  select(-c(fnlwgt, education, capital_gain, capital_loss)) %>% 
  mutate(Income = ifelse(Income == "<=50K" | Income == "<=50K.", "0", "1")) %>% 
  mutate(across(c(workclass, occupation, native_country), ~na_if(., "?"))) %>% 
  mutate(native_country = ifelse(str_detect(native_country, 
            paste0(c("United-", "Outlying-US"), collapse = "|")), "USA", "Other")) %>% 
  mutate(across(where(is.character), ~ factor(., levels = unique(.)))) %>% 
  mutate(marital_status = 
    fct_collapse(marital_status, 
      Married = c("Married-civ-spouse", "Married-spouse-absent", "Married-AF-spouse"))) 
#
xxC34Adult <- bb
f_setRDS(xxC34Adult)
```

### Merge Factor Levels {.unlisted .unnumbered}

```{r 'C34-Merge', eval=FALSE}
# #Levels of each Factor Variable
#lapply(aa[ , sapply(aa, is.factor)], levels)
#levels(aa$marital_status)
summary(aa$marital_status)
ii <- aa %>% select(marital_status) %>% 
  mutate(marital_status = 
    fct_collapse(marital_status, 
      Married = c("Married-civ-spouse", "Married-spouse-absent", "Married-AF-spouse")))
#summary(ii$marital_status)
```

### Check Numeric {.unlisted .unnumbered}

```{r 'C34-Numeric', eval=FALSE}
# #Check Numeric Columns by summary()
if(TRUE) aa %>% select(!where(is.numeric)) %>% summary()
if(TRUE) sort(unique(aa$age))
if(TRUE) length(sort(unique(aa$age)))
if(TRUE) aa %>% count(age) %>% mutate(PROP = n/sum(n)) #%>% arrange(desc(n)) %>% head(10)
#
# #Find Missing Numbers in a Sequence of Numbers
#ii <- unique(aa$age) 
ii <- unique(aa$hours_per_week)
jj <- min(ii):max(ii)
jj[!jj %in% ii]
#
# #Equivalent
setdiff(jj, ii)
```

### Search String {.unlisted .unnumbered}

```{r 'C34-StrSearch', eval=FALSE}
# #To Search For Question Mark in All Factor Columns, Question Mark needs to be escaped
# #The Backslash used for escaping itself needs to be escaped using Backslash
aa %>% rowwise() %>%
  mutate(find_me = any(str_detect(c_across(where(is.factor)), 
                                  regex("\\?", ignore_case = TRUE)), na.rm = TRUE)) %>% 
  filter(find_me)
#
# #To Get the Column Names containing a String i.e. '?'
which(vapply(aa, function(x) any(stri_detect(x, regex = "\\?", max_count = 1)), logical(1)))
```

### Replace Mutiple Partial Matches {.unlisted .unnumbered}

```{r 'C34-StrReplace', eval=FALSE}
# #Search and Replace for Multiple Partial Matches 
# #NOTE: "|" should be used to collpase NOT " | "
# #NOTE: Question Marks Replaced as Other
aa %>% mutate(native_country = ifelse(str_detect(native_country, 
            paste0(c("United-", "Outlying-US"), collapse = "|")), "USA", "Other")) %>% 
  count(native_country)
```

### Check Factor {.unlisted .unnumbered}

```{r 'C34-Factor', eval=FALSE}
# #Check Factor Columns by summary()
if(TRUE) aa %>% select(!where(is.factor)) %>% summary()
ii <- factor(aa$native_country)
if(TRUE) levels(ii)
if(TRUE) nlevels(ii)
aa %>% count(native_country) #%>% tail(10) 
```

## Correlation Matrix 

Note that the correlations, although statistically significant in several cases, are overall much weaker than the correlations from the 'houses' data set. A weaker correlation structure should pose more of a challenge for the dimension-reduction method. 

NOTE: While the Book created 'Net Captial', I have skipped that because Capital Gain and Capital Loss Columns have extremely high number of zeroes. Further, an ID column 'fnlwgt' was also dropped.


```{r 'C34-PartAdult', include=FALSE}
#wwww
# #Select Numeric Variables and Break the Original Training Dataset into further Train & Test 
bb <- xxC34Adult %>% filter(source == "Train") %>% 
  select(age, edu = education_num, hours = hours_per_week, Income) 
set.seed(3)  
brk_bb = c(train = 25000, test = nrow(bb))
idx_bb = sample(cut(seq_len(nrow(bb)), c(0, brk_bb), labels = names(brk_bb)))
# #List of Multiple Tibbles
part_l = split(bb, idx_bb)
#
# #nrow(), ncol(), dim() can be applied
vapply(part_l, nrow, FUN.VALUE = integer(1))
stopifnot(identical(nrow(bb), sum(vapply(part_l, nrow, FUN.VALUE = integer(1)))))
#
# #Training Data
adl_xsyw <- part_l$train 
adl_xw <- adl_xsyw %>% select(-Income)
# #Scaling
adl_zsyw <- adl_xsyw %>% mutate(across(where(is.numeric), ~ as.vector(scale(.))))
adl_zw <- adl_zsyw %>% select(-Income)
# #Long
f_wl(adl_xw, adl_zw)
```

```{r 'C34T08', echo=FALSE}
hh <- cor(adl_zw)
cap_hh <- paste0("(C34T08) ", "Adult: Correlation Matrix") 
f_pKblM(x = hh, caption = cap_hh, negPos = c(-0.5, 0.5), dig = 3, debug = FALSE)
```

Factor analysis requires a certain level of correlation in order to function appropriately. The following tests have been developed to ascertain whether there exists sufficiently high correlation to perform factor analysis.

- Note, however, that statistical tests in the context of huge data sets can be misleading. With huge sample sizes, even the smallest effect sizes become statistically significant. This is why data mining methods rely on cross-validation methodologies, not statistical inference.

- The proportion of variability within the standardized predictor variables which is shared in common, and therefore might be caused by underlying factors, is measured by the \textcolor{pink}{Kaiser-Meyer-Olkin (KMO) Measure of Sampling Adequacy}. 
  - Values of the KMO statistic less than 0.50 indicate that factor analysis may not be appropriate. 
-  \textcolor{pink}{Bartlett Test of Sphericity} tests the null hypothesis that the correlation matrix is an identity matrix, that is, that the variables are really uncorrelated. 
  - The statistic reported is the p-value, so that very small values would indicate evidence against the null hypothesis, that is, the variables really are correlated. 
  - For p-values much larger than 0.10, there is insufficient evidence that the variables are correlated, and so factor analysis may not be suitable.
  - It compares an observed correlation matrix to the identity matrix. 
    - Essentially it checks to see if there is a certain redundancy between the variables that we can summarize with a few number of factors. 
    - The null hypothesis of the test is that the variables are orthogonal, i.e. not correlated. 
    - If the numbers in the matrix represent correlation coefficients, like Identity Matrix, it means that each variable is perfectly orthogonal (i.e. "uncorrelated") to every other variable and thus a data reduction technique like PCA or factor analysis would not be able to "compress" the data in any meaningful way. 

- The KMO statistic has a value of 0.52, which is not less than 0.5, meaning that this test does not find the level of correlation to be too low for factor analysis. 
- The p-value for Bartlett Test of Sphericity rounds to zero, so that the null hypothesis that no correlation exists among the variables is rejected. We therefore proceed with the factor analysis.

## KMO Test

```{r 'C34-KMO'}
# #KMO Test: Measure of Sampling Adequacy (MSA) 
KMO(cor(adl_zw))
```


## Bartlett Test of Sphericity

- \textcolor{orange}{Caution:} It is NOT same as 'Bartlett Test for Equality of Variances'
- This tests requires multivariate normality. If this condition is not met, KMO can still be used. 


```{r 'C34-BartSphere'}
# #Bartlett Test of Sphericity
bartsph <- cortest.bartlett(cor(adl_zw), n = nrow(adl_zw))
if(bartsph$p.value < 0.05) {
  cat("Null Rejected. Variables are Correlated. Dimension Reduction can be performed.\n")
} else {
  cat("Failed to reject the Null. Uncorrelated Variables. No benefit in Dimension Reduction.\n")
}
```

## FA {.tabset .tabset-fade}

- To allow us to view the results using a scatter plot, we decide a priori to extract only two factors.
- The following factor analysis is performed using the \textcolor{pink}{principal axis factoring} option. 

  - In principal axis factoring, an iterative procedure is used to estimate the communalities and the factor solution. 
  - This particular analysis required 152 such iterations before reaching convergence. 
  - The eigenvalues and the proportions of the variance explained by each factor are shown in Table

- \textcolor{pink}{fa()}
  - Trying with 'pa' as given in Book, even though 'pa' produces warnings whereas 'minres' does not
- \textcolor{orange}{Warning:} The estimated weights for the factor scores are probably incorrect.  Try a different factor score estimation method.
  - It is coming most probably because age and edu might be serially correlated i.e. 
    - As the Age is increasing, education might also increasing with a lag. Age values might be shifted forward in time with Education values
- \textcolor{orange}{Warning:} An ultra-Heywood case was detected.  Examine the results carefully
  - communality > 1
  - Heywood cases should be treated as invalid. 
  - Try to reduce the number of factors, try other initial communalities (in PAF method), try to drop variables with low KMO, check multicollinearity
  - These are encountered typically when there are too few variables to support the requested number of factors.
    - Both Warnings go away when requested factors were reduced from 3 to 2

```{conjecture 'CorMat'}
\textcolor{brown}{Error in if (prod(R2) < 0) { : missing value where TRUE/FALSE needed}}
```

- Add 'SMC = FALSE' in fa()

- NOTE
  - Cummulative Variance is only 48% i.e. less than half is explained by first two factors extracted.
    - In contrast, the Housing data had ~76% explained by first two factors because there the correlation was strong
  
### pa {.unlisted .unnumbered}

```{r 'C34-FA'}
adl_fa <- fa(adl_zw, nfactors = 2, #ncol(adl_zw)
   fm = "pa", rotate = "none", SMC = FALSE)
# #Loadings
adl_fa$loadings
#
# #Values
adl_fa$values
#
# #Communalities
adl_fa$communalities
```

### pa {.unlisted .unnumbered}

```{r 'C34-FA-PA'}
# #Warnings
# #The estimated weights for the factor scores are probably incorrect.  
# #Try a different factor score estimation method.
# #An ultra-Heywood case was detected.  Examine the results carefully
fa(adl_zw, nfactors = 2, #ncol(adl_zw)
   fm = "pa", rotate = "none", SMC = FALSE)
```

### With cor() {.unlisted .unnumbered}

```{r 'C34-FA-Cor'}
# #Warnings
# #The estimated weights for the factor scores are probably incorrect.  
# #Try a different factor score estimation method.
# #An ultra-Heywood case was detected.  Examine the results carefully
fa(cor(adl_zw), nfactors = 2, #ncol(adl_zw)
   fm = "pa", rotate = "none", n.obs = 25000, SMC = FALSE)
```

### minres {.unlisted .unnumbered}

```{r 'C34-FA-MinRes'}
# #No Warning
fa(adl_zw, nfactors = ncol(adl_zw), fm = "minres", rotate = "none")
```

## Factor Rotation

To assist in the interpretation of the factors, factor rotation may be performed. Factor rotation corresponds to a transformation (usually orthogonal) of the coordinate axes, leading to a different set of factor loadings.

The sharpest focus occurs when each variable has high factor loadings on a single factor, with low-to-moderate loadings on the other factors.

- "ForLater" Figure 4.6 Page 115 - Biplot

- No significant difference was observed with different Rotations, unlike BOOK.


```{r 'C34-BiPlot', include=FALSE, eval=FALSE}
# #This does not look good
# #Works but really bad. Limited Options. Arrow Colour is matched with points Not visible
#hh <- adl_fa$scores#[1:10, ]
biplot.psych(adl_fa, cex = 1, group = adl_zsyw[["Income"]], 
             col = viridis(2), 
             pch=c(21, 24)[adl_zsyw[["Income"]]], main="x")
#
```


```{r 'C34-Rotation'}
# #Huge number of Rotations are available including "none", "varimax", "quartimax", "equamax" ...
# #No significant difference observed
fa(adl_zw, nfactors = 2, fm = "pa", rotate = "none", SMC = FALSE)$loadings
#
fa(adl_zw, nfactors = 2, fm = "pa", rotate = "varimax", SMC = FALSE)$loadings
#
fa(adl_zw, nfactors = 2, fm = "pa", rotate = "quartimax", SMC = FALSE)$loadings
#
fa(adl_zw, nfactors = 2, fm = "pa", rotate = "equamax", SMC = FALSE)$loadings
```

## User Defined Composites


- User Defined Composites or Summated Scales
  - A user-defined composite is simply a linear combination of the variables, which combines several variables together into a single composite measure.
  - The simplest user-defined composite is simply the mean of the variables.
  - When compared to the use of individual variables, user-defined composites provide a way to diminish the effect of measurement error. 
    - Measurement error refers to the disparity between the observed variable values, and the "true" variable value. Measurement error contributes to the background error noise, interfering with the ability of models to accurately process the signal provided by the data, with the result that truly significant relationships may be missed. 
    - User-defined composites reduce measurement error by combining multiple variables into a single measure.
  - Appropriately constructed user-defined composites allow the analyst to represent the manifold aspects of a particular concept using a single measure. 
    - Thus, user-defined composites enable the analyst to embrace the range of model characteristics, while retaining the benefits of a parsimonious model.



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C34-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, c32churn, xxB16Cars, xxB18Churn, brk_bb, hh, idx_bb, part_l,
          xxC34Housing, C34P01, cap_hh, lgd_hh, loc_png, names_bb, sub_hh, ttl_hh, xsyg, xsyl, 
          zsyl, C34P02, mm, nn, oo, zsyw, c_xsyw, C34P03, C34P04, xl, xsyw, xw, zl, zw, c_zsyw, 
          labels_hh, C34P05, C34P06, pca_zw, y_hh, C34P07, C34P08, pca_eigen, com_ii, com_jj,
          eigen_pca, load_pca, psy_zw, rec_ii, rec_jj, sd_pca, swp_ii, swp_jj, vv, adl_fa, adl_xl,
          adl_xsyw, adl_xw, adl_zl, adl_zsyw, adl_zw, bartsph, xxC34Adult, ii_num, jj_chr)
```

```{r 'C34-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/334-DimensionReduction.Rmd-->

# Model Data {#c37}

```{r 'C37', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

> "Univariate Statistical Analysis (335)" was a summary view of Hypothesis Testing.

> "Multivariate Statistics (336)" was a summary view of ANOVA, Goodness of Fit etc.

> "Simple Linear Regression (338)" has been merged in [Anderson C14](#c14 "c14").

> "Multiple Regression and Model Building (339)" has been merged in [Anderson C15](#c15 "c15").

- "Preparing to Model the Data"

## Data Mining

- [Data Mining Methods](#mining-b19 "b19") and [Definitions](#mining-def-c31 "c31")
  - Data mining methods may be categorized as either supervised or unsupervised.
  - Most data mining methods are supervised methods.
  - Unsupervised : Clustering, PCA, Factor Analysis, Association Rules, RFM
  - Supervised : 
    - Regression (Continuous Target) : Linear Regression, Regularised Regression, Decision trees, Ensemble learning 
      - Linear Regression : Ridge, Lasso and Elastic Regression
      - Ensemble learning : Bagging, Boosting (AdaBoost, XGBoost), Random forests
    - Classification (Categorical Target) : Decision trees, Ensemble learning, Logistic Regression, k-nearest neighbor (k-NN), Naive-Bayes 
    - Deep Learning : Neural Networks

```{definition 'Unsupervised-Methods'}
In \textcolor{pink}{unsupervised methods}, no target variable is identified as such. Instead, the data mining algorithm searches for patterns and structures among all the variables. The most common unsupervised data mining method is clustering. Ex: Voter Profile.
```

```{definition 'Supervised-Methods'}
\textcolor{pink}{Supervised methods} are those in which there is a particular prespecified target variable and the algorithm is given many examples where the value of the target variable is provided. This allows the algorithm to learn which values of the target variable are associated with which values of the predictor variables. 
```

## Statistical Inference vs. Data Mining

- Statistical methodology and data mining methodology differ in the following two ways: 
  - Applying statistical inference using the huge sample sizes encountered in data mining tends to result in statistical significance, even when the results are not of practical significance. 
  - In statistical methodology, the data analyst has an a priori hypothesis in mind. Data mining procedures usually do not have an a priori hypothesis.

```{definition 'A-Priori-Hypothesis'}
An \textcolor{pink}{a priori hypothesis} is one that is generated prior to a research study taking place.
```

## Cross-validation {#cross-c37}

```{definition 'Cross-Validation'}
\textcolor{pink}{Cross-validation} is a technique for ensuring that the results uncovered in an analysis are generalizable to an independent, unseen, data set. Generally it is done either with two-fold (1-model) or k-fold (k-models). 
```

- Cross-validation requires the model to show good prediction performance on 'novel' data.
- In data mining, the most common methods are \textcolor{pink}{twofold} cross-validation and \textcolor{pink}{k-fold} cross-validation. 
  - In twofold cross-validation, the data are partitioned, using random assignment, into a training data set and a test data set. The test data set should then have the target variable omitted. Thus, the only systematic difference between the \textcolor{pink}{training data set} and the \textcolor{pink}{test data set} is that the training data includes the target variable and the test data does not. 
  - A provisional data mining model is then constructed using the training samples provided in the training data set.
  - However, the algorithm needs to guard against \textcolor{pink}{"memorizing"} the training set and blindly applying all patterns found in the training set to the future data. Ex: Just because all people named 'David' in the training set are in the high income bracket, it may not be True for all people in general.
  - Therefore, the next step is to examine how the provisional model performs on a test set of data. In the test set the provisional model performs classification according to the patterns and structures it learned from the training set. 
  - The efficacy of the classifications is then evaluated by comparing them against the true values of the target variable. 
  - The provisional model is then adjusted to minimize the error rate on the test set.

- We must insure that the training and test data sets are independent, by \textcolor{pink}{validating the partition}. 
  - By performing graphical and statistical comparisons between the two sets. 
  - For example, we may find that, even though the assignment of records was made randomly, a significantly higher proportion of positive values of an important flag variable were assigned to the training set, compared to the test set. This would bias our results.
  - It is especially important that the characteristics of the target variable be as similar as possible between the training and test data sets. 
  - Hypothesis tests for validating the target variable, based on the type of target variable: t-test (for difference in means), z-test (for difference in proportions), test for homogeneity of proportions

- Cross-validation guards against spurious results, as it is highly unlikely that the same random variation would be found to be significant in both the training set and the test set.
- In k-fold cross validation, the original data is partitioned into k independent and similar subsets. 
  - The model is then built using the data from k−1 subsets, using the ${k}^{\text{th}}$ subset as the test set. 
  - This is done iteratively until we have k different models. The results from the k models are then combined using averaging or voting. 
  - A popular choice for k is 10. 
  - A benefit of using k-fold cross-validation is that each record appears in the test set exactly once; a drawback is that the requisite validation task is made more difficult.

## Overfitting

```{r 'C37D01', comment="", echo=FALSE, results='asis'}
f_getDef("Overfitting")
```

- Increasing the complexity of the model in order to increase the accuracy on the training set eventually and inevitably leads to a degradation in the generalizability of the provisional model to the test set.
  - As the model complexity increases, the error rate on the training set continues to fall in a monotone manner. 
  - However, as the model complexity increases, the test set error rate soon begins to flatten out and increase because the provisional model has memorized the training set rather than leaving room for generalizing to unseen data.

## Bias-Variance Trade-off {#bias-var-c37}

```{r 'C37P01', echo=FALSE, fig.cap="(C37P01) Bias-Variance Trade-off"}
knitr::include_graphics(paste0(.z$PX, "C37P01", "-Bias-Variance", ".png")) #iiii
```

- [(External) Image Source](https://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9 "https://towardsdatascience.com")
- The low complexity model suffers from some classification errors. The classification errors can be reduced by a more complex model. 
  - We might be tempted to adopt the greater complexity in order to reduce the error rate. 
  - However, we should be careful not to depend on the idiosyncrasies of the training set. 
  - The low-complexity model need not change very much to accommodate new data points. i.e. low-complexity model has \textcolor{pink}{low variance}. 
  - However, the high-complexity model must alter considerably if it is to maintain its low error rate.  i.e. high-complexity model has a \textcolor{pink}{high variance}. 

```{definition 'Bias-Variance-Trade-off'}
Even though the high-complexity model has low bias (error rate), it has a high variance; and even though the low-complexity model has a high bias, it has a low variance. This is known as the \textcolor{pink}{Bias-Variance Trade-off}. It is another way of describing the overfitting-underfitting dilemma.
```


- The goal is to construct a model in which neither the bias nor the variance is too high
  - A common method of evaluating how accurate model estimation is proceeding for a continuous target variable is to use the mean-squared error (MSE). (Target: Low MSE) 
    - MSE is a good evaluative measure because it combines both bias and variance. i.e. $\text{MSE} = \text{variance} + \text{bias}^2$

```{r 'C37D02', comment="", echo=FALSE, results='asis'}
f_getDef("MSE") #dddd
```

## Balancing the Training Dataset

- For classification models, in which one of the target variable classes has much lower relative frequency than the other classes, balancing is recommended. 
  - I guess Adult dataset can be suitable candidate for this because there is 75:25 ratio between the two levels of Target variable (income)
  - A benefit of balancing the data is to provide the classification algorithms with a rich balance of records for each classification outcome, so that the algorithms have a chance to learn about all types of records, not just those with high target frequency. 
  - For example, suppose we are running a fraud classification model and our training data set consists of 100000 transactions, of which only 1000 are fraudulent. Then, our classification model could simply predict "non-fraudulent" for all transactions, and achieve 99% classification accuracy. However, clearly this model is useless. Instead, the analyst should balance the training data set so that the relative frequency of fraudulent transactions is increased. 

- There are two ways to accomplish this, which are as follows:
  - Resample a number of fraudulent (rare) records - Discouraged
  - Set aside a number of non-fraudulent (non-rare) records

```{definition 'Resampling'}
\textcolor{pink}{Resampling} refers to the process of sampling at random and with replacement from a data set. It is discouraged.
```

- Suppose we wished our 1000 fraudulent records to represent 25% of the balanced training set, rather than the 1% represented by these records in the raw training data set. 
  - $x = \frac{p(\text{records}) - \text{rare}}{1 - p}$
  - where ${x}$ is the required number of resampled records, ${p}$ represents the desired proportion of rare values in the balanced data set, 'records' represents the number of records in the unbalanced data set, and 'rare' represents the current number of rare target values
  - Thus $x = \frac{0.25 * 100000 - 1000}{1 - 0.25} = 32000$ more records can be added to achieve 25% proportion of fraudulent records in balanced set.
  - \textcolor{orange}{Caution:} Some people discourage this practice because they feel this amounts to fabricating data.

- Alternatively, a sufficient number of non-fraudulent transactions would instead be set aside, thereby increasing the proportion of fraudulent transactions. 
  - To achieve a 25% balance proportion, we would retain only 3000 non-fraudulent records. i.e. discard 96000 of the 99000 non-fraudulent records from the analysis, using random selection.
  - \textcolor{orange}{Caution:} Data mining models might suffer as a result of starving them of data in this way. 
  - Thus, it is advised to decrease the desired balance proportion to something like 10%.

- The test data set should never be balanced. 
  - The test data set represents new data that the models have not seen yet.
  - Note that all model evaluation will take place using the test data set, so that the evaluative measures will all be applied to unbalanced (real-world-like) data.

- Direct overall comparisons between the original and balanced data sets are futile, as changes in character are inevitable. 
  - Because some predictor variables have higher correlation with the target variable than do other predictor variables, the character of the balanced data will change. 
  - For example, suppose we are working with the Churn data set, and suppose that churners have higher levels of 'day minutes' than non-churners. Then, when we balance the data set, the overall mean of 'day minutes' will increase, as we have eliminated so many non-churner records. Such changes cannot be avoided when balancing data sets. 
  - However, apart from these unavoidable changes, and although the random sampling tends to protect against systematic deviations, data analysts should provide evidence that their balanced data sets do not otherwise differ systematically from the original data set. 
  - This can be accomplished by examining the graphics and summary statistics from the original and balanced data set, partitioned on the categories of the target variable. 
  - Hypothesis tests may be applied. 
  - If deviations are uncovered, the balancing should be reapplied. 
  - Cross-validation measures can be applied if the analyst is concerned about these deviations. 
    - Multiple randomly selected balanced data sets can be formed, and the results averaged, for example.

## Baseline Performance

For example, suppose we report that "only" 28.4% of customers adopting our International Plan will churn. That does not sound too bad, until we recall that, among all of our customers, the overall churn rate is only 14.49%. This overall churn rate may be considered our \textcolor{pink}{baseline}, against which any further results can be calibrated. Thus, belonging to the International Plan actually nearly doubles the churn rate, which is clearly not good.

For example, suppose the algorithm your analytics company currently uses succeeds in identifying 90% of all fraudulent online transactions. Then, your company will probably expect your new data mining model to outperform this 90% baseline.


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C37-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C37-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/337-PrepModel.Rmd-->

# (C40) {#c40 .unlisted .unnumbered}

```{r 'C40', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C40-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C40-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/340-C40.Rmd-->

# (C41) {#c41 .unlisted .unnumbered}

```{r 'C41', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C41-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C41-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/341-C41.Rmd-->

# (C42) {#c42 .unlisted .unnumbered}

```{r 'C42', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C42-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C42-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/342-C42.Rmd-->

# (C43) {#c43 .unlisted .unnumbered}

```{r 'C43', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C43-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C43-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/343-C43.Rmd-->

# (C44) {#c44 .unlisted .unnumbered}

```{r 'C44', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C44-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C44-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/344-C44.Rmd-->

# (C45) {#c45 .unlisted .unnumbered}

```{r 'C45', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C45-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C45-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/345-C45.Rmd-->

# (C46) {#c46 .unlisted .unnumbered}

```{r 'C46', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C46-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C46-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/346-C46.Rmd-->

# (C47) {#c47 .unlisted .unnumbered}

```{r 'C47', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C47-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C47-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/347-C47.Rmd-->

# (C48) {#c48 .unlisted .unnumbered}

```{r 'C48', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C48-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C48-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/348-C48.Rmd-->

# Hierarchical and K-means Clustering {#c49}

```{r 'C49', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview


```{definition 'Clustering'}
The goal of cluster analysis is to ascertain whether the observations fall into relatively distinct groups. \textcolor{pink}{Clustering} refers to the grouping of records, observations, or cases into classes of similar objects. Clustering differs from classification in that there is no target variable for clustering.
```

```{definition 'Cluster'}
A \textcolor{pink}{cluster} is a collection of records that are similar to one another and dissimilar to records in other clusters.
```

- Cluster vs. Classification
  - Clustering differs from classification in that there is no target variable for clustering. 
  - The clustering task does not try to classify, estimate, or predict the value of a target variable.
  - Instead, clustering algorithms seek to segment the entire data set into relatively homogeneous subgroups or clusters, where the similarity of the records within the cluster is maximized, and the similarity to records outside this cluster is minimized.
    - Example: All zipcodes of a country can be described in terms of distinct lifestyle types (e.g. 66 types). One of them "Upper Crust" might be the wealthiest lifestyle of the country i.e. couples between ages of 45-64 without any dependents (children, parents) living at their home. This segment might be having median earnings of ~1-million dollars and might be possessing a postgraduate degree. No other lifestyle type would have a more opulent standard of living.
    - Methods: Hierarchical and k-means clustering, Kohonen networks, BIRCH clustering
  - For optimal performance, clustering algorithms, just like algorithms for classification, require the data to be normalized so that no particular variable or subset of variables dominates the analysis.
  - Clustering algorithms seek to construct clusters of records such that the \textcolor{pink}{between-cluster} variation is large compared to the \textcolor{pink}{within-cluster} variation. 
  - For continuous variables, we can use euclidean distance
  - For categorical variables, we may again define the "different from" function for comparing the $i^{\text{th}}$ attribute values of a pair as 0 when $x_i = y_i$ and 1 otherwise.

```{definition 'Euclidean-Distance'}
\textcolor{pink}{Euclidean distance} between records is given by equation, $d_{\text{Euclidean}}(x,y) = \sqrt{\sum_i{\left(x_i - y_i\right)^2}}$, where $x = \{x_1, x_2, \ldots, x_m\}$ and $y = \{y_1, y_2, \ldots, y_m\}$ represent the ${m}$ attribute values of two records.
```

## Hierarchical Clustering

```{definition 'Hierarchical-Clustering'}
In \textcolor{pink}{hierarchical clustering}, a treelike cluster structure (\textcolor{pink}{dendrogram}) is created through recursive partitioning (divisive methods) or combining (agglomerative) of existing clusters. 
```

```{definition 'Agglomerative-Clustering'}
\textcolor{pink}{Agglomerative clustering} methods initialize each observation to be a tiny cluster of its own. Then, in succeeding steps, the two closest clusters are aggregated into a new combined cluster. In this way, the number of clusters in the data set is reduced by one at each step. Eventually, all records are combined into a single huge cluster. mMost computer programs that apply hierarchical clustering use agglomerative methods.
```

```{definition 'Divisive-Clustering'}
\textcolor{pink}{Divisive clustering} methods begin with all the records in one big cluster, with the most dissimilar records being split off recursively, into a separate cluster, until each record represents its own cluster. 
```

## Distance between Clusters

```{definition 'Single-Linkage'}
\textcolor{pink}{Single linkage}, the nearest-neighbor approach, is based on the minimum distance between any record in cluster A and any record in cluster B. Cluster similarity is based on the similarity of the most similar members from each cluster. It tends to form long, slender clusters, which may sometimes lead to heterogeneous records being clustered together. 
```

```{definition 'Complete-Linkage'}
\textcolor{pink}{Complete linkage}, the farthest-neighbor approach, is based on the maximum distance between any record in cluster A and any record in cluster B. Cluster similarity is based on the similarity of the most dissimilar members from each cluster. It tends to form more compact, spherelike clusters. 
```

```{definition 'Average-Linkage'}
\textcolor{pink}{Average linkage} is designed to reduce the dependence of the cluster-linkage criterion on extreme values, such as the most similar or dissimilar records. The criterion is the average distance of all the records in cluster A from all the records in cluster B. The resulting clusters tend to have approximately equal within-cluster variability. In general, average linkage leads to clusters more similar in shape to complete linkage than does single linkage.
```

## k-means Clustering

- Refer [k-means Algorithm](#k-means-b20 "b20")
- Refer [Pseudo F-Statistic](#f-stat-c49 "c49")

- One potential problem for applying the k-means algorithm is: Who decides how many clusters to search for i.e. who decides k
  - Unless the analyst has a priori knowledge of the number of underlying clusters; therefore, an "outer loop" should be added to the algorithm, which cycles through various promising values of k. 
  - Clustering solutions for each value of k can therefore be compared, with the value of k resulting in the largest F statistic being selected. 
  - Alternatively, some clustering algorithms, such as the \textcolor{pink}{BIRCH} clustering algorithm, can select the optimal number of clusters.

- What if some attributes are more relevant than others to the problem formulation
  - As cluster membership is determined by distance, we may apply the axis-stretching methods for quantifying attribute relevance.

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C49-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C49-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/349-Clustering.Rmd-->

# (C50) {#c50 .unlisted .unnumbered}

```{r 'C50', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C50-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C50-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/350-C50.Rmd-->

# (C51) {#c51 .unlisted .unnumbered}

```{r 'C51', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C51-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C51-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/351-C51.Rmd-->

# Cluster Goodness {#c52}

```{r 'C52', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Measuring Cluster Goodness"

```{definition 'Cluster-Separation'}
\textcolor{pink}{Cluster separation} represents how distant the clusters are from each other.
```

```{definition 'Cluster-Cohesion'}
\textcolor{pink}{Cluster cohesion} refers to how tightly related the records within the individual clusters are. SSE accounts only for cluster cohesion.
```

## Silhouette Method {#sil-c52}

```{definition 'Silhouette'}
The \textcolor{pink}{silhouette} is a characteristic of each data value. For each data value i,
$\text{Silhouette}_i = s_i = \frac{b_i - a_i}{\text{max}(b_i, a_i)} \to s_i \in [-1, 1]$, where $a_i$ is the distance between the data value (Cohesion) and its cluster center, and $b_i$ is the distance between the data value and the next closest cluster center (Separation).
```

- The silhouette value is used to gauge how good the cluster assignment is for that particular point. 
  - A positive value indicates that the assignment is good, with higher values being better than lower values. 
  - A value that is close to zero is considered to be a weak assignment, as the observation could have been assigned to the next closest cluster with limited negative consequence. 
  - A negative silhouette value is considered to be misclassified, as assignment to the next closest cluster would have been better. 
  - It accounts for both separation and cohesion. 
    - $a_i$ represents cohesion, as it measures the distance between the data value and its cluster center
    - $b_i$ represents separation, as it measures the distance between the data value and a different cluster. 
  - Taking the average silhouette value over all records yields a useful measure of how well the cluster solution fits the data.

## Silhouette on Iris Data

"ForLater" - Nothing groundbreaking there for now.

## Pseudo F-Statistic {#f-stat-c49}

```{definition 'pseudo-F'}
The \textcolor{pink}{pseudo-F statistic} is measures the ratio of (i) the separation between the clusters, as measured by the \textcolor{pink}{mean square between the clusters (MSB)}, to (ii) the spread of the data within the clusters as measured by the \textcolor{pink}{mean square error (MSE)}. i.e. $F_{k-1, N-k} = \frac{\text{MSB}}{\text{MSE}} = \frac{\text{SSB}/{k-1}}{\text{SSE}/{N-k}}$
```

- Pseudo F-Statistic
  - Clustering algorithms seek to construct clusters of records such that the between-cluster variation is large compared to the within-cluster variation. Because this concept is analogous to the analysis of variance, we may define a pseudo-F statistic
  - MSB represents the between-cluster variation and MSE represents the within-cluster variation. 
  - Thus, a "good" cluster would have a large value of the pseudo-F statistic, representing a situation where the between-cluster variation is large compared to the within-cluster variation. 
  - Hence, as the k-means algorithm proceeds, and the quality of the clusters increases, we would expect MSB to increase, MSE to decrease, and F to increase.
  - \textcolor{orange}{Caution:} 
    - pseudo-F statistic should not be used to test for the presence of clusters in data.
    - However, if we have reason to believe that clusters do exist in the data, but we do not know how many clusters there are, then pseudo-F can be helpful.

## Silhouette on Iris Data

"ForLater" - NOTE that Pseudo F-Statistic prefer k=3 in contrast to the Silhouette which preferred k=2.

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C52-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C52-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/352-ClusterGoodness.Rmd-->

# Association Rules {#c53}

```{r 'C53', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

```{definition 'Affinity-Analysis'}
\textcolor{pink}{Affinity analysis}, (or Association Rules or Market Basket Analysis), is the study of attributes or characteristics that "go together". It seeks to uncover rules for quantifying the relationship between two or more attributes. Association rules take the form \textcolor{pink}{"If antecedent, then consequent"}, along with a measure of the support and confidence associated with the rule.
```

- Example Beer-Diaper: 
  - If out of 1000 customers, 200 bought diapers and of the 200 who bought diapers, 50 bought beer.
  - Then Association Rule: "If buy diapers, then buy beer."
  - Prior Proportion = Support = Consequent / Total = 50/1000 = 5%
  - Confidence = Consequent / Antecedent = 50 / 200 = 25%
- Problem: Dimensionality
  - The number of possible association rules grows exponentially in the number of attributes.
  - Ex: Suppose that a store has 100 items and any combination of them can be purchased by a customer. i.e. a customer can buy or not buy each of the items. Then there are $2^{100}$ association rules
  - The a priori algorithm for mining association rules, however, takes advantage of structure within the rules themselves to reduce the search problem to a more manageable size. 

## Data Representation for Market Basket Analysis

- Assuming we ignore quantity purchased. Currently, we are trying to identify which items go together.
- Transaction data: Each row represents a transaction. Two variables ID & Items ("Apple, Banana, Orange")
  - It can be converted to long format where Two variables ID & Items and Items contain only 1 unique item and ID is not unique any more.
- Tabular Data: (Wider) ID Column and each item has its own unique column. ID is unique. Columns are Binary with 1 as Yes /Purchased and 0 representing No /did not buy.
  - Note: For simplicity, the variable here is Flag (Categorical, Binary). However, the a priori algorithm can take Categorical data with more than 2 levels without any issue.

## Set Theory {#arules-c53}

- Refer [Association Rules](#arules-b22 "b22")
- Let $I$ represent set of items. 
- Let $D$ be the set of transactions represented where each Transaction 'T' in D represents a set of items contained in I.
- Suppose that we have a particular set of items A (e.g., potato and tomato), and another set of items B (e.g., onion). 
- Then an \textcolor{pink}{association rule} takes the form if A, then B (i.e., $A \Rightarrow B$), where the \textcolor{pink}{antecedent A} and the \textcolor{pink}{consequent B} are proper subsets of I, and A and B are mutually exclusive. 
  - This definition would exclude, for example, trivial rules such as if potato and tomato, then potato.

```{definition 'Support'}
The \textcolor{pink}{support (s)} for a particular association rule $A \Rightarrow B$ is the proportion of transactions in the set of transactions D that contain both \textcolor{pink}{antecedent A} and \textcolor{pink}{consequent B}. Support is Symmetric. $\text{Support} = P(A \cap B) = \frac{\text{Number of transactions containing both A and B}}{\text{Total Number of Transactions}}$
```

```{definition 'Confidence'}
The \textcolor{pink}{confidence (c)} of the association rule $A \Rightarrow B$ is a measure of the accuracy of the rule, as determined by the percentage of transactions in the set of transactions D containing \textcolor{pink}{antecedent A} that also contain \textcolor{pink}{consequent B}. Confidence is Asymmetric $\text{Confidence} = P(B|A) = \frac{P(A \cap B)}{P(A)} = \frac{\text{Number of transactions containing both A and B}}{\text{Total Number of Transactions containing A}}$
```

- Analysts may prefer rules that have either high support or high confidence, and usually both. 
  - Strong rules are those that meet or surpass certain minimum support and confidence criteria. 
  - For example, an analyst interested in finding which supermarket items are purchased together may set a minimum support level of 20% and a minimum confidence level of 70%. 
  - However, a fraud detection analyst or a terrorism detection analyst would need to reduce the minimum support level to 1% or less, because comparatively few transactions are either fraudulent or terror-related. 
  - To provide an overall measure of usefulness for an association rule, analysts sometimes multiply the support with confidence. This allows the analyst to rank the rules according to a combination of prevalence and accuracy.

```{definition 'Itemset'}
An \textcolor{pink}{itemset} is a set of items contained in I, and a k-itemset is an itemset containing k items. For example, {Potato, Tomato} is a 2-itemset, and {Potato, Tomato, Onion} is a 3-itemset, each from the vegetable stand set I. 
```

```{definition 'Itemset-Frequency'}
The \textcolor{pink}{itemset frequency} is simply the number of transactions that contain the particular itemset. 
```

```{definition 'Frequent-Itemset'}
A \textcolor{pink}{frequent itemset} is an itemset that occurs at least a certain minimum number of times, having itemset frequency $\geq \phi$. We denote the set of frequent k-itemsets as $F_k$.
```

## Mining Association Rules

- It is a Two-step Process
  - Find all frequent itemsets; that is, find all itemsets with frequency $\geq \phi$. 
  - From the frequent itemsets, generate association rules satisfying the minimum support and confidence conditions. 
  
```{definition 'A-Priori-Property'}
\textcolor{pink}{a priori property}: If an itemset Z is not frequent, then for any item A, $Z \cup A$ will not be frequent. In fact, no superset of Z (itemset containing Z) will be frequent. 
```

- The \textcolor{pink}{a priori algorithm} takes advantage of the a priori property to shrink the search space. 
  - This property reduces significantly the search space for the a priori algorithm. 
  - DataType:
    - It can handle Categorical data without any issue.
    - Numerical attributes need to be supplied after discretisation. However, this will result in loss of information. 
      - Alternative method for mining association rules is \textcolor{pink}{generalised rule induction (GRI)} which can handle either categorical or numerical variables as inputs, but still requires categorical variables as outputs.


- "ForLater" - Apply a priori on Adult Dataset

- "ForLater" - "Association Rules are easy to do badly" - Example: Adult Dataset
  - If 'workclass is Private' then 'Sex is Male' with Support 69.5% and Confidence 65.6%
    - One needs to take into account the raw (prior) proportion of males in the data set, which in this case is 66.8%. In other words, applying this association rule actually reduces the probability of randomly selecting a male from 0.6684 to 0.6563. 
  - Why, then, if the rule is so useless, did the software report it 
    - The quick answer is that the default ranking mechanism is confidence.
  - (Aside), So maybe we can drop those rules which have lower confidence than the proportion in the dataset
  - We can provide a priori association rules using the \textcolor{pink}{confidence difference} as the evaluative measure.
    - Here, rules are favored that provide the greatest increase in confidence from the prior to the posterior. 
    - Ex: If 'Marital status= Divorced' then 'Sex=Female', Support 13.%, Confidence 60%
      - The data set contains 33.16% females, so an association rule that can identify females with 60% confidence is useful. 
      - The confidence difference for this association rule is 0.60029−0.3316=0.26869 between the prior and posterior confidences.
  - Alternatively, analysts may prefer to use the \textcolor{pink}{confidence ratio} to evaluate potential rules
    - Confidence Ratio of above rule is 0.4476

## Usefulness of Association Rules

```{definition 'Lift'}
\textcolor{pink}{Lift} is a measure that can quantify the usefulness of an association rule. Lift is Symmetric. $\text{Lift} = \frac{\text{Rule Confidence}}{\text{Prior proportion of Consequent}}$
```

Not all association rules are equally useful. Thus Lift can be used to quantify its usefulness.

- Example Beer-Diaper: 
  - If out of 1000 customers, 200 bought diapers and of the 200 who bought diapers, 50 bought beer.
  - Then Association Rule: "If buy diapers, then buy beer."
  - Prior Proportion = Support = Consequent / Total = 50/1000 = 5%
  - Confidence = Consequent / Antecedent = 50 / 200 = 25%
  - Lift = Confidence / Support = 25/5 = 5
  - "Customers who buy diapers are five times as likely to buy beer as customers from the entire data set."

- \textcolor{pink}{Question:} We know 50 beer out of 200 diaper. But we do not know how many beer overall out of total 1000. 
  - See the next example of diaper-makeup. Here we have separate proportions available. 40 makeup in 1000, 5 makeup in 200.
  - The above diaper-beer woule be valid if the missing information is added in the form that 50 beer in 1000 and 50 beer in 200 diaper. In that case it is obvious that people who buy diapers are definitely 5 times as likely to buy beer.
  - In fact we can say that people who are not buying diapers are 'somehow' not buying beer at all. ~~Extrapolate from Diapers to Babies and from Beer to Alchoholism and we can say with full judgemental eyes that "Babies are the cause of Alchocholism". Hence Proved!~~


- Diaper-Makeup Situation:
  - "40 of the 1000 customers bought expensive makeup, whereas, of the 200 customers who bought diapers, only 5 bought expensive makeup."
  - Then Association Rule: "If buy diapers, then buy expensive makeup"
  - Prior Proportion = Support = Consequent / Total = 40/1000 = 4%
  - Confidence = Consequent / Antecedent = 5 / 200 = 2.5%
  - Lift = Confidence / Support = 2.5/4 = 0.625
  - So, customers who buy diapers are only 62.5% as likely to buy expensive makeup as customers in the entire data set.

- Lift
  - Lift value of 1 implies that A and B are independent events, meaning that knowledge of the occurrence of A does not alter the probability of the occurrence of B. Such relationships are not useful from a data mining perspective, and thus we prefer our association rules to have a lift value different from 1.

- Association Rules are Supervised or Unsupervised
  - most data mining methods represent supervised learning
  - Association rule mining, however, can be applied in either a supervised or an unsupervised manner. 
  - Analysis of purchase patterns would be unsupervised because we are interested in which items go together. However, analysis of voter profile can be supervised because voting preference, naturally, acts as a Target and fulfill the role of consequent not antecedent.

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'C53-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C53-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/353-AssocitationRules.Rmd-->

# (C54) {#c54 .unlisted .unnumbered}

```{r 'C54', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C54-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C54-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/354-C54.Rmd-->

# (C55) {#c55 .unlisted .unnumbered}

```{r 'C55', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C55-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C55-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/355-C55.Rmd-->

# (C56) {#c56 .unlisted .unnumbered}

```{r 'C56', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C56-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C56-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/356-C56.Rmd-->

# (C57) {#c57 .unlisted .unnumbered}

```{r 'C57', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C57-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C57-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/357-C57.Rmd-->

# (C58) {#c58 .unlisted .unnumbered}

```{r 'C58', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C58-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C58-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/358-C58.Rmd-->

# (C59) {#c59 .unlisted .unnumbered}

```{r 'C59', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C59-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C59-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/359-C59.Rmd-->

# (C60) {#c60 .unlisted .unnumbered}

```{r 'C60', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C60-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C60-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/360-C60.Rmd-->

# (C61) {#c61 .unlisted .unnumbered}

```{r 'C61', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C61-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C61-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/361-C61.Rmd-->

# (C62) {#c62 .unlisted .unnumbered}

```{r 'C62', include=FALSE, cache=FALSE, eval=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

<!-- 
## Overview

- "ForLater"

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

-->

```{r 'C62-Cleanup', include=FALSE, cache=FALSE, eval=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'C62-Validation', include=FALSE, cache=FALSE, eval=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/362-C62.Rmd-->

# Statistical Learning (F65) {#f65}
> Definitions and Exercises are from the Book [@James]

```{r 'F65', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

> "Introduction (364)" has the Basics only.

\textcolor{pink}{This section assumes a basic understanding of statistical concepts and R. Focus is on implementation.}

## Packages

```{r 'F65-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("ISLR2", dependencies = TRUE)
}
```

## Introduction

```{definition 'Statistical-Learning'}
\textcolor{pink}{Statistical learning} refers to a vast set of tools for understanding data. These tools can be classified as \textcolor{pink}{supervised} or \textcolor{pink}{unsupervised}. 
```

## Statistical Learning

- $Y = f(X) + \epsilon$
  - Where Y is the quantitative response of predictors $X = \{x_1, x_2, \ldots, x_p\}$. 
  - 'f' is some fixed but unknown function of predictors. 'f' represents the systematic information that X provides about Y
  - $\epsilon$ is a 'random error' term, which is independent of X and has mean zero. 

- \textcolor{pink}{Prediction}: a set of inputs X are readily available, but the output Y cannot be easily obtained
  - Prediction: $\hat{Y} = \hat{f}\!(X)$
  - where $\hat{f}$ represents our estimate for 'f', and $\hat{Y}$ represents the resulting prediction for Y. 
  - Here, typically, we are not concerned with the exact form of $\hat{f}$, provided that it yields accurate predictions for Y.
  - The accuracy of $\hat{Y}$ as a prediction for Y depends on two quantities: reducible error and the irreducible error. 
    - In general, $\hat{f}$ will not be a perfect estimate for 'f', and this inaccuracy will introduce some error. This is \textcolor{pink}{reducible error} because we can potentially improve the accuracy of $\hat{f}$ by using the most appropriate statistical learning technique to estimate 'f'. 
    - However, our prediction would still have some error in it. This is because Y is also a function of $\epsilon$, which, by definition, cannot be predicted using X. 
    - Therefore, variability associated with $\epsilon$ also affects the accuracy of our predictions. This is known as the \textcolor{pink}{irreducible error}, because no matter how well we estimate 'f', we cannot reduce the error introduced by $\epsilon$.
  - Why is the irreducible error larger than zero
    - The quantity $\epsilon$ may contain unmeasured variables or unmeasurable variation that are useful in predicting Y. Since we did not or can not measure them, 'f' cannot use them for its prediction. 

- \textcolor{pink}{Inference}: We are often interested in understanding the association between Y and X. 
  - In this situation we wish to estimate 'f', but our goal is not necessarily to make predictions for Y.
  - Now $\hat{f}$ cannot be treated as a black box, because we need to know its exact form. 
  - In this setting, one may be interested in answering the following questions:
    - Which predictors are associated with the response
    - What is the extent of the association (Weak, Strong etc.)
    - What is the relationship between the response and each predictor (positive, negtive etc.)
    - What is the type of relationship (linear, polynomial etc.)
    
- Our goal is to apply a statistical learning method to the training data in order to estimate the unknown function 'f'. 
  - In other words, we want to find a function $\hat{f}$ such that $\hat{Y} \approx \hat{f}\!(X)$ for any observation (X, Y). 
  - Broadly speaking, most statistical learning methods for this task can be characterized as either parametric or non-parametric.
  
- \textcolor{pink}{Parametric methods} involve a two-step model-based approach.
  - First, we make an assumption about the functional form, or shape, of 'f'. 
    - Ex: 'f' is linear in X: $\hat{f}\!(X) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p$
      - Once we have assumed that 'f' is linear, the problem of estimating 'f' is greatly simplified.
      - Instead of having to estimate an entirely arbitrary p-dimensional function f(X), one only needs to estimate the p + 1 coefficients $\{\beta_0, \beta_1, \beta_2, \ldots, \beta_p\}$.
  - After a model has been selected, we need a procedure that uses the training data to fit or train the model. 
    - Ex: In the case of the linear model, we need to estimate the parameters $\{\beta_0, \beta_1, \beta_2, \ldots, \beta_p\}$.
    - The most common approach to fitting the model is referred to as (ordinary) least squares. However, there are many possible ways to fit the linear model. 
  - This model-based approach is referred to as parametric; it reduces the problem of estimating 'f' down to one of estimating a set of parameters. Assuming a parametric form for 'f' simplifies the problem of estimating 'f' because it is generally much easier to estimate a set of parameters than it is to fit an entirely arbitrary function 'f'. 
  - The potential \textcolor{pink}{disadvantage} of a parametric approach is that the model we choose will usually not match the true unknown form of 'f'. 
    - To address this problem we can choose flexible models that can fit many different possible functional forms flexible for 'f'. But in general, fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to 'overfitting'.


- \textcolor{pink}{Non-parametric methods} do not make explicit assumptions about the functional form of 'f'. 
  - Instead they seek an estimate of 'f' that gets as close to the data points as possible without being too rough or wiggly. 
  - Such approaches can have a major advantage over parametric approaches: by avoiding the assumption of a particular functional form for f, they have the potential to accurately fit a wider range of possible shapes for 'f'. 
  - Any parametric approach brings with it the possibility that the functional form used to estimate 'f' is very different from the true 'f', in which case the resulting model will not fit the data well. In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of 'f' is made. 
  - But non-parametric approaches do suffer from a major \textcolor{pink}{disadvantage}: since they do not reduce the problem of estimating 'f' to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for 'f'.
  - Ex: \textcolor{pink}{spline}


- why would we ever choose to use a more restrictive method instead of a very flexible approach
  - Trade-Off Between Prediction Accuracy and Model Interpretability
  - If we are mainly interested in inference, then restrictive models are much more interpretable. Ex: In linear model, it is easy to understand the relationship between Y and X.
  - In contrast, very flexible approaches, (e.g. splines, boosting) can lead to such complicated estimates of 'f' that it is difficult to understand how any individual predictor is associated with the response.
    - Note: We might obtain more accurate predictions using a less flexible method. This phenomenon, which may seem counterintuitive at first glance, has to do with the potential for overfitting in highly flexible methods.
  - Examples:
    - Least squares linear regression is relatively inflexible but is quite interpretable. 
    - The lasso relies upon the linear model but uses an alternative fitting procedure for estimating the coefficients. It is more restrictive in estimating the coefficients, and sets a number of them to exactly zero. Hence in this sense the lasso is a less flexible approach than linear regression. It is also more interpretable than linear regression, because in the final model the response variable will only be related to a small subset of the predictors. 
    - Generalized additive models (GAM) instead extend the linear model to allow for certain non-linear relationships. Consequently GAM are more flexible than linear regression. They are also somewhat less interpretable than linear regression, because the relationship between each predictor and the response is now modeled using a curve. 
    - Finally, fully non-linear methods such as bagging, boosting, support vector machines, and neural networks (deep learning) are highly flexible approaches that are harder to interpret.
  
- Sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear-cut. 
  - Supervised vs. Unsupervised Learning
  - For instance, suppose that we have a set of 'n' observations. For 'm' of the observations, where m < n, we have both predictor measurements and a response measurement. For the remaining (n − m) observations, we have predictor measurements but no response measurement. 
  - Such a scenario can arise if the predictors can be measured relatively cheaply but the corresponding responses are much more expensive to collect. We refer to this setting as a \textcolor{pink}{semi-supervised learning} problem.

- Why is it necessary to introduce so many different statistical learning approaches, rather than just a single best method
  - No one method dominates all others over all possible data sets. 
  - On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. 
  
- Regression Model Accuracy or Quality of Fit (MSE)
  - we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data.

- \textcolor{pink}{Refer:} Figure 2.9 (page 31) For impact of degrees of freedom on MSE and the explanation (and extended to Figure 2.10, 2.11)


- What do we mean by the variance and bias of a statistical learning method
  - The Bias-Variance Trade-Off
    - The expected test MSE, for a given value $x_0$, can always be decomposed into the sum of three fundamental quantities: the variance of $\hat{f}\!(x_0)$, the squared bias of $\hat{f}\!(x_0)$ and the variance of the error term $\epsilon$
    - In order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves \textcolor{pink}{low variance and low bias}. 
    - Note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative.
    - Hence, we see that the expected test MSE can never lie below Var$(\epsilon)$ i.e. the irreducible error
  - \textcolor{pink}{Variance} refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. Different training data sets will result in a different $\hat{f}$. But ideally the estimate for 'f' should not vary too much between training sets. In general, more flexible statistical methods have higher variance.
  - On the other hand, \textcolor{pink}{bias} refers to the error that is introduced by approximating a complicated problem by a much simpler model. 
    - For example, linear regression assumes that there is a linear relationship between Y and X. It is unlikely that any real-life problem truly has such a simple linear relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of f. Generally, more flexible methods result in less bias.
  - As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. 
    - The relative rate of change of these two quantities determines whether the test MSE increases or decreases. 
    - As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. 
    - However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases. 
  - Good test set performance of a statistical learning method requires low variance as well as low squared bias. 
    - This is referred to as a trade-off because it is easy to obtain a method with extremely low bias but high variance (for instance, by drawing a curve that passes through every single training observation) or a method with very low variance but high bias (by fitting a horizontal line to the data). 
    - The challenge lies in finding a method for which both the variance and the squared bias are low.

## Classification Models

- Classification Model Accuracy (Error Rate)
  - The most common approach for quantifying the accuracy of our estimate $\hat{f}$ is the training \textcolor{pink}{error rate}, the proportion of mistakes that are made if we apply our estimate $\hat{f}$ to the training observations.
  - $\frac{1}{n}\displaystyle\sum_{i=1}^nI\!\left(y_i \neq \hat{y}_i \right)$
  - Where $I\!\left(y_i \neq \hat{y}_i \right)$ is an \textcolor{pink}{indicator variable} that is 1 when actual and predicted are NOT equal and 0 if they are classified correctly.
  
- The Bayes Classifier
  - The test error rate is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values.
    - In other words, we should simply assign a test observation with predictor vector $x_0$ to the class 'j' for which \textcolor{pink}{conditional probability} $\text{Pr}(Y = j \phantom{0} | \phantom{0} X = x_0)$ is Maximum
    - It is the probability that 'Y = j' given the observed predictor is $x_0$
    - The line with the probability 0.5 (for binary response variable) is called the \textcolor{pink}{Bayes decision boundary}.
    - The Bayes classifier produces the lowest possible test error rate, called the \textcolor{pink}{Bayes error rate}. The Bayes error rate is analogous to the irreducible error.
  - In theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of Y given X, and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods like KNN. 


## K-Nearest Neighbors (KNN)

- KNN
  - (Like others) It approaches attempt to estimate the conditional distribution of Y given X, and then classify a given observation to the class with highest estimated probability. 
  - Given a positive integer 'K' and a test observation $x_0$, the KNN classifier first identifies the 'K' points in the training data that are closest to $x_0$, represented by $\mathcal{N}_0$. It then estimates the conditional probability for class 'j' as the fraction of points in $\mathcal{N}_0$ whose response values equal j: $\text{Pr}(Y = j \phantom{0} | \phantom{0} X = x_0) = \frac{1}{K}\displaystyle\sum_{i \in \mathcal{N}_0}I\!(y_i = j)$
  - Finally, KNN classifies the test observation $x_0$ to the class with the largest probability.

- The choice of $K \in (1, \infty)$ has a drastic effect on the KNN classifier obtained. 
  - When K = 1, the decision boundary is overly flexible and finds patterns in the data that do not correspond to the Bayes decision boundary. This corresponds to a classifier that has low bias but very high variance. (Overfitting)
  - As K grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier. 
  - With K = 1, the KNN training error rate might be 0, but the test error rate may be quite high. 
  - In general, as we use more flexible classification methods, the training error rate will decline but the test error rate may not. 
  - As $1/K \in (0, 1)$ increases, the method becomes more flexible. 
    - As in the regression setting, the training error rate consistently declines as the flexibility increases. However, the test error exhibits a characteristic U-shape, declining at first before increasing again when the method becomes excessively flexible and overfits. 

- \textcolor{pink}{Refer:} Figure 2.17 (page 42) For U-curve of KNN


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'F65-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'F65-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/365-F65.Rmd-->

# Linear Regression (F66) {#f66}

```{r 'F66', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Data: Boston {#set-boston-f66 .tabset .tabset-fade}

### Glance {.unlisted .unnumbered}

- About: ISLR2::Boston [506, 13]
- \textcolor{orange}{Caution:} ISLR2::Boston has 13 columns whereas MASS::Boston has 14 columns with 1 extra column being named as 'black'. For now, ISLR2 is being used because the book is using that.


### EDA {.unlisted .unnumbered}

```{r 'F66-Data-Boston', include=TRUE, eval=FALSE}
aa <- as_tibble(ISLR2::Boston)
# #Relocate Y | 
bb <- aa %>% relocate(lstat)
zzF66Boston <- xfw <- bb
```

```{r 'F66-Save-Boston', include=FALSE, eval=FALSE}
f_setRDS(zzF66Boston)
```

```{r 'F66-Get-Boston', include=FALSE}
zzF66Boston <- xfw <- bb <- aa <- f_getRDS(zzF66Boston)
```

### Structure {.unlisted .unnumbered}

```{r 'F66-Structure-Boston'}
str(zzF66Boston)
```

### Summary {.unlisted .unnumbered}

```{r 'F66-Summary-Boston'}
summary(zzF66Boston)
```

### ETC {.unlisted .unnumbered}

```{r 'F66-ETC', include=TRUE, eval=FALSE}
# #Count NA in Columns
if(FALSE) colSums(is.na(bb)) %>% as_tibble(rownames = "Cols") %>% filter(value > 0)
# #Subset Rows
if(FALSE) bb %>% select(1) %>% slice(1:10)
# #Comma separated string having each item within quotes for easy pasting as character not objects
if(FALSE) cat('"', paste0(names(which(sapply(bb, is.factor))), collapse = '", "'), '"\n', sep = '')
if(FALSE) cat('"', paste0(levels(bb$Arrival), collapse = '", "'), '"\n', sep = '')
# #Filter
if(FALSE) bb %>% filter(is.na(bpl)) %>% select(id, bph, bpl)
# #Count Yes/No or True/False in ALL such Columns
if(FALSE) bb %>% select(iFemale, iMarried) %>% 
  pivot_longer(everything()) %>% count(name, value) %>% 
  pivot_wider(names_from = value, values_from = n) 
# #Count Unique of all Columns to decide which should be Factors
if(FALSE) bb %>% summarise(across(everything(), ~ length(unique(.)))) %>% pivot_longer(everything())
# #Summary of Columns of a class: is.factor is.numeric is.character
if(FALSE) bb %>% select(where(is.factor)) %>% summary()
# #Names and Indices of Columns of class: is.factor is.numeric is.character
if(FALSE) which(sapply(bb, is.factor))
# #Levels of Factor Columns
if(FALSE) lapply(bb[c(3, 6:9, 15)], levels)
# #Frequency of Each level of Factor
if(FALSE) bb %>% count(Own) %>% arrange(desc(n))
# #Coding for Dummy Variables
if(FALSE) contrasts(bb$Married) 
```


## Data: CarSeats {#set-carseats-f66 .tabset .tabset-fade}

### Glance {.unlisted .unnumbered}

- About: ISLR2::Carseats [400, 11]
  - 'Sales' is the Response Variable
  - It has categorical variables also which should be converted to dummy
    - [Dummies should be converted Explicitly](#exp-dummy-f66 "f66")
  - Continuous Age and Education Levels
    - [Variables like Age or Education Levels should be treated as Continuous](#age-con-f66 "f66")


### EDA {.unlisted .unnumbered}

```{r 'F66-Data-CarSeats', include=TRUE, eval=FALSE}
aa <- as_tibble(ISLR2::Carseats)
# #Dummy | Change Reference | Drop | 
bb <- aa %>% 
  mutate(iUrban = ifelse(Urban == "Yes", 1, 0)) %>% 
  mutate(iUS = ifelse(US == "Yes", 1, 0)) %>% 
  mutate(across(ShelveLoc, relevel, ref = "Medium")) %>% 
  select(-c(Urban, US))
zzF66CarSeats <- xfw <- bb
```

```{r 'F66-Save-CarSeats', include=FALSE, eval=FALSE}
f_setRDS(zzF66CarSeats)
```

```{r 'F66-Get-CarSeats', include=FALSE}
zzF66CarSeats <- xfw <- bb <- aa <- f_getRDS(zzF66CarSeats)
```

### Structure {.unlisted .unnumbered}

```{r 'F66-Structure-CarSeats'}
str(zzF66CarSeats)
```

### Summary {.unlisted .unnumbered}

```{r 'F66-Summary-CarSeats'}
summary(zzF66CarSeats)
```


## Data: Advertising {#set-ads-f66 .tabset .tabset-fade}

### Glance {.unlisted .unnumbered}

- About: [200, 4]
  - Source: https://github.com/abjidge/The-Complete-Guide-to-Linear-Regression-Analysis-with-Business-Problem/blob/master/Advertising.csv
    - \textcolor{orange}{Caution:} (Do not use the below link) 
      - Different Values in Sales Column : ~~https://www.kaggle.com/ashydv/sales-prediction-simple-linear-regression/data~~
  - 'Sales' is the Response Variable
  - It has sales of product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper.
    - Advertising Budgets for each media is in 1000 dollars 
    - Sales is the number of units of product sold (in thousands of units)


### EDA {.unlisted .unnumbered}

\textcolor{pink}{Please import the "F66-Advertising.csv"}

```{r 'F66-Data-Ads-x', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum
#tools::md5sum(paste0(.z$XL, "F66-Advertising.csv"))
xxF66Ads <- f_getObject("xxF66Ads", "F66-Advertising.csv", "ee7780d02d13787d88792c4ab9a19622")
```

```{r 'F66-Get-Ads-x', include=FALSE, eval=FALSE}
xxF66Ads <- f_getRDS(xxF66Ads)
```

```{r 'F66-Data-Ads', eval=FALSE}
aa <- xxF66Ads
# #Drop SN | Rename | Relocate Y | 
bb <- aa %>% select(-1) %>% rename_with(tolower) %>% relocate(sales)
zzF66Ads <- xfw <- bb
```

```{r 'F66-Save-Ads', include=FALSE, eval=FALSE}
f_setRDS(zzF66Ads)
```

```{r 'F66-Get-Ads', include=FALSE}
zzF66Ads <- xfw <- bb <- aa <- f_getRDS(zzF66Ads)
```

### Structure {.unlisted .unnumbered}

```{r 'F66-Structure-Ads'}
str(zzF66Ads)
```

### Summary {.unlisted .unnumbered}

```{r 'F66-Summary-Ads'}
summary(zzF66Ads)
```

## Data: Credit {#set-credit-f66 .tabset .tabset-fade}

### Glance {.unlisted .unnumbered}

- About: ISLR2::Credit [400, 11]
  - 'Balance' is the Response Variable
    - It is the average credit card debt for each individual
  - It has categorical variables also which should be converted to dummy
    - [Dummies should be converted Explicitly](#exp-dummy-f66 "f66")

### EDA {.unlisted .unnumbered}

```{r 'F66-Data-Credit', eval=FALSE}
aa <- as_tibble(ISLR2::Credit)
# #Using the default levels to Match with the Book analysis
# #Relocate Y | 
bb <- aa %>% 
  #mutate(across(Region, relevel, ref = "South")) %>% 
  #mutate(across(c(Own, Married), relevel, ref = "Yes")) %>% 
  relocate(Balance)
zzF66Credit <- bb
```

```{r 'F66-Save-Credit', include=FALSE, eval=FALSE}
f_setRDS(zzF66Credit)
```

```{r 'F66-Get-Credit', include=FALSE}
zzF66Credit <- xfw <- bb <- aa <- f_getRDS(zzF66Credit)
```

### Structure {.unlisted .unnumbered}

```{r 'F66-Structure-Credit'}
str(zzF66Credit)
```

### Summary {.unlisted .unnumbered}

```{r 'F66-Summary-Credit'}
summary(zzF66Credit)
```

## Data: Auto {#set-auto-f66 .tabset .tabset-fade}

### Glance {.unlisted .unnumbered}

- About: ISLR2::Auto [392, 9]


### EDA {.unlisted .unnumbered}

```{r 'F66-Data-Auto', eval=FALSE}
zzF66Auto <- bb <- aa <- as_tibble(ISLR2::Auto)
```

```{r 'F66-Save-Auto', include=FALSE, eval=FALSE}
f_setRDS(zzF66Auto)
```

```{r 'F66-Get-Auto', include=FALSE}
zzF66Auto <- xfw <- bb <- aa <- f_getRDS(zzF66Auto)
```

### Structure {.unlisted .unnumbered}

```{r 'F66-Structure-Auto'}
str(zzF66Auto)
```

### Summary {.unlisted .unnumbered}

```{r 'F66-Summary-Auto'}
summary(zzF66Auto)
```



## Explicit Dummy Conversion {#exp-dummy-f66}

- [Dummies should be converted Explicitly](#exp-dummy-f66 "f66")
  - By default, lm() can convert dummies but it is better to explictly handle those. - "ForLater"
- Also, Keep the base dataset (i.e. with factors), not including the dummies
  - The Name of the Reference Level is lost and cannot be recovered. If there are 3 regions, {East, West, South} and 'East' is being treated as the reference, the resulting dataset, with dummies, does not know whether the {0, ..., 0} reference level would be East or North.
  - Implicit Dummy conversion retains the reference level because it keeps the variable as factor. However, implicit is not available for some models.


## Continuous Age {#age-con-f66}

- [Variables like Age or Education Levels should be treated as Continuous](#age-con-f66 "f66")
  - Keeping them as Continuous is better (in general)
  - Converting to Factor would result in loss of information about their relative location i.e. age 80 is near 70 compared to age 30 or education 20 is better than education 10
  - Many techniques (like randomForest) become exponentially resource intensive as the number of variable increases and Age /Level type of variables would create lots of dummies if converted to categorical
  - Marks given to Students (e.g. 44 or 63 out of 100, 3 or 4 out of 10) although look continuous but inherently these are categorical. However, almost always, they are assumed to be continous to calculate a Grade point averge (GPA) etc. 
    - \textcolor{orange}{Caution:} According to Statistical theory this is wrong but it happens this way only.
  - As the number of levels increases, we tend to assume that the variable is continuous. So, there is no set boundary when a variable is categorical or continous
  - Treating them as continous, automatically handles the missing levels that are not in the data i.e. age (e.g. 40, 55 might be missing). Converting this to categorical would create another problem of how to treat those.
    - Distance between 39 and 41 would change with the prsence or absence of an observation value of 40 if we treat it as categorical.


## Ordered Factor {#lm-poly-f66}

- [Ordered Factors in lm() will give polynomials](#lm-poly-f66 "f66")
- \textcolor{orange}{Caution:} Incomplete. "ForLater"
- Ordered Factors are converted into polynomial by defualt in R during regression whereas unordered factors are kept simple
    - How to convert ordered factors into polynomials for usage
    - Further, what would happen if in the 'Good < Better < Best' ordering, Better is the most frequent level. Currently, the basic rule is to convert most frequent level to the reference to minimise multicollinearity. How this would be handled is unknown for now.




## Simple Linear Regression {#reg-f66}

- Dataset Advertising - Goal 
  - To develop an accurate model that can be used to predict sales on the basis of the three media budgets.

- \textcolor{pink}{Question:} Refer [Answers](#ans-f66 "f66")

1. Is there a relationship between advertising budget and sales
2. How strong is the relationship between advertising budget and sales
3. Which media are associated with sales
4. How large is the association between each medium and sales
5. How accurately can we predict future sales
6. Is the relationship linear
7. Is there synergy among the advertising media (interaction effect)


```{definition 'Simple-Linear-Regression-F66'}
\textcolor{pink}{Simple Linear Regression} is the approach for predicting a quantitative response (Y) on the basis of a single predictor variable (X). It assumes that there is approximately a linear relationship between X and Y: \textcolor{pink}{$Y \approx \beta_0 + \beta_1 X$}, where $\beta_0$ represent the intercept and $\beta_1$ represent the slope in the linear model. Together $\beta_0$ and $\beta_1$ are known as the model \textcolor{pink}{coefficients} or \textcolor{pink}{parameters}.
```

- Our goal is to obtain coefficient estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ such that the linear model fits the available data well
  - $y_i \approx \hat{\beta}_0 + \hat{\beta}_1 x_i \phantom{0} \forall \phantom{0} i \in \{1, 2, \ldots, n\}$
  - In other words, we want to find an intercept $\hat{\beta}_0$ and a slope $\hat{\beta}_1$ such that the resulting line is as close as possible to the observed data points. 
  - The most common approach, to measure closeness, involves minimizing the \textcolor{pink}{least squares criterion}.
  - The least squares approach chooses $\hat{\beta}_0$ and $\hat{\beta}_1$ to minimize the RSS.


```{definition 'Residuals-F66'}
\textcolor{pink}{$i^{\text{th}}$ Residual $(e_i)$} is the difference between observed response $(y_i)$ and predicted response $(\hat{y}_i)$. i.e. $e_i = y_i - \hat{y}_i$
```


```{definition 'RSS'}
\textcolor{pink}{Residual Sum of Squares} $\text{RSS} = \displaystyle\sum_{i=1}^n (y_i - \hat{y}_i)^2$
```

## Population Regression Line

- Regression Model $Y = \beta_0 + \beta_1 X + \epsilon$ 
  - $\beta_0$ represent the intercept i.e. the expected value of Y when X = 0
  - $\beta_1$ represent the slope i.e. the average increase in Y associated with a one-unit increase in X
  - $\epsilon$ is the error term. We typically assume that the error term is independent of X.
  - The model given by this equation defines the \textcolor{pink}{population regression line}, is the best linear approximation to the true relationship between X and Y.
  - The true relationship is generally not known for real data, but the least squares line can always be computed using the coefficient estimates. 
  - In other words, in real applications, we have access to a set of observations from which we can compute the least squares line; however, the population regression line is unobserved.

## Unbiased Estimator

```{definition 'Unbiased-Estimator'}
An \textcolor{pink}{unbiased estimator} does not systematically over-estimate or under-estimate the true parameter. 
```


- The sample mean $(\hat{\mu})$ and the population mean $({\mu})$ are different, but in general the sample mean will provide a good estimate of the population mean.
- If we use the sample mean $\hat{\mu}$ to estimate ${\mu}$, this estimate is unbiased, in the sense that on average, we expect $\hat{\mu}$ to equal ${\mu}$. 
  - It means that on the basis of one particular set of observations $\hat{\mu}$ might overestimate ${\mu}$, and on the basis of another set of observations, $\hat{\mu}$ might underestimate ${\mu}$. But if we could average a huge number of estimates of ${\mu}$ obtained from a huge number of sets of observations, then this average would exactly equal ${\mu}$. 

## Assessing the Accuracy of the Coefficient Estimates

- How accurate is the sample mean $(\hat{\mu})$ as an estimate of $({\mu})$ 
  - Calculate Standard Error of $\hat{\mu}$ i.e. $\text{SE}(\hat{\mu})$
  - $\text{Var}(\hat{\mu}) = \text{SE}(\hat{\mu})^2 = \frac{\sigma^2}{n}$
  - Roughly, the standard error tells us the average amount that this estimate $\hat{\mu}$ differs from the actual value of ${\mu}$.
  - It also tells us that as the 'n' becomes larger, the standard error of $\hat{\mu}$ decreases

- Similarly standard errors for $\beta_0, \beta_1$ can be estimated
  - $\text{SE}(\hat{\beta}_1)$ would be smaller when the $x_i$ are more spread out; intuitively we have more leverage to estimate a slope when this is the case. 
  - We also see that $\text{SE}(\hat{\beta}_0)$ would be the same as $\text{SE}(\hat{\mu})$ if ${\overline{x}}$ were zero, in which case $\hat{\beta}_0$ would be equal to ${\overline{y}}$. 
  - In general, $\sigma^2$ is not known, but can be estimated from the data. This estimate of $\sigma$ is known as the RSE. 
    - Definition given below is applicable similarly to the estimation of model coeffcients. 


```{definition 'RSE'}
The \textcolor{pink}{residual standard error (RSE)} is an estimate of the standard deviation of $\epsilon$ or the estimate of $\sigma$. It is the average amount that the response will deviate from the true regression line. RSE represents the \textcolor{pink}{lack of fit} of the model. Lower RSE is desired. \textcolor{pink}{$\text{RSE} = \sqrt{\frac{\text{RSS}}{(n-2)}}$}
```


```{definition 'Confidence-Interval-F66'}
A 95% \textcolor{pink}{confidence interval} is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. Standard errors can be used to compute confidence intervals. 
```


- Standard errors can also be used to perform hypothesis tests on the coefficients. 
  - ${H_0} : \beta_1 = 0$ : There is no relationship between X and Y
  - ${H_a} :\beta_1 \neq 0$ : There is some relationship between X and Y
- To test the null hypothesis, we need to determine whether $\hat{\beta}_1$ (estimate) is sufficiently far from zero that we can be confident that $\beta_1$ (parameter) is non-zero. 
  - How far is far enough, depends on the accuracy of $\hat{\beta}_1$ i.e. $\text{SE}(\hat{\beta}_1)$
  - If $\text{SE}(\hat{\beta}_1)$ is small, then even relatively small values of $\hat{\beta}_1$ may provide strong evidence that $\beta_1 \neq 0$, and hence that there is a relationship between X and Y. 
  - In contrast, if $\text{SE}(\hat{\beta}_1)$ is large, then $\hat{\beta}_1$ must be large in absolute value in order for us to reject the null hypothesis.

- We compute a t-statistic as: $t = \frac{\hat{\beta}_1 - 0}{\text{SE}(\hat{\beta}_1)}$
  - It measures the number of standard deviations that $\hat{\beta}_1$ is away from 0. 
  - If there really is no relationship between X and Y, then we expect that it will have a t-distribution with \textcolor{pink}{(n − 2)} degrees of freedom. 
  - The t-distribution has a bell shape and for values of 'n' greater than approximately 30 it is quite similar to the standard normal distribution. 
  - Consequently, we can compute the probability of observing any number equal to |t| or larger in absolute value, assuming $\beta_1 = 0$. We call this probability the p-value. 

- p-value
  - A small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real association between the predictor and the response. 
  - Hence, if we see a small p-value, then we can infer that there is an association between the predictor and the response. 
  - We reject the null hypothesis —that is, we declare a relationship to exist between X and Y —if the p-value is small enough. 
    - Typical p-value cutoffs for rejecting the null hypothesis are 5% or 1%. 
    - When n = 30, these correspond to t-statistics of around 2 and 2.75, respectively.

## lm() {.tabset .tabset-fade}

- \textcolor{pink}{lm()} for Linear Regression
  - \textcolor{pink}{stargazer()} for summary
    - ~~summary() for coefficients~~
  - \textcolor{pink}{confint()} for Confidence Interval
  - \textcolor{pink}{predict()} for prediction i.e. $\hat{y}$ 
    - It can provide confidence intervals and prediction intervals for given value

- Dataset Advertising - Linear Regression of Y (Sales) vs. Single X (TV)
  - Least squares model for the regression of number of units sold on TV advertising budget for the Advertising data. 
  - $\beta_0 = 7.03, \beta_1 = 0.0475$
  - According to this approximation, an additional unit (1,000 dollars) spent on TV advertising is associated with selling approximately 0.0475 additional units of the product (in 1000 i.e. 47.5 units)
  - Confidence Interval
    - $\beta_0 \in [6.130, 7.935]$ : In the absence of any advertising, sales will, on average, fall somewhere between 6,130 and 7,935 units
    - $\beta_1 \in [0.042, 0.053]$ : For each 1,000 dollar increase in television advertising, there will be an average increase in sales of between 42 and 53 units
  - Hypothesis Test
     - Coefficients for $\hat{\beta}_0$ and $\hat{\beta}_1$ are very large relative to their standard errors, so the t-statistics are also large; the probabilities of seeing such values if ${H_0}$ is true are virtually zero.
    - Hence we can conclude that $\beta_0 \neq 0, \beta_1 \neq 0$
      - $\beta_1 \neq 0$ allows us to conclude that there is a relationship between TV and sales. 
       - $\beta_0 \neq 0$ allows us to conclude that in the absence of TV expenditure, sales are non-zero. 


```{r 'F66-Regression-Ads-1'}
# #Simple Linear Regression
mod_ads_tv <- lm(sales ~ tv, data = zzF66Ads)
mod_ads_rad <- lm(sales ~ radio, data = zzF66Ads)
mod_ads_paper <- lm(sales ~ newspaper, data = zzF66Ads)
```


```{r 'F66-ConfidenceInterval-Ads-1'}
# #confint() for Confidence Interval of the Estimated Coefficients
round(confint(mod_ads_tv), 3)
```


```{r 'F66-Summary-Ads-1'}
# #Coefficient Estimates, Standard Error, t-value, p-value and Significance
if(TRUE) f_pNum(summary(mod_ads_tv)$coefficients, 3) %>% as_tibble(rownames = "Coefficients") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
```

## Assessing the Accuracy of the Model {.tabset .tabset-fade}

- Goal: To quantify the extent to which the model fits the data. 
  - The quality of a linear regression fit is typically assessed using two related quantities: 
    - the Residual Standard Error (RSE) 
    - the $R^2$ statistic

```{r 'F66D01', comment="", echo=FALSE, results='asis'}
f_getDef("RSE") #dddd
```


```{definition 'Rsq'}
The \textcolor{pink}{$R^2 \in [0, 1] $}, is the proportion of variance explained. It is independent of the scale of Y. \textcolor{pink}{$R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}$}
```


```{definition 'TSS'}
\textcolor{pink}{Total Sum of Squares} $\text{TSS} = \displaystyle\sum_{i=1}^n (y_i - {\overline{y}}_i)^2$
```

- $R^2$
  - The RSE provides an absolute measure of 'lack of fit' of the model to the data. But since it is measured in the units of Y, it is not always clear what constitutes a good RSE. The $R^2$ statistic provides an alternative measure of fit.
  - TSS measures the total variance in the response Y, and can be thought of as the amount of variability inherent in the response before the regression is performed. 
  - In contrast, RSS measures the amount of variability that is left unexplained after performing the regression. 
  - Hence, (TSS - RSS) measures the amount of variability in the response that is explained (or removed) by performing the regression, and $R^2$ measures the proportion of variability in Y that can be explained using X. 
  - An $R^2$ statistic that is close to 1 indicates that a large proportion of the variability in the response is explained by the regression. 
  - A $R^2$ value near 0 indicates that the regression does not explain much of the variability in the response; this might occur because the linear model is wrong, or the error variance ${\sigma}^2$ is high, or both.

- Dataset Advertising - Linear Regression of Y (Sales) vs. Single X (TV)
  - RSE = 3.26 (1000 dollars)
    - i.e. Actual sales in each market deviate from the true regression line by approximately 3,260 units, on average. 
    - i.e. Even if the model were correct and the true values of the unknown coefficients $\beta_0$ and $\beta_1$ were known exactly, any prediction of sales on the basis of TV advertising would still be off by about 3,260 units on average. 
  - Percentage Error
    - Mean Y = 14.02 (1000 units)
    - Percentage error = MSE / Mean Y $= 3258 / 14022 \approx 23\%$
  - $R^2 = 0.61$
    - i.e. Just under two-thirds of the variability in 'sales' is explained by a linear regression on 'TV'.


### stargazer() {.unlisted .unnumbered}

```{r 'F66-Print-Ads-1'}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Linear Regression: Advertising: Y ~ X (3 Models)"
col_hh <- c("TV", "Radio", "Newspaper")
#
stargazer(mod_ads_tv, mod_ads_rad, mod_ads_paper, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```

### Deprecated {.unlisted .unnumbered}

```{r 'F66-Summary-Ads-1-OLD'}
# #RSE, R2, F-statistic, Percentage Error
summary(mod_ads_tv)$sigma
summary(mod_ads_tv)$r.squared
summary(mod_ads_tv)$fstatistic
#
# #Percentage Error = RSE/Mean
cat(paste0("Percentage Error (RSE / Mean Y) = ", 
           round(100 * summary(mod_ads_tv)$sigma / mean(zzF66Ads$sales), 2), "%\n"))
```



## Multiple Linear Regression

Instead of fitting a separate simple linear regression model for each predictor, a better approach is to extend the simple linear regression model so that it can directly accommodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model.

- $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon$
  - where $X_j$ represents the $j^{th}$ predictor and $\beta_j$ quantifies the association between that variable and the response. 
  - We interpret $\beta_j$ as the average effect on Y of a one unit increase in $X_j$, \textcolor{pink}{holding all other predictors fixed}.


- Dataset Advertising - Linear Regression of Y (Sales)
  - For a given amount of TV and newspaper advertising, spending an additional 1,000 dollars on radio advertising is associated with approximately 188 units of additional sales. 
  - Comparing these coefficient estimates, we notice that the multiple regression coefficient estimates for 'TV' and 'radio' are pretty similar to the simple linear regression coefficient estimates.
    - TV: Simple (0.0475), Multiple (0.0458)
    - Radio: Simple (0.2025), Multiple (0.1885)
  - However, while the 'newspaper' regression coefficient estimate in Simple Linear Regression was significantly non-zero, the coefficient estimate for newspaper in the multiple regression model is close to zero, and the corresponding p-value is no longer significant. 
    - Newspaper: Simple (0.0547 & significant), Multiple (-0.0010 & not significant)
  - This illustrates that the simple and multiple regression coefficients can be quite different. 
    - This difference stems from the fact that in the simple regression case, the slope term represents the average increase in product sales associated with a 1,000 dollars increase in newspaper advertising, \textcolor{pink}{ignoring other predictors} such as TV and radio. 
    - By contrast, in the multiple regression setting, the coefficient for newspaper represents the average increase in product sales associated with increasing newspaper spending by 1,000 dollars while \textcolor{pink}{keeping other predictors fixed}.


```{conjecture 'stargazer-nchar'}
\textcolor{brown}{Error in if (nchar(text.matrix[r, c]) > max.length[real.c]) : missing value where TRUE/FALSE needed}
```

- stargazer() cannot handle underscore in Headers for Table Printing


```{r 'F66-Regression-Ads-2'}
# #Multiple Linear Regression 
# #Use Dot (.) to indicate ALL (except Y)
# #Use Plus (+) to add or Minus (-) to remove variable names
mod_ads_tvrad <- lm(sales ~ tv + radio, data = zzF66Ads)
mod_ads <- lm(sales ~ ., data = zzF66Ads)
```


```{r 'F66-Print-Ads-2'}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Linear Regression: Advertising"
col_hh <- c("ALL", "TV.Radio", "TV", "Radio", "Newspaper")
#
stargazer(mod_ads, mod_ads_tvrad, mod_ads_tv, mod_ads_rad, mod_ads_paper, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```


## Correlation {.tabset .tabset-fade}

- Does it make sense for the multiple regression to suggest no relationship between 'sales' and 'newspaper' while the simple linear regression implies the opposite
  - Yes, it does make sense. 
  - The correlation matrix shows that the correlation between 'radio' and 'newspaper' is 0.35. 
    - This indicates that markets with high newspaper advertising tend to also have high radio advertising. 
  - Now suppose that the multiple regression is correct and newspaper advertising is not associated with sales, but radio advertising is associated with sales. 
    - Then in markets where we spend more on radio our sales will tend to be higher, and as our correlation matrix shows, we also tend to spend more on newspaper advertising in those same markets.
  - Hence, in a simple linear regression which only examines sales versus newspaper, we will observe that higher values of newspaper tend to be associated with higher values of sales, even though newspaper advertising is not directly associated with sales. 
    - So newspaper advertising is a \textcolor{pink}{surrogate} for radio advertising; newspaper gets 'credit' for the association between radio on sales.


### Image {.unlisted .unnumbered}

```{r 'F66-ADS-Corr-Set', include=FALSE}
# #Setup for Corrplot
ii <- zzF66Ads 
hh <- cor(ii)
corr_hh <- cor.mtest(ii)
# #p-value Higher than this is insignificant and should be skipped
sig_corr_hh <- 0.05 
#
cap_hh <- "F66P08"
ttl_hh <- "Advertisement: Corrplot"
loc_png <- paste0(.z$PX, "F66P08", "-Ads-Corrplot", ".png")
```

```{r 'F66P08-Save', include=FALSE, ref.label=c('F66-Corrplot')}
#
```

```{r 'F66P08', echo=FALSE, fig.cap="(F66P08) Advertisement: Corrplot"}
knitr::include_graphics(paste0(.z$PX, "F66P08", "-Ads-Corrplot", ".png"))
```

### Code {.unlisted .unnumbered}

```{r 'F66-ADS-Corr-Set-A', eval=FALSE, ref.label=c('F66-ADS-Corr-Set')}
#
```

```{r 'F66-Corrplot', eval=FALSE}
# #IN: hh, corr_hh, sig_corr_hh
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  corrplot(hh, method = "circle", type = "lower", diag = FALSE, col = COL2('BrBG', 200),
                   cl.pos = 'r', tl.pos = 'ld', addCoef.col = "black", tl.col = "black",
                   p.mat = corr_hh$p, sig.level = sig_corr_hh, insig = 'blank', 
        #order = "hclust", hclust.method = "ward.D", addrect = 2, rect.col = 3, rect.lwd = 3, 
                   title = NULL #, col = RColorBrewer::brewer.pal(3, "BrBG")
				   )
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  F66 <- recordPlot()
  dev.off()
  assign(cap_hh, F66)
  rm(F66)
  #eval(parse(text = cap_hh))
}
```


## Questions to be answered by Regression

### Is There a Relationship Between the Response and Predictors

- F-statistic is used to perform following Hypothesis Test
  - ${H_0} :$ All regression coefficients are zero.
  - ${H_a} :$ At least one regression coefficient is non-zero.
  - $F = \frac{(\text{TSS} - \text{RSS})/p}{\text{RSS} /(n - p - 1)}$
  - When there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to 1.
  - If ${H_a}$ is true then F-statistic would be greater than 1

- How large does the F-statistic need to be before we can reject ${H_0}$ and conclude that there is a relationship
  - It depends on the values of 'n' and 'p'. 
  - When 'n' is large, an F-statistic that is just a little larger than 1 might still provide evidence against ${H_0}$. 
  - In contrast, a larger F-statistic is needed to reject ${H_0}$ if 'n' is small. 

- For each individual predictor a t-statistic and a p-value were reported. 
  - These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. 
  - It turns out that each of these is exactly equivalent to the F-test that omits that single variable from the model, leaving all the others in. 
    - The square of each t-statistic is the corresponding F-statistic
  - So it reports the partial effect of adding that variable to the model. 
    - For instance these p-values indicate that TV and radio are related to sales, but that there is no evidence that newspaper is associated with sales, when TV and radio are held fixed.

- Given these individual p-values for each variable, why do we need to look at the overall F-statistic
  - ~~After all, it seems likely that if any one of the p-values for the individual variables is very small, then at least one of the predictors is related to the response.~~
    - However, this logic is flawed, especially when the number of predictors 'p' is large.
  - For instance, consider an example in which p = 100 and ${H_0} : \beta_1 = \beta_2 = \ldots = \beta_p = 0$ is true, so no variable is truly associated with the response. 
    - In this situation, about 5 % of the p-values associated with each variable will be below 0.05 by chance. 
    - In other words, we expect to see approximately five small p-values even in the absence of any true association between the predictors and the response. (Related to "Multiple Testing")
    - In fact, it is likely that we will observe at least one p-value below 0.05 by chance! 
    - Hence, if we use the individual t-statistics and associated p-values in order to decide whether or not there is any association between the variables and the response, there is a very high chance that we will incorrectly conclude that there is a relationship. 
  - However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors. 
    - Hence, if ${H_0}$ is true, there is only a 5 % chance that the F-statistic will result in a pvalue below 0.05, regardless of the number of predictors or the number of observations.

- \textcolor{orange}{Caution:} In high-dimensionality situation (p >> n), this approach of using an F-statistic to test for any association between the predictors and the response fails.

### Deciding on Important Variables


```{definition 'Variable-Selection'}
The task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is referred to as \textcolor{pink}{variable selection}.
```


- Various statistics can be used to judge the quality of a model. 
  - Mallows cp
  - Akaike information criterion (AIC)
  - Bayesian information criterion (BIC)
  - Adjusted $R_a^2$
- Approaches
    - Forward selection
      - Forward selection might include variables early that later become redundant.
    - Backward selection
      - Backward selection cannot be used if p > n
    - Mixed selection
      - We continue to perform forward and backward steps until all variables in the model have a sufficiently low p-value, and all variables outside the model would have a large p-value if added to the model.


### Model Fit

```{r 'F66-Print-Ads-2-A', ref.label=c('F66-Print-Ads-2')}
#
```

- Dataset Advertising - Linear Regression of Y (Sales)
  - Model with All 3 variables: $R^2 = 0.89721$
  - Model with 2 variables excluding newspaper: $R^2 = 0.89719$
  - In other words, there is a small increase in $R^2$ if we include newspaper advertising in the model that already contains TV and radio advertising, even though we saw earlier that the p-value for newspaper advertising is not significant. 
  - It turns out that $R^2$ will always increase when more variables are added to the model, even if those variables are only weakly associated with the response. 
    - This is due to the fact that adding another variable always results in a decrease in the residual sum of squares on the training data (though not necessarily the testing data). Thus, the $R^2$ statistic, which is also computed on the training data, must increase. The fact that adding newspaper advertising to the model containing only TV and radio advertising leads to just a tiny increase in $R^2$ provides additional evidence that newspaper can be dropped from the model. 
    - Essentially, newspaper provides no real improvement in the model fit to the training samples, and its inclusion will likely lead to poor results on independent test samples due to overfitting.

- We should look at \textcolor{pink}{Adjusted R Square $R_a^2$}
  - Model with All 3 variables: $R_a^2 = 0.89564$
  - Model with 2 variables excluding newspaper: $R_a^2 = 0.89615$
  - Model with 2 variabls has higher (better) $R_a^2$ and thus should be used

### Predictions

- The inaccuracy in the coefficient estimates is related to the reducible error. We can compute a confidence interval in order to determine how close $\hat{Y}$ will be to $f(X)$.

- Even if we knew $f(X)$ —that is, even if we knew the true values for coefficients— the response value cannot be predicted perfectly because of the random error $\epsilon$ in the model. 
  - This is the irreducible error. 
  - How much will Y vary from $\hat{Y}$
  - We use prediction intervals to answer this question. 
    - Prediction intervals are always wider than confidence intervals, because they incorporate both the error in the estimate for $f(X)$ (the reducible error) and the uncertainty as to how much an individual point will differ from the population regression plane (the irreducible error).


- Dataset Advertising - Linear Regression of Y (Sales)
  - We use a confidence interval to quantify the uncertainty surrounding confidence interval the average sales over a large number of cities. 
    - Ex: Given that 100,000 dollars are spent on TV advertising and 20,000 dollars are spent on radio advertising in each city, the 95% confidence interval is [10985, 11528]. 
    - We interpret this to mean that 95% of intervals of this form will contain the true value of $f(X)$.
  - On the other hand, a prediction interval can be used to quantify the prediction interval uncertainty surrounding sales for a particular city. 
    - Ex: Given that 100,000 dollars are spent on TV advertising and 20,000 dollars are spent on radio advertising in that city the 95% prediction interval is [7930, 14583]. 
    - We interpret this to mean that 95% of intervals of this form will contain the true value of Y for this city. 
  - Note that both intervals are centered at 11256, but that the prediction interval is substantially wider than the confidence interval, reflecting the increased uncertainty about sales for a given city in comparison to the average sales over many locations.


```{r 'F66-Predict-Ads'}
# #predict() for confidence intervals (95%)
predict(mod_ads_tvrad, tibble(tv = c(100), radio = c(20)), interval = "confidence")
#
# #predict() for prediction intervals (95%)
predict(mod_ads_tvrad, tibble(tv = c(100), radio = c(20)), interval = "prediction")
```

##  Qualitative Predictors

- For a categorical variable with 'k' levels, we need to create $(k - 1)$ \textcolor{pink}{indicator or dummy variables}. Each of them would take on two possible numerical values (0, 1)
  - Also known as 'one-hot encoding'
  - We can assign (0, 1) to different levels i.e. reference will change, values will change, interpretation will change, but the conclusion would remain the same. 
  - Further, we can even use (-1, 1)
  - All of these approaches lead to equivalent model fits, but the coefficients are different and have different interpretations, and are designed to measure particular \textcolor{pink}{contrasts}.

### Predictors with Only Two Levels

- Dataset Credit - Linear Regression of Y (Balance) vs. Single Categorical X (Own)
  - Reference: 0 (does not own house), 1 (owns the house)
  - The average credit card debt for non-owners is estimated to be 509.80 dollars (Constant), whereas owners are estimated to carry 19.73 dollars in additional debt for a total of 529.53 dollars. 
  - However, the p-value for the dummy variable is higher than 0.05. This indicates that there is no statistical evidence of a difference in average credit card balance based on house ownership. 
  - Alternate Reference: 1 (does not own house), 0 (owns the house)
    - The average credit card debt for owners is estimated to be 529.53 dollars (Constant), whereas non-owners are estimated to carry slightly lower debt of 19.73 dollars for a total of 509.80 dollars. 
  - Alternate Reference: -1 (does not own house), 1 (owns the house)
    - The average credit card debt for owners is estimated to be 519.67 dollars (Constant)
    - Non-owners are estimated to carry slightly lower debt of (-1) * 9.87 dollars for a total of 509.80 dollars. 
    - Owners are estimated to carry additional debt of (1) * 9.87 dollars for a total of 529.53 dollars.


```{r 'F66-Regression-Credit-1'}
# #Simple Linear Regression with Categorical X
# #Dummy |
zzF66Credit_dum <- dummy_cols(zzF66Credit, 
                              remove_first_dummy = TRUE, remove_selected_columns = TRUE)
#
# #Reference Own: No = 0, Yes = 1
mod_credit_Own_Yes <- lm(Balance ~ Own_Yes, data = zzF66Credit_dum)
#
# #Change Reference Own: No = 1, Yes = 0
ii <- zzF66Credit %>% 
  mutate(across(c(Own), relevel, ref = "Yes")) %>% dummy_cols(., 
                              remove_first_dummy = TRUE, remove_selected_columns = TRUE)
mod_credit_Own_No <- lm(Balance ~ Own_No, data = ii)
#
# #Change Reference Own: No = -1, Yes = 1
jj <- ii %>% mutate(Own_NoYes = replace(Own_No, Own_No == 0, -1)) %>% select(-Own_No)
mod_credit_Own_NoYes <- lm(Balance ~ Own_NoYes, data = jj)
```


```{r 'F66-Print-Credit-1'}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Linear Regression: Credit"
col_hh <- c("Own.Yes", "Own.No", "Own.NoYes")
#
stargazer(mod_credit_Own_Yes, mod_credit_Own_No, mod_credit_Own_NoYes, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```


### Qualitative Predictors with More than Two Levels

- Dataset Credit - Linear Regression of Y (Balance) vs. Single Categorical X (Region)
  - Reference: East
  - \textcolor{orange}{Caution:} Book might have interchanged the Coefficients of Region_South and Region_West
    - The estimated balance for the baseline, East, is 531 dollars. 
    - It is estimated that those in the South will have 12 dollars less debt than those in the East
    - Those in the West will have 18 dollars less debt than those in the East. 
    - However, the p-values associated with the coefficient estimates for the two dummy variables are greater than 0.05, suggesting no statistical evidence of a real difference in average credit card balance between 'South and East' or between 'West and East'.
  - Selection of baseline category is arbitrary (suggested most frequent, not applied here)
    - Final predictions for each group will be same regardless of this choice
    - However, p-values do depend on the choice of dummy variable coding. 
    - Rather than rely on the individual coefficients, we can use an F-test.
      - This does not depend on the coding. 
      - This F-test has a p-value of 0.9575 (Bottom of Summary) i.e. higher than 0.05, indicating that we cannot reject the null hypothesis that there is no relationship between balance and region.


```{r 'F66-Regression-Credit-2'}
# #Simple Linear Regression with Categorical X (Region)
mod_credit_rEast <- lm(Balance ~ Region_South + Region_West, data = zzF66Credit_dum)
```


```{r 'F66-Print-Credit-2'}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Linear Regression: Credit"
col_hh <- c("Region")
#
# #Model p-value
if(TRUE) round(glance(mod_credit_rEast)$p.value, 4)
#
stargazer(mod_credit_rEast, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```


- Dataset CarSeats - Linear Regression of Y (Sales) vs. Categorical X (ShelveLoc)
  - Reference: Medium
  - Estimate for 'Bad' Shelves is Negative, indicating that the impact on Price due to Bad Shelving is negative compared to Medium
  - Estimate for 'Good' Shelves is Positive, indicating that the impact on Price due to Good Shelving is positive compared to Medium


```{r 'F66-Regression-Cars'}
# #Linear Regression with Categorical X (ShelveLoc)
# #Dummy |
zzF66CarSeats_dum <- dummy_cols(zzF66CarSeats, 
                              remove_first_dummy = TRUE, remove_selected_columns = TRUE)
mod_cars <- lm(Sales ~ ., data = zzF66CarSeats_dum)
```


```{r 'F66-Print-Cars'}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Linear Regression: CarSeats"
col_hh <- c("ShelveLoc")
#
stargazer(mod_cars, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```


## Extensions of the Linear Model

- The standard linear regression model makes several highly restrictive assumptions that are often violated in practice. 
  - Assumptions: The relationship between the predictors and response are additive and linear. 
  - The \textcolor{pink}{additivity assumption} means that the association between a predictor $X_j$ and the response Y does not depend on the values of the other predictors. 
  - The \textcolor{pink}{linearity assumption} states that the change in the response Y associated with a one-unit change in $X_j$ is constant, regardless of the value of $X_j$. 

### Removing the Additive Assumption

```{definition 'Interaction-Effect'}
An \textcolor{pink}{interaction effect (or synergy effect)} exists when the effect of an independent variable on a dependent variable changes, depending on the value(s) of one or more other independent variables. \textcolor{pink}{Interaction term} as product of the variables $(X_1 \times X_2)$ is introduced in the model to evaluate this effect.
```

- In our previous analysis of the Advertising data, we concluded that both TV and radio seem to be associated with sales. 
  - The linear models that formed the basis for this conclusion assumed that the effect on sales of increasing one advertising medium is independent of the amount spent on the other media. 
  - For example, the linear model states that the average increase in sales associated with a one-unit increase in TV is always $\beta_1$, regardless of the amount spent on radio. 
  - However, this simple model may be incorrect. 
    - Suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases. 
    - In this situation, given a fixed budget of 100,000 dollars, spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or to radio. 
    - In marketing, this is known as a \textcolor{pink}{synergy effect}, and in statistics it is referred to as an \textcolor{pink}{interaction effect}. 

- According to the simple model, a one-unit increase in $X_1$ is associated with an average increase in Y of $\beta_1$ units. 
  - The presence of $X_2$ does not alter this statement —that is, regardless of the value of $X_2$, a one unit increase in $X_1$ is associated with a $\beta_1$-unit increase in Y.
  - One way of extending this model is to include a third predictor, called an \textcolor{pink}{interaction term}, which is constructed by computing the product of $X_1$ and $X_2$. 
  

```{definition 'Hierarchical-Principle'}
The \textcolor{pink}{hierarchical principle} states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant. 
```

- What if the interaction term has small p-value (significant) but the associated main effects do not
  - Include them as stated by the 'Hierarchical Principle'
  - If the interaction between $X_1$ and $X_2$ seems important, then we should include both $X_1$ and $X_2$ in the model even if their coefficient estimates have large p-values. 
  - The rationale for this principle is that if $(X_1 \times X_2)$ is related to the response, then whether or not the coefficients of $X_1$ or $X_2$ are exactly zero is of little interest. 
  - Also $(X_1 \times X_2)$ is typically correlated with $X_1$ and $X_2$, and so leaving them out tends to alter the meaning of the interaction.

#### Interaction between Two Quantitative X

- Dataset Advertising - Linear Regression of Y (Sales)
  - Model with the interaction effect has better (higher) $R_a^2 = 0.9673$ (vs. 0.8962)
    - i.e. (96.8 − 89.7)/(100 − 89.7) = 69% of the variability in sales that remains after fitting the additive model has been explained by the interaction term
  - Model with the interaction effect has better (lower) RSE = 0.9435 (vs. 1.6814)
  - Coefficients
    - An increase in TV advertising of 1,000 dollars is associated with increased sales of $(\hat{\beta}_1 + \hat{\beta}_3 \times \text{Radio}) \times 1000 = 19 + 1.1 \times \text{Radio}$ units.
    - An increase in Radio advertising of 1,000 dollars will be associated with an increase in sales of $(\hat{\beta}_2 + \hat{\beta}_3 \times \text{TV}) \times 1000 = 29 + 1.1 \times \text{TV}$ units.


```{r 'F66-Regression-Ads-3'}
# #Multiple Linear Regression 
mod_ads_tvrad <- lm(sales ~ tv + radio, data = zzF66Ads)
#
# #Linear Regression with Interaction Term. 
# #The Colon (:) indicates interaction between i.e. a:b implies 'a' multiplied by 'b'
# #The Star (*) is shorthand to include both terms and their interaction 
#mod_ads_intr <- lm(sales ~ tv + radio + tv:radio, data = zzF66Ads)
mod_ads_intr <- lm(sales ~ tv*radio, data = zzF66Ads)
```


```{r 'F66-Print-Ads-3'}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Linear Regression: Advertising"
col_hh <- c("TVxRadio", "TV.Radio", "ALL")
#
stargazer(mod_ads_intr, mod_ads_tvrad,
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```


- ANOVA on Two Models
  - The null hypothesis is that the two models fit the data equally well, whereas Alternative is that they are different
  - p-value less than 0.05 confirms that null is rejected and the models are different


```{r 'F66-ANOVA-Ads-3'}
# #Comparison of Models by ANOVA 
if(FALSE) Anova(mod_ads_intr, mod_ads_tvrad)
if(TRUE) anova(mod_ads_intr, mod_ads_tvrad)
```


#### Interaction between Quantitative X and Qualitative X


- Dataset Credit - Linear Regression of Y (Balance) vs. Continuous X and Categorical X
  - For the Model without interaction
    - For Non-Student: Balance = $\beta_1 \times$ Income + $\beta_0$
    - For Student: Balance = $\beta_1 \times$ Income + $(\beta_0 + \beta_2)$
    - Lines for students and non-students have different intercepts $(\beta_0 + \beta_2)$ vs $\beta_0$. 
    - However both have same slope $\beta_1$
    - The fact that the lines are parallel means that the average effect on balance of a one-unit increase in income does not depend on whether or not the individual is a student. This is a Limitation for the model.
  - For the Model with interaction 
    - For Non-Student (Same as Above): Balance = $\beta_1 \times$ Income + $\beta_0$
    - For Student: Balance = $(\beta_1 + \beta_3) \times$ Income + $(\beta_0 + \beta_2)$
    - (Same as Above) Lines for students and non-students have different intercepts $(\beta_0 + \beta_2)$ vs $\beta_0$. 
    - Now, we have different slopes also. $\beta_1$ for non-students but $(\beta_1 + \beta_3)$ for students.
      - This allows for the possibility that changes in income may affect the credit card balances of students and non-students differently.
    - $\beta_3 = -1.99$
      - i.e. slope for students is lower than the slope for non-students. 
      - This suggests that increases in income are associated with smaller increases in credit card balance among students as compared to non-students.



```{r 'F66-Regression-Credit-4'}
# #Linear Regression with Continuous X and Categorical X
mod_credit_x2 <- lm(Balance ~ Income + Student_Yes, data = zzF66Credit_dum)
mod_credit_intr <- lm(Balance ~ Income * Student_Yes, data = zzF66Credit_dum)
```


```{r 'F66-Print-Credit-4'}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Linear Regression: Credit"
col_hh <- c("No Interaction", "Interaction")
#
# #Model p-value
if(FALSE) round(glance(mod)$p.value, 4)
#
stargazer(mod_credit_x2, mod_credit_intr, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```

### Non-linear Relationships

- The true relationship between the response and the predictors may be nonlinear. 
  - \textcolor{pink}{Polynomial Regression} can accommodate non-linear relationships
  - It is still a linear model! 
    - It is simply a multiple linear regression model with $X_1$ and $X_2 = X_1^2$
  

- Dataset Auto - Polynomial Regression of Y (mpg) 
  - The near-zero p-value associated with the quadratic term suggests that it leads to an improved model.
  - The polynomial model has higher $R_a^2$ and lower RSE compared to the linear model
  - Note that higher order polynomial does not result in any major gain in $R_a^2$ or reduction in RSE.
  - Further, With 5th order, we might be experiencing 'overfitting'


- \textcolor{pink}{I()}
  - Within Formula Caret (^) symbol has special meaning. 
  - To use any special symbol in a formula as their general usage, use \textcolor{pink}{I()}
  - I() implies that the symbols within should be applied as regular
- \textcolor{pink}{poly()}
  - It is better to use \textcolor{pink}{poly()} for higher orders
    - Although, we can use \textcolor{pink}{I(X^3)} for Cubic Fit
  - By default, the poly() function orthogonalizes the predictors. 
    - This means that the features output by this function are not simply a sequence of powers of the argument. 
    - However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). 
    - raw = TRUE : To obtain the raw polynomials from the poly() function

```{r 'F66-Regression-Auto-1'}
# #Polynomial Regression with Non-linear predictors using I()
mod_auto_x <- lm(mpg ~ horsepower, data = zzF66Auto)
mod_auto_x2 <- lm(mpg ~ horsepower + I(horsepower^2), data = zzF66Auto)
mod_auto_x5 <- lm(mpg ~ horsepower + I(horsepower^2) + I(horsepower^3) + I(horsepower^4) +
                    I(horsepower^5), data = zzF66Auto)
# #Polynomial Regression with Non-linear predictors using poly()
mod_auto_poly <- lm(mpg ~ poly(horsepower, 5), data = zzF66Auto)
mod_auto_raw <- lm(mpg ~ poly(horsepower, 5, raw = TRUE), data = zzF66Auto)
```


```{r 'F66-Print-Auto-1'}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Polynomial Regression: Auto"
col_hh <- c("Linear", "I2", "I5", "Raw", "Poly5")
#
# #Model p-value
if(FALSE) round(glance(mod)$p.value, 4)
#
stargazer(mod_auto_x, mod_auto_x2, mod_auto_x5, mod_auto_raw, mod_auto_poly, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```
## Potential Problems

1. Non-linearity of the response-predictor relationships.
2. Correlation of error terms.
3. Non-constant variance of error terms.
4. Outliers.
5. High-leverage points.
6. Collinearity.

### Non-linearity of the Data

- The linear regression model assumes that there is a straight-line relationship between the predictors and the response. 
  - If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are suspect. In addition, the prediction accuracy of the model can be significantly reduced. 
  - \textcolor{pink}{Residual plots, $(y_i - \hat{y}_i)$ vs. $\hat{y}_i$}, are a useful graphical tool for identifying non-linearity.
    - There should be no discernible pattern observed in the plot

- Dataset Auto - Polynomial Regression of Y (mpg) 
  - Quadratic Term improves the fit of the data.

```{r 'F66-Auto-Results-1', include=FALSE}
# #Augment the Tibble with Fitted and Residuals
res_auto_x <- augment(mod_auto_x)
res_auto_x2 <- augment(mod_auto_x2)
```


```{r 'F66-Auto-ResidFit-1-Set', include=FALSE}
# #Setup for Residual vs. Fitted Plot of lm()
hh <- res_auto_x
#
cap_hh <- "F66P09"
ttl_hh <- "Auto: Y (mpg) vs. X (horsepower): Residuals vs. Fitted"
sub_hh <- "Large residuals at edges, Small at Centre: Non-linearity!"
```

```{r 'F66-Auto-Residuals-1-Plot', include=FALSE, ref.label=c('F66-Plot-ResidFit')}
#
```

```{r 'F66P09-Save', include=FALSE}
loc_png <- paste0(.z$PX, "F66P09", "-Auto-ResidFit-1x", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = F66P09, device = "png", dpi = 144) 
}
```

```{r 'F66P09', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "F66P09", "-Auto-ResidFit-1x", ".png"))
```


```{r 'F66-Auto-ResidFit-2-Set', include=FALSE}
# #Setup for Residual vs. Fitted Plot of lm()
hh <- res_auto_x2
#
cap_hh <- "F66P10"
ttl_hh <- "Auto: Y (mpg) vs. Square (horsepower): Residuals vs. Fitted"
sub_hh <- "No strong pattern is visible!"
```

```{r 'F66-Auto-Residuals-2-Plot', include=FALSE, ref.label=c('F66-Plot-ResidFit')}
#
```

```{r 'F66P10-Save', include=FALSE}
loc_png <- paste0(.z$PX, "F66P10", "-Auto-ResidFit-2x", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = F66P10, device = "png", dpi = 144) 
}
```

```{r 'F66P10', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "F66P10", "-Auto-ResidFit-2x", ".png"))
```


```{r 'F66P0910', echo=FALSE, ref.label=c('F66P09', 'F66P10'), fig.cap="(F66P09, F66P10) Auto: Residuals vs. Fitted: Linear (Left) vs. Square (Right)"}
#
```

### Correlation of Error Terms

- An important assumption of the linear regression model is that the error terms are uncorrelated. 
  - i.e. error of one observation $e_i$ provides no information about error of another observation $e_j$
  - The standard errors are based on the assumption of uncorrelated error terms. 
    - If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors. As a result, confidence and prediction intervals will be narrower than they should be. 
    - In addition, p-values associated with the model will be lower than they should be; this could cause us to erroneously conclude that a parameter is statistically significant. 
    - In short, if the error terms are correlated, we may have an unwarranted sense of confidence in our model. 
  - Such correlations frequently occur in the context of time series data, which consists of observations for which measurements are obtained at discrete points in time. In many cases, observations that are obtained at adjacent time points will have positively correlated errors.
    - If we plot residuals vs. time, we might see \textcolor{pink}{tracking} in the plot i.e. adjacent residuals may have similar values.

### Non-constant Variance of Error Terms


```{definition 'Heteroscedasticity'}
\textcolor{pink}{Heteroscedasticity} is the non-constant variances in the errors. It is a major concern because it invalidates several key assumptions. It can be identified by the presence of a funnel shape in the residual plot. A possible solution is the transformation of response Y.
```


- Another important assumption of the linear regression model is that the error terms have a constant variance, $\text{Var}(\epsilon_i) = {\sigma}^2$. 
  - The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption. 
  - Unfortunately, it is often the case that the variances of the error terms are non-constant. 
  - Transformation
    - To transform the response Y using a concave function such as $\log{Y}$ or $\sqrt{Y}$. 
      - Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity. 
  - Sometimes we have a good idea of the variance of each response. 
    - For example, the $i^{th}$ response could be an average of $n_i$ raw observations. 
    - If each of these raw observations is uncorrelated with variance ${\sigma}^2$, then their average has variance ${\sigma}_i^2 = \frac{{\sigma}^2}{n_i}$. 
    - In this case a simple remedy is to fit our model by weighted least squares, with weights proportional to the inverse weighted least squares variances—i.e. $w_i = n_i$ in this case. 


### Outliers

- Residual Plots can be used to identify \textcolor{pink}{Outliers}. To address this problem, instead of plotting the residuals, we can plot the \textcolor{pink}{studentized residuals}, computed by dividing each residual $e_i$ by its estimated standard error. 
  - Observations whose studentized residuals are greater than 3 in absolute value are possible outliers.

### High Leverage Points

- Generally, \textcolor{pink}{outliers} are observations for which the response $y_i$ is unusual given the predictor $x_i$. In contrast, observations with \textcolor{pink}{high leverage} have an unusual value for $x_i$.
- A large value of \textcolor{pink}{leverage statistic ($h_i$)} indicates an observation with high leverage. 
  - $h_i = \frac{1}{n} + \frac{(x_i - {\overline{x}})^2}{\sum_{j = 1}^n (x_j - {\overline{x}})^2}$
  - i.e. $h_i$ increases with the distance of $x_i$ from ${\overline{x}}$.
  - $h_i \in [\frac{1}{n}, 1]$ 
  - Average $h_i = \frac{(p + 1)}{n}$
  - So if a given observation has a leverage statistic that greatly exceeds (p+1)/n, then we may suspect that the corresponding point has high leverage.

### Collinearity

- \textcolor{pink}{Collinearity} refers to the situation in which two or more predictor variables collinearity are closely related to one another.
  - "ForLater" If we add polynomial terms or interaction terms, would those not increase the multicollinearity
  - The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. 
  - Since collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for $\hat{\beta}_j$ to grow. 
    - The t-statistic for each predictor is calculated by dividing $\hat{\beta}_j$ by its standard error.     - Consequently, collinearity results in a decline in the t-statistic. 
    - As a result, in the presence of collinearity, we may fail to reject ${H_0} : \beta_j = 0$. 
    - This means that the power of the hypothesis test —the probability of correctly detecting a non-zero coefficient— is reduced by collinearity.
  - A simple way to detect collinearity is to look at the correlation matrix of the predictors.


```{definition 'Multicollinearity-F66'}
\textcolor{pink}{Multicollinearity} is the situation when collinearity exists between three or more variables even if no pair of variables has a particularly high correlation. It is assessed by VIF.
```


```{definition 'VIF-F66'}
The \textcolor{pink}{variance inflation factor (VIF)} is the ratio of the variance of $\hat{\beta}_j$ when fitting the full model divided by the variance of $\hat{\beta}_j$ if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity. 
```


- Multicollinearity is difficult to detect by inspection of the correlation matrix.
  - To assess multicollinearity, compute the variance inflation factor (VIF). 
  - There are two solutions
    - Either drop one of the multicollinear variable because the information it provides about the Respone is redundant in the presence of other variable
    - Or combine collinear variables into a single predictor e.g. by taking average of their standardised versions.
      - Standardisation is required so that unit of one variable does not overshadow the other


- Dataset Credit - Linear Regression of Y (Balance) 
  - Limit and Rating are collinear
    - So, they tend to increase or decrease together. Thus, it can be difficult to determine how each one separately is associated with the response (Balance). 
  - Model with Predictors having No collinearity e.g. Age & Limit
    - Both Age and Limit are Significant 
  - Model with Predictors which are collinear e.g. Rating & Limit
    - The collinearity between 'limit' and 'rating' has caused the standard error for the limit coefficient estimate to increase by a factor of 12 (from 0.005 to 0.0638) and changed its status from significant to not significant based on p-value.
    - In other words, the importance of the 'limit' variable has been masked due to the presence of collinearity. 


```{r 'F66-Credit-Corr-Set', include=FALSE}
# #Setup for Corrplot
ii <- zzF66Credit %>% select(where(is.numeric))
hh <- cor(ii)
corr_hh <- cor.mtest(ii)
# #p-value Higher than this is insignificant and should be skipped
sig_corr_hh <- 0.05 
#
cap_hh <- "F66P11"
ttl_hh <- "Credit: Corrplot"
loc_png <- paste0(.z$PX, "F66P11", "-Credit-Corrplot", ".png")
```

```{r 'F66P11-Save', include=FALSE, ref.label=c('F66-Corrplot')}
#
```

```{r 'F66P11', echo=FALSE, fig.cap="(F66P11) Credit: Corrplot"}
knitr::include_graphics(paste0(.z$PX, "F66P11", "-Credit-Corrplot", ".png"))
```


```{r 'F66-Regression-Credit-5'}
# #Linear Regression with Predictors without Collinearity
mod_credit_nocol <- lm(Balance ~ Age + Limit, data = zzF66Credit)
# #Linear Regression with Predictors having Collinearity
mod_credit_col <- lm(Balance ~ Rating + Limit, data = zzF66Credit)
# #All 3
mod_credit_col3 <- lm(Balance ~ Age + Rating + Limit, data = zzF66Credit)
```


```{r 'F66-Print-Credit-5'}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Linear Regression: Credit: Collinearity"
col_hh <- c("No.Coll", "Yes.Coll", "Three")
#
# #Model p-value
if(FALSE) round(glance(mod)$p.value, 4)
#
stargazer(mod_credit_nocol, mod_credit_col, mod_credit_col3, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*s", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```

```{r 'F66-VIF-Credit-5'}
# #VIF
vif(mod_credit_nocol)
vif(mod_credit_col)
#
vif_ii <- vif(mod_credit_col3)
vif_ii[vif_ii > 5]
```


## The Marketing Plan {#ans-f66}

```{r 'F66-Print-Ads-4'}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Linear Regression: Advertising"
col_hh <- c("TVxRadio", "TV.Radio", "ALL")
#
stargazer(mod_ads_intr, mod_ads_tvrad, mod_ads, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```

```{r 'F66-ConfidenceInterval-Ads-4'}
# #Confidence Interval of the Estimated Coefficients
round(confint(mod_ads_intr), 3)
round(confint(mod_ads), 3)
# #VIF
vif(mod_ads)
```

- Refer [The Original Questions](#reg-f66 "f66")
  - Best Model : 
    - Exclude Newspaper
    - Include Interaction between TV and Radio


1. Is there a relationship between advertising budget and sales
    - This question can be answered by fitting a multiple regression model of sales onto TV, radio, and newspaper and testing the hypothesis \textcolor{pink}{${H_0} : \beta_\text{TV} = \beta_\text{Radio} = \beta_\text{Newspaper} = 0$}
    - The F-statistic can be used to determine whether or not we should reject this null hypothesis. 
    - In this case the p-value corresponding to the F-statistic is very low, indicating clear evidence of a relationship between advertising and sales.
2. How strong is the relationship between advertising budget and sales
    - $\text{RSE} = 0.9435$ estimates the standard deviation of the response from the population regression line. 
      - Percentage Error $= \frac{\text{RSE}}{{\overline{y}}} = \frac{0.9435}{14.022} \approx 6.7\%$
    - $(R_a^2 = 0.9673)$ statistic records the percentage of variability in the response that is explained by the predictors. The predictors explain almost 96% of the variance in sales.
3. Which media are associated with sales
    - To answer this question, we can examine the p-values associated with t-statistic of each predictor.
    - The p-values for TV and radio are low, but the p-value for newspaper is not. This suggests that only TV and radio are related to sales. 
4. How large is the association between each medium and sales
    - The standard error of $\hat{\beta}_j$ can be used to construct confidence intervals for ${\beta}_j$. 
    - 95% confidence intervals for the coefficients: 
      - All Predictors Model: (0.043, 0.049) for TV, (0.172, 0.206) for radio, and (-0.013, 0.011) for Newspaper. 
      - Best Model: (0.016, 0.022) for TV, (0.011, 0.046) for radio, and (0.001, 0.001) for interaction betwen TV and Radio. 
    - The confidence intervals for TV and radio are narrow and far from zero, providing evidence that these media are related to sales. 
    - But the interval for newspaper includes zero, indicating that the variable is not statistically significant given the values of TV and radio. 
    - Collinearity can result in very wide standard errors. Collinearity might be the reason that the confidence interval associated with newspaper is so wide
      - However, VIF scores are 1.005, 1.145, and 1.145 for TV, radio, and newspaper, suggesting no evidence of collinearity. 
5. How accurately can we predict future sales
    - Prediction Interval is used to predict an individual response 
    - Confidence Interval is used to predict an average response
    - Prediction intervals will always be wider than confidence intervals because they account for the uncertainty associated with $\epsilon$, the irreducible error.
6. Is the relationship linear
    - Residual Plot does not show any non-linearity
7. Is there synergy among the advertising media (interaction effect)
    - A small p-value associated with the interaction term indicates the presence of such relationships.
    - Including an interaction term in the model has resulted in a substantial increase in $R_a^2$



## Comparison of Linear Regression with K-Nearest Neighbors (KNN)

- Linear regression is an example of a parametric approach because it assumes a linear functional form for f(X). Parametric methods have several advantages. They are often easy to fit, because one need estimate only a small number of coefficients. In the case of linear regression, the coefficients have simple interpretations, and tests of statistical significance can be easily performed. 

- But parametric methods do have a disadvantage: by construction, they make strong assumptions about the form of f(X). If the specified functional form is far from the truth, and prediction accuracy is our goal, then the parametric method will perform poorly. 

- In contrast, non-parametric methods do not explicitly assume a parametric form for f(X), and thereby provide an alternative and more flexible approach for performing regression. 
  - The simplest and best-known non-parametric methods is \textcolor{pink}{K-nearest neighbors regression (KNN regression)}. 
  - The KNN regression method is closely related to the KNN classifier. 
  - Given a value for K and a prediction point $x_0$, KNN regression first identifies the K training observations that are closest to $x_0$, represented by $\mathcal{N}_0$. It then estimates f($x_0$) using the average of all the training responses in $\mathcal{N}_0$. 
  - A small value for K provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent on just one observation. 
  - In contrast, larger values of K provide a smoother and less variable fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in f(X). 

- Note: 
  - The parametric approach (linear regression) will outperform the nonparametric approach (KNN regression) if the parametric form that has been selected is close to the true form of f (i.e. the actual relationship is linear).
  - KNN performs better than linear regression for non-linear situations. 
  - Thus, in a real life situation in which the true relationship is unknown, one might suspect that KNN should be favored over linear regression because it will at worst be slightly inferior to linear regression if the true relationship is linear, and may give substantially better results if the true relationship is non-linear. 
    - But in reality, even when the true relationship is highly non-linear, KNN may still provide inferior results to linear regression. 
    - It is because of the dimensionality problem

- \textcolor{pink}{dimensionality problem}
  - Read data has \textcolor{pink}{noise predictors} that are not associated with the response.
  - The increase in dimension causes a small deterioration in the linear regression test set MSE, but it might cause a multi-fold increase in the MSE for KNN. 
  - This decrease in performance as the dimension increases is a common problem for KNN, and results from the fact that in higher dimensions there is effectively a reduction in sample size. 
  - If, there are 50 training observations in a data set; when p = 1, this provides enough information to accurately estimate f(X). However, spreading 50 observations over p = 20 dimensions results in a phenomenon in which a given observation has no nearby neighbors—this is the so-called \textcolor{pink}{curse of dimensionality}. 
    - i.e. the K observations that are nearest to a given test observation $x_0$ may be very far away from $x_0$ in p-dimensional space when p is large, leading to a very poor prediction of f($x_0$) and hence a poor KNN fit. 
    

- \textcolor{pink}{Note}
  - Parametric methods tend to outperform non-parametric approaches when there is a small number of observations per predictor.
  - Further, even when the dimension is small, we might prefer linear regression to KNN from an interpretability standpoint. 

## Hands on ...

### Boston {.tabset .tabset-fade}

- Dataset Boston - Linear Regression of Y (medv)


```{r 'F66-Regression-Boston-1'}
# #Linear Regression 
mod_boston_lstat <- lm(medv ~ lstat, data = zzF66Boston)
mod_boston <- lm(medv ~ ., data = zzF66Boston)
```


```{r 'F66-Print-Boston-1'}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Linear Regression: Boston"
col_hh <- c("ALL")
#
stargazer(mod_boston, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```

### Save Results

- \textcolor{pink}{augment()}
  - .cooksd, .fitted, .hat, .lower, .resid, .se.fit, .sigma, .std.resid, .upper 
  
```{r 'F66-Augment-Boston'}
# #Augment the Tibble with Fitted and Residuals
res_boston_lstat <- augment(mod_boston_lstat)
res_boston <- augment(mod_boston)
#
mod <- mod_boston
res <- augment(mod)
#
stopifnot(identical(res$.fitted, unname(predict(mod))))
stopifnot(identical(round(res$.resid, 3), round(unname(residuals(mod)), 3)))
stopifnot(identical(res$.std.resid, unname(rstandard(mod))))
stopifnot(identical(res$.hat, unname(hatvalues(mod))))
#
# #rstudent(), not same as rstandard()
# #studentized residuals = residual / estimated standard error
mm <- rstudent(mod)
```

### Confidence Interval

```{r 'F66-ConfidenceInterval-Boston-1'}
mod <- mod_boston_lstat
#
# #Confidence Interval of the Estimated Coefficients
confint(mod)
#
# #predict() for confidence intervals (95%)
predict(mod, tibble(lstat = c(5, 10, 15)), interval = "confidence")
#
# #predict() for prediction intervals (95%)
predict(mod, tibble(lstat = c(5, 10, 15)), interval = "prediction")
```

### Basic 4 Plots {.tabset .tabset-fade}

#### Plot {.unlisted .unnumbered}

```{r 'F66-Boston-Plots-Base-Set', include=FALSE}
# #Setup for Base 4 Plots of Model
hh <- mod_boston_lstat
#
cap_hh <- "F66P01"
ttl_hh <- "Boston: Simple Linear Regression of X (lstat) vs. Y (medv)"
loc_png <- paste0(.z$PX, "F66P01", "-Boston-lm-1x-lstat", ".png")
```

```{r 'F66P01-Save', include=FALSE, ref.label=c('F66-Plot-Model')}
#
```

```{r 'F66P01', echo=FALSE, fig.cap="(F66P01) Boston: Simple Linear Regression of X (lstat) vs. Y (medv)"}
knitr::include_graphics(paste0(.z$PX, "F66P01", "-Boston-lm-1x-lstat", ".png"))
```

#### Code {.unlisted .unnumbered}

```{r 'F66-Boston-Plots-Base-Set-A', eval=FALSE, ref.label=c('F66-Boston-Plots-Base-Set')}
#
```

```{r 'F66-Plot-Model', eval=FALSE}
# #IN: ttl_hh, cap_hh, hh <- mod_xfw
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) #, width = k_width, height = k_height, units = "in", res = 144
  #dev.control('enable') 
  par(mfrow = c(2, 2))
  plot(hh)
  title(main = ttl_hh, line = -2, adj = 0, outer = TRUE)
  title(sub = cap_hh, line = 4, adj = 1)
  F66 <- recordPlot()
  dev.off()
  assign(cap_hh, F66)
  rm(F66)
  #eval(parse(text = cap_hh))
}
```


### Response vs X with Residuals {.tabset .tabset-fade}

#### Plot {.unlisted .unnumbered}

```{r 'F66-Boston-Response-1-Set', include=FALSE}
# #Setup for Response Plot with Residuals
hh <- res_boston_lstat %>% select(Response = medv, X = lstat, .fitted, .resid)
#
cap_hh <- "F66P02"
ttl_hh <- "Boston: Y (medv) vs. X (lstat) with Residuals"
sub_hh <- "Large residuals at edges, Small at Centre: Non-linearity!"
```

```{r 'F66-Boston-Response-1-Plot', include=FALSE, ref.label=c('F66-Plot-Response-Single')}
#
```

```{r 'F66P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "F66P02", "-Boston-1x-YX", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = F66P02, device = "png", dpi = 144) 
}
```

```{r 'F66P02', echo=FALSE, fig.cap="(F66P02) Boston: Y (medv) vs. X (lstat) with Residuals"}
knitr::include_graphics(paste0(.z$PX, "F66P02", "-Boston-1x-YX", ".png"))
```

#### Code {.unlisted .unnumbered}

```{r 'F66-Boston-Response-1-Set-A', eval=FALSE, ref.label=c('F66-Boston-Response-1-Set')}
#
```

```{r 'F66-Plot-Response-Single', eval=FALSE}
# #IN: sub_hh, cap_hh, ttl_hh, hh (augmented)
F66 <- hh %>% { ggplot(data = ., mapping = aes(x = X, y = Response)) +
    geom_smooth(formula = 'y ~ x', method = "lm", se = FALSE, color = "#440154FF") +
    geom_segment(aes(xend = X, yend = .fitted), alpha = 0.2) +
    geom_point(aes(color = abs(.resid)), size = 1) + 
    #geom_point(aes(y = .fitted), shape = 1, size = 1) + 
    scale_colour_distiller(palette = "BrBG") + 
    scale_y_continuous(breaks = breaks_pretty()) + 
    scale_x_continuous(breaks = breaks_pretty()) + 
    theme(panel.grid = element_blank()) +
    labs(subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, F66)
rm(F66)
#eval(parse(text = cap_hh))
```

### Multiple ScatterPlots with Residuals {.tabset .tabset-fade}

#### Plot {.unlisted .unnumbered}

```{r 'F66-Boston-Response-Set', include=FALSE}
# #Setup for Response Plot with Residuals
hh <- res_boston %>% 
  rename(Response = medv) %>% 
  select(-c(chas, rad, zn, .hat, .sigma, .cooksd, .std.resid)) %>% 
  pivot_longer(!c(Response, .fitted, .resid)) %>% 
  mutate(across(name, factor, levels = unique(name)))
#
cap_hh <- "F66P03"
ttl_hh <- "Boston: Response (medv) vs. X with Residuals"
sub_hh <- "Except: chas, rad, zn"
```

```{r 'F66-Boston-Response-Plot', include=FALSE, ref.label=c('F66-Plot-Response')}
#
```

```{r 'F66P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "F66P03", "-Boston-YX", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = F66P03, device = "png", dpi = 144, width = k_width, height = k_height) 
}
```

```{r 'F66P03', echo=FALSE, out.width='100%', fig.cap="(F66P03) Boston: Response (medv) vs. X with Residuals"}
knitr::include_graphics(paste0(.z$PX, "F66P03", "-Boston-YX", ".png"))
```

#### Code {.unlisted .unnumbered}

```{r 'F66-Boston-Response-Set-A', eval=FALSE, ref.label=c('F66-Boston-Response-Set')}
#
```

```{r 'F66-Plot-Response', eval=FALSE}
# #IN: sub_hh, cap_hh, ttl_hh, hh (long)
F66 <- hh %>% { ggplot(data = ., mapping = aes(x = value, y = Response)) +
    geom_smooth(formula = 'y ~ x', method = "lm", se = FALSE, color = "#440154FF") +
    geom_segment(aes(xend = value, yend = .fitted), alpha = 0.2) +
    geom_point(aes(color = abs(.resid)), size = 1) + 
    #geom_point(aes(y = .fitted), shape = 1, size = 1) + 
    facet_wrap(~ name, scales = "free_x") +
    scale_colour_distiller(palette = "BrBG") + 
    scale_y_continuous(breaks = breaks_pretty()) + 
    scale_x_continuous(breaks = breaks_pretty()) + 
    theme(panel.grid = element_blank()) +
    labs(subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, F66)
rm(F66)
#eval(parse(text = cap_hh))
```

### Residual vs. Fitted {.tabset .tabset-fade}

```{conjecture 'broom-dataframe'}
\textcolor{brown}{Error: `data_frame()` was deprecated in tibble 1.1.0. Please use `tibble()` instead.}
```

- \textcolor{orange}{Warning:} "Data frame tidiers are deprecated and will be removed in an upcoming release of broom."
- The Error & Warning occur if model resul is passed to tidy() in place of actual model

#### Plot {.unlisted .unnumbered}

```{r 'F66-Boston-ResidFit-Set', include=FALSE}
# #Setup for Residual vs. Fitted Plot of lm()
hh <- res_boston_lstat
#
cap_hh <- "F66P04"
ttl_hh <- "Boston: Residuals vs. Fitted for 1x (lstat)"
sub_hh <- NULL
```

```{r 'F66-Boston-ResidFit-Plot', include=FALSE, ref.label=c('F66-Plot-ResidFit')}
#
```

```{r 'F66P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "F66P04", "-Boston-1x-Resid-Fit", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = F66P04, device = "png", dpi = 144) 
}
```

```{r 'F66P04', echo=FALSE, fig.cap="(F66P04) Boston: Residuals vs. Fitted for 1x (lstat)"}
knitr::include_graphics(paste0(.z$PX, "F66P04", "-Boston-1x-Resid-Fit", ".png"))
```

#### Code {.unlisted .unnumbered}

```{r 'F66-Boston-ResidFit-Set-A', eval=FALSE, ref.label=c('F66-Boston-ResidFit-Set')}
#
```

```{r 'F66-Plot-ResidFit', eval=FALSE}
# #IN: sub_hh, cap_hh, ttl_hh, hh (augmented)
F66 <- hh %>% { ggplot(data = ., mapping = aes(x = .fitted, y = .resid)) +
    geom_smooth(formula = 'y ~ x', method = "loess", se = FALSE, color = "#440154FF") +
    geom_point(aes(color = abs(.resid)), size = 1) + 
    geom_hline(yintercept = 0, linetype = 2) +
    scale_colour_distiller(palette = "BrBG") + 
    scale_y_continuous(breaks = breaks_pretty()) + 
    scale_x_continuous(breaks = breaks_pretty()) + 
    theme(panel.grid = element_blank()) +
    labs(subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, F66)
rm(F66)
#eval(parse(text = cap_hh))
```

### Normal QQ {.tabset .tabset-fade}

#### Plot {.unlisted .unnumbered}

```{r 'F66-Boston-QQ-Set', include=FALSE}
# #Setup for QQ Plot of Results by lm()
hh <- res_boston_lstat
#
cap_hh <- "F66P05"
ttl_hh <- "Boston: QQ Plot for Linear Regression"
x_hh <- "Theoretical"
y_hh <- "Sample"
sub_hh <- NULL 
```

```{r 'F66-Boston-QQ-Plot', include=FALSE, ref.label=c('F66-Plot-QQ')}
#
```

```{r 'F66P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "F66P05", "-Boston-lm-QQ", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = F66P05, device = "png", dpi = 144) 
}
```

```{r 'F66P05', echo=FALSE, fig.cap="(F66P05) Boston: QQ Plot for Linear Regression"}
knitr::include_graphics(paste0(.z$PX, "F66P05", "-Boston-lm-QQ", ".png"))
```

#### Code {.unlisted .unnumbered}

```{r 'F66-Boston-QQ-Set-A', eval=FALSE, ref.label=c('F66-Boston-QQ-Set')}
#
```

```{r 'F66-Plot-QQ', eval=FALSE}
# #IN: sub_hh, cap_hh, ttl_hh, hh (augmented) 
F66 <- hh %>% { ggplot(data = ., mapping = aes(sample = .std.resid)) +
    geom_qq(size = 1) + 
    geom_qq_line(color = "#440154FF") + 
    labs(x = x_hh, y = y_hh, subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, F66)
rm(F66)
#eval(parse(text = cap_hh))
```


### Scale vs. Location {.tabset .tabset-fade}

#### Plot {.unlisted .unnumbered}

```{r 'F66-Boston-ScaleLoc-Set', include=FALSE}
# #Setup for Scale vs. Location Plot of Results by lm()
hh <- res_boston_lstat
#
cap_hh <- "F66P06"
ttl_hh <- "Boston: lm() Scale vs. Location"
sub_hh <- NULL 
```

```{r 'F66-Boston-ScaleLoc-Plot', include=FALSE, ref.label=c('F66-Plot-ScaleLoc')}
#
```

```{r 'F66P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "F66P06", "-Boston-lm-Scale-Loc", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = F66P06, device = "png", dpi = 144) 
}
```

```{r 'F66P06', echo=FALSE, fig.cap="(F66P06) Boston: lm() Scale vs. Location"}
knitr::include_graphics(paste0(.z$PX, "F66P06", "-Boston-lm-Scale-Loc", ".png"))
```

#### Code {.unlisted .unnumbered}

```{r 'F66-Boston-ScaleLoc-Set-A', eval=FALSE, ref.label=c('F66-Boston-ScaleLoc-Set')}
#
```

```{r 'F66-Plot-ScaleLoc', eval=FALSE}
# #IN: sub_hh, cap_hh, ttl_hh, hh (augmented) 
F66 <- hh %>% { ggplot(data = ., mapping = aes(x = .fitted, y = sqrt(abs(.std.resid)))) +
    geom_point(aes(color = .std.resid), size = 1) + 
    geom_smooth(formula = 'y ~ x', method = "loess", se = FALSE, color = "#440154FF") +
    scale_colour_distiller(palette = "BrBG") + 
    labs(subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, F66)
rm(F66)
#eval(parse(text = cap_hh))
```


### Residuals vs. Leverage {.tabset .tabset-fade}

#### Plot {.unlisted .unnumbered}

```{r 'F66-Boston-Leverage-Set', include=FALSE}
# #Setup for Residuals vs. Leverage Plot of Results by lm()
hh <- res_boston_lstat
#
cap_hh <- "F66P07"
ttl_hh <- "Boston: lm() Residuals vs. Leverage"
sub_hh <- NULL
```

```{r 'F66-Boston-Leverage-Plot', include=FALSE, ref.label=c('F66-Plot-Leverage')}
#
```

```{r 'F66P07-Save', include=FALSE}
loc_png <- paste0(.z$PX, "F66P07", "-Boston-lm-Leverage", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = F66P07, device = "png", dpi = 144) 
}
```

```{r 'F66P07', echo=FALSE, fig.cap="(F66P07) Boston: lm() Residuals vs. Leverage"}
knitr::include_graphics(paste0(.z$PX, "F66P07", "-Boston-lm-Leverage", ".png"))
```

#### Code {.unlisted .unnumbered}

```{r 'F66-Boston-Leverage-Set-A', eval=FALSE, ref.label=c('F66-Boston-Leverage-Set')}
#
```

```{r 'F66-Plot-Leverage', eval=FALSE}
# #IN: sub_hh, cap_hh, ttl_hh, hh (augmented) 
F66 <- hh %>% { ggplot(data = ., mapping = aes(x = .cooksd, y = .std.resid)) +
    geom_point(size = 1) + 
    geom_smooth(formula = 'y ~ x', method = "loess", se = FALSE, color = "#440154FF") +
    geom_hline(yintercept = 0, linetype = 2) +
    labs(subtitle = sub_hh, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, F66)
rm(F66)
#eval(parse(text = cap_hh))
```

### Basic 4 Plots (GGPlot)

```{r 'F66P0407', echo=FALSE, ref.label=c('F66P04', 'F66P05', 'F66P06', 'F66P07'), fig.cap="(F66P04, F66P05, F66P06, F66P07) Boston: Basic 4 Graphs by GGPlot"}
#
```


## Model Summary {#lm-all-f66 .tabset .tabset-fade}

- [What to do with lm() object!](#lm-all-f66 "f66")
- Dataset Credit - Linear Regression of Y (Balance) 
  

- Comparison of Models with Explicit Dummies vs. Implicit Dummies
  - \textcolor{pink}{Conclusion:} NO Difference. Keep using Explicit for now (Although, No additional benefit).
  - Dataframe and Number of Columns are different because Explicit already has $(k - 1)$ columns.
  - Class of these columns is different because Implicit shows these columns as Factors whereas Explicit has numeric dummies.
  - names(mod) : Implicit has additional 'contrasts' which is NOT present in Explicit
  - mod$xlevels : Applicable to Implicit. It includes Factor Variables and their Levels
  - Variables : For Implicit the predictor X is the original variable with multiple levels whereas for Explicit the predictor X is numeric

### Model {.unlisted .unnumbered}

```{r 'F66-Regression-Credit-3'}
# #Linear Regression with Continuos & Categorical X
# #Dummy |
zzF66Credit_dum <- dummy_cols(zzF66Credit, 
                              remove_first_dummy = TRUE, remove_selected_columns = TRUE)
#
# #Remove underscores to match with implicit dummies
names(zzF66Credit_dum) <- sub("_", "", names(zzF66Credit_dum))
#
# #Explicit Dummies and Implicit Dummies
mod_credit <- lm(Balance ~ ., data = zzF66Credit_dum)
mod_credit_imp <- lm(Balance ~ ., data = zzF66Credit)
```


```{r 'F66-Print-Credit-3'}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Linear Regression: Credit"
#col_hh <- c("ALL")
col_hh <- c("Explicit", "Implicit")
#
stargazer(mod_credit, mod_credit_imp,
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```

### Summary Explicit {.unlisted .unnumbered}

```{r 'F66-Summary-Credit-Explicit'}
# #The model itself, summary(), glance()
mod <- mod_credit #mod_credit_imp #mod_credit
```


```{r 'F66-Summary-lm-Explicit', ref.label = c('F66-Summary-lm')}
#
```

### Summary Implicit {.unlisted .unnumbered}

```{r 'F66-Summary-Credit-Implicit'}
# #The model itself, summary(), glance()
mod <- mod_credit_imp #mod_credit_imp #mod_credit
```


```{r 'F66-Summary-lm-Implicit', ref.label = c('F66-Summary-lm')}
#
```

### Summary Code {.unlisted .unnumbered}

```{r 'F66-Summary-lm', eval=FALSE}
# #The model itself, summary(), glance()
#mod <- mod_credit_imp #mod_credit_imp #mod_credit
#
# #Available Objects of the List in Model object of class "lm" 
names(mod)
# # ? 
str(mod$effects)
#
# #Rank of the Coefficient Matrix (including Intercept, excluding Y)
mod$rank
stopifnot(identical(mod$qr[["rank"]], mod$rank))
#
# #Predicted /Calculated Resonse Y i.e. hat{Y}
str(mod$fitted.values)
#
# # ? 
mod$assign
#
# #qr is a List of List: Available Objects of this List
names(mod$qr)
#
# # ? 
str(mod$qr[["qr"]], give.attr = FALSE)
#
# # ? 
mod$qr[["qraux"]]
#
# # ? 
mod$qr[["pivot"]]
#
# # ? 
mod$qr[["tol"]]
#
# #Factor Variables and their Levels (if Implicit Conversion)
mod$xlevels
#
# #'terms' is a List of List & is exactly same for Model & Summary: Available Objects of this List
stopifnot(identical(mod$terms, summary(mod)$terms))
names(attributes(mod$terms))
#
# #List of Variables: Y & X - To be checked if default lm for EAST
attributes(mod$terms)$variables
stopifnot(identical(attributes(mod$terms)$variables, attributes(mod$terms)$predvars))
#
# #Matrix of Coefficients (but obviously missing reference like Region = East)
attributes(mod$terms)$factors %>% as_tibble(rownames = "Coefficients") 
#
# #Only Independent Variables X
attributes(mod$terms)$term.labels
# # ? 
attributes(mod$terms)$order
# # ? It is Always 1. Probably shows either the position of intercept or How Many Intercepts
attributes(mod$terms)$intercept
# # ? It is Always 1. Probably shows how many Repsonse Y are in the model
attributes(mod$terms)$response
#
# #Class and Environment
attributes(mod$terms)$class 
attributes(mod$terms)$.Environment
#
# #Class of each variable (Y and X)
attributes(mod$terms)$dataClasses
#
# #Original Dataset
str(mod$model, give.attr = FALSE)
#
# #Available Objects of the List in Model object of class "summary.lm" 
names(summary(mod))
#
# #Explanation moved to separate Tab
if(FALSE) summary(mod)$cov.unscaled 
#
# #Model: 
summary(mod)$call 
stopifnot(identical(summary(mod)$call, mod$call))
#
# #Residuals of each observation (Length N)
stopifnot(identical(summary(mod)$residuals, mod$residuals))
str(summary(mod)$residuals)
#
# #Coefficient Estimates, Standard Error, t-value, p-value and Significance (< 0.05)
if(FALSE) summary(mod)$coefficients
stopifnot(identical(summary(mod)$coefficients[ ,"Estimate"], mod$coefficients))
if(TRUE) f_pNum(summary(mod)$coefficients, 3) %>% as_tibble(rownames = "Coefficients") %>% 
  rename(pVal = "Pr(>|t|)") %>% 
  mutate(pVal = ifelse(pVal < 0.001, 0, pVal), isSig = ifelse(pVal < 0.05, TRUE, FALSE))
#
# # ? It is FALSE for each variable X. 
summary(mod)$aliased
#
# #Estimated standard error of the residuals OR Residual Standard Error (RSE)
#glance(mod)$sigma 
summary(mod)$sigma 
#
# #Degrees of Freedom: 3 Types: [1] ? [2] Residual degrees of freedom: (n-p-2) [3] ?
summary(mod)$df
stopifnot(all(identical(mod$df.residual, glance(mod)$df.residual), 
              identical(mod$df.residual, summary(mod)$df[2])))
#
# #R squared statistic OR percent of variation explained OR coefficient of determination
#glance(mod)$r.squared
summary(mod)$r.squared
#
# #Adjusted R squared statistic
#glance(mod)$adj.r.squared 
summary(mod)$adj.r.squared
#
# #F-statistic value, numdf (Numerator DOF) and dendf (Denominator DOF)
summary(mod)$fstatistic
# #Test statistic for the Model e.g. F-statistic for lm()
if(FALSE) summary(mod)$fstatistic[["value"]] #summary(mod)$fstatistic[1L]
glance(mod)$statistic 
#
# #The degrees for freedom from the numerator of the overall F-statistic i.e. numdf
if(FALSE) summary(mod)$fstatistic[["numdf"]] #summary(mod)$fstatistic[2L]
glance(mod)$df 
#
# #NOTE: summary() contains p-value of the model i.e. of the F-statistic (Not of the Coefficients)
# #However it is not available in any list object from summary().
# #glance() provides the p-value corresponding to the test statistic e.g. F-statistic for lm()
# #For simple regression with one predictor ... 
# #the model p-value (of the F-statistic) and the p-value for the coefficient will be the same. 
round(glance(mod)$p.value, 4)
#
# #Column Names of the Tibble:
names(glance(mod)) 
#
# #Number of Observations N
glance(mod)$nobs 
#
# #The log-likelihood of the model
glance(mod)$logLik 
#
# #Akaike Information Criterion for the model
glance(mod)$AIC 
#
# #Bayesian Information Criterion for the model
glance(mod)$BIC 
#
# #Deviance of the model
glance(mod)$deviance 
```

### Covariance {.unlisted .unnumbered}

```{r 'F66-Covariance-Credit-3'}
mod <- mod_credit
# #Matrix of Covariance (Unscaled): 
# #NOTE: "unscaled" means that it is not scaled by the estimated variance sigma^2
# #IF X refers to the design-matrix:
# #unscaled: solve(t(X) %*% X)
# #scaled  : solve(t(X) %*% X) * sigma^2
summary(mod)$cov.unscaled %>% as_tibble(rownames = "Coefficients") 
#
# #To get Scaled Variance 
ii <- summary(mod)$cov.unscaled * summary(mod)$sigma^2 
jj <- vcov(mod)
stopifnot(identical(ii, jj))
```

### Investigate p-value {.unlisted .unnumbered}

```{r 'F66-pval'}
# #Why the p-value of summary() is not available as an object
mod <- mod_credit_rEast
#
# #Get the class of the object 
class(summary(mod))
#
# #Get the code for the print method and look for "p-value:"
if(FALSE) stats:::print.summary.lm
#
# #Which shows the calculation 
if(FALSE) format.pval(pf(x$fstatistic[1L], x$fstatistic[2L], x$fstatistic[3L], 
               lower.tail = FALSE), digits = digits)
#
# #Thus, the p-value can be calculated as
if(FALSE) pf(q = summary(mod)$fstatistic[["value"]], df1 = summary(mod)$fstatistic[["numdf"]], 
             df2 = summary(mod)$fstatistic[["dendf"]], lower.tail = FALSE)
if(TRUE) pf(q = summary(mod)$fstatistic[1L], df1 = summary(mod)$fstatistic[2L], 
            df2 = summary(mod)$fstatistic[3L], lower.tail = FALSE)
#
# #Equivalent: glance()
if(TRUE) round(glance(mod)$p.value, 4)
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'F66-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, cap_hh, col_hh, corr_hh, F66P02, F66P03, F66P04, F66P05, F66P06,
          F66P07, F66P09, F66P10, hh, loc_png, mm, mod, mod_ads, mod_ads_intr, mod_ads_paper,
          mod_ads_rad, mod_ads_tv, mod_ads_tvrad, mod_auto_poly, mod_auto_raw, mod_auto_x, 
          mod_auto_x2, mod_auto_x5, mod_boston, mod_boston_lstat, mod_cars, mod_credit, 
          mod_credit_col, mod_credit_col3, mod_credit_imp, mod_credit_intr, mod_credit_nocol,
          mod_credit_Own_No, mod_credit_Own_NoYes, mod_credit_Own_Yes, mod_credit_rEast, 
          mod_credit_x2, res, res_auto_x, res_auto_x2, res_boston, res_boston_lstat, sig_corr_hh, 
          sub_hh, ttl_hh, vif_ii, x_hh, xfw, y_hh, zzF66Ads, zzF66Auto, zzF66Boston, 
          zzF66CarSeats, zzF66CarSeats_dum, zzF66Credit, zzF66Credit_dum)
```

```{r 'F66-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/366-RegL.Rmd-->

# Tree-Based Methods (F72) {#f72}

```{r 'F72', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Definitions {#def-trees-f72}

```{definition 'Decision-Tree-Methods'}
The approaches which involve 'stratifying' or 'segmenting' the predictor space into a number of simple regions are known as \textcolor{pink}{decision tree methods}. These also include bagging, random forests, boosting, and Bayesian additive regression trees.
```


```{definition 'Decision-Tree'}
A \textcolor{pink}{decision tree} is a flowchart-like structure in which each \textcolor{pink}{internal node} represents a 'test' on an attribute, each \textcolor{pink}{branch} represents the outcome of the test, and each \textcolor{pink}{leaf node} represents a class label (decision taken after computing all attributes). The paths from root to leaf represent \textcolor{pink}{classification rules}. 
```


```{definition 'Tree-Node'}
A \textcolor{pink}{node} is a structure which may contain a value or condition, or represent a separate data structure (which could be a tree of its own). 
```


```{definition 'Tree-Node-Parent-Child'}
A \textcolor{pink}{child node} is a node extending from another node. Each node in a tree has zero or more \textcolor{pink}{child nodes}, which are below it in the tree. A node that has a child node is called the \textcolor{pink}{parent node (superior)}. A node has at most one parent. Child nodes with the same parent are \textcolor{pink}{sibling nodes}. 
```


```{definition 'Tree-Node-Internal-Leaf'}
The points along the tree where the predictor space is split are referred to as internal nodes. An \textcolor{pink}{internal node} is any node of a tree that has child nodes. An \textcolor{pink}{external node (outer node, leaf node, terminal node)} is any node that does not have child nodes. 
```


```{definition 'Tree-Node-Root'}
The highest point on a tree structure is called a \textcolor{pink}{root node}, which does not have a parent node. 
```


```{definition 'Tree-Node-Degree'}
The \textcolor{pink}{degree} of a node is the number of children of the node. A leaf node has degree zero. The degree of a tree is the maximum degree of a node in the tree.
```


```{definition 'Tree-Node-Depth'}
The \textcolor{pink}{depth (or level)} of node A is the length of the path from A to the root node. The root node is said to have depth 0.
```


```{definition 'Tree-Node-Height'}
The \textcolor{pink}{height} of node A is the length of the longest path through children to a leaf node. The height of the tree is equal to the height of the root node.
```


```{definition 'Tree-Breadth'}
The number of leaves are given as the \textcolor{pink}{breadth} of the tree.
```


```{definition 'Decision-Trees-Summary'}
The regions $\{R_1, R_2, R_3, \ldots \}$ are known as \textcolor{pink}{terminal nodes} or \textcolor{pink}{leaves of the tree}. \textcolor{pink}{Decision trees} are typically drawn upside down, in the sense that the leaves are at the bottom of the tree. The points along the tree where the predictor space is split are referred to as \textcolor{pink}{internal nodes}. The segments of the trees that connect the nodes are referred as \textcolor{pink}{branches}. Observations satisfying the condition at each \textcolor{pink}{junction} are assigned to the \textcolor{pink}{left branch}, and the others to the \textcolor{pink}{right branch}.
```


```{definition 'Splitting-Pruning'}
\textcolor{pink}{Splitting} is a process of dividing a node into two or more sub-nodes. Remoal of sub-nodes of a decision node, is called \textcolor{pink}{Pruning}
```


```{definition 'Recursive-Partitioning'}
\textcolor{pink}{Recursive partitioning} creates a decision tree that strives to correctly classify members of the population by splitting it into sub-populations based on several dichotomous independent variables. The process is termed recursive because each sub-population may in turn be split an indefinite number of times until the splitting process terminates after a particular stopping criterion is reached. It includes methods like \textcolor{pink}{C4.5, CART} etc. 
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'F72-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'F72-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z3BusinessEconomics/372-Trees.Rmd-->

# Wieringen (E01) {#e01}

```{r 'E01', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

> Ridge Regression by Wessel N. van Wieringen

## Matrix Definitions

```{definition 'Design-Matrix'}
The data from an experiment, in which 'p' characteristics of 'n' samples are measured, is denoted by the the \textcolor{pink}{design matrix $\underset{n \times p}{\mathbf{X}}$}. 
```


```{definition 'Matrix-Rank'}
The \textcolor{pink}{rank} (or the column rank) of a matrix is the dimension of space spanned by the column vectors. Hence, the rank of X is equal to the number of linearly independent columns: $\text{rank}(\underset{n \times p}{\mathbf{X}}) = p$
```


```{definition 'Singular-Matrix'}
A square matrix that does not have an inverse is called \textcolor{pink}{Singular Matrix}. A matrix $\mathbf{A}$ is singular if and only if its determinant is zero i.e. $\text{det}(\mathbf{A}) = 0$. As $\text{det}(\mathbf{A})$ is equal to the product of the eigenvalues $\nu_j$ of $\mathbf{A}$, the matrix $\mathbf{A}$ is singular if one (or more) of the eigenvalues of A is zero. 
```


```{definition 'Orthogonal-Matrix'}
An \textcolor{pink}{orthogonal matrix} $\mathbf{A}$, is a real square matrix with the property: $\{\mathbf{A}^T\mathbf{A} = \mathbf{A}\mathbf{A}^T = \mathbf{I} \}\to \mathbf{A}^T = \mathbf{A}^{-1}$
```


```{definition 'Identity-Matrix'}
The \textcolor{pink}{identity matrix $\mathbf{I}$} is the square matrix with ones on the main diagonal and zeros elsewhere. 
```


```{definition 'Illconditioned-Matrix'}
A matrix is \textcolor{pink}{illconditioned} if its condition number is high. The \textcolor{pink}{condition number} of a square positive definite matrix $\mathbf{A}$ is the ratio of its largest and smallest eigenvalue. If the smallest eigenvalue is zero, the conditional number is undefined and so is $\mathbf{A}^{-1}$
```



## Ridge Regression

- Collinearity leads to uncertainty in the estimates. Regression analysis has difficulty to decide to which covariate the explained proportion of variation in the response should be attributed.


```{definition 'Ridge-Estimator'}
The \textcolor{pink}{ridge regression estimator} proposes to replace $\underset{p \times n}{\mathbf{X}^T}\underset{n \times p}{\mathbf{X}}$ by $\underset{p \times n}{\mathbf{X}^T}\underset{n \times p}{\mathbf{X}} + \lambda \underset{p \times p}{\mathbf{I}}$ with $\lambda \in (0,\infty)$. The estimator $\lambda$ is called \textcolor{pink}{penalty parameter}. 
```

- Each choice of $\lambda$ leads to a different ridge regression estimate.
  - For any $\lambda > 0$, the 'ridge fit' $\hat{Y}(\lambda)$ is not orthogonal to the observation Y. 
  - In other words, the 'ridge residuals' $Y - \hat{Y}(\lambda)$ are not orthogonal to the fit $\hat{Y}(\lambda)$. 
  - Hence, the ad-hoc fix of the ridge regression estimator resolves the non-evaluation of the estimator in the face of super-collinearity but yields a 'ridge fit' that is not optimal in explaining the observation.


- Principal component regression is a close relative to ridge regression that can also be applied in a high-dimensional context. Principal components regression explains the response not by the covariates themselves but by linear combinations of the covariates as defined by the principal components of $\mathbf{X}$.


- All regression coefficients are shrunken towards zero as the penalty parameter increases. 
  -  In particular, the larger $\lambda$, the larger the contribution of the penalty to the loss function, the stronger the tendency to shrink non-zero regression coefficients to zero (and decrease the contribution of the penalty to the loss function).
- The variance of the ridge regression coefficient estimates decreases towards zero as the penalty parameter becomes large. 

- For small $\lambda$, the variance of the ridge estimator dominates the MSE. 
- For large $\lambda$, the variance vanishes and the bias dominates the MSE. 
- For small enough values of $\lambda$, with increse in $\lambda$, the decrease in variance of the ridge regression estimator exceeds the increase in its bias. 
  - As the MSE is the sum of these two, the MSE first decreases as $\lambda$ moves away from zero and then increases.
  - In particular, $\lambda = 0$ corresponds to the ML regression estimator.
  - More noisy data benefits the ridge regression estimator
  - Ridge regression will perform better in the case where the regression parameter is not dominated by a few elements but rather all contribute comparably to the explanation of the variation in the response.

- Overfitting
  - Overfitting refers to the phenomenon of modelling the noise rather than the signal. 
  - In case the true model is parsimonious (few covariates driving the response) and data on many covariates are available, it is likely that a linear combination of all covariates yields a higher likelihood than a combination of the few that are actually related to the response. 
    - As only the few covariates related to the response contain the signal, the model involving all covariates then cannot but explain more than the signal alone: it also models the error. 
    - Hence, it overfits the data. 
  - In high-dimensional settings $(n \leq p)$ overfitting is a real threat. 
    - The number of explanatory variables exceeds the number of observations. 
    - It is thus possible to form a linear combination of the covariates that perfectly explains the response, including the noise. 
  - Large estimates of regression coefficients are often an indication of overfitting. 
    - Augmentation of the estimation procedure with a constraint on the regression coefficients is a simple remedy to large parameter estimates. 
    - As a consequence it decreases the probability of overfitting.


- Constraints
  - Penalty parameter $\lambda$ should not be too close to zero to avoid illconditioned matrix
  - Practically plot a conditional numbers for different $\lambda$ and then choose the optimum one.

## Cross-validation

- Two-fold cross-validation
  - Model building and prediction evaluation on training and test set, respectively, is done for a collection of possible penalty parameter choices. 
  - The penalty parameter that yields the model with the best prediction performance is to be preferred. 

\textcolor{pink}{The thus obtained performance evaluation depends on the actual split of the data set.}

To remove this dependence, the data set is split many times into a training and test set. Per split the model parameters are estimated for all choices of $\lambda$ using the training data and estimated parameters are evaluated on the corresponding test set. The penalty parameter, that on average over the test sets performs best (in some sense), is then selected.

When the repetitive splitting of the data set is done randomly, samples may accidently end up in a fast majority of the splits in either training or test set. Such samples may have an unbalanced influence on either model building or prediction evaluation. 

To avoid this \textcolor{pink}{k-fold cross-validation} structures the data splitting. 

- The samples are divided into 'k' more or less equally sized exhaustive and mutually exclusive subsets. In turn (at each split) one of these subsets plays the role of the test set while the union of the remaining subsets constitutes the training set. 
  - Such a splitting warrants a balanced representation of each sample in both training and test set over the splits. 
  - \textcolor{pink}{Still the division into the k subsets involves a degree of randomness.}

- Not only the shape of the distribution, but also its moments are affected by the randomness of the penalty parameter. 

- Should we standardize the covariates prior to ridge regression analysis
  - The covariate with a larger variance yields the larger ridge regression estimator.
  - Although, the answer is data and context dependent but in general the variables should be standardised

- VIF
  - The VIF measures the change in the variance of the estimate due to the collinearity. 
  - The penalization does not remove collinearity but it reduces the effect of collinearity on the variance of the ridge estimator (as measured by the VIF). 
  - Thus the VIF may guide the choice of the penalty parameter: choose $\lambda$ such that the variance of the estimator is increased at most by a user-specified factor.


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'E01-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'E01-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z4Articles/401-Wieringen.Rmd-->

# MWE (Z01) {#z01}

```{r 'Z01', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- Minimum Working Examples (MWE) for the most common /interesting /obscure 'how to do in R'.
  - It does not cover any function extensively or their finer points, cautions etc.
  - It assumes that reader knows R basics.

- [How to Load Packages](#loadpkg-z01 "z01")
- [How to Copy Clipboard as Tibble](#loaddata-z01 "z01")
- [How to Generate correlated vectors of normal distribution](#cor-vec-z01 "z01")
- [How to Count Unique Values in ALL Columns](#n-uniq-z01  "z01")
- [How to Rename ALL Headers to Proper Names](#rename-z01  "z01")

## Load Multiple Packages {#loadpkg-z01 .tabset .tabset-fade}

### Packages {.unlisted .unnumbered}

```{r 'Z01-LoadPackages', eval=FALSE}
if(FALSE) {# #Load Packages that were already installed
  # #Install Packages if: Error in library(...) : there is no package called '...'
  # #WARNING: Installation may take some time.
  #install.packages("dplyr", dependencies = TRUE)
  library(dplyr)
  library(readr)
  library(tibble)
}
```

### ALL {.unlisted .unnumbered}

```{r 'Z01-LoadPackagesALL', eval=FALSE}
if(FALSE) {# #Load Packages that were already installed
  # #Install Packages if: Error in library(...) : there is no package called '...'
  # #WARNING: Installation may take some time.
  #install.packages("dplyr", dependencies = TRUE)
  pkg <- c("dplyr", "readr", "tibble")
  lapply(pkg, FUN = function(x) {suppressMessages(library(x, character.only = TRUE))})
  rm(pkg)
}
```

## Load Data {#loaddata-z01 .tabset .tabset-fade}

### Clipboard {.unlisted .unnumbered}

```{r 'Z01-LoadClipBoard', eval=FALSE}
if(FALSE) {
  library(readr)
}
# #Assuming that the Data has been copied to clipboard. Output is a Tibble.
ii <- read_delim(clipboard())
```

## Generate Correlated Vectors {#cor-vec-z01 .tabset .tabset-fade}

```{r 'Z01-CorrelatedVectors'}
# #Set Seed
set.seed(3)
#
# #Generate correlated vectors of normal distribution
ii <- rnorm(n = 50, mean = 0, sd = 1)
jj <- ii + rnorm(n = 50, mean = 1, sd = 0.1)
#
# #Check Correlation
cor(ii, jj)
```

## Count Unique Values in ALL Columns {#n-uniq-z01 .tabset .tabset-fade}

```{r 'Z01-CountUnique'}
# #Count Unique Values in ALL Columns
sapply(mtcars, function(x) length(unique(x)))
```

## Rename ALL Headers to Proper Names {#rename-z01 .tabset .tabset-fade}

```{r 'Z01-RenameHeaders'}
if(FALSE) {
  library(dplyr)
}
# #Rename to Proper Names | To Lower, Replace by Underscore | 
aa <- tibble("A_Underscore" = 1, "B.Dot" = 2, "C Space" = 3, "D-Dash" = 4, `E'apostrophe` = 5) #'
bb <- aa %>% rename_with(make.names) %>% 
  rename_with(~ tolower(gsub(".", "_", .x, fixed = TRUE))) 
```



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'Z01-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll)
```

```{r 'Z01-Validation', include=TRUE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****

<!--chapter:end:z9End/901-MWE.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
<div id="refs"></div>
'`


<!--chapter:end:z9End/969-References.Rmd-->

# Glossary {#Glossary .unnumbered .tabset .tabset-fade}

## THEOREMS {.unlisted .unnumbered}
```{r '979-Theorems', comment="", echo=FALSE, results='asis', cache=FALSE}
# #New items will NOT be added immediately if cache=TRUE
for(x in knitr::all_labels(engine == 'theorem')){
# # Use (x, "\n\n>") in place of (x, "\n\n") to show definitions as Quotes
   cat(paste0("\n\n", "\\@ref(thm:", x, "): ", x, "\n\n", knitr:::knit_code$get(x), collapse = "\n\n"))
} 
```

## DEFINITIONS {.unlisted .unnumbered}

```{r '979-Definitions', comment="", echo=FALSE, results='asis', cache=FALSE}
# #New items will NOT be added immediately if cache=TRUE
for(x in knitr::all_labels(engine == 'definition')){
# # Use (x, "\n\n>") in place of (x, "\n\n") to show definitions as Quotes
   cat(paste0("\n\n", "\\@ref(def:", x, "): ", x, "\n\n", knitr:::knit_code$get(x), collapse = "\n\n"))
} 
```

## ERRORS {.unlisted .unnumbered}

```{r '979-Errors', comment="", echo=FALSE, results='asis', cache=FALSE}
# #New items will NOT be added immediately if cache=TRUE
for(x in knitr::all_labels(engine == 'conjecture')){
# # Use (x, "\n\n>") in place of (x, "\n\n") to show definitions as Quotes
   cat(paste0("\n\n", "\\@ref(cnj:", x, "): ", x, "\n\n", knitr:::knit_code$get(x), collapse = "\n\n"))
} 
```

<!--
## Validation {.unlisted .unnumbered}

```{r 'l979-Validation', cache=FALSE}
# #Summarised Packages and Objects
rm(x)
f_()
```
-->

****

<!--chapter:end:z9End/979-Glossary.Rmd-->

# END {#LAST .unnumbered}
[Goto FIRST](#FIRST "Top of the Document")
```{r '999-ShowRunTime', cache=FALSE, include=FALSE}
f_()
#
difftime(Sys.time(), k_start)
#
if(!identical(0L, length(k_all_chunk_times_lt))) {
  print(k_all_chunk_times_lt[order(unlist(k_all_chunk_times_lt), decreasing=TRUE)], digits = 2) 
}
```

```{js '999-DetailsDisclosureElement', include=FALSE, eval=FALSE}
(function() {
// JavaScript code to find output blocks, and wrap them into the <details> tags.
// It needs 'collapse = FALSE'
  var codes = document.querySelectorAll('pre:not([class])');
  var code, i, d, s, p;
  for (i = 0; i < codes.length; i++) {
    code = codes[i];
    p = code.parentNode;
    d = document.createElement('details');
    s = document.createElement('summary');
    s.innerText = 'Details';
    // <details><summary>Details</summary></details>
    d.appendChild(s);
    // move the code into <details>
    p.replaceChild(d, code);
    d.appendChild(code);
  }
})();
```

****

<!--chapter:end:z9End/999-END.Rmd-->

