# Random Forest (B35, Mar-06) {#b35}

```{r 'B35', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Capstone Project

- Steps:
  - Identify any dataset, from any source e.g. Kaggle
  - Work in your group
  - Implement various applicable algorithms
  - Prepare a comprehensive report including data visualizations, EDA e.g. ~25 pages
  - Output Format: PDF generated by 'R Markdown'
  - Interactions: Weekly 2/3 times ~9 PM with the Professor of 15-30 minutes
  - Target: June-End
  - Ensure that the data has enough depth in the sense multiple algorithms can be applied on it
  - Try to have a dataset which can cover both supervised and unsupervised learning methods

- Another faculty in April to cover - Social Media Analytics. IIM will purchase relevant software license
- 3 sessions from Eruditus Global Faculty on Deep Learning /Neural Networks
- Some case studies would be shared for hand-on experience
- "ForLater" Professor will share some material with decision trees, random forest and logistic regression
- "ForLater" Customer Churn dataset

## Data

- [Import Data Bank - B34](#set-bank-b34 "b34")

```{r 'B35-GetBank', ref.label=c('B34-Get-Bank')}
# #zzB34Bank, xbank, test_bank
```

```{r 'B35-Get-Bank', include=FALSE, eval=TRUE}
zzB34Bank <- f_getRDS(zzB34Bank)
#
# #Create Dummies | Replaced All NA in original data so no replacement in dummy columns |
dum_xfw <- dummy_cols(zzB34Bank, 
  select_columns = c("wed", "edu", "default", "housing", "loan", "call", "poutcome"),
                      remove_first_dummy = TRUE, remove_selected_columns = TRUE) 
#
# #Partition Data
set.seed(3)
#idx_xsyw <- sample.int(n = nrow(dum_xfw), size = floor(0.8 * nrow(dum_xfw)), replace = FALSE)
idx_xsyw <- createDataPartition(dum_xfw$y, p = 0.8, list = FALSE)
train_xfw <- dum_xfw[idx_xsyw[, 1], ] 
test_xfw  <- dum_xfw[-idx_xsyw[, 1], ]
#
xbank <- train_xfw
test_bank <- test_xfw
```

## Install TinyTex Package & Distribution

- Skipped "23:00-??" on basics of R Markdown

```{r 'B35-TinyTex', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
# #Install Package 'tinytex' in R
  install.packages("tinytex", dependencies = TRUE)
# #'latexpdf' might be required also
  if(FALSE) install.packages("latexpdf", dependencies = TRUE)
# #Use the package to install the TinyTeX distribution
  tinytex::install_tinytex()
# #Verification: Success =TRUE
  tinytex:::is_tinytex()
}
```


## Bank: Logistic Regression

- Refer [How to Disable Scientific Notation in R](#scipen-b09 "b09")
- Discussion continued from previous lecture: Logistic Regression on the Bank dataset


- Dataset Default - Logistic Regression
  - Because the coefficient is positive, as the "Age" increases, the chances of subscribing the Bank Term Deposit also increases
  - The marital status of Single or Divorced, both have positive coefficients. Thus, their chances of subscribing are higher compared to Married (control /reference)
  - Credit status of people (default) was found to insignificant for predicting the term deposit subscription
  - Coefficients are in the form of log odds.
    - Thus, the direction is same as sign of the coefficients (similar to linear regression). 
    - However, the magnitude needs to be calculated by taking exponential of model coefficients (unlike linear regression)


```{r 'B35-Logistic-Bank-1'}
# #Logistic Regression 
mod_1_glm <- glm(formula = y ~ ., family = binomial, data = xbank)
# #Step-wise Regression
mod_2_step <- step(mod_1_glm, direction = "both", trace = 0)
```

```{r 'B35-Print-Bank-1', echo=FALSE, collapse=FALSE, class.output="models"}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Logistic Regression: Bank: Y ~ X"
col_hh <- c("GLM", "Stepwise")
#
stargazer(mod_1_glm, mod_2_step, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```


```{r 'B35-VIF-Bank-1'}
# #VIF
vif(mod_1_glm) %>% `[`(. > 2)
vif(mod_2_step) %>% `[`(. > 2)
```


```{r 'B35-Contrasts-Bank-1'}
# #contrasts() to identify the criteria: 0 (control, no event), 1 (case, event)
contrasts(xbank$y)
```

```{r 'B35-Exponential-Bank-1'}
# #exp() to get the magnitude of model coefficients
exp(mod_2_step$coefficients)
```


## Imbalanced data: Impact on Random Sampling

```{r 'B35D01', comment="", echo=FALSE, results='asis'}
f_getDef("Imbalanced-Data") #dddd
```

- Effect of Imbalanced Data on the Random Sampling (during partition)
  - Generally, in the case of logistic regression the data imbalance is present 
  - Thus, the Sampling is generally biased towards the majority class i.e. for the training set, most of the selection would be from majority class 


```{definition 'Stratified-Random-Sampling'}
\textcolor{pink}{Stratified Random Sampling} is a method of sampling from a population which can be partitioned into subpopulations. It ensures that the proportion of classes would remain similar in sample and population.
```


```{r 'B35-T7', eval=FALSE}
# #Example with extreme difference between two classes 
bb <- as_tibble(ISLR2::Default)
bb %>% count(default) %>% mutate(PROP = n /sum(n))
#
# #Partition Data by Simple Random Sampling
set.seed(3)
index <- sample.int(n = nrow(bb), size = floor(0.8 * nrow(bb)), replace = FALSE)
train_simple <- bb[c(index), ]
#test_simple <- bb[-c(index), ]
# #Slightly different proportions as compared to the original dataset
train_simple %>% count(default) %>% mutate(PROP = n /sum(n))
#
# #Partition Data by Stratified Random Sampling
set.seed(3)
index <- createDataPartition(bb$default, p = 0.8, list = FALSE)
train_strat <- bb[c(index), ]
#test_strat <- bb[-c(index), ]
# #Same proportions as compared to the original dataset
train_strat %>% count(default) %>% mutate(PROP = n /sum(n))
```

## Predict

```{r 'B35-Predict'}
# #For glm(): predict() provides probabilities 
prob_2_step_train <- predict(mod_2_step, type = "response")
prob_2_step_test <- predict(mod_2_step, newdata = test_bank, type = "response")
#
# #These probabilities cannot be directly applied, conversion into classes is required
# #contrasts() to identify the criteria: 0 (control, no event), 1 (case, event)
contrasts(xbank$y)
#
# #Convert Probabilities into levels
res_2_step_train <- factor(ifelse(prob_2_step_train >= 0.5, "yes", "no"), levels = c("no", "yes"))
res_2_step_test <- factor(ifelse(prob_2_step_test >= 0.5, "yes", "no"), levels = c("no", "yes"))
```

## Confusion Matrix

```{conjecture 'Dollar-Invalid-Atomic'}
\textcolor{brown}{Error ... operator is invalid for atomic vectors}
```

- Check the class and structure of the object. 
- Dollar can not be applied on atomic vectors (including matrix)

```{conjecture 'Confusion-Factor-Levels'}
\textcolor{brown}{Error: `data` and `reference` should be factors with the same levels}
```

- Check the class and structure of the data and reference.
- Convert the numeric probabilities into factor levels and match factor levels (case-sensitive)

```{r 'B35-Cmat'}
# #Confusion Matrix 
cmat_2_train <- confusionMatrix(res_2_step_train, 
                                reference = xbank$y, positive = "yes")
cmat_2_test <- confusionMatrix(res_2_step_test, 
                                reference = test_bank$y, positive = "yes")
```


```{r 'B35-vs-Cmat-Bank', echo=FALSE, collapse=FALSE, class.output="models"}
# #Class Comparisons: LDA vs. QDA vs. Naive Bayes
cmat <- cmat_2_train
hh_names <- c("Accuracy", "N", "True Positive (TP_A)", "False Positive (FP_B)", 
              "False Negative (FN_C)", "True Negative (TN_D) ", 
              names(cmat$byClass))
ii <- c(cmat$overall[["Accuracy"]], round(sum(cmat$table), 0), 
        cmat$table["yes", "yes"], cmat$table["yes", "no"], 
        cmat$table["no", "yes"], cmat$table["no", "no"], round(cmat$byClass, 4))
cmat <- cmat_2_test
jj <- c(cmat$overall[["Accuracy"]], sum(cmat$table), 
        cmat$table["yes", "yes"], cmat$table["yes", "no"], 
        cmat$table["no", "yes"], cmat$table["no", "no"], round(cmat$byClass, 4))
#
if(TRUE) tibble(Names = hh_names, 
     TRAIN = ii, TEST = jj)
```



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B35-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, cmat, cmat_2_test, cmat_2_train, col_hh, dum_xfw, hh_names, 
          idx_xsyw, mod_1_glm, mod_2_step, prob_2_step_test, prob_2_step_train, res_2_step_test,
          res_2_step_train, test_bank, test_xfw, train_xfw, ttl_hh, xbank, zzB34Bank)
```

```{r 'B35-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
