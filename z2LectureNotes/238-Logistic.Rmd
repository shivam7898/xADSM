# Logistic Regression (B38, Apr-03) {#b38}

```{r 'B38', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Data: Churn {#set-churn-b38 .tabset .tabset-fade}

### Glance {.unlisted .unnumbered}

- About: Churn [3333, 22]
  - This is similar to [Dataset - Churn - B18](#set-churn-b18 "b18"). 
  - 'Churn' is the Response Variable
  - Dropped 4 Columns: sl_no_, state, area_code, phone

### EDA {.unlisted .unnumbered}

\textcolor{pink}{Please import the "B38-Customer-Churn.csv"}

```{r 'B38-Data-Churn', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum tools::md5sum(paste0(.z$XL, "B38-Customer-Churn.csv"))
if(FALSE) xxB38Churn <- f_getObject("xxB38Churn", "B38-Customer-Churn.csv",
                                "b8747f2433f7ada18afec51f74b9a10c")
```

```{r 'B38-Get-Churn', include=FALSE}
xxB38Churn <- f_getRDS(xxB38Churn)  #3333 x 22
xxB18Churn <- f_getRDS(xxB18Churn)  #3333 x 21
```

```{r 'B38-Compare-Churn'}
# #B38 vs. B18 : B18 has Serial Number as additional column and "True." "False." as Churn values 
aa <- xxB18Churn |>  rename_with(make.names) |> 
  mutate(across(Churn, ~case_when(. == "True." ~ TRUE, . == "False." ~ FALSE)))
bb <- xxB38Churn |> select(-1)
stopifnot(identical(aa, bb))
```


```{r 'B38-Clean-Churn'}
# #Rename | Modify Levels | Logical to Factor Y | Drop | Relocate Y |
xfw <- xxB38Churn |> rename_with(make.names) |> 
  rename_with(~ tolower(gsub(".", "_", .x, fixed = TRUE))) |> 
  mutate(across(c(int_l_plan, vmail_plan), factor, 
                levels = c("no", "yes"), labels = c("No", "Yes"))) |> 
  mutate(across(churn, factor, levels = c("FALSE", "TRUE"), labels = c("No", "Yes"))) |> 
  select(-c(sl_no_, state, area_code, phone)) |> 
  relocate(churn)

```

### Structure {.unlisted .unnumbered}

```{r 'B38-Structure-Insurance'}
str(xfw)
```

### Summary {.unlisted .unnumbered}

```{r 'B38-Summary-Insurance'}
summary(xfw)
```

### ETC {.unlisted .unnumbered}

```{r 'B38-ETC', include=TRUE, eval=FALSE}
# #Count NA in Columns
if(FALSE) colSums(is.na(bb)) |> as_tibble(rownames = "Cols") |> filter(value > 0)
# #Subset Rows
if(FALSE) bb |> select(1) |> slice(1:10)
# #Comma separated string having each item within quotes for easy pasting as character not objects
if(FALSE) cat('"', paste0(names(which(sapply(bb, is.factor))), collapse = '", "'), '"\n', sep = '')
if(FALSE) cat('"', paste0(levels(bb$Arrival), collapse = '", "'), '"\n', sep = '')
# #Filter
if(FALSE) bb |> filter(is.na(account_length)) |> select(churn, account_length, vmail_plan)
# #Count Yes/No or True/False in ALL such Columns
if(FALSE) bb |> select(churn) |> 
  pivot_longer(everything()) |> count(name, value) |> 
  pivot_wider(names_from = value, values_from = n)
# #Count Unique of all Columns to decide which should be Factors
if(FALSE) bb |> summarise(across(everything(), ~ length(unique(.)))) |> pivot_longer(everything())
# #Summary of Columns of a class: is.factor is.numeric is.character
if(FALSE) bb |> select(where(is.factor)) |> summary()
# #Names and Indices of Columns of class: is.factor is.numeric is.character
if(FALSE) which(sapply(bb, is.factor))
# #Levels of Factor Columns
if(FALSE) lapply(bb[c(3, 6:9, 15)], levels)
# #Frequency of Each level of Factor
if(FALSE) bb |> count(churn) |> arrange(desc(n))
# #Coding for Dummy Variables
if(FALSE) contrasts(bb$Married) 
# #Vector of all column classes
if(FALSE) bb |> {\(x) vapply(x, class, character(1))}()
```

## Dummies

```{r 'B38-Dummy-Churn'}
# #Create Dummies 
dum_xfw <- dummy_cols(xfw, 
  select_columns = c("int_l_plan", "vmail_plan"),
                      remove_first_dummy = TRUE, remove_selected_columns = TRUE) 
```

## Data Partition

```{r 'B38-Partition-Churn'}
# #Partition Data
set.seed(3)
idx_xsyw <- createDataPartition(dum_xfw$churn, p = 0.8, list = FALSE)
zChurn <- dum_xfw[idx_xsyw[, 1], ] 
zTest <- dum_xfw[-idx_xsyw[, 1], ]
```

## Data Balancing 

```{conjecture 'ROSE-Too-Few'}
\textcolor{brown}{Error in ... : Too few observations.}
```

- Check for NA in the dataset and Remove them.


```{r 'B38-Balancing-Churn'}
# #Data has imbalance
zChurn |> count(churn) |> mutate(PROP = n /sum(n))
#
# #Data Balancing with 50% balance
both_churn <- ovun.sample(churn ~ ., data = zChurn, method = "both", seed = 3)$data
zBalanced <- as_tibble(both_churn) 
#
zBalanced |> count(churn) |> mutate(PROP = n /sum(n))
```

## Logistic Regression

- Even though 'vmail_message' is non-significant, step-wise does not drop this variable when the data is unbalanced.

```{r 'B38-GLM-Churn-1'}
# #Logistic Regression: Unbalanced data
mod_1_glm <- glm(formula = churn ~ ., family = binomial, data = zChurn)
# #Step-wise Regression
mod_2_step <- step(mod_1_glm, direction = "both", trace = 0)
```

```{r 'B38-GLM-Churn-2'}
# #Logistic Regression: Balanced data
mod_3_glm <- glm(formula = churn ~ ., family = binomial, data = zBalanced)
# #Step-wise Regression
mod_4_step <- step(mod_3_glm, direction = "both", trace = 0)
```

```{r 'B38-GLM-Churn-3'}
# #Logistic Regression: Balanced data and without high VIF predictor (vmail_message)
mod_5_glm <- glm(formula = churn ~ . -vmail_message, family = binomial, data = zBalanced)
# #Step-wise Regression
mod_6_step <- step(mod_5_glm, direction = "both", trace = 0)
```


```{r 'B38-Print-Churn-1', echo=FALSE, collapse=FALSE, class.output="models"}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Logistic Regression: Churn: Y ~ X"
col_hh <- c("Unbalanced", "U.Step", "Balanced", "B.Step", "LowVIF", "L.Step")
#
stargazer(mod_1_glm, mod_2_step, mod_3_glm, mod_4_step, mod_5_glm, mod_6_step, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```

## VIF

```{r 'B38-VIF-Churn'}
# #There are very high VIF value column with non-significant coefficient
vif(mod_2_step) |> {\(x) x[x > 2]}()
vif(mod_4_step) |> {\(x) x[x > 2]}()
vif(mod_6_step) |> {\(x) x[x > 2]}()
```


## Prediction on Train 

```{r 'B38-Predict-Churn-Train-2'}
# #For glm() predict() provides probabilities 
prob_2_step <- predict(mod_2_step, type = "response")
#
# #Convert Probabilities into levels
res_2_step <- factor(ifelse(prob_2_step >= 0.5, "Yes", "No"), levels = c("No", "Yes"))
#
# #Confusion Matrix
cmat_2_step <- confusionMatrix(res_2_step, 
                                reference = zChurn$churn, positive = "Yes")
cmat <- cmat_2_step
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

```{r 'B38-Predict-Churn-Train-4'}
# #For glm() predict() provides probabilities 
prob_4_step <- predict(mod_4_step, type = "response")
#
# #Convert Probabilities into levels
res_4_step <- factor(ifelse(prob_4_step >= 0.5, "Yes", "No"), levels = c("No", "Yes"))
#
# #Confusion Matrix
cmat_4_step <- confusionMatrix(res_4_step, 
                                reference = zBalanced$churn, positive = "Yes")
cmat <- cmat_4_step
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

```{r 'B38-Predict-Churn-Train-6'}
# #For glm() predict() provides probabilities 
prob_6_step <- predict(mod_6_step, type = "response")
#
# #Convert Probabilities into levels
res_6_step <- factor(ifelse(prob_6_step >= 0.5, "Yes", "No"), levels = c("No", "Yes"))
#
# #Confusion Matrix
cmat_6_step <- confusionMatrix(res_6_step, 
                                reference = zBalanced$churn, positive = "Yes")
cmat <- cmat_6_step
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

## Confusion Matrix

- \textcolor{pink}{Question:} What does the p-value i.e. P-Value [Acc > NIR] indicate
  - (Aside) we want to know if the accuracy being higher than NIR is significant. It is a simple one-tailed binomial test. The p-value from which is what the \textcolor{pink}{P-Value [Acc > NIR]} displays. 
  - Ideally, Accuracy (0.862) should be higher than NIR (0.855) and the result should be significant.
  - However, in this case, the result is not-significant (p-value = 0.168)

- \textcolor{pink}{Question:} The thumb rule is high sensitivity means that the model is good
  - Yes but only in this context. In other cases, either sensitivity or specificity or both needs to be considered.


```{r 'B38-Cmat-Churn-Train'}
# #Confusion Matrix
cmat <- cmat_2_step
#
# #Ideally, Accuracy should be higher than NIR and the result should be significant.
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$overall[["AccuracyNull"]]
if(TRUE) cmat$overall[["AccuracyPValue"]]
#
if(TRUE) cmat$table |> as_tibble() |> 
  pivot_wider(names_from = Reference, values_from = n) |> 
  rename(Prediction_Reference = 1) |> 
  {\(x) add_row(x, summarise(x, across(1, ~"Total")), 
                summarise(x, across(where(is.numeric), sum)))}() |> 
  mutate(SUM = rowSums(across(where(is.numeric))))
```

## Prediction on Test 

```{r 'B38-Predict-Churn-Test-2'}
# #For glm() predict() provides probabilities 
prob_2_step_test <- predict(mod_2_step, newdata = zTest, type = "response")
#
# #Convert Probabilities into levels
res_2_step_test <- factor(ifelse(prob_2_step_test >= 0.5, "Yes", "No"), levels = c("No", "Yes"))
#
# #Confusion Matrix
cmat_2_step_test <- confusionMatrix(res_2_step_test, 
                                reference = zTest$churn, positive = "Yes")
cmat <- cmat_2_step_test
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

```{r 'B38-Predict-Churn-Test-4'}
# #For glm() predict() provides probabilities 
prob_4_step_test <- predict(mod_4_step, newdata = zTest, type = "response")
#
# #Convert Probabilities into levels
res_4_step_test <- factor(ifelse(prob_4_step_test >= 0.5, "Yes", "No"), levels = c("No", "Yes"))
#
# #Confusion Matrix
cmat_4_step_test <- confusionMatrix(res_4_step_test, 
                                reference = zTest$churn, positive = "Yes")
cmat <- cmat_4_step_test
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

```{r 'B38-Predict-Churn-Test-6'}
# #For glm() predict() provides probabilities 
prob_6_step_test <- predict(mod_6_step, newdata = zTest, type = "response")
#
# #Convert Probabilities into levels
res_6_step_test <- factor(ifelse(prob_6_step_test >= 0.5, "Yes", "No"), levels = c("No", "Yes"))
#
# #Confusion Matrix
cmat_6_step_test <- confusionMatrix(res_6_step_test, 
                                reference = zTest$churn, positive = "Yes")
cmat <- cmat_6_step_test
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

## Comparison between glm() only

- Model with Low VIF perform slightly worse than the Model with all variables.

```{r 'B38-vs-Cmat', echo=FALSE, collapse=FALSE, class.output="models"}
# #Class Comparisons: Unbalanced, Balanced, Balanced & LowVIF
cmat <- cmat_2_step
hh_names <- c("Accuracy", "NIR", "N", "True Positive (TP_A)", "False Positive (FP_B)", 
              "False Negative (FN_C)", "True Negative (TN_D) ", 
              names(cmat$byClass))
iio <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
cmat <- cmat_2_step_test
iin <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
cmat <- cmat_4_step
jjo <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
cmat <- cmat_4_step_test
jjn <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
cmat <- cmat_6_step
kko <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
cmat <- cmat_6_step_test
kkn <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
#
if(TRUE) tibble(Names = hh_names, 
            Unbalanced = iio, Balanced = jjo, LowVIF = kko, tst_U = iin, tst_B = jjn, tst_L = kkn)
```

## Decision Trees: rpart

```{r 'B38-rpart-Churn'}
# #Logistic Regression : rpart
mod_7_rpart <- rpart(formula = churn ~ ., data = zBalanced, method = 'class')
```

```{r 'B38-Predict-Churn-rpart-Train'}
prob_7_rpart <- predict(mod_7_rpart, type = "prob")
#
# #Convert Probabilities into levels
res_7_rpart <- factor(ifelse(prob_7_rpart[, 2] >= 0.5, "Yes", "No"), 
                      levels = c("No", "Yes"))
#
# #Confusion Matrix
cmat_7_rpart <- confusionMatrix(res_7_rpart, 
                                reference = zBalanced$churn, positive = "Yes")
cmat <- cmat_7_rpart
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
#
if(FALSE){#Same Result
  res_70_rpart <- predict(mod_7_rpart, type = "class")
#
# #Confusion Matrix
cmat_70_rpart <- confusionMatrix(res_70_rpart, 
                                reference = zBalanced$churn, positive = "Yes")
cmat <- cmat_70_rpart
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
}
```

```{r 'B38-Predict-Churn-rpart-Test'}
prob_7_rpart_test <- predict(mod_7_rpart, newdata = zTest, type = "prob")
#
# #Convert Probabilities into levels
res_7_rpart_test <- factor(ifelse(prob_7_rpart_test[, 2] >= 0.5, "Yes", "No"), 
                           levels = c("No", "Yes"))
#
# #Confusion Matrix
cmat_7_rpart_test <- confusionMatrix(res_7_rpart_test, 
                                reference = zTest$churn, positive = "Yes")
cmat <- cmat_7_rpart_test
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```


## Decision Trees: caret

```{r 'B38-ModelControl'}
cv <- trainControl(method = "repeatedcv", number = 10, repeats = 5, allowParallel = TRUE)
```

```{r 'B38-CV-Churn'}
# #train() tuneLength decides the granularity in the tuning parameter grid
mod_8_cv <- suppressWarnings(train(churn ~ ., data = zBalanced, method = 'rpart', 
                trControl = cv, tuneLength = 10))
#
# #Best Tuning Cp value (Linear: RMSE, Logistic: Accuracy): 0.00308404	
mod_8_cv$bestTune
```

```{r 'B38-Predict-Churn-CV-Train'}
prob_8_cv <- predict(mod_8_cv, type = "prob")
res_8_cv <- predict(mod_8_cv, type = "raw")
#
# #Confusion Matrix
cmat_8_cv <- confusionMatrix(res_8_cv, 
                                reference = zBalanced$churn, positive = "Yes")
cmat <- cmat_8_cv
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

```{r 'B38-Predict-Churn-CV-Test'}
prob_8_cv_test <- predict(mod_8_cv, newdata = zTest, type = "prob")
res_8_cv_test <- predict(mod_8_cv, newdata = zTest, type = "raw")
#
# #Confusion Matrix
cmat_8_cv_test <- confusionMatrix(res_8_cv_test, 
                                reference = zTest$churn, positive = "Yes")
cmat <- cmat_8_cv_test
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

## Random Forest

```{r 'B38-Forest-Churn'}
# #Logistic Regression : randomForest
mod_9_forest <- randomForest(formula = churn ~ ., data = zBalanced)
```

```{r 'B38-Predict-Churn-Forest-Train'}
# #The fraction can be taken as predicted probabilities for the classes. 
#str(mod_9_forest$votes[40:50, ], vec.len = Inf)
#str(mod_9_forest$votes[40:50, 2], vec.len = Inf)
res_9_forest <- predict(mod_9_forest, type = "response")
#
# #Confusion Matrix
cmat_9_forest <- confusionMatrix(res_9_forest, 
                                reference = zBalanced$churn, positive = "Yes")
cmat <- cmat_9_forest
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

```{r 'B38-Predict-Churn-Forest-Test'}
res_9_forest_test <- predict(mod_9_forest, newdata = zTest, type = "response")
#
# #Confusion Matrix
cmat_9_forest_test <- confusionMatrix(res_9_forest_test, 
                                reference = zTest$churn, positive = "Yes")
cmat <- cmat_9_forest_test
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

## Random Forest: Tuned

- mtry = 4 has the lowest OOB
- ntree between 600 and 1000 does not impact the outcome in any significant manner

```{r 'B38-TuneModel', eval=FALSE}
# #Not Plotted
if(FALSE) tuneRF(x = zBalanced[, -1], y = zBalanced$churn, stepFactor = 2, plot = TRUE, 
       ntreeTry = 600, improve = .05)
```


```{r 'B38-Forest-Tuned-Churn'}
# #Logistic Regression : randomForest
mod_10_forest <- randomForest(formula = churn ~ ., data = zBalanced, mtry = 4)
```

```{r 'B38-Predict-Churn-Forest-Tuned-Train'}
# #The fraction can be taken as predicted probabilities for the classes. 
#str(mod_10_forest$votes[40:50, ], vec.len = Inf)
#str(mod_10_forest$votes[40:50, 2], vec.len = Inf)
res_10_forest <- predict(mod_10_forest, type = "response")
#
# #Confusion Matrix
cmat_10_forest <- confusionMatrix(res_10_forest, 
                                reference = zBalanced$churn, positive = "Yes")
cmat <- cmat_10_forest
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

```{r 'B38-Predict-Churn-Forest-Tuned-Test'}
res_10_forest_test <- predict(mod_10_forest, newdata = zTest, type = "response")
#
# #Confusion Matrix
cmat_10_forest_test <- confusionMatrix(res_10_forest_test, 
                                reference = zTest$churn, positive = "Yes")
cmat <- cmat_10_forest_test
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

## Comparison of Test: GLM, Rpart, Caret, Random Forest 

- Performance of Random Forest is better than all others. Further, tuned Random Forest model is better than the default.

```{r 'B38-vs-Cmat-Models', echo=FALSE, collapse=FALSE, class.output="models"}
# #Class Comparisons: GLM, Rpart, Caret, RandomForest
cmat <- cmat_4_step_test
hh_names <- c("Accuracy", "NIR", "N", "True Positive (TP_A)", "False Positive (FP_B)", 
              "False Negative (FN_C)", "True Negative (TN_D) ", 
              names(cmat$byClass))
iiglm <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
cmat <- cmat_7_rpart_test
jjrpt <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
cmat <- cmat_8_cv_test
kkcvt <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
cmat <- cmat_9_forest_test
llfrm <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
cmat <- cmat_10_forest_test
mmfrm4 <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
#
if(TRUE) tibble(Names = hh_names, 
            GLM = iiglm, Rpart = jjrpt, Caret = kkcvt, Forest = llfrm, Forest4 = mmfrm4)
```

## ROC {.tabset .tabset-fade}

### Plot {.unlisted .unnumbered}

```{r 'B38-ROC-01-Set', include=FALSE}
# #Setup for ROC Plots
hh <- roc(response = zBalanced$churn, predictor = prob_4_step, 
    levels = c("No", "Yes"), direction = "<", plot = FALSE)
lst_roc_hh <- list(
  Rpart = roc(response = zBalanced$churn, predictor = prob_7_rpart[, 2], 
    levels = c("No", "Yes"), direction = "<", plot = FALSE),
  Caret = roc(response = zBalanced$churn, predictor = prob_8_cv[, 2], 
    levels = c("No", "Yes"), direction = "<", plot = FALSE),
  Forest = roc(response = zBalanced$churn, predictor = mod_9_forest$votes[, 2], 
    levels = c("No", "Yes"), direction = "<", plot = FALSE), 
  Tuned = roc(response = zBalanced$churn, predictor = mod_10_forest$votes[, 2], 
    levels = c("No", "Yes"), direction = "<", plot = FALSE)
  )
#
lgd_hh <- c(GLM = "GLM Logistic (Stepwise)", Rpart = "Decision Trees", Caret = "Caret", 
            Forest = "Random Forest", Tuned = "Random Forest (Tuned)") 
stopifnot(identical(names(lst_roc_hh), names(lgd_hh)[-1]))
col_hh <- setNames(c("#440154FF", "#3B528BFF", "#21908CFF", "#5DC863FF", "#FDE725FF"), 
                   nm = names(lgd_hh))
auc_hh <- setNames(seq(from = 0.20, by = -0.045, length.out = length(lgd_hh)), 
                   nm = names(lgd_hh))
#
cap_hh <- "B38P01"
ttl_hh <- "Churn: ROC: Train"
loc_png <- paste0("./images/", "B38P01", "-Churn-Train-ROC", ".png")
```

```{r 'B38-ROC-01-Plot', include=FALSE, ref.label=c('B38-ROC-Multi')}
#
```

```{r 'B38P01', echo=FALSE, fig.cap="(B38P01) Churn: ROC: Train"}
loc_png <- paste0("./images/", "B38P01", "-Churn-Train-ROC", ".png")
if(file.exists(loc_png)) include_graphics(loc_png) else include_graphics(here(loc_png))
```


### Code {.unlisted .unnumbered}

```{r 'B38-ROC-01-Set-A', eval=FALSE, ref.label=c('B38-ROC-01-Set')}
#
```

```{r 'B38-ROC-Multi', eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh (ROC Plot)
if(!file.exists(here(loc_png))) {
  png(filename = here(loc_png))
  #dev.control('enable') 
  plot(hh, print.auc.y = auc_hh[["GLM"]], col = col_hh[["GLM"]], legacy.axes = TRUE, 
       print.auc.x = 0.70, print.auc = TRUE)
  plot(lst_roc_hh$Rpart, print.auc.y = auc_hh[["Rpart"]], col = col_hh[["Rpart"]], add = TRUE, 
       print.auc.x = 0.70, print.auc = TRUE)
  plot(lst_roc_hh$Caret, print.auc.y = auc_hh[["Caret"]], col = col_hh[["Caret"]], add = TRUE, 
       print.auc.x = 0.70, print.auc = TRUE)
  plot(lst_roc_hh$Forest, print.auc.y = auc_hh[["Forest"]], col = col_hh[["Forest"]], add = TRUE, 
       print.auc.x = 0.70, print.auc = TRUE)
  plot(lst_roc_hh$Tuned, print.auc.y = auc_hh[["Tuned"]], col = col_hh[["Tuned"]], add = TRUE, 
       print.auc.x = 0.70, print.auc = TRUE)
  legend("bottomright", bg ="transparent", legend=lgd_hh, col=col_hh, lwd = 1)
  title(main = ttl_hh, line = 3, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B38 <- recordPlot()
  dev.off()
  assign(cap_hh, B38)
  rm(B38)
  #eval(parse(text = cap_hh))
}
```


## Data: Earnings

### Import

- \textcolor{pink}{Earnings Prediction} [1239, 11]
  - \textcolor{pink}{Please import the "B38-Predicting-Earnings.xlsx"}
  - 'Manipulater' is the Response Variable
    - Converted from character to factor and relocated to the start
  - Dropped 1 variable: C-MANIPULATOR
  - 'Company ID' has been kept because it helps in identifying unique or duplicated observations after data balancing
  - All predictors are numeric so no need to create dummy variables
  - However, data balancing is required


```{r 'B38-Load-Data-Earnings', include=FALSE, eval=FALSE}
# #Path to the Excel File #read_delim(clipboard())
loc_src <- here("data", "B38-Predicting-Earnings.xlsx")
if(FALSE) excel_sheets(loc_src)
# #Read Sheets
#zdata <- read_excel(path = loc_src, sheet = 4, range = "A1:K1240")
xxB38Earnings <- read_excel(path = loc_src, sheet = 4) #Works in Console. Not in Chunk. Works in Knit.
```

```{r 'B38-Cleanup-Earnings', eval=FALSE}
#Logical to Factor Y | Rename | Coerce to int | Drop | Relocate Y |
if(FALSE) xfw <- xxB38Earnings |> 
  mutate(across(Manipulater, factor, levels = c("No", "Yes"))) |> 
  rename(ID = "Company ID") |> 
  mutate(across(ID, as.integer)) |> 
  select(-c("C-MANIPULATOR")) |> 
  relocate(Manipulater)
```

```{r 'B38-Save-RDS-Earnings', include=FALSE, eval=FALSE}
if(FALSE) {
  loc_rds <- here("data", "zzB38Earnings.rds")
  # #Write Object as RDS
  saveRDS(xfw, loc_rds)  
}
```

```{r 'B38-Get-RDS-Earnings', include=FALSE}
loc_rds <- here("data", "zzB38Earnings.rds")
# #Read Object
zzB38Earnings <- xfw <- readRDS(loc_rds)
```

```{r 'B38-Structure-Earnings'}
str(xfw)
```

```{r 'B38-Summary-Earnings'}
summary(xfw)
```

### Data Partition

```{r 'B38-Partition-Earnings'}
# #Partition Data
set.seed(3)
idx_xfw <- createDataPartition(xfw$Manipulater, p = 0.8, list = FALSE)
zTrn <- xfw[ idx_xfw[, 1], ] 
zTst <- xfw[-idx_xfw[, 1], ]
```

### Data Balancing 

```{r 'B38-Balancing-Earnings'}
# #Data Balancing with 50% balance
both_train <- ovun.sample(Manipulater ~ ., data = zTrn, method = "both", seed = 3)$data
zBalanced <- as_tibble(both_train) 
#
zBalanced |> count(Manipulater) |> mutate(PROP = n /sum(n))
```


### Logistic Regression

```{r 'B38-GLM-Earnings-1', collapse=FALSE, class.output="redwarn"}
# #Logistic Regression: Balanced data
tryCatch(mod_1_glm <- glm(formula = Manipulater ~ . -ID, family = binomial, data = zBalanced), 
        warning = function(w) {
          print(paste0(w))
          suppressWarnings(#Double Arrow Assignment
            mod_1_glm <<- glm(formula = Manipulater ~ . -ID, family = binomial, data = zBalanced))
          }
        )
```


- \textcolor{orange}{Warning:} glm.fit: fitted probabilities numerically 0 or 1 occurred
  - The warning will arise whenever a predicted probability is effectively indistinguishable from 1.
  - No clear separation was apparent between Yes, No for Grouped Min & Max values
  - 3 observations are causing the issue (ID = 2, 22, 26). They all have extreme values of one or more covariates. 
    - ID 26: Max GMI 46.5, Min LEVI 0
    - ID 2, 22: Max SGI 13.1
    - These 3 observations were moved to Test from Train
  - However, there are lots of other observations that are similarly close to 1. There are some observations with high leverage, but none of those are problematic.
  - Excluding these 3 observations and re-calculating the model coefficients
    - None of the coefficients changed by more than 10^-6! So essentially unchanged results. 
    - Probably, this warning is a "false positive", nothing to worry about. 
    - This warning arises with complete separation, but in that case we would expect to see the coefficient for one or more covariates get very large, with a standard error that is even larger. 
    - That is not occurring here, and we have seen that the Yes and No occur across overlapping ranges of all covariates. 
    - So the warning occurs because a few observations have very extreme values of the covariates. That could be a problem if those observations were also highly influential. But they are not.


```{r 'B38-Separation-Earnings'}
# #Check if there is any clear separation between Yes, No for Grouped Min & Max values
zBalanced |> 
  group_by(Manipulater) |> 
  summarise(across(everything(), list(MIN = min, MAX = max), .names = "{.fn}_{.col}")) |> 
  mutate(across(where(is.numeric), round, 3)) |> 
  t()
#
# #Source Code of glm.fit() provide the reason for the warning.
eps <- 10 * .Machine$double.eps
res_1_glm <- augment(mod_1_glm) |> 
  mutate(p = 1 / (1 + exp(-.fitted)),
         warning = p > 1-eps) |> 
  select(-c(".resid", ".std.resid", ".sigma", ".cooksd")) |> 
  rename(Y = 1) 

# #Problem is with following 3 unique observations
res_1_glm |> arrange(desc(.fitted)) |> filter(warning == "TRUE") |> distinct()

zRemoved <- zBalanced |> filter( GMI > 46 | SGI > 13 |LEVI <= 0) |> distinct()

# #Observations with high leverage are not a problem
res_1_glm |> arrange(desc(.hat)) |> distinct()
```

```{r 'GLM-Filtered'}
# #Exclude Observations from Train
zFiltered <- zBalanced |> filter(!(GMI > 46 | SGI > 13 |LEVI <= 0))
#
# #Add the excluded rows from Train back into the Test
zAppended <- bind_rows(zRemoved, zTst)
```

```{r 'B38-GLM-Earnings-2'}
# #Logistic Regression: Filtered data
mod_2_glm <- glm(formula = Manipulater ~ . -ID, family = binomial, data = zFiltered)

# #Comparison of Coefficients: Max. Difference is lower than 10^-6
stopifnot(10^-6 > max(tidy(mod_2_glm)$estimate - tidy(mod_1_glm)$estimate))
```


```{r 'B38-Step-Earnings-2', warning=TRUE}
# #Step-wise Regression: It also triggers same warning and there is no benefit
mod_3_step <- step(mod_2_glm, direction = "both", trace = 0)
```

```{r 'B38-Separation-Earnings-2'}
# #Source Code of glm.fit() provide the reason for the warning.
eps <- 10 * .Machine$double.eps
res_3_step <- augment(mod_3_step) |> 
  mutate(p = 1 / (1 + exp(-.fitted)),
         warning = p > 1-eps) |> 
  select(-c(".resid", ".std.resid", ".sigma", ".cooksd")) |> 
  rename(Y = 1) 

# #Although there are warnings in the code, actually no warning is found
res_3_step |> arrange(desc(.fitted)) |> filter(warning == "TRUE") |> distinct()
```


```{r 'B38-Print-Earnings', echo=FALSE, collapse=FALSE, class.output="models"}
# #Coefficient Estimates, Significance, DF
ttl_hh <- "Logistic Regression: Earning: Y ~ X"
col_hh <- c("Balanced", "Filtered", "F.Stepwise")
#
stargazer(mod_1_glm, mod_2_glm, mod_3_step,
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```

### VIF

```{r 'B38-VIF-Earnings'}
# #Some predictors have small multicollinearity
vif(mod_2_glm) |> {\(x) x[x > 2]}()
```


### Prediction on Train 

```{r 'B38-Pred-Trn-GLM-Earn'}
# #For glm() predict() provides probabilities 
prob_2_glm <- predict(mod_2_glm, type = "response")

# #Convert Probabilities into levels
res_2_glm <- factor(ifelse(prob_2_glm >= 0.5, "Yes", "No"), levels = c("No", "Yes"))

# #Confusion Matrix
cmat_2_glm <- confusionMatrix(res_2_glm, 
                                reference = zFiltered$Manipulater, positive = "Yes")
cmat <- cmat_2_glm
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

### Confusion Matrix

```{r 'B38-Cmat-Trn-GLM-Earn'}
# #Confusion Matrix
cmat <- cmat_2_glm
if(TRUE) cmat$table |> as_tibble() |> 
  pivot_wider(names_from = Reference, values_from = n) |> 
  rename(Prediction_Reference = 1) |> 
  {\(x) add_row(x, summarise(x, across(1, ~"Total")), 
                summarise(x, across(where(is.numeric), sum)))}() |> 
  mutate(SUM = rowSums(across(where(is.numeric))))
```

### Prediction on Test 

```{r 'B38-Pred-Tst-GLM-Earn'}
# #For glm() predict() provides probabilities 
prob_2_glm_test <- predict(mod_2_glm, newdata = zAppended, type = "response")

# #Convert Probabilities into levels
res_2_glm_test <- factor(ifelse(prob_2_glm_test >= 0.5, "Yes", "No"), levels = c("No", "Yes"))

# #Confusion Matrix
cmat_2_glm_test <- confusionMatrix(res_2_glm_test, 
                                reference = zAppended$Manipulater, positive = "Yes")
cmat <- cmat_2_glm_test
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```


### Decision Trees: rpart

```{r 'B38-Rpart-Earn'}
# #Logistic Regression : rpart
mod_7_rpart <- rpart(formula = Manipulater ~ . -ID, data = zFiltered, method = 'class')
```

```{r 'B38-Pred-Trn-Rpart-Earn'}
prob_7_rpart <- predict(mod_7_rpart, type = "prob")
#
# #Convert Probabilities into levels
res_7_rpart <- factor(ifelse(prob_7_rpart[, 2] >= 0.5, "Yes", "No"), 
                      levels = c("No", "Yes"))
#
# #Confusion Matrix
cmat_7_rpart <- confusionMatrix(res_7_rpart, 
                                reference = zFiltered$Manipulater, positive = "Yes")
cmat <- cmat_7_rpart
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

```{r 'B38-Pred-Tst-Rpart-Earn'}
prob_7_rpart_test <- predict(mod_7_rpart, newdata = zAppended, type = "prob")
#
# #Convert Probabilities into levels
res_7_rpart_test <- factor(ifelse(prob_7_rpart_test[, 2] >= 0.5, "Yes", "No"), 
                           levels = c("No", "Yes"))
#
# #Confusion Matrix
cmat_7_rpart_test <- confusionMatrix(res_7_rpart_test, 
                                reference = zAppended$Manipulater, positive = "Yes")
cmat <- cmat_7_rpart_test
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```


### Decision Trees: caret

```{r 'B38-ModelControl-2'}
cv <- trainControl(method = "repeatedcv", number = 10, repeats = 5, allowParallel = TRUE)
```

```{r 'B38-CV-Earn'}
# #train() tuneLength decides the granularity in the tuning parameter grid
mod_8_cv <- train(Manipulater ~ . -ID, data = zFiltered, method = 'rpart', 
                trControl = cv, tuneLength = 10)
#
# #Best Tuning Cp value (Linear: RMSE, Logistic: Accuracy): 0 (i.e. do not prune the Tree)
mod_8_cv$bestTune
```

```{r 'B38-Pred-Trn-CV-Earn'}
prob_8_cv <- predict(mod_8_cv, type = "prob")
res_8_cv <- predict(mod_8_cv, type = "raw")
#
# #Confusion Matrix
cmat_8_cv <- confusionMatrix(res_8_cv, 
                                reference = zFiltered$Manipulater, positive = "Yes")
cmat <- cmat_8_cv
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

```{r 'B38-Pred-Tst-CV-Earn'}
prob_8_cv_test <- predict(mod_8_cv, newdata = zAppended, type = "prob")
res_8_cv_test <- predict(mod_8_cv, newdata = zAppended, type = "raw")
#
# #Confusion Matrix
cmat_8_cv_test <- confusionMatrix(res_8_cv_test, 
                                reference = zAppended$Manipulater, positive = "Yes")
cmat <- cmat_8_cv_test
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```


### Random Forest

```{r 'B38-Forest-Earn'}
# #Logistic Regression : randomForest
mod_9_forest <- randomForest(formula = Manipulater ~ . -ID, data = zFiltered)
```

```{r 'B38-Pred-Trn-Forest-Earn'}
# #The fraction can be taken as predicted probabilities for the classes. 
#str(mod_9_forest$votes[40:50, ], vec.len = Inf)
#str(mod_9_forest$votes[40:50, 2], vec.len = Inf)
res_9_forest <- predict(mod_9_forest, type = "response")
#
# #Confusion Matrix
cmat_9_forest <- confusionMatrix(res_9_forest, 
                                reference = zFiltered$Manipulater, positive = "Yes")
cmat <- cmat_9_forest
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

```{r 'B38-Pred-Tst-Forest-Earn'}
res_9_forest_test <- predict(mod_9_forest, newdata = zAppended, type = "response")
#
# #Confusion Matrix
cmat_9_forest_test <- confusionMatrix(res_9_forest_test, 
                                reference = zAppended$Manipulater, positive = "Yes")
cmat <- cmat_9_forest_test
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

### Random Forest: Tuned

- mtry = 1 has the lowest OOB
  - Its value 1 suggests that most of the predictors are inherently correlated /interconnected
- ntree is not changing anything much so have used 500
- stepFactor should not be less than 1
- lower improve is better

```{r 'B38-TuneModel-2', eval=FALSE}
# #Not Plotted
if(FALSE) tuneRF(x = zFiltered[, -c(1:2)], y = zFiltered$Manipulater, stepFactor = 2, plot = TRUE, 
       ntreeTry = 500, improve = 0.00001, trace = FALSE)
```


```{r 'B38-TForest-Earn'}
# #Logistic Regression : randomForest
mod_10_forest <- randomForest(formula = Manipulater ~ . -ID, data = zFiltered, mtry = 1)
```

```{r 'B38-Pred-Trn-TForest-Earn'}
# #The fraction can be taken as predicted probabilities for the classes. 
#str(mod_10_forest$votes[40:50, ], vec.len = Inf)
#str(mod_10_forest$votes[40:50, 2], vec.len = Inf)
res_10_forest <- predict(mod_10_forest, type = "response")
#
# #Confusion Matrix
cmat_10_forest <- confusionMatrix(res_10_forest, 
                                reference = zFiltered$Manipulater, positive = "Yes")
cmat <- cmat_10_forest
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```

```{r 'B38-Pred-Tst-TForest-Earn'}
res_10_forest_test <- predict(mod_10_forest, newdata = zAppended, type = "response")
#
# #Confusion Matrix
cmat_10_forest_test <- confusionMatrix(res_10_forest_test, 
                                reference = zAppended$Manipulater, positive = "Yes")
cmat <- cmat_10_forest_test
cmat$table
if(TRUE) cmat$overall[["Accuracy"]]
if(TRUE) cmat$byClass[["Sensitivity"]]
if(TRUE) cmat$byClass[["Specificity"]]
```


### Comparison of Test: GLM, Rpart, Caret, Random Forest 

- Model Performance is lower than NIR. GLM performs better than others
- Random Forest has good performance in identifying companies with "No". We are interested in "Yes" where it fails.

```{r 'B38-vs-Cmat-Earn', echo=FALSE, collapse=FALSE, class.output="models"}
# #Class Comparisons: GLM, Rpart, Caret, RandomForest
cmat <- cmat_2_glm_test
hh_names <- c("Accuracy", "NIR", "N", "True Positive (TP_A)", "False Positive (FP_B)", 
              "False Negative (FN_C)", "True Negative (TN_D) ", 
              names(cmat$byClass))
iiglm <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
cmat <- cmat_7_rpart_test
jjrpt <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
cmat <- cmat_8_cv_test
kkcvt <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
cmat <- cmat_9_forest_test
llfrm <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
cmat <- cmat_10_forest_test
mmfrm1 <- c(cmat$overall[["Accuracy"]], cmat$overall[["AccuracyNull"]], sum(cmat$table), 
        cmat$table["Yes", "Yes"], cmat$table["Yes", "No"], 
        cmat$table["No", "Yes"], cmat$table["No", "No"], round(cmat$byClass, 4))
#
if(TRUE) tibble(Names = hh_names, 
            GLM = iiglm, Rpart = jjrpt, Caret = kkcvt, Forest = llfrm, Forest1 = mmfrm1)
```



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B38-Cleanup', include=FALSE, cache=FALSE}
if(TRUE) f_rmExist(aa, bb, ii, jj, kk, ll, auc_hh, both_churn, cap_hh, cmat, cmat_10_forest, 
          cmat_10_forest_test, cmat_2_step, cmat_2_step_test, cmat_4_step, cmat_4_step_test, 
          cmat_6_step, cmat_6_step_test, cmat_7_rpart, cmat_7_rpart_test, cmat_8_cv, 
          cmat_8_cv_test, cmat_9_forest, cmat_9_forest_test, col_hh, cv, dum_xfw, hh, hh_names, 
          idx_xsyw, iiglm, iin, iio, jjn, jjo, jjrpt, kkcvt, kkn, kko, lgd_hh, llfrm, loc_png, 
          lst_roc_hh, mmfrm4, mod_1_glm, mod_10_forest, mod_2_step, mod_3_glm, mod_4_step, 
          mod_5_glm, mod_6_step, mod_7_rpart, mod_8_cv, mod_9_forest, prob_2_step, 
          prob_2_step_test, prob_4_step, prob_4_step_test, prob_6_step, prob_6_step_test, 
          prob_7_rpart, prob_7_rpart_test, prob_8_cv, prob_8_cv_test, res_10_forest, 
          res_10_forest_test, res_2_step, res_2_step_test, res_4_step, res_4_step_test, 
          res_6_step, res_6_step_test, res_7_rpart, res_7_rpart_test, res_8_cv, res_8_cv_test,
          res_9_forest, res_9_forest_test, ttl_hh, xfw, xxB18Churn, xxB38Churn, zBalanced, zChurn, 
          zTest, both_train, cmat_2_glm, cmat_2_glm_test, eps, idx_xfw, loc_rds, mmfrm1, 
          mod_2_glm, mod_3_step, prob_2_glm, prob_2_glm_test, res_1_glm, res_2_glm, 
          res_2_glm_test, res_3_step, zAppended, zFiltered, zRemoved, zTrn, zTst, zzB38Earnings)
```

```{r 'B38-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
