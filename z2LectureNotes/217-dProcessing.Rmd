# Data Preprocessing (B17, Oct-31) {#b17}

```{r 'B17', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
invisible(lapply(f_getPathR(A10getUtil), knitr::read_chunk))
```

## Data

\textcolor{pink}{Please import the "B16-Cars2.csv".}

```{r 'B17-ImportData', include=FALSE}
xxB16Cars <- f_getRDS(xxB16Cars)
bb <- aa <- xxB16Cars
```

## Z-score Standardisation {.tabset .tabset-fade}

```{r 'B17D01', comment="", echo=FALSE, results='asis'}
f_getDef("z-Scores")
```

- \@ref(eq:z-scores) $z_i = \frac{{x}_i - {\overline{x}}}{{s}}$
  - $z_i \notin \pm 3 \to z_i \in \text{Outlier}$ 
  - After Standardisation, we can compare values of different orders because these would be scaled into a dimensionless quantity
  - While comparing the Eucledean distance between Two Variables like Age and Income, the standardisation allow these to be scaled to a similar range
    - Ex: If Age range is 20-30 but Income range is 10k-30k, directly using these values will ignore any impact of change in Age.
  - For some data mining algorithms, large differences in the ranges will lead to a tendency for the variable with greater range to have undue influence on the results. Therefore, these numeric variables should be normalised, in order to standardize the scale of effect each variable has on the results. 
  - Scaling also benefits Neural networks, and algorithms that make use of distance measures, such as the k-nearest neighbors algorithm. 
  - Note that Mean itself includes the impact of extreme values thus it is not very robust. IQR is better because it is based on position
  - Standardisation does not convert the non-normal data to normal. It does not change the shape of the data. Outliers remain outliers, Skewness remain in the data. It simply changes the Scale.
- \textcolor{pink}{Question:} Can we use different methods for outlier identification on different variables
  - Yes, you can remove outliers of one variable usig IQR and of another variable using standardisation
- \textcolor{pink}{Question:} From the Normalised values, can be convert back to original data
  - Tiring process
  - (Aside) The scales() function attaches mean and sd as attributes to the output matrix. That can be used to convert back the data.

- \textcolor{pink}{scale()}
  - It converts any vector to standard normal
- (Aside) 
  - Scaling is less effective if the outliers are present. Extremely low value (e.g. 192.5 Weight) or  extremely high value (527 Mileage) are obviously going to impact the scale applied. Probably, it is better to follow: Scaling | Outlier Treatment (Identification, Removal or Imputation) | Scaling of original data with Outlier Treatment


### Normalisation {.unlisted .unnumbered}

```{r 'B17-Normalisation'}
# #Normalising Weight
bb <- aa %>% select(weightlbs) %>% mutate(z = as.vector(scale(weightlbs)))
str(bb)
#
# #Excluding Outliers
kept_bb <- bb[bb$z >= -3 & bb$z <= 3, ]
str(kept_bb)
#
# #Similarly with mpg
kept_bb <- aa %>% select(mpg) %>% mutate(z = as.vector(scale(mpg))) %>% filter(z >= -3 & z <= 3)
str(kept_bb)
summary(kept_bb)
```

### scale() {.unlisted .unnumbered}

```{r 'B17-Scale'}
# #scale(x, center = TRUE, scale = TRUE) output is Nx1 Matrix
bb <- aa %>% select(weightlbs) 
ii <- bb %>% mutate(z = as.vector(scale(weightlbs)))
#bb %>% mutate(z = across(weightlbs, scale)) #matrix
#bb %>% mutate(z = across(weightlbs, ~ as.vector(scale(.)))) #tibble
jj <- bb %>% mutate(across(weightlbs, list(z = ~ as.vector(scale(.))), .names = "{.fn}"))
kk <- bb
kk$z <- as.vector(scale(kk$weightlbs))
stopifnot(all(identical(ii, jj), identical(ii, kk)))
```

## Min-Max Scaling 

- $x^* = \frac{{x}_i - \text{min}(x)}{\text{range}(x)} = \frac{{x}_i - \text{min}(x)}{\text{max}(x) - \text{min}(x)} \to x^* \in [0, 1]$
  - This is for scaling only, not for removal of outliers

```{r 'B17-ScalingMinMax'}
# #Min-Max Scaling
min_aa <- min(aa$weightlbs)
max_aa <- max(aa$weightlbs)
bb <- aa %>% select(weightlbs) %>% mutate(z = {weightlbs - min_aa}/{max_aa - min_aa})
str(bb)
```

## Decimal Scaling 

- $x^* = \frac{{x}_i}{10^d} \to x^* \in [-1, 1]$
  - Where 'd' represents the number of digits in the largest absolute value i.e. if max(abs(x)) is 6997, d will be 4


```{r 'B17-ScalingDecimal'}
# #Count Digits in Maximum (NOTE: Take care of NA, 0, [-1, 1] values)
d_bb <- 10^{floor(log10(max(abs(aa$weightlbs)))) + 1}
# #Decimal Scaling
bb <- aa %>% select(weightlbs) %>% mutate(z = weightlbs/d_bb)
str(bb)
```

## Comparison {.tabset .tabset-fade}

### Histogram {.unlisted .unnumbered}

```{r 'B17-Histogram', include=FALSE}
# #Histogram
bb <- aa %>% select(weightlbs) %>% mutate(z = as.vector(scale(weightlbs)))
hh <- tibble(ee = bb$z)
ttl_hh <- "Cars: Histogram of Weight (Scaled)"
cap_hh <- "B17P01"
# #Basics
median_hh <- round(median(hh[[1]]), 1)
mean_hh <- round(mean(hh[[1]]), 1)
sd_hh <- round(sd(hh[[1]]), 1)
len_hh <- nrow(hh)
#
B17 <- hh %>% { ggplot(data = ., mapping = aes(x = ee)) + 
  geom_histogram(bins = 50, alpha = 0.4, fill = '#FDE725FF') + 
  geom_vline(aes(xintercept = mean_hh), color = '#440154FF') +
  geom_text(data = tibble(x = mean_hh, y = -Inf, 
                          label = paste0("Mean= ", mean_hh)), 
            aes(x = x, y = y, label = label), 
            color = '#440154FF', hjust = -0.5, vjust = 1.3, angle = 90) +
  geom_vline(aes(xintercept = median_hh), color = '#3B528BFF') +
  geom_text(data = tibble(x = median_hh, y = -Inf, 
                          label = paste0("Median= ", median_hh)), 
            aes(x = x, y = y, label = label), 
            color = '#3B528BFF', hjust = -0.5, vjust = -0.7, angle = 90) +
  theme(plot.title.position = "panel") + 
  labs(x = "x", y = "Frequency", 
       subtitle = paste0("(N=", len_hh, "; ", "Mean= ", mean_hh, 
                         "; Median= ", median_hh, "; SD= ", sd_hh,
                         ")"), 
        caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B17)
rm(B17)
```

```{r 'B17P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B17P01","-Cars-Hist-Weight-Scaled", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B17P01, device = "png", dpi = 144) 
}
```

```{r 'B17P01', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B17P01","-Cars-Hist-Weight-Scaled", ".png"))
```

```{r 'B17P0102', echo=FALSE, ref.label=c('B16P02', 'B17P01'), fig.cap="Cars: Histogram of Weight  (Original vs Scaled)"}
# #Ref another file chunk
```

### hist() {.unlisted .unnumbered}

```{r 'B17-PlotHist', eval=FALSE}
par(mfrow = c(1,2))
# Create two histograms
hist(bb$weightlbs, breaks = 20,
     xlim = c(1000, 5000),
     main = "Histogram of Weight",
     xlab = "Weight",
     ylab = "Counts")
box(which = "plot",
    lty = "solid",
    col = "black")
#
hist(bb$z,
     breaks = 20,
     xlim = c(-2, 3),
     main = "Histogram of Zscore
of Weight",
     xlab = "Z-score of Weight",
     ylab = "Counts")
box(which = "plot",
    lty = "solid",
    col = "black")
```

## Skewness

- Refer [Skewness](#skewness-c03 "c03")
- Scaling does not change the skewness

```{r 'B17-Skewness'}
# #Skewness
bb <- aa %>% select(weightlbs) %>% mutate(z = as.vector(scale(weightlbs)))
ii <- bb$weightlbs
#
3 * {mean(ii) - median(ii)} / sd(ii)
#
ii <- bb$z
3 * {mean(ii) - median(ii)} / sd(ii)
```

## Non-linear Transformations {.tabset .tabset-fade}

- It is done for conversion of non-normal data to normal
  - Note that scaling is linear transformation
- Transformations
  - Sqare Root - \textcolor{pink}{sqrt()}
  - Log - \textcolor{pink}{log(), log10()}
  - Inverse Sqare Root
- 

```{r 'B17-Transformations'}
bb <- aa %>% select(weightlbs) %>% 
  mutate(z = as.vector(scale(weightlbs)), Sqrt = sqrt(weightlbs),
         Log = log(weightlbs), InvSqr = 1/Sqrt)
#
# #Check Skewness
vapply(bb, function(x) round(3 * {mean(x) - median(x)} / sd(x), 3), numeric(1))
```

### Histogram {.unlisted .unnumbered}

```{r 'B17-FacetWrap', include=FALSE}
# #Histogram
bb <- aa %>% select(weightlbs) %>% 
  mutate(z = as.vector(scale(weightlbs)), Sqrt = sqrt(weightlbs), 
         Log = log(weightlbs), InvSqr = 1/Sqrt) %>% 
  pivot_longer(everything(), names_to = "Key", values_to = "Values") %>% 
  mutate(across(Key, factor, levels = c("Sqrt", "Log", "InvSqr", "weightlbs", "z"), 
    labels = c("Square Root", "Natural Log", "Inverse Square", 
               "Original Weight", "Scaled Weight")))
#
hh <- bb
mean_hh <- hh %>% group_by(Key) %>% summarize(Mean = mean(Values))
#
ttl_hh <- "Cars: Weight with Transformed values and Mean"
cap_hh <- "B17P03"
#
B17 <- hh %>% { ggplot(data = ., mapping = aes(Values)) + 
    geom_histogram(bins = 50, alpha = 0.4, fill = '#FDE725FF') + 
    geom_vline(data = mean_hh, aes(xintercept = Mean), color = '#440154FF') +
	  geom_text(data = mean_hh, aes(x = Mean, y = -Inf, label = paste0("Mean= ", f_pNum(Mean))), 
            color = '#440154FF', hjust = -0.5, vjust = 1.3, angle = 90) +
    facet_wrap(~Key, scales = 'free_x') +
    theme(plot.title.position = "panel") + 
    labs(x = "x", y = "Frequency", caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B17)
rm(B17)
```

```{r 'B17P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B17P03","-Cars-Weight-Transform", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B17P03, device = "png", dpi = 144) 
}
```

```{r 'B17P03', echo=FALSE, out.width="100%", fig.cap="Cars: Weight Transformed with Original & Scaled"}
knitr::include_graphics(paste0(.z$PX, "B17P03","-Cars-Weight-Transform", ".png"))
```

### facet_wrap() {.unlisted .unnumbered}

```{r 'B17-FacetWrap-A', ref.label=c('B17-FacetWrap'), eval=FALSE}
#
```

### f_pNum() {.unlisted .unnumbered}

```{r 'B17-f_pNum', ref.label=c('A10B-pNum'), eval=FALSE}
#
```

### hist() {.unlisted .unnumbered}

```{r 'B17-PlotHistInvSqare', eval=FALSE}
# #Histogram with Normal Distribution Overlay
par(mfrow=c(1,1))
hist(bb$InvSqr,
     breaks = 30,
     xlim=c(0.0125, 0.0275),
     col = "lightblue",
     prob = TRUE,
     border = "black",
     xlab="Inverse Square Root of Weight",
     ylab = "Counts",
     main = "Histogram of Inverse Square Root of Weight")
box(which = "plot",
    lty = "solid",
    col="black")
# #Overlay Normal density
lines(density(bb$InvSqr), col="red")
```


## QQ Plot {.tabset .tabset-fade}

Q–Q (quantile-quantile) plot is a probability plot for comparing two probability distributions by plotting their quantiles against each other. A point $(x, y)$ on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). If the two distributions being compared are similar, the points in the Q–Q plot will approximately lie on the line $y = x$.

- Refer figure \@ref(fig:B12P09)
  - The QQ Plot shows whether the values are within the specified limits of Normal Curve

### Image {.unlisted .unnumbered}

```{r 'B17-WeightQQ', include=FALSE}
# #QQ Plot
bb <- aa %>% select(weightlbs) %>% 
  filter(weightlbs > min(weightlbs)) %>% 
  mutate(z = as.vector(scale(weightlbs)), Sqrt = sqrt(weightlbs), 
         Log = log(weightlbs), InvSqr = 1/Sqrt) %>% 
  pivot_longer(everything(), names_to = "Key", values_to = "Values") %>% 
  mutate(across(Key, factor, levels = c("Sqrt", "Log", "InvSqr", "weightlbs", "z"), 
    labels = c("Square Root", "Natural Log", "Inverse Square", 
               "Original Weight", "Scaled Weight")))
#
hh <- bb
#hh %>% group_by(Key) %>% summarize(Max = max(Values), Min = min(Values))
max_hh <- min_hh <- hh %>% group_by(Key) %>% summarise(Values = 0)
#
# #Modify Number of Y-Axis Major Gridlines for Horizontal Comparison
max_hh$Values  <- c(100, 8.55, 0.0300, 5000, 2.35) #c(72, 8.55, 0.0255, 5000, 2.35)
min_hh$Values  <- c(20, 7.35, 0.0135, 1500, -1.65) #c(40, 7.35, 0.0135, 1500, -1.65)
#
ttl_hh <- "QQ Plots of Transformed Weight"
sub_hh <- "Excluded 1 Outlier and Modified Y-axis for alignment"
cap_hh <- "B17P04"
#
B17 <- hh %>% { ggplot(., aes(sample = Values)) +
    stat_qq() +
    stat_qq_line() +
    geom_blank(data=max_hh, aes(y = Values)) +
    geom_blank(data=min_hh, aes(y = Values)) +
    facet_wrap(~Key, scales = 'free') +
    scale_x_continuous(limits = c(-3, 3)) + 
    #coord_flip() +
    labs(caption = cap_hh, subtitle = sub_hh, title = ttl_hh)
}
assign(cap_hh, B17)
rm(B17)
```

```{r 'B17P04-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B17P04","-Cars-Weight-QQ", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B17P04, device = "png", dpi = 144, width = k_width, height = k_height) 
}
```

```{r 'B17P04', echo=FALSE, out.width="100%", fig.cap="Cars: QQ Plots of Transformed Weight"}
knitr::include_graphics(paste0(.z$PX, "B17P04","-Cars-Weight-QQ", ".png"))
```

### Flipped Axis {.unlisted .unnumbered}

```{r 'B17-WeightQQ-Flip', include=FALSE}
ttl_hh <- "QQ Plots of Transformed Weight (Flipped)"
cap_hh <- "B17P05"
B17P05 <- B17P04 + coord_flip() + labs(caption = cap_hh, title = ttl_hh)
```

```{r 'B17P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B17P05","-Cars-Weight-QQ-Flip", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B17P05, device = "png", dpi = 144, width = k_width, height = k_height) 
}
```

```{r 'B17P05', echo=FALSE, out.width="100%", fig.cap="Cars: QQ Plots of Transformed Weight"}
knitr::include_graphics(paste0(.z$PX, "B17P05","-Cars-Weight-QQ-Flip", ".png")) #iiii
```

### Code {.unlisted .unnumbered}

```{r 'B17-WeightQQ-A', eval=FALSE, ref.label=c('B17-WeightQQ')}
#
```

### qqnorm() {.unlisted .unnumbered}

```{r 'B17-PlotQQ', eval=FALSE}
# Normal Q-Q Plot
qqnorm(bb$InvSqr,
       datax = TRUE,
       col = "red",
       ylim = c(0.01, 0.03),
       main = "Normal
Q-Q Plot of Inverse Square Root of Weight")
qqline(bb$InvSqr,
       col = "blue",
       datax = TRUE)
```

## Shapiro

```{r 'B17D02', comment="", echo=FALSE, results='asis'}
f_getDef("Shapiro-Wilk-Test") #dddd
```


- Refer [Shapiro-Wilk Test for Normality](#shapiro-c10 "c10")


### rnorm()

- \textcolor{pink}{rnorm(n, mean = 0, sd = 1)}
  - Create Vector of Random Numbers with given mean and sd

```{r 'B17-rnorm'}
set.seed(3)
ii <- rnorm(n = 100, mean = 50, sd = 5.99)
#
# #Check Normality of randomly generated Normal dataset
shapiro.test(ii)
#
# #Check Normality of Weight
ii <- aa %>% select(weightlbs) %>% 
  #filter(weightlbs > min(weightlbs)) %>%
  mutate(z = as.vector(scale(weightlbs)), Sqrt = sqrt(weightlbs), 
         Log = log(weightlbs), InvSqr = 1/Sqrt) %>% 
  pivot_longer(everything(), names_to = "Key", values_to = "Values") %>% 
  mutate(across(Key, factor, levels = c("Sqrt", "Log", "InvSqr", "weightlbs", "z"), 
    labels = c("Square Root", "Natural Log", "Inverse Square", 
               "Original Weight", "Scaled Weight")))
#
# #No Transformation was able to convert the data to Normality 
# #Even after excluding 1 outlier (Not shown here)
ii %>% group_by(Key) %>% 
  summarise(p_Shapiro = shapiro.test(Values)$p.value, 
            isNormal = ifelse(p_Shapiro > 0.05, TRUE, FALSE))
```

### cut()

- Refer [cut() for creating bins](#cut-c02 "c02")
- \textcolor{pink}{cut()} - It slightly increases the range

```{r 'B17-Cut'}
# #Continuous to Categorical (Bins)
cut_ii <- cut(aa$weightlbs, breaks = 3, dig.lab = 4, include.lowest = TRUE, ordered_result = TRUE)
levels(cut_ii)
#
# #ggplot2::cut_interval()
cut_jj <- cut_interval(aa$weightlbs, n = 3, dig.lab = 4, ordered_result = TRUE)
levels(cut_jj)
#
# #With Labels: NOTE default ordering is ascending
levels(cut(aa$weightlbs, breaks = 3, dig.lab = 4, include.lowest = TRUE, ordered_result = TRUE, 
           labels = c("low", "medium", "high")))
levels(cut_interval(aa$weightlbs, n = 3, dig.lab = 4, ordered_result = TRUE, 
                    labels = c("low", "medium", "high")))
```

## Continuos to Categorical Groups

```{r 'B17-ToCategorical'}
bb <- aa %>% select(weightlbs) %>% rename(Weight = 1)
#
# #Subsetting
# #Create Column explicitly to prevent Warning message: Unknown or uninitialised column: `ii`. 
bb$ii <- NA
bb$ii[bb$Weight >= 3000] <- 1
bb$ii[bb$Weight < 3000] <- 2
#
# #Using ifelse() or case_when()
bb <- bb %>% mutate(jj = ifelse(Weight >= 3000, 1, 2), 
                    kk = case_when(Weight >= 3000 ~ 1, Weight < 3000 ~ 2))
stopifnot(all(identical(bb$ii, bb$jj), identical(bb$ii, bb$kk)))
```

## Index

```{r 'B17-Index'}
# #Create Data
set.seed(3)
bb <- tibble(x = rnorm(n = 10, mean = 5, sd = 0.55), 
             y = rnorm(n = 10, mean = 4.5, sd = 0.66))
#
# #Basic Indexing
bb$i <- 1:nrow(bb)
# #Indexcan be started from anywhere. However it is not recommended.
bb$j <- 5:{nrow(bb) + 5L - 1L}
# 
# #Other Methods
bb$k <- seq_along(bb[[1]])
bb$l <- seq_len(nrow(bb))
bb$m <- seq.int(nrow(bb))
# #Note the placement of column at the beginning i.e. column index modified
bb <- cbind(n = 1:nrow(bb), bb)
bb <- rowid_to_column(bb, "o")
#
bb <- bb %>% mutate(p = row_number())
#
# #Excluding 'j' all other columns are equal. However, 'n' & 'o' modify column index
stopifnot(all(identical(bb$i, bb$k), identical(bb$i, bb$k), identical(bb$i, bb$l), 
  identical(bb$i, bb$m), identical(bb$i, bb$n), identical(bb$i, bb$o), identical(bb$i, bb$p)))
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B17-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ee, hh, ii, jj, kk, ll, mm, nn, oo, rr, vv, xx, yy, zz, B17P01, B17P03, B17P04, 
          B17P05, cap_hh, cut_ii, cut_jj, d_bb, kept_bb, len_hh, loc_png, max_aa, max_hh, 
          mean_hh, median_hh, min_aa, min_hh, sd_hh, sub_hh, ttl_hh, xxB16Cars)
```

```{r 'B17-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
