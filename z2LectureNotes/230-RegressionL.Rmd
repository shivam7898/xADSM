# Linear Regression (B30, Jan-30) {#b30}

```{r 'B30', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Decision Tree Algorithm"

## Data: KC House {#set-kc-b30 .tabset .tabset-fade}

\textcolor{pink}{Please import the "B30-KC-House.csv"}

```{r 'B30-KC', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum
#tools::md5sum(paste0(.z$XL, "B30-KC-House.csv"))
xxB30KC <- f_getObject("xxB30KC", "B30-KC-House.csv", "13e2be1e90780e7331b6a24ad950799d")
```

```{r 'B26-GetKC', include=FALSE}
# #Load Data: KC House
xxB30KC <- f_getRDS(xxB30KC)
```

### EDA

- About: [21613, 21]
  - Source: https://www.kaggle.com/harlfoxem/housesalesprediction
  - Description
    1. id: A notation for a house
    1. date: Date house was sold
    1. price: Price is prediction target
    1. bedrooms: Number of bedrooms
    1. bathrooms: Number of bathrooms
    1. sqft_living: Square footage of the home
    1. sqft_lot: Square footage of the lot
    1. floors: Total floors (levels) in house
    1. waterfront: House which has a view to a waterfront
    1. view: Has been viewed
    1. condition: How good the condition is overall
    1. grade: overall grade given to the housing unit, based on King County grading system
    1. sqft_above: Square footage of house apart from basement
    1. sqft_basement: Square footage of the basement
    1. yr_built: Built Year
    1. yr_renovated: Year when house was renovated
    1. zipcode: Zip code
    1. lat: Latitude coordinate
    1. long: Longitude coordinate
    1. sqft_living15: Living room area in 2015 (implies some renovations) This might or might not have     affected the lotsize area
    1. sqft_lot15: LotSize area in 2015 (implies some renovations)
  - Date 
    - Depending upon the import method, date might be character and need conversion
  - Integer to Categorical conversion is needed
  - yr_renovated needs to be handled 0 means no renovation - We can convert to Factor of Yes/No
  - sqft_basement - Similarly Yes/No
  - There are big area houses without any bedroom or bathroom
  - Renovated House is NOT a new house.
  - Calculate Age = Date of Sales - Year Built
    - There are 8 houses with negative age i.e. sold first completed later
    - There are 430 houses with 0 age i.e. sold in the year it was built
    - These can happen anyway

- Rows 21613
  - sqft_above: NA: -2 
  - Beds: 33: -1 : 
    - ~~No point in predicting price of a castle~~
    - With Price at 640000, and single floor with low sqft, it might be an Error for 3 beds
- Columns
  - Dropped sqft_living because of Multicollinearity issue with sqft_above. Together the sqft_above and sqft_basement take care of sqft_living
  - Dropped sqft_living15 and sqft_lot15 

- \textcolor{pink}{Question:} Property price are affected by location. Why are we removing lat/long
  - We are already including different types like waterfront etc.
  - It would have been better if we have rural, urban, city center, market type categories 
  - (Aside) We should identify clusters of zipcodes, some of them are clearly different from others

- \textcolor{pink}{Question:} Based on description of sqft_living15, would this not cause Multicollinearity issue
  - We will check "ForLater"

- \textcolor{pink}{Question:} Average Price over zipcode has clear distinctions
  - We will check "ForLater"
  - (Aside) We should identify clusters of zipcodes, some of them are clearly different from others

- \textcolor{pink}{Question:} Why the age is not taken as Today
  - Price is of the date it was sold. Our analysis date does not change the price.

```{r 'B30-PrepKC'}
aa <- xxB30KC
# #Drop NA | Get Age | Dummy | Rename | Filter | Factor | Relevel Most Frequent Reference | 
# #Drop | Relocate | 
bb <- aa %>% 
  drop_na() %>% 
  #mutate(across(date, as_date)) %>% 
  mutate(sold = year(date), age = sold - yr_built) %>% 
  mutate(iRenew = ifelse(yr_renovated == 0, 0, 1)) %>% 
  rename(beds = bedrooms, baths = bathrooms, sqAbove = sqft_above, sqLot = sqft_lot, 
         iWater = waterfront) %>% 
  filter(beds != 33) %>% 
  mutate(across(c(beds, baths, floors, iWater, condition, grade, iRenew), factor)) %>% 
  mutate(across(beds, relevel, ref = "3")) %>% 
  mutate(across(baths, relevel, ref = "2")) %>% 
  mutate(across(floors, relevel, ref = "1")) %>% 
  mutate(across(condition, relevel, ref = "3")) %>% 
  mutate(across(grade, relevel, ref = "7")) %>% 
  select(-c(id, view, zipcode, lat, long, date, sold, yr_renovated, yr_built, 
            sqft_living, sqft_living15, sqft_lot15)) %>% 
  relocate(price)
#
xfw <- bb
zfw <- xfw %>% mutate(across(where(is.numeric), ~ as.vector(scale(.)))) 
xnw <- xfw %>% select(where(is.numeric))
znw <- zfw %>% select(where(is.numeric))
f_wl(znw)
```

### Structure {.unlisted .unnumbered}

```{r 'B30-Temp-KC', include=FALSE, eval=FALSE}
# #Count NA 
if(FALSE) colSums(is.na(aa)) %>% as_tibble(rownames = "Cols") %>% filter(value > 0)
#
if(FALSE) bb %>% select(isFemale) %>% slice(1:10)
if(FALSE) cat('"', paste0(levels(bb$Arrival), collapse = '", "'), '"\n', sep = '')
if(FALSE) bb %>% filter(is.na(bpl)) %>% select(id, bph, bpl)
if(FALSE) bb %>% select(isImplant, imp_c) %>% summary()
if(FALSE) bb %>% select(iFemale, iMarried, iEmergency, iImplant) %>% 
  pivot_longer(everything()) %>% 
  count(name, value) %>% 
  pivot_wider(names_from = value, values_from = n) 
# #Count Unique of all Columns to decide which should be Factors
if(FALSE) bb %>% summarise(across(everything(), ~ length(unique(.)))) %>% pivot_longer(everything())
# #Summary of Columns of a class: is.factor is.numeric is.character
if(FALSE) bb %>% select(where(is.factor)) %>% summary()
# #Names and Indices of Columns of class: is.factor is.numeric is.character
if(FALSE) which(sapply(bb, is.factor))
# #Comma separated string having each item within quotes for easy pasting as character not objects
if(FALSE) cat('"', paste0(names(which(sapply(bb, is.factor))), collapse = '", "'), '"\n', sep = '')
# #Levels of Factor Columns
if(FALSE) lapply(bb[c(3, 6:9, 15)], levels)
# #Frequency of Each level of Factor
if(FALSE) bb %>% count(baths) %>% arrange(desc(n))
```

```{r 'B30-Str-KC'}
str(bb)
```

### Summary {.unlisted .unnumbered}

```{r 'B30-Summary-KC'}
summary(bb)
```

### 33 Beds {.unlisted .unnumbered}

```{r 'B30-Beds'}
# #With Price at 640000, and single floor with low sqft, it might be an Error for 3 beds
aa %>% filter(bedrooms == 33 | price > 5000000) %>% 
  mutate(across(price, num, notation = "dec"))
```

### zipcode {.unlisted .unnumbered}

```{r 'B30-zipcode'}
# #zipcode might be converted into clusters
# #Some of these have clearly higher than average prices
aa %>% filter(zipcode == 98112) %>% mutate(across(price, num, notation = "eng"))
#
aa %>% filter(zipcode == 98039) %>% mutate(across(price, num, notation = "eng"))
```

### relevel() {.unlisted .unnumbered}

```{r 'B30-Relevel'}
ii <- aa %>% select(beds = bedrooms) %>% mutate(across(beds, factor))
levels(ii$beds)
#
# #Relevel 3rd Index as Reference
jj <- ii %>% mutate(across(beds, relevel, ref = 3))
levels(jj$beds)
#
# #Relevel Value 3 as Reference
kk <- ii %>% mutate(across(beds, relevel, ref = "3"))
levels(kk$beds)
```


## SPLOM

- Note: Scatterplot Matrix (SPLOM) of all variables was not plotted because it takes long time and does not provide good actionable insight. Notes of earlier lectures have the code to generate them. - "ForLater" add link


## Correlation {.tabset .tabset-fade}

- \textcolor{pink}{Question:} The dependent variable (price) is skewed towards left but some independent variables (e.g. sqft_living) also are left skewed. Just like we are focusing on skewness of Y, should we also be concerned about X
  - Skewness of Y is more important. X can have skewness.

- \textcolor{pink}{Question:} Price and Age have correlation value of -0.05 and is significant. This is really small. Should this not show stronger correlation
  - Age might not come out as stronger predictor. 
  - Sometimes a variable might show a strong influence with the help of another independent variable
  
- \textcolor{pink}{Question:} If the correlation between Bedrooms and Price is 0.3 and is significant, then 30% of price can be explained by bedrooms
 - When we are looking for correlation between two variables, it does not consider presence or absence of other variables. However, regression consider that.
  - Correlation tells us if the variables are related and what type of relationship that is. Correlation is only about the association. It does not talk about whether the X leads to or explains Y.
  - Correlation is bidirectional i.e. $Y \Leftrightarrow X$, whereas regression is unidirectional i.e. $Y \Leftarrow X$
 - (Aside) No, Bedroom does not explain 30% of price. Sum of all correlations to Y is not 100%.


```{conjecture 'cor-numeric'}
\textcolor{brown}{Error in cor(...) : 'x' must be numeric}
```

- cor() can take only the numeric columns, remove any factor or character
  
### Image {.unlisted .unnumbered}

```{r 'B30-SetCorr-KC', include=FALSE}
# #Setup for Corrplot
ii <- znw
hh <- cor(ii)
corr_hh <- cor.mtest(ii)
# #p-value Higher than this is insignificant and should be skipped
sig_corr_hh <- 0.05 
#
cap_hh <- "B30P01"
ttl_hh <- "KC: Corrplot (Scaled)"
loc_png <- paste0(.z$PX, "B30P01", "-KC-Corrplot-z", ".png")
```

```{r 'B30P01-Save', include=FALSE, ref.label=c('B30-Corrplot')}
#
```

```{r 'B30P01', echo=FALSE, fig.cap="(B30P01) KC: Corrplot (Scaled)"}
knitr::include_graphics(paste0(.z$PX, "B30P01", "-KC-Corrplot-z", ".png"))
```

### Code {.unlisted .unnumbered}

```{r 'B30-SetCorr-KC-A', eval=FALSE, ref.label=c('B30-SetCorr-KC')}
#
```

```{r 'B30-Corrplot', eval=FALSE}
# #IN: hh, corr_hh, sig_corr_hh
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  corrplot(hh, method = "circle", type = "lower", diag = FALSE, col = COL2('BrBG', 200),
                   cl.pos = 'r', tl.pos = 'ld', addCoef.col = "black", tl.col = "black",
                   p.mat = corr_hh$p, sig.level = sig_corr_hh, insig = 'blank', 
        #order = "hclust", hclust.method = "ward.D", addrect = 2, rect.col = 3, rect.lwd = 3, 
                   title = NULL #, col = RColorBrewer::brewer.pal(3, "BrBG")
				   )
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B30 <- recordPlot()
  dev.off()
  assign(cap_hh, B30)
  rm(B30)
}
```


## Boxplot {.tabset .tabset-fade}

### Image {.unlisted .unnumbered}

```{r 'B30-SetBox-KC', include=FALSE}
hh <- znl
#levels(hh$Keys) <- names(znw)
#
ttl_hh <- "KC: Boxplot (Scaled)"
cap_hh <- "B30P02"
sub_hh <- NULL 
lgd_hh  <- NULL
```

```{r 'B30-KCBox', include=FALSE, ref.label=c('B30-ScaleBox')}
#
```

```{r 'B30P02-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B30P02", "-KC-Box-Z", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B30P02, device = "png", dpi = 144) 
}
```

```{r 'B30P02', echo=FALSE, fig.cap="(B30P02) KC: Boxplot (Scaled)"}
knitr::include_graphics(paste0(.z$PX, "B30P02", "-KC-Box-Z", ".png"))
```

### Code {.unlisted .unnumbered}

```{r 'B30-SetBox-KC-A', eval=FALSE, ref.label=c('B30-SetBox-KC')}
#
```

```{r 'B30-ScaleBox', eval=FALSE}
# #IN: hh(Keys, Values) 
B30 <- hh %>% { ggplot(data = ., mapping = aes(x = Keys, y = Values, fill = Keys)) +
    geom_boxplot() +
    k_gglayer_box +
    scale_y_continuous(breaks = breaks_pretty()) + 
    coord_flip() +
    theme(legend.position = 'none') +
    labs(x = NULL, y = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B30)
rm(B30)
```


## Outliers

- Multiple criteria to identify Outliers
  - Scaling 406
  - IQR 406 + 733
  - MAD 406 + 733 + 32
- Note: None of the identified outliers are actually deleted. All have been kept for processing.
  - I am interested in looking at the RMSE of the model before deleting any outlier

- \textcolor{pink}{Question:} Instead of deleting, can we separate the outliers and treat them as a different dataset. This will give us two models for the two distributions
  - Yes, we can do that. Anyway removal of more than 5% datapoints is not recommended.

```{r 'B30-Outliers'}
# # Select Price | Rename | Get MAD | Outliers by Z, IQR, MAD | Count |
xnw %>% select(Y = price) %>% 
  mutate(MADY = (Y - median(Y))/mad(Y)) %>% 
  mutate(isOut_Z = ifelse(abs(scale(Y)) > 3, TRUE, FALSE), 
         isOut_MAD = ifelse(abs(MADY) > 3, TRUE, FALSE),
         isOut_IQR = ifelse(Y < {quantile(Y)[2] - 1.5 * IQR(Y)} | 
                            Y > {quantile(Y)[4] + 1.5 * IQR(Y)}, TRUE, FALSE)) %>% 
  count(isOut_Z, isOut_IQR, isOut_MAD)
#
# #NOT DONE: But To Show How to Remove anythind beyond 2 SD of Price
if(FALSE) ii <- xnw %>% filter(!(abs(price - median(price)) > 2 * sd(price)))
#
# #NOT DONE: But To Show How to Remove anythind beyond 2.5 Scaled of Price
if(FALSE) ii <- znw %>% filter(between(price, -2.5, +2.5))
```

## Partition Data 

```{r 'B30-Partition-KC'}
# #Partition Data
set.seed(3)
# #Create Dummies | Replaced All NA in original data so no replacement in dummy columns |
dum_xfw <- dummy_cols(xfw, remove_first_dummy = TRUE, remove_selected_columns = TRUE) 
idx_xsyw <- sample.int(n = nrow(dum_xfw), size = floor(0.8 * nrow(dum_xfw)), replace = FALSE)
train_xfw <- dum_xfw[idx_xsyw, ] 
test_xfw  <- dum_xfw[-idx_xsyw, ]
```

```{r 'B30-Save-KC', include=FALSE, eval=FALSE}
# #EDA | Dummy | Partition | 
zzB30KC <- train_xfw
f_setRDS(zzB30KC)
```

```{r 'B30-Get-KC', include=FALSE}
zzB30KC <- f_getRDS(zzB30KC)
```


## Model

```{r 'B30-Model-KC'}
# #Linear Regression
mod_kc_all <- lm(price ~ ., data = zzB30KC)
```

## Stepwise Model

- Stepwise Regression did not remove multicollinearity 

```{r 'B30-StepModel-KC', eval=FALSE}
# #Stepwise Regression #"both", "backward", "forward"
yyB30KC_back <- step(mod_kc_all, direction = "backward", trace = 0)
yyB30KC_both <- step(mod_kc_all, direction = "both", trace = 0)
```

```{r 'B30-Save-KC-StepModel', include=FALSE, eval=FALSE}
# #EDA | Dummy | Partition | 
f_setRDS(yyB30KC_back)
f_setRDS(yyB30KC_both)
```

```{r 'B30-Get-KC-StepModel'}
yyB30KC_back <- f_getRDS(yyB30KC_back)
yyB30KC_both <- f_getRDS(yyB30KC_both)
```


```{r 'B30-Print-KC-1'}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Linear Regression: KC"
col_hh <- c("ALL", "StepBack", "StepBoth")
#
stargazer(mod_kc_all, yyB30KC_back, yyB30KC_both, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```

## VIF

- \textcolor{pink}{Question:} If we keep the outliers and get $R_a^2 \approx 72$ and after removing the outliers we get $R_a^2 \approx 58$, which model would be considered better
  - Do not compare $R_a^2$ built on different number of observations. $R_a^2$ is to compare models built on same dataset but with different number of independent variables


- Multicollinearity is an issue that needs to be handled 

```{r 'B30-VIF-KC'}
# #VIF
ii <- vif(mod_kc_all) 
ii[ii > 2]
# #After step-wise regression, some (not all) multicollinear variables have been dropped
ii <- vif(yyB30KC_both) 
ii[ii > 2]
```


## Normality

```{conjecture 'shapiro-5000'}
\textcolor{brown}{Error in shapiro.test(...) : sample size must be between 3 and 5000}
```

- 5000 is a safety limit on Shapiro. Randomly choose a sample of 5000 observations for testing

```{r 'B30-Normal-KC'}
# #Normality Test
set.seed(3)
test_shapiro <- shapiro.test(sample(x = mod_kc_all$residuals, size = 5000))
test_shapiro
#
if(test_shapiro$p.value > 0.05) cat("Normal. H0.\n") else cat("Not Normal. Ha.\n")
```

## Semi Log

```{r 'B30-SemiLog-KC', eval=FALSE}
# #Run Log Transformed Y
yyB30KC_l <- lm(log(price) ~ ., data = zzB30KC)
yyB30KC_l_both <- step(yyB30KC_l, direction = "both", trace = 0)
```


```{r 'B30-Save-KC-SemiLog', include=FALSE, eval=FALSE}
f_setRDS(yyB30KC_l)
f_setRDS(yyB30KC_l_both)
```

```{r 'B30-Get-KC-SemiLog'}
yyB30KC_l <- f_getRDS(yyB30KC_l)
yyB30KC_l_both <- f_getRDS(yyB30KC_l_both)
```


```{r 'B30-KC-SemiLog-Normal'}
# #VIF
ii <- vif(yyB30KC_l_both) 
ii[ii > 2]
#
summary(yyB30KC_l_both)$adj.r.squared
#names(yyB30KC_l_both$coefficients)
#
# #Normality Test
set.seed(3)
test_shapiro <- shapiro.test(sample(x = yyB30KC_l_both$residuals, size = 5000))
test_shapiro
#
if(test_shapiro$p.value > 0.05) cat("Normal. H0.\n") else cat("Not Normal. Ha.\n")
```

## Log-Log

```{r 'B30-Transform-Log-KC'}
# #Log Transform Y and X
train_log <- zzB30KC %>% 
  mutate(across(c(age), ~ . + 2)) %>% 
  mutate(across(c(sqft_basement), ~ . + 1)) %>% 
  mutate(across(c(price, sqLot, sqAbove, sqft_basement, age), log)) 
```

```{r 'B30-KC-Log', eval=FALSE}
# #Run Log Transformed Y and X
yyB30KC_ll <- lm(price ~ ., data = train_log)
yyB30KC_ll_both <- step(yyB30KC_ll, direction = "both", trace = 0)
```

```{r 'B30-Save-KC-Log', include=FALSE, eval=FALSE}
f_setRDS(yyB30KC_ll)
f_setRDS(yyB30KC_ll_both)
```

```{r 'B30-Get-KC-Log'}
yyB30KC_ll <- f_getRDS(yyB30KC_ll)
yyB30KC_ll_both <- f_getRDS(yyB30KC_ll_both)
```

```{r 'B30-KC-Log-Normal'}
# #VIF
ii <- vif(yyB30KC_ll_both) 
ii[ii > 2]
#
summary(yyB30KC_ll_both)$adj.r.squared
#names(yyB30KC_ll_both$coefficients)
#
# #Normality Test
set.seed(3)
test_shapiro <- shapiro.test(sample(x = yyB30KC_ll_both$residuals, size = 5000))
test_shapiro
#
if(test_shapiro$p.value > 0.05) cat("Normal. H0.\n") else cat("Not Normal. Ha.\n")
```


## Models

```{r 'F66-Print-KC'}
if(FALSE) lapply(list(mod_kc_all, yyB30KC_l, yyB30KC_l_both, yyB30KC_ll, yyB30KC_ll_both), glance)
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Linear Regression: KC"
col_hh <- c("SemiLog", "Log")
#
# #Log Transformation did not result in successful conversion to normal Model
# #RSE of Base Model should be compared with Log Transformed Model only after applying exponential 
stargazer(yyB30KC_l_both, yyB30KC_ll_both, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```


## Predict

- Subtract Y from 'exp(predicted)' to get the RMSE etc. in case of log transformed Y.

```{r 'B30-Predict-KC'}
# #Apply Same Transformation on Test Data (except the Actual Y)
test_log <- test_xfw %>% 
  mutate(across(c(age), ~ . + 2)) %>% 
  mutate(across(c(sqft_basement), ~ . + 1)) %>% 
  mutate(across(c(sqLot, sqAbove, sqft_basement, age), log)) 
#
res_kc_ll <- test_log %>% 
  mutate(CalYlog = predict(yyB30KC_ll_both, .), Y_Yc = price - exp(CalYlog), MAPE = Y_Yc / price)
#
mod <- res_kc_ll
summary(mod$Y_Yc)
#
# #RMSE: Root Mean Squared Error
#res_w %>% summarise(across(everything(), ~ sqrt(mean((.)^2))))
sqrt(mean((mod$Y_Yc)^2))

#
# #MAE: Mean Absolute Error (MAE)
#res_w %>% summarise(across(everything(), ~ mean(abs(.))))
mean(abs(mod$Y_Yc))
#
# #MAPE: Mean Absolute Percentage Error
mean(abs(mod$MAPE))
#
# #Accuracy 
1 - mean(abs(mod$MAPE))
```

## Ridge Regression

```{r 'B30-Ridge-KC'}
# #Ridge Regression i.e. alpha = 0, lambda = (0, Inf)
set.seed(3)
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
mod_ridge <- train(price ~ ., train_log, method = "glmnet", trControl = custom,
    tuneGrid = expand.grid(alpha = 0, 
                           lambda = seq(from = 0.01, to = 0.1, length.out = 10)))
#
# #Sample Subset of Results
mod_ridge$results %>% as_tibble() %>% slice_sample(n = 5)
#
# #Best Tuned Model
mod_ridge$results %>% as_tibble() %>% filter(lambda == mod_ridge$bestTune$lambda)
```


```{r 'B30-Predict-KC-Ridge'}
res_lll_ridge <- test_log %>% 
  mutate(CalYlog = predict(mod_ridge, .), Y_Yc = price - exp(CalYlog), MAPE = Y_Yc / price)
#
sqrt(mean((res_lll_ridge$Y_Yc)^2))
mean(abs(res_lll_ridge$Y_Yc))
mean(abs(res_lll_ridge$MAPE))
1 - mean(abs(res_lll_ridge$MAPE))
```

## Lasso Regression

```{r 'B30-Lasso-KC'}
# #Lasso Regression i.e. alpha = 1, lambda = (0, Inf)
set.seed(3)
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
mod_lasso <- train(price ~ ., train_log, method = "glmnet", trControl = custom,
    tuneGrid = expand.grid(alpha = 1, 
                           lambda = seq(from = 0.0005, to = 0.005, length.out = 10)))
#
# #Sample Subset of Results
mod_lasso$results %>% as_tibble() %>% slice_sample(n = 5)
#
# #Best Tuned Model
mod_lasso$results %>% as_tibble() %>% filter(lambda == mod_lasso$bestTune$lambda)
```

```{r 'B30-Predict-KC-Lasso'}
res_lll_lasso <- test_log %>% 
  mutate(CalYlog = predict(mod_lasso, .), Y_Yc = price - exp(CalYlog), MAPE = Y_Yc / price)
#
sqrt(mean((res_lll_lasso$Y_Yc)^2))
mean(abs(res_lll_lasso$Y_Yc))
mean(abs(res_lll_lasso$MAPE))
1 - mean(abs(res_lll_lasso$MAPE))
```

## Elastic Net

- It takes long time to run and multiple attempts resulted in different values of alpha and lambda so on hold for now - "ForLater"
  - Need a better method than providing different ranges

```{r 'B30-Elastic', eval=FALSE}
# #Elastic Net Regression i.e. alpha = [0, 1], lambda = (0, Inf)
set.seed(3)
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
mod_enet <- train(price ~ ., train_log, method = "glmnet", trControl = custom,
    tuneGrid = expand.grid(alpha = seq(from = 0.001, to = 0.1, length.out = 100), 
                           lambda = seq(from = 0.01, to = 0.1, length.out = 10)))
#
# #Sample Subset of Results
mod_enet$results %>% as_tibble() %>% slice_sample(n = 5)
#
# #Best Tuned Model
mod_enet$results %>% as_tibble() %>% 
  filter(alpha == mod_enet$bestTune$alpha & lambda == mod_enet$bestTune$lambda)
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B30-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, B30P02, cap_hh, corr_hh, custom, dum_xfw, hh, idx_xsyw, lgd_hh, 
          loc_png, mod_lasso, mod_ridge, mod_xfw, mod_xfw_lll, mod_xfw_log, res_lll, 
          res_lll_lasso, res_lll_ridge, sig_corr_hh, stp_xfw, stp_xfw_lll, stp_xfw_log, sub_hh, 
          test_log, test_shapiro, test_xfw, train_log, train_xfw, ttl_hh, vif_ii, xfw, xnw, 
          xxB30KC, zfw, znl, znw, B30P01, col_hh, mod, mod_kc_all, res_kc_ll, yyB30KC_back, 
          yyB30KC_both, yyB30KC_l, yyB30KC_l_both, yyB30KC_ll, yyB30KC_ll_both, zzB30KC)
```

```{r 'B30-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
