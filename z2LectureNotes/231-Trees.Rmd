# Decision Tree Algorithm (B31, Feb-06) {#b31}

```{r 'B31', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "Decision Tree Algorithm"

## Packages

```{r 'B31-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("rpart", dependencies = TRUE)
  install.packages("rpart.plot", dependencies = TRUE)
  install.packages("partykit", dependencies = TRUE)
  install.packages("treeheatr", dependencies = TRUE)
}
```

## Data

- [Import Data CarDekho - B26](#set-cardekho-b26 "b26")


```{r 'B31-GetCarDekho', ref.label=c('B26-GetCarDekho', 'B26-PrepCar')}
# #xxB26CarDekho, aa, bb, xsyw, xw,xsw, zfw, xnw, znw
```

```{r 'B31-GetCarDekhoKnit', include=FALSE, eval=TRUE}
xxB26CarDekho <- f_getRDS(xxB26CarDekho)
aa <- xxB26CarDekho
bb <- aa %>% 
  separate(name, c("brand", NA), sep = " ", remove = FALSE, extra = "drop") %>% 
  filter(fuel != "Electric") %>% 
  #mutate(across(where(is.character), factor)) %>% 
  mutate(across(fuel, factor, levels = c("Diesel", "Petrol", "CNG", "LPG"))) %>% 
  mutate(across(transmission, factor, levels = c("Manual", "Automatic"), 
                labels = c("Manual", "Auto"))) %>% 
  mutate(across(owner, factor, 
levels = c("First Owner", "Second Owner", "Third Owner", "Fourth & Above Owner", "Test Drive Car"), 
labels = c("I", "II", "III", "More", "Test"))) %>% 
  mutate(across(seller_type, factor, levels = c("Individual", "Dealer", "Trustmark Dealer"), 
                labels = c("Indiv", "Dealer", "mDealer"))) %>% 
  rename(price = selling_price, km = km_driven, 
         s = seller_type, o = owner, t = transmission, f = fuel) %>% 
  mutate(age = 2022 - year) %>% 
  select(-c(year, name, brand))
# 
xfw <- bb
zfw <- xfw %>% mutate(across(where(is.numeric), ~ as.vector(scale(.)))) 
xnw <- xfw %>% select(where(is.numeric))
znw <- zfw %>% select(where(is.numeric))
```

## Definitions


```{r 'B31D01', comment="", echo=FALSE, results='asis'}
f_getDef("Decision-Tree-Methods")
```


```{r 'B31D02', comment="", echo=FALSE, results='asis'}
f_getDef("Decision-Tree")
```


```{r 'B31D03', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node")
```


```{r 'B31D04', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node-Parent-Child")
```


```{r 'B31D05', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node-Internal-Leaf")
```


```{r 'B31D06', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node-Root")
```


```{r 'B31D07', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node-Degree")
```


```{r 'B31D08', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node-Depth")
```


```{r 'B31D09', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Node-Height")
```


```{r 'B31D10', comment="", echo=FALSE, results='asis'}
f_getDef("Tree-Breadth")
```


```{r 'B31D11', comment="", echo=FALSE, results='asis'}
f_getDef("Decision-Trees-Summary")
```


```{r 'B31D12', comment="", echo=FALSE, results='asis'}
f_getDef("Splitting-Pruning")
```


```{r 'B31D13', comment="", echo=FALSE, results='asis'}
f_getDef("Recursive-Partitioning") #dddd
```



## Decision Trees

- Decision Trees can be used for both regression (Continuous Y) and classification (Categorical Y)
  - Objective is to partition the dependent variable based on given set of predictors
  - At each step The algorithm try to use a variable which can split the dataset into two groups A & B which have maximum heterogeneity between them (between A & B) and minimum heterogeneity within the groups (within A & within B).
  - The process stops when there is no variance in data that can be separated further i.e. All leaf nodes are homogeneous (within variance is zero)

- \textcolor{pink}{Question:} At each decision node, would the variable be changed
  - At each decision node, the algorithm loops through all the variabls, selects one of them and does a binary split on the value.
  - It is a sequential process, not simultaneous

- Note: If there is a variable in the dataset which does not change i.e. its variance is zero e.g. gender is male for all observations
  - In linear regression model the variable would be kept unless we eliminate it in stepwise regression
  - In Decision Tree model the variable will not be included


## Recursive Tree Algorithm {.tabset .tabset-fade}

### Data {.unlisted .unnumbered}

```{r 'B31-Tree'}
# #24 Observations with 12 who defaulted (isDefaulted = 1) and 12 who paid their loan
bb <- tibble(Income = c(51000, 63000, 59000, 47000, 64000, 84000, 49000, 66000, 
                        33000, 75000, 43000, 53000, 85000, 108000, 60000, 110000, 
                        69000, 81000, 93000, 61000, 65000, 52000, 83000, 87000), 
             Credit = c(524, 548, 596, 602, 627, 640, 638, 667, 680, 708, 730, 748, 
                        620, 648, 682, 701, 731, 716, 749, 752, 767, 788, 796, 840), 
             iDefault = factor(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)))
str(bb)
```

### Split 01 {.unlisted .unnumbered}

```{r 'B31-Split-1'}
# #Split: 01 Credit=681
mod_tree <- rpart(iDefault ~ ., data = bb, method = 'class', 
                  control = rpart.control(cp = 0.5, minsplit = 2))
```

```{r 'B31-PrintTree-1', ref.label=c('B31-PrintTree')}
#
```

### 02 {.unlisted .unnumbered}

```{r 'B31-Split-2'}
# #Split: 01 Credit=681 |02 Income=84500
mod_tree <- rpart(iDefault ~ ., data = bb, method = 'class', 
                  control = rpart.control(cp = 0.15, minsplit = 2))
```

```{r 'B31-PrintTree-2', ref.label=c('B31-PrintTree')}
#
```

### 03 {.unlisted .unnumbered}

```{r 'B31-Split-3'}
# #Split: 01 Credit=681 |02 Income=56500 |03 Income=84500
mod_tree <- rpart(iDefault ~ ., data = bb, method = 'class', 
                  control = rpart.control(cp = 0.083, minsplit = 4))
```

```{r 'B31-PrintTree-3', ref.label=c('B31-PrintTree')}
#
```

### 04 {.unlisted .unnumbered}

```{r 'B31-Split-4'}
# #Split: 01 Credit=681 |02 Income=56500 |03 Income=84500 |04 Credit=768
mod_tree <- rpart(iDefault ~ ., data = bb, method = 'class', 
                  control = rpart.control(cp = 0.08, minsplit = 2))
```

```{r 'B31-PrintTree-4', ref.label=c('B31-PrintTree')}
#
```

### Full Split {.unlisted .unnumbered}

```{r 'B31-Split-Full'}
# #Split: 01 Credit=681 |02 Income=56500 |03 Income=84500 |04 Credit=768 | ... | FULL
mod_tree <- rpart(iDefault ~ ., data = bb, method = 'class', 
                  control = rpart.control(cp = 0.005, minsplit = 3))
```

```{r 'B31-PrintTree-Full', ref.label=c('B31-PrintTree')}
#
```


### as.party() {.unlisted .unnumbered}

```{r 'B31-PrintTree', eval=FALSE}
# #Printing Tree
ii <- as.party(mod_tree)
length(ii)
width(ii)
depth(ii)
ii
```

## Plot Tree {.tabset .tabset-fade}

### Images {.unlisted .unnumbered}

```{r 'B31-Tree-Base-Set', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P01"
ttl_hh <- "Example Tree Basic"
loc_png <- paste0(.z$PX, "B31P01", "-Tree-Basic", ".png")
```

```{r 'B31P01-Save', include=FALSE, ref.label=c('B31-Tree-Base')}
#
```

```{r 'B31P01', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P01", "-Tree-Basic", ".png"))
```

```{r 'B31-Tree-rpart-Set', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P02"
ttl_hh <- "Example Tree by rpart"
loc_png <- paste0(.z$PX, "B31P02", "-Tree-rpart", ".png")
```

```{r 'B31P02-Save', include=FALSE, ref.label=c('B31-Tree-rpart')}
#
```

```{r 'B31P02', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P02", "-Tree-rpart", ".png"))
```

```{r 'B31-Tree-fancy-Set', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P03"
ttl_hh <- "Example Tree by fancyRpartPlot"
loc_png <- paste0(.z$PX, "B31P03", "-Tree-fancy", ".png")
```

```{r 'B31P03-Save', include=FALSE, ref.label=c('B31-Tree-fancy')}
#
```

```{r 'B31P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P03", "-Tree-fancy", ".png"))
```

```{r 'B31-Tree-prp-Set', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P04"
ttl_hh <- "Example Tree by prp"
loc_png <- paste0(.z$PX, "B31P04", "-Tree-prp", ".png")
```

```{r 'B31P04-Save', include=FALSE, ref.label=c('B31-Tree-prp')}
#
```

```{r 'B31P04', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P04", "-Tree-prp", ".png"))
```

```{r 'B31-Tree-heat-Set', include=FALSE}
hh <- as.party(mod_tree)
#
cap_hh <- "B31P05"
ttl_hh <- "(B31P05) Example Tree by heat_tree"
```

```{r 'B31-Tree-heat-Plot', include=FALSE, ref.label=c('B31-Tree-heat')}
#
```

```{r 'B31P05-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B31P05", "-Tree-treeheatr", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B31P05, device = "png", dpi = 144) 
}
```

```{r 'B31P05', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P05", "-Tree-treeheatr", ".png"))
```

```{r 'B31-Tree-ggparty-Set', include=FALSE}
hh <- ctree(iDefault ~ ., data = bb, 
      control = ctree_control(alpha = 0.9999, minsplit = 1, minbucket = 1)) 
#
cap_hh <- "B31P06"
ttl_hh <- "Example Tree (& counts) by ctree & ggparty"
```

```{r 'B31-Tree-ggparty-Plot', include=FALSE, ref.label=c('B31-Tree-ggparty')}
#
```

```{r 'B31P06-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B31P06", "-Tree-ctree-ggparty", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B31P06, device = "png", dpi = 144) 
}
```

```{r 'B31P06', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P06", "-Tree-ctree-ggparty", ".png"))
```

```{r 'B31P0106', echo=FALSE, ref.label=c('B31P01', 'B31P02', 'B31P03', 'B31P04', 'B31P05', 'B31P06'), fig.cap="(B31P01 B31P02 B31P03 B31P04 B31P05 B31P06) Example Tree: Basic, rpart, fancyRpartPlot, prp, treeheatr and ctree & ggparty"}
#
```

### Code Base Tree {.unlisted .unnumbered}

```{r 'B31-Tree-Base-Set-A', eval=FALSE, ref.label=c('B31-Tree-Base-Set')}
#
```

```{r 'B31-Tree-Base', eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh <- mod_tree
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plot(hh)
  text(hh, pretty = 1)
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B31 <- recordPlot()
  dev.off()
  assign(cap_hh, B31)
  rm(B31)
}
```

### rpart.plot() {.unlisted .unnumbered}

```{conjecture 'rpart-plot-extra'}
\textcolor{brown}{Error: extra=2 is legal only for "class", "poisson" and "exp" models (you have an "anova" model)}
```

- Use extra = 'auto' or appropriate code for extra information


```{r 'B31-Tree-rpart-Set-A', eval=FALSE, ref.label=c('B31-Tree-rpart-Set')}
#
```

```{r 'B31-Tree-rpart', eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh <- mod_tree
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  rpart.plot(hh, extra = 'auto')
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B31 <- recordPlot()
  dev.off()
  assign(cap_hh, B31)
  rm(B31)
}
```

### fancyRpartPlot() {.unlisted .unnumbered}

```{r 'B31-Tree-fancy-Set-A', eval=FALSE, ref.label=c('B31-Tree-fancy-Set')}
#
```

```{r 'B31-Tree-fancy', eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh <- mod_tree
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  fancyRpartPlot(hh)
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B31 <- recordPlot()
  dev.off()
  assign(cap_hh, B31)
  rm(B31)
}
```

### prp() {.unlisted .unnumbered}

```{conjecture 'prp-extra'}
\textcolor{brown}{Error: extra=104 is legal only for "class" models (you have an "anova" model)}
```

- Use extra = 0 or appropriate code for extra information

```{r 'B31-Tree-prp-Set-A', eval=FALSE, ref.label=c('B31-Tree-prp-Set')}
#
```

```{r 'B31-Tree-prp', eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh <- mod_tree
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  prp(hh, type = 2, extra = 0, nn = TRUE, fallen.leaves = TRUE, 
      faclen = 0, varlen = 0, shadow.col = "grey", branch.lty = 3)
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B31 <- recordPlot()
  dev.off()
  assign(cap_hh, B31)
  rm(B31)
}
```

### heat_tree() {.unlisted .unnumbered}

```{r 'B31-Tree-heat-Set-A', eval=FALSE, ref.label=c('B31-Tree-heat-Set')}
#
```

```{r 'B31-Tree-heat', eval=FALSE}
# #IN: cap_hh, ttl_hh, hh <- as.party(mod_tree)
#
B31 <- hh %>% {# cannot use labs because it is a gtable object
  suppressWarnings(heat_tree(x = ., label_map = c('0'= 'Paid', '1'= 'Fault'), 
                             target_cols = c('#FDE725FF', '#440154FF'), 
      panel_space = 0.03, target_space = 0.2, tree_space_bottom = 0.05, title = ttl_hh))
}
assign(cap_hh, B31)
rm(B31)
#
if(FALSE){# Modifying a gtable object. Finding the Coordinates of specific grob is Manual.
  ii <- suppressWarnings(heat_tree(x = hh, label_map = c('0'= 'Paid', '1'= 'Fault'), 
                             target_cols = c('#FDE725FF', '#440154FF'), 
      panel_space = 0.03, target_space = 0.2, tree_space_bottom = 0.05, title = ttl_hh))
  # #Works
  if(FALSE) ii 
  if(FALSE) grid.draw(ii)
  # #Plot with grey squares in backgroud if plot or save (like base R plot objects)
  if(FALSE) plot(ii)
  #
  # #Explore gtable Object
  names(ii)
  # #layout is a data.frame indicating the position of each grob.
  # #z defines drawing order i.e. overlap
  if(FALSE) ii$layout #t = 9, l = 7, b = 9, r = 7
  # # t   l  b  r   z   clip       name
  # # 1   2  1  14  21  0          on background
  left.foot = textGrob("Caption", x = 0, y = 0.8, 
                       just = c("left", "top"), gp = gpar(fontsize = 11, col =  "black"))
  labs.foot = gTree("LabsFoot", children = gList(left.foot))
  jj <- gtable_add_grob(ii, labs.foot, t = 3, l = 2, b = 3, r = 2, z = 13, clip = 'off')
  jj 
}
```

### ggparty() {.unlisted .unnumbered}

```{r 'B31-Tree-ggparty-Set-A', eval=FALSE, ref.label=c('B31-Tree-ggparty-Set')}
#
```

```{r 'B31-Tree-ggparty', eval=FALSE}
# #IN: cap_hh, ttl_hh, hh as ctree object
#
B31 <- hh %>% { ggparty(.) + 
    geom_edge() + 
    geom_edge_label() + 
    geom_node_label(aes(label = splitvar), ids = "inner") + 
    geom_node_label(aes(label = paste("n =", nodesize)), nudge_y = 0.03, ids = "terminal") +
    geom_node_plot(shared_legend = FALSE,
                   gglist = list(geom_bar(aes(x = !!.$terms[[2]], 
                                              fill = !!.$terms[[2]])),
                                 theme_minimal(), theme(legend.position = "none"))) + 
    theme(plot.background = element_rect(fill = 'white', colour = 'white')) +
    labs(caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B31)
rm(B31)
#
if(FALSE) {#With Bars
  B31 <- hh %>% { ggparty(.) + 
    geom_edge() + 
    geom_edge_label() + 
    geom_node_label(aes(label = splitvar), ids = "inner") + 
    geom_node_label(aes(label = paste("n =", nodesize)), nudge_y = 0.03, ids = "terminal") +
    geom_node_plot(gglist = list(geom_bar(aes(x = "", fill = !!.$terms[[2]]), 
                                          position = position_fill()), 
        theme_classic(), theme(axis.title = element_blank(), axis.ticks.x = element_blank()))) + 
    theme(plot.background = element_rect(fill = 'white', colour = 'white')) +
    labs(caption = cap_hh, title = ttl_hh)
  }
}
```

## Gini Index

```{definition 'Gini-Index'}
\textcolor{pink}{Gini index} is the probability of misclassifying an observation. It is a measure of total variance across the 'K' classes. If $\hat{p}_{mk}$ represents the proportion of training observations in the $m^{\text{th}}$ region that are from the $k^{\text{th}}$ class. Then \textcolor{pink}{Gini index: $G = \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk}) \to G \in [0, 0.5]$}. 
```

- To determine the particular split that maximized homogeneity the algorithm used a purity measure, Gini impurity. 
  - If a set was very mixed (impure), the Gini impurity would be high. 
  - As the set become more pure, the Gini impurity would decrease. 
  - Gini index is referred to as a measure of node purity. A small Gini value indicates that a node contains predominantly observations from a single class.


```{definition 'Information-Gain'}
The difference in Gini impurity from the prior state to the next is called the \textcolor{pink}{information gain}. It is inversely related with Gini impurity.
```


- \textcolor{pink}{Question:} What are the chances of a person being defaulter if the dataset has 12 defaulters and 12 not defaulters
  - For being defaulter : 12/24 = 50%
- \textcolor{pink}{Question:} What are the chances of a person being defaulter if the dataset has 6 defaulters and 18 not defaulters
  - For being defaulter : 6/24 = 25% (more homogeneour sample compared to above, thus Gini will be reduced)

- \textcolor{pink}{Question:} If the Tree splits at Credit Score = 681, what is the Gini index
  - $R_1$ [2] Credit >= 681: 0 (n = 13, err = 23.1%) 3 deaulters, 10 not defaulters
  - $R_2$ [3] Credit < 681: 1 (n = 11, err = 18.2%) 9 defaulters, 2 not defaulters
  - Gini impurity of $R_1 : G_{R_1} = \frac{10}{13} \times \frac{3}{13} + \frac{3}{13} \times \frac{10}{13} = 0.356$
  - Gini impurity of $R_2 : G_{R_2} = \frac{2}{11} \times \frac{9}{11} + \frac{9}{11} \times \frac{2}{11} = 0.298$
  - Proportion of $R_1 : P_{R_1} = \frac{13}{24}$
  - Proportion of $R_2 : P_{R_2} = \frac{11}{24}$
  - $G = P_{R_1} \times G_{R_1} + P_{R_2} \times G_{R_2} = \frac{13}{24} \times 0.356 + \frac{11}{24} \times 0.298 = 0.3286$
  - Thus, Gini reduced from 0.5 to 0.3286 after the first split.

- \textcolor{pink}{Question:} Why the first cut is at Credit Score = 681, why can it not be at 850, 700, 650, 500
  - At this value the Gini becomes lowest for single cut compared to all other cuts (Horizontal or Vertical)
  - (Aside) At the extreme ends (higher than max or lower than min) the Gini index will be 50% (maximum for binary choices)
    - As we start to move towards inside (from max to lower or from min to higher) the Gini would start decreasing once and then reaching a local minima and then increasing again
    - The algorithm has identified this local minima at the value of Credit Score = 681
    - This is the line that reduces that variances within $(R_1)$ and within $(R_2)$. And is maximises the variance between $(R_1 , R_2)$

- \textcolor{pink}{Question:} For a completely pure node, what would be the Gini impurity
  - Zero

## Pruning

- If we allow to grow to any extent, it might result in overfitting and if we prune it too much, it might lead to underfitting
- We can use complexity parameter $C_p \in [0, 1]$ for pruning
  - It decides the minimum information gain. Any branch having lower gain than that will be pruned
  - High value of $C_p$ will result in small tree

## Building a Regression Tree

1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations. 
2. Apply cost complexity pruning $(C_p)$ to the large tree in order to obtain a sequence of best subtrees, as a function of ${\alpha}$. 
3. Use K-fold cross-validation to choose ${\alpha}$. 
    - Divide the training observations into K folds. 
    - For each $k = \{1, 2, \ldots, K\}$: 
      - Repeat Steps 1 and 2 on all but the $k^{\text{th}}$ fold of the training data. i.e. keep at least one separate as Test. 
      - Evaluate the mean squared prediction error on the data in the left-out $k^{\text{th}}$ fold, as a function of ${\alpha}$. 
    - Average the results for each value of ${\alpha}$, and pick ${\alpha}$ to minimize the average error. 
4. Return the subtree from Step 2 that corresponds to the chosen value of ${\alpha}$.


## Dummy Variable Creation

- Some algorithms in R and Python can automatically convert factors to dummy variable and perform the analysis. Ex: lm()
  - However, it is recommended to perform the conversion yourself for fewer bugs and better control over reference variables etc. 
    - Most of other software platform would not do this automatically. We need factor variables to be converted to the dummy variables
    - Some of the advanced modelling methods in R or Python would not do this conversion automatically


```{r 'B31-Dummies'}
# #Dataset: iris (Assume Y = Petal.Width, 3 numerical X and 1 factor X)
str(iris)
# #Create Linear Model without converting the factor variable to dummies
mod_lm <- lm(Petal.Width ~ ., data = iris)
# #Note that it automatically converted factor to dummy variables with "setosa" as reference
summary(mod_lm)
```

## Car Dekho

```{r 'B31-Dummies-Car'}
# #Dataset: Car Dekho
str(xfw)
#
# #Dummy | Drop First Level i.e. Reference | Drop Selected Columns i.e. Original |
dum_xfw <- xfw %>% dummy_cols(.data = ., 
                  select_columns = c("f", "s", "t", "o"), 
                  remove_first_dummy = TRUE, remove_selected_columns = TRUE)
#
# #Partition Data
set.seed(3)
idx_xfw <- sample.int(n = nrow(dum_xfw), size = floor(0.8 * nrow(dum_xfw)), replace = FALSE)
train_xfw <- dum_xfw[idx_xfw, ]
test_xfw  <- dum_xfw[-idx_xfw, ]
#
# #Decision Trees: Convert Dummy Variables to Factor Variables of two levels "0", "1"
train_xfw_ii <- train_xfw %>% mutate(across(starts_with(c("s_", "f_", "t_", "o_")), factor))
test_xfw_ii  <- test_xfw  %>% mutate(across(starts_with(c("s_", "f_", "t_", "o_")), factor))
#
str(train_xfw_ii)
```

## Build Tree by rpart

- \textcolor{pink}{rpart()}
  - For Regression Tree (Y Continuous), Method needs to be 'anova'
  - For Classification Tree (Y Categorical), Method needs to be 'class'
  - If there is a problem in plotting then mention the cp parameter in rpart() with the earlier identified value
  
- \textcolor{orange}{Caution:} Unlike Regression, for Tree Model: Convert Dummy Variables to Factor Variables of two levels "0", "1"
  - Otherwise these would be treated as numeric i.e. Petrol < 0.5 etc.

```{r 'B31-Tree-Car'}
# #rpart() for Tree Model
mod_tree <- rpart(price ~ ., data = train_xfw_ii, method = 'anova')
```

```{r 'B31-PrintTree-Car', ref.label=c('B31-PrintTree')}
#
```

## Plot Tree

- "ForLater"
  - Unable to run heat_tree() on this dataset.
  - Error in get_fit.party : Please ensure the tree was trained on a dataset with dependent variable of class factor or switch task to regression.


```{r 'B31-Tree-Base-Set-Car', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P07"
ttl_hh <- "CarDekho: Tree Basic"
loc_png <- paste0(.z$PX, "B31P07", "-Car-Basic", ".png")
```

```{r 'B31P07-Save', include=FALSE, ref.label=c('B31-Tree-Base')}
#
```

```{r 'B31P07', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P07", "-Car-Basic", ".png"))
```

```{r 'B31-Tree-rpart-Set-Car', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P08"
ttl_hh <- "CarDekho: Tree by rpart"
loc_png <- paste0(.z$PX, "B31P08", "-Car-rpart", ".png")
```

```{r 'B31P08-Save', include=FALSE, ref.label=c('B31-Tree-rpart')}
#
```

```{r 'B31P08', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P08", "-Car-rpart", ".png"))
```

```{r 'B31-Tree-fancy-Set-Car', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P09"
ttl_hh <- "CarDekho: Tree by fancyRpartPlot"
loc_png <- paste0(.z$PX, "B31P09", "-Car-fancy", ".png")
```

```{r 'B31P09-Save', include=FALSE, ref.label=c('B31-Tree-fancy')}
#
```

```{r 'B31P09', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P09", "-Car-fancy", ".png"))
```

```{r 'B31-Tree-prp-Set-Car', include=FALSE}
hh <- mod_tree
#
cap_hh <- "B31P10"
ttl_hh <- "CarDekho: Tree by prp"
loc_png <- paste0(.z$PX, "B31P10", "-Car-prp", ".png")
```

```{r 'B31P10-Save', include=FALSE, ref.label=c('B31-Tree-prp')}
#
```

```{r 'B31P10', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P10", "-Car-prp", ".png"))
```

```{r 'B31-Tree-heat-Set-Car', include=FALSE, eval=FALSE}
# #NOT WORKING. THROWING ERROR
hh <- as.party(mod_tree)
#
cap_hh <- "B31P11"
ttl_hh <- "(B31P11) CarDekho: Tree by heat_tree"
# 'B31-Tree-heat-Regression'
# #IN: cap_hh, ttl_hh, hh <- as.party(mod_tree)
#
B31 <- hh %>% {# cannot use labs because it is a gtable object
  suppressWarnings(heat_tree(x = ., task = 'regression', title = ttl_hh))
}
assign(cap_hh, B31)
rm(B31)
# 'B31P11-Save'
loc_png <- paste0(.z$PX, "B31P11", "-Car-treeheatr", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B31P11, device = "png", dpi = 144) 
}
# 'B31P11'
knitr::include_graphics(paste0(.z$PX, "B31P11", "-Car-treeheatr", ".png"))
```

```{r 'B31-Tree-ggparty-Set-Car', include=FALSE}
# #, minsplit = 1, minbucket = 1
hh <- ctree(price ~ ., data = train_xfw_ii, 
      control = ctree_control(alpha = 0.000001)
      ) 
#
cap_hh <- "B31P12"
ttl_hh <- "CarDekho: Tree (& counts) by ctree & ggparty"
```

```{r 'B31-Tree-ggparty-Plot-Car'}
# #'B31-Tree-ggparty'
# #IN: cap_hh, ttl_hh, hh as ctree object
#
B31 <- hh %>% { ggparty(.) + 
    geom_edge() + 
    geom_edge_label() + 
    geom_node_label(aes(label = splitvar), ids = "inner") + 
    geom_node_label(aes(label = paste("n =", nodesize)), nudge_y = 0.03, ids = "terminal") +
    #geom_node_plot(shared_legend = FALSE,
    #               gglist = list(geom_bar(aes(x = !!.$terms[[2]], 
    #                                          fill = !!.$terms[[2]])),
    #                             theme_minimal(), theme(legend.position = "none"))) + 
    #theme_bw() +
    #theme_minimal() +
    theme(plot.background = element_rect(fill = 'white', colour = 'white')) +
    labs(caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B31)
rm(B31)
```

```{r 'B31P12-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B31P12", "-Car-ctree-ggparty", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B31P12, device = "png", dpi = 144) 
}
```

```{r 'B31P12', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B31P12", "-Car-ctree-ggparty", ".png"))
```

```{r 'B31P0708', echo=FALSE, ref.label=c('B31P07', 'B31P08'), fig.cap="(B31P07 B31P08) CarDekho: Tree: Basic and rpart"}
#
```

```{r 'B31P0910', echo=FALSE, ref.label=c('B31P09', 'B31P10'), fig.cap="(B31P09 B31P10) CarDekho: Tree: fancyRpartPlot and prp"}
#
```

```{r 'B31P12-A', echo=FALSE, ref.label=c('B31P12'), fig.cap="(B31P12) CarDekho: Tree: ctree & ggparty"}
#
```


## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B31-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, B31P01, B31P02, B31P03, B31P04, B31P05, B31P06, B31P07, B31P08, 
          B31P09, B31P10, B31P12, cap_hh, dum_xfw, hh, idx_xfw, loc_png, mod_lm, mod_tree, 
          test_xfw, test_xfw_ii, train_xfw, train_xfw_ii, ttl_hh, xfw, xnw, xxB26CarDekho, zfw, 
          znw)
```

```{r 'B31-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
