# Logistic Regression (B36, Mar-13) {#b36}

```{r 'B36', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Packages

```{r 'B36-Installations', include=FALSE, eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("pROC", dependencies = TRUE)
}
```

## Data

- [Import Data Bank - B34](#set-bank-b34 "b34")

```{r 'B36-GetBank', ref.label=c('B34-Get-Bank')}
# #zzB34Bank, xbank, test_bank
```

```{r 'B36-Get-Bank', include=FALSE, eval=TRUE}
zzB34Bank <- f_getRDS(zzB34Bank)
#
# #Create Dummies | Replaced All NA in original data so no replacement in dummy columns |
dum_xfw <- dummy_cols(zzB34Bank, 
  select_columns = c("wed", "edu", "default", "housing", "loan", "call", "poutcome"),
                      remove_first_dummy = TRUE, remove_selected_columns = TRUE) 
#
# #Partition Data
set.seed(3)
#idx_xsyw <- sample.int(n = nrow(dum_xfw), size = floor(0.8 * nrow(dum_xfw)), replace = FALSE)
idx_xsyw <- createDataPartition(dum_xfw$y, p = 0.8, list = FALSE)
train_xfw <- dum_xfw[idx_xsyw[, 1], ] 
test_xfw  <- dum_xfw[-idx_xsyw[, 1], ]
#
xbank <- train_xfw
test_bank <- test_xfw
```

## Confusion Matrix Revision {.tabset .tabset-fade}

### Link {.unlisted .unnumbered}

- Refer [Confusion Matrix](#confusion-f67 "f67")

### Explanation {.unlisted .unnumbered}

```{r 'B36T01', echo=FALSE, ref.label=c('F67T01')}
#
```

```{r 'B36E01', comment="", echo=FALSE, results='asis'}
f_getExm("Confusion-Setup")
```

```{r 'B36E02', comment="", echo=FALSE, results='asis'}
f_getExm("Confusion-Definitions")
```

## ROC and AUC

- \textcolor{pink}{Question:} What is the problem with 'Overall Accuracy' $= \frac{A+D}{N}$
  - Sometimes we require better sensitivity and other times better specificity is required. However, the Accuracy does not take that into account.

- We have assumed a cut-off probability of 0.50. However, we can change this threshold according to our requirements

```{r 'B36D01', comment="", echo=FALSE, results='asis'}
f_getDef("ROC") #dddd
```

- We should select an algorithm which provides a consistent performance (max AUC) across various levels of threshold.

- \textcolor{pink}{Question:} Is it possible that a model perform better below a certain threshold (e.g. 0.20) and worse after that, in comparison to another model
  - Yes, it is possible. That is why we are considering AUC 
  - Instead of only looking at the curve, we calculate AUC and select the model with max. AUC


## Effect of Change in Threshold

```{r 'B36-vs-Cmat-ROC', echo=FALSE, collapse=FALSE, class.output="models"}
# #Class Comparisons
cmat_50 <- confusionMatrix(factor(c("1", "1", "0", "0", "0", "1"), levels = c("0", "1")), 
      reference = factor(c("1", "0", "1", "1", "0", "1"), levels = c("0", "1")), positive = "1")
cmat_00 <- confusionMatrix(factor(c("1", "1", "1", "1", "1", "1"), levels = c("0", "1")), 
      reference = factor(c("1", "0", "1", "1", "0", "1"), levels = c("0", "1")), positive = "1")
cmat_20 <- confusionMatrix(factor(c("1", "1", "1", "1", "0", "1"), levels = c("0", "1")), 
      reference = factor(c("1", "0", "1", "1", "0", "1"), levels = c("0", "1")), positive = "1")
#
cmat <- cmat_50 
hh_names <- c("Accuracy", "N", "True Positive (TP_A)", "False Positive (FP_B)", 
              "False Negative (FN_C)", "True Negative (TN_D) ", 
              names(cmat$byClass))
ii <- c(cmat$overall[["Accuracy"]], sum(cmat$table), 
        cmat$table["1", "1"], cmat$table["1", "0"], 
        cmat$table["0", "1"], cmat$table["0", "0"], round(cmat$byClass, 4))
cmat <- cmat_00
jj <- c(cmat$overall[["Accuracy"]], sum(cmat$table), 
        cmat$table["1", "1"], cmat$table["1", "0"], 
        cmat$table["0", "1"], cmat$table["0", "0"], round(cmat$byClass, 4))
cmat <- cmat_20
kk <- c(cmat$overall[["Accuracy"]], sum(cmat$table), 
        cmat$table["1", "1"], cmat$table["1", "0"], 
        cmat$table["0", "1"], cmat$table["0", "0"], round(cmat$byClass, 4))
#
if(TRUE) tibble(Names = hh_names, 
     p50 = ii, p00 = jj, p20 = kk)
```

## Example 

- Using glm() and rpart()
  - predict.glm() provides posterior probability of event
  - predict.rpart() provides the probabilities of both control and case (no-event and event). One is compliment of the other in case of 'binary' class. Thus, the output is in the form of matrix. So, it needs subset as a vector.

```{r 'B36-Example-glm'}
set.seed(3)
bb <- tibble(income = c(rnorm(n = 500, mean = 500, sd = 50), 
                        rnorm(n = 500, mean = 300, sd = 50)),
             status = factor(c(rep("1", 500), rep("0", 500)), levels = c("0", "1")))
#
str(bb)
#
# #Logistic Regression : glm
mod_1_glm <- glm(formula = status ~ income, family = binomial, data = bb)
prob_1_glm <- predict(mod_1_glm, type = "response")
stopifnot(identical(prob_1_glm, mod_1_glm$fitted.values))
#
res_1_glm <- factor(ifelse(prob_1_glm >= 0.5, "1", "0"), levels = c("0", "1"))
cmat_1_glm <- confusionMatrix(res_1_glm, reference = bb$status, positive = "1")
if(FALSE) cmat_1_glm
if(FALSE) plot(x=bb$income, y = bb$status)
#
```


```{r 'B36-Example-rpart'}
# #Logistic Regression : rpart
mod_2_rpart <- rpart(formula = status ~ income, data = bb, method = 'class')
prob_2_rpart <- predict(mod_2_rpart, type = "prob")
```

```{r 'B36-Example-Forest'}
# #Logistic Regression : randomForest
mod_3_forest <- randomForest(formula = status ~ income, data = bb)
#
# #The fraction can be taken as predicted probabilities for the classes. 
str(mod_3_forest$votes[40:50, ], vec.len = Inf)
str(mod_3_forest$votes[40:50, 2], vec.len = Inf)
if(FALSE) res_3_forest <- predict(mod_3_forest, type = "response")
```

## pROC {.tabset .tabset-fade}

- \textcolor{pink}{roc()}
  - Colors: https://r-graph-gallery.com/ggplot2-color.html
  
### Plot {.unlisted .unnumbered}

```{r 'B36-ROC-01-Set', include=FALSE}
# #Setup for ROC Plot
hh <- roc(response = bb$status, predictor = prob_1_glm, 
    levels = c("0", "1"), direction = "<", plot = FALSE)
glm_hh <- roc(response = bb$status, predictor = prob_1_glm, 
    levels = c("0", "1"), direction = "<", plot = FALSE)
rpart_hh <- roc(response = bb$status, predictor = prob_2_rpart[, 2], 
    levels = c("0", "1"), direction = "<", plot = FALSE)
forest_hh <- roc(response = bb$status, predictor = mod_3_forest$votes[, 2], 
    levels = c("0", "1"), direction = "<", plot = FALSE)
#
cap_hh <- "B36P01"
ttl_hh <- "Example: ROC"
loc_png <- paste0(.z$PX, "B36P01", "-Example-ROC", ".png")
```

```{r 'B36-ROC-01-Plot', include=FALSE, ref.label=c('B36-ROC')}
#
```

```{r 'B36P01', echo=FALSE, fig.cap="(B36P01) Example: ROC"}
include_graphics(paste0(".", "/images/", "B36P01", "-Example-ROC", ".png"))
```

### Code {.unlisted .unnumbered}

```{r 'B36-ROC-01-Set-A', eval=FALSE, ref.label=c('B36-ROC-01-Set')}
#
```

```{r 'B36-ROC', eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh (ROC Plot)
if(!file.exists(loc_png)) {
  png(filename = loc_png)
  #dev.control('enable') 
  #plot(hh, legacy.axes = TRUE, print.auc = TRUE, print.auc.y = 0.60)
  plot(glm_hh, legacy.axes = TRUE, print.auc = TRUE, print.auc.y = 0.60, col = "#5DC863FF")
  plot(forest_hh, print.auc = TRUE, print.auc.y = 0.50, col = "#3B528BFF", add = TRUE)
  plot(rpart_hh, print.auc = TRUE, print.auc.y = 0.40, col = "#440154FF", add = TRUE)
  legend("bottomright", bg ="transparent", 
         legend=c("Logisitic Regression", "Random Forest", "Decision Trees"), 
         col=c("#5DC863FF", "#3B528BFF","#440154FF"), lwd = 1)
  title(main = ttl_hh, line = 3, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B36 <- recordPlot()
  dev.off()
  assign(cap_hh, B36)
  rm(B36)
  #eval(parse(text = cap_hh))
}
```


## Bank: Logistic Regression : Revision

- The model has low sensitivity because the data is imbalanced


```{r 'B36-Imbalance-Bank-1'}
# #Data has imbalance
xbank %>% count(y) %>% mutate(PROP = n /sum(n))
```

```{r 'B36-Logistic-Bank-1-Show', eval=FALSE}
# #Logistic Regression 
mod_1_glm_bank <- glm(formula = y ~ ., family = binomial, data = xbank)
# #Step-wise Regression
mod_2_step_bank <- step(mod_1_glm_bank, direction = "both", trace = 0)
```

```{r 'B36-Logistic-Bank-1', include=FALSE}
# #Logistic Regression #Reduce the Build Time #Copied Data & Model from B35
loc_rds1 <- here("data", "zzB35-mod_1_glm_bank.rds")
loc_rds2 <- here("data", "zzB35-mod_2_step_bank.rds")
if(q_fastbuild && file.exists(loc_rds1) && file.exists(loc_rds2)){
  mod_1_glm_bank <- readRDS(loc_rds1)
  mod_2_step_bank <- readRDS(loc_rds2)
} else {
  mod_1_glm_bank <- glm(formula = y ~ ., family = binomial, data = xbank)
  mod_2_step_bank <- step(mod_1_glm_bank, direction = "both", trace = 0)
  saveRDS(mod_1_glm_bank, loc_rds1)
  saveRDS(mod_2_step_bank, loc_rds2)
}
```

```{r 'B36-Print-Bank-1', echo=FALSE, collapse=FALSE, class.output="models", eval=FALSE}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Logistic Regression: Bank: Y ~ X"
col_hh <- c("GLM", "Stepwise")
#
stargazer(mod_1_glm_bank, mod_2_step_bank, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```

```{r 'B36-Cmat-Bank-1'}
prob_2_step_bank_train <- predict(mod_2_step_bank, type = "response")
prob_2_step_bank_test <- predict(mod_2_step_bank, newdata = test_bank, type = "response")
#
# #Convert Probabilities into levels
res_2_step_bank_train <- factor(ifelse(prob_2_step_bank_train >= 0.5, "yes", "no"), 
                                levels = c("no", "yes"))
res_2_step_bank_test <- factor(ifelse(prob_2_step_bank_test >= 0.5, "yes", "no"), 
                               levels = c("no", "yes"))
#
# #Confusion Matrix 
cmat_2_step_bank_train <- confusionMatrix(res_2_step_bank_train, 
                                          reference = xbank$y, positive = "yes")
cmat_2_step_bank_test <- confusionMatrix(res_2_step_bank_test, 
                                         reference = test_bank$y, positive = "yes")
```


```{r 'B36-vs-Cmat-Bank', echo=FALSE, collapse=FALSE, class.output="models"}
# #Class Comparisons
cmat <- cmat_2_step_bank_train
hh_names <- c("Accuracy", "N", "True Positive (TP_A)", "False Positive (FP_B)", 
              "False Negative (FN_C)", "True Negative (TN_D) ", 
              names(cmat$byClass))
ii <- c(cmat$overall[["Accuracy"]], round(sum(cmat$table), 0), 
        cmat$table["yes", "yes"], cmat$table["yes", "no"], 
        cmat$table["no", "yes"], cmat$table["no", "no"], round(cmat$byClass, 4))
cmat <- cmat_2_step_bank_test
jj <- c(cmat$overall[["Accuracy"]], sum(cmat$table), 
        cmat$table["yes", "yes"], cmat$table["yes", "no"], 
        cmat$table["no", "yes"], cmat$table["no", "no"], round(cmat$byClass, 4))
#
if(TRUE) tibble(Names = hh_names, 
     TRAIN = ii, TEST = jj)
```


## Package: ROSE {.tabset .tabset-fade}

- ROSE contains functions to deal with binary classification problems in the presence of imbalanced classes. Synthetic balanced samples are generated.
  - \textcolor{pink}{ovun.sample()}
    - Oversampling, Under-sampling or both can be applied
    - Balancing is required only for the Train data, not the Test data


- \textcolor{pink}{Question:} Why do we need to specify N (The desired sample size of the resulting data set). The dataset itself contains the count of major and minor classes. 
  - We can mention any number but it would not match with our concept

- \textcolor{pink}{Question:} How would we select one of the three methods
  - We would apply all the three methods and then check AUC values of the models and then make a selection
  - (Aside) Or we can simply go with 'both' as a good starting point
  - "ForLater" Is it a good practice to compare these 3 models because the underlying datasets have been changed effectively. As can be seen by change in list of variables that are considered to be significant

- Dataset Bank - 
  - Using different methods of balancing leads to some variables to be identified as significant which were discarded earlier

### ovun.sample() {.unlisted .unnumbered}

```{r 'B36-Balancing-Bank'}
# #Data has imbalance
xbank %>% count(y) %>% mutate(PROP = n /sum(n))
# #
over_xbank <- ovun.sample(formula = y ~ ., data = xbank, method = "over", seed = 3)$data
over_xbank %>% count(y) %>% mutate(PROP = n /sum(n))
#
under_xbank <- ovun.sample(formula = y ~ ., data = xbank, method = "under", seed = 3)$data
under_xbank %>% count(y) %>% mutate(PROP = n /sum(n))
#
both_xbank <- ovun.sample(formula = y ~ ., data = xbank, method = "both", seed = 3)$data
both_xbank %>% count(y) %>% mutate(PROP = n /sum(n))
```

```{r 'B36-Logistic-Bank-2-Show', eval=FALSE}
# #Logistic Regression 
mod_3_glm_over  <- glm(formula = y ~ ., family = binomial, data = over_xbank)
mod_3_step_over <- step(mod_3_glm_over, direction = "both", trace = 0)
mod_4_glm_under <- glm(formula = y ~ ., family = binomial, data = under_xbank)
mod_4_step_under <- step(mod_4_glm_under, direction = "both", trace = 0)
mod_5_glm_both  <- glm(formula = y ~ ., family = binomial, data = both_xbank)
mod_5_step_both <- step(mod_5_glm_both, direction = "both", trace = 0)
rm(mod_3_glm_over, mod_4_glm_under, mod_5_glm_both)
```

```{r 'B36-Logistic-Bank-2', include=FALSE}
# #Logistic Regression #Reduce the Build Time
loc_rds <- here("data", "zzB36-mod_3_step_over.rds")
if(q_fastbuild && file.exists(loc_rds)){
  mod_3_step_over <- readRDS(loc_rds)
} else {
  mod_3_glm_over <- glm(formula = y ~ ., family = binomial, data = over_xbank)
  mod_3_step_over <- step(mod_3_glm_over, direction = "both", trace = 0)
  saveRDS(mod_3_step_over, loc_rds)
  rm(mod_3_glm_over)
}
loc_rds <- here("data", "zzB36-mod_4_step_under.rds")
if(q_fastbuild && file.exists(loc_rds)){
  mod_4_step_under <- readRDS(loc_rds)
} else {
  mod_4_glm_under <- glm(formula = y ~ ., family = binomial, data = under_xbank)
  mod_4_step_under <- step(mod_4_glm_under, direction = "both", trace = 0)
  saveRDS(mod_4_step_under, loc_rds)
  rm(mod_4_glm_under)
}
loc_rds <- here("data", "zzB36-mod_5_step_both.rds")
if(q_fastbuild && file.exists(loc_rds)){
  mod_5_step_both <- readRDS(loc_rds)
} else {
  mod_5_glm_both <- glm(formula = y ~ ., family = binomial, data = both_xbank)
  mod_5_step_both <- step(mod_5_glm_both, direction = "both", trace = 0)
  saveRDS(mod_5_step_both, loc_rds)
  rm(mod_5_glm_both)
}
```

```{r 'B36-Print-Bank-2', echo=FALSE, collapse=FALSE, class.output="models"}
# #Coefficient Estimates, RSE, R2, F-statistic, Significance, DF
ttl_hh <- "Logistic Regression (Stepwise): Bank: Y ~ X"
col_hh <- c("Imbalanced", "Oversampling", "Undersampling", "Both")
#
stargazer(mod_2_step_bank, mod_3_step_over, mod_4_step_under, mod_5_step_both, 
    title = ttl_hh, column.labels = col_hh, model.numbers = FALSE, df = FALSE, report = "vc*", 
    type = "text", single.row = TRUE, intercept.bottom = FALSE, dep.var.caption = "", digits = 4)
```

```{r 'B36-Cmat-Bank-Over', include=FALSE}
# #Over
prob_3_over_train <- predict(mod_3_step_over, type = "response")
prob_3_over_test <- predict(mod_3_step_over, newdata = test_bank, type = "response")
#
# #Convert Probabilities into levels
res_3_over_train <- factor(ifelse(prob_3_over_train >= 0.5, "yes", "no"), levels = c("no", "yes"))
res_3_over_test <- factor(ifelse(prob_3_over_test >= 0.5, "yes", "no"), levels = c("no", "yes"))
#
# #Confusion Matrix 
cmat_3_over_train <- confusionMatrix(res_3_over_train, reference = over_xbank$y, positive = "yes")
cmat_3_over_test <- confusionMatrix(res_3_over_test, reference = test_bank$y, positive = "yes")
```

```{r 'B36-Cmat-Bank-Under', include=FALSE}
# #Under
prob_4_under_train <- predict(mod_4_step_under, type = "response")
prob_4_under_test <- predict(mod_4_step_under, newdata = test_bank, type = "response")
#
# #Convert Probabilities into levels
res_4_under_train <- factor(ifelse(prob_4_under_train >= 0.5, "yes", "no"), 
                            levels = c("no", "yes"))
res_4_under_test <- factor(ifelse(prob_4_under_test >= 0.5, "yes", "no"), 
                           levels = c("no", "yes"))
#
# #Confusion Matrix 
cmat_4_under_train <- confusionMatrix(res_4_under_train, 
                                      reference = under_xbank$y, positive = "yes")
cmat_4_under_test <- confusionMatrix(res_4_under_test, 
                                     reference = test_bank$y, positive = "yes")
```

```{r 'B36-Cmat-Bank-Both', include=FALSE}
# #Both
prob_5_both_train <- predict(mod_5_step_both, type = "response")
prob_5_both_test <- predict(mod_5_step_both, newdata = test_bank, type = "response")
#
# #Convert Probabilities into levels
res_5_both_train <- factor(ifelse(prob_5_both_train >= 0.5, "yes", "no"), 
                           levels = c("no", "yes"))
res_5_both_test <- factor(ifelse(prob_5_both_test >= 0.5, "yes", "no"), 
                          levels = c("no", "yes"))
#
# #Confusion Matrix 
cmat_5_both_train <- confusionMatrix(res_5_both_train, 
                                     reference = both_xbank$y, positive = "yes")
cmat_5_both_test <- confusionMatrix(res_5_both_test, 
                                    reference = test_bank$y, positive = "yes")
```


```{r 'B36-vs-Cmat-Bank-2', echo=FALSE, collapse=FALSE, class.output="models"}
# #Class Comparisons
cmat <- cmat_2_step_bank_train
hh_names <- c("Accuracy", "N", "True Positive (TP_A)", "False Positive (FP_B)", 
              "False Negative (FN_C)", "True Negative (TN_D) ", 
              names(cmat$byClass))
iio <- c(cmat$overall[["Accuracy"]], round(sum(cmat$table), 0), 
        cmat$table["yes", "yes"], cmat$table["yes", "no"], 
        cmat$table["no", "yes"], cmat$table["no", "no"], round(cmat$byClass, 4))
cmat <- cmat_2_step_bank_test
iin <- c(cmat$overall[["Accuracy"]], sum(cmat$table), 
        cmat$table["yes", "yes"], cmat$table["yes", "no"], 
        cmat$table["no", "yes"], cmat$table["no", "no"], round(cmat$byClass, 4))
cmat <- cmat_3_over_train
jjo <- c(cmat$overall[["Accuracy"]], round(sum(cmat$table), 0), 
        cmat$table["yes", "yes"], cmat$table["yes", "no"], 
        cmat$table["no", "yes"], cmat$table["no", "no"], round(cmat$byClass, 4))
cmat <- cmat_3_over_test
jjn <- c(cmat$overall[["Accuracy"]], sum(cmat$table), 
        cmat$table["yes", "yes"], cmat$table["yes", "no"], 
        cmat$table["no", "yes"], cmat$table["no", "no"], round(cmat$byClass, 4))
cmat <- cmat_4_under_train
kko <- c(cmat$overall[["Accuracy"]], round(sum(cmat$table), 0), 
        cmat$table["yes", "yes"], cmat$table["yes", "no"], 
        cmat$table["no", "yes"], cmat$table["no", "no"], round(cmat$byClass, 4))
cmat <- cmat_4_under_test
kkn <- c(cmat$overall[["Accuracy"]], sum(cmat$table), 
        cmat$table["yes", "yes"], cmat$table["yes", "no"], 
        cmat$table["no", "yes"], cmat$table["no", "no"], round(cmat$byClass, 4))
cmat <- cmat_5_both_train
llo <- c(cmat$overall[["Accuracy"]], round(sum(cmat$table), 0), 
        cmat$table["yes", "yes"], cmat$table["yes", "no"], 
        cmat$table["no", "yes"], cmat$table["no", "no"], round(cmat$byClass, 4))
cmat <- cmat_5_both_test
lln <- c(cmat$overall[["Accuracy"]], sum(cmat$table), 
        cmat$table["yes", "yes"], cmat$table["yes", "no"], 
        cmat$table["no", "yes"], cmat$table["no", "no"], round(cmat$byClass, 4))
#
if(TRUE) tibble(Names = hh_names, 
     Basic = iio, Basic_Test = iin, 
     Over = jjo, Over_Test = jjn, 
     Under = kko, Under_Test = kkn, 
     Both = llo, Both_Test = lln)
```

### Confusion Matrices {.unlisted .unnumbered}

```{r 'B36-Cmat-Bank-ALL', eval=FALSE, ref.label=c('B36-Cmat-Bank-Over', 'B36-Cmat-Bank-Under', 'B36-Cmat-Bank-Both')}
#
```


## Bank: ROC {.tabset .tabset-fade}

### Plot {.unlisted .unnumbered}

```{r 'B36-ROC-02-Set', include=FALSE}
# #Setup for ROC Plot of 4: Base, Over, Under, Both
hh <- roc(response = xbank$y, predictor = prob_2_step_bank_train, 
    levels = c("no", "yes"), direction = "<", plot = FALSE)
over_hh <- roc(response = over_xbank$y, predictor = prob_3_over_train, 
    levels = c("no", "yes"), direction = "<", plot = FALSE)
under_hh <- roc(response = under_xbank$y, predictor = prob_4_under_train, 
    levels = c("no", "yes"), direction = "<", plot = FALSE)
both_hh <- roc(response = both_xbank$y, predictor = prob_5_both_train, 
    levels = c("no", "yes"), direction = "<", plot = FALSE)
#
cap_hh <- "B36P02"
ttl_hh <- "Bank: ROC: Train"
loc_png <- paste0(.z$PX, "B36P02", "-Bank-Train-ROC", ".png")
auc_hh <- seq(from = 0.15, by = -0.04, length.out = 4)
```

```{r 'B36-ROC-02-Plot', include=FALSE, ref.label=c('B36-ROC-Four')}
#
```

```{r 'B36P02', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
include_graphics(paste0(".", "/images/", "B36P02", "-Bank-Train-ROC", ".png"))
```

```{r 'B36-ROC-03-Set', include=FALSE}
# #Setup for ROC Plot of 4: Base, Over, Under, Both
hh <- roc(response = test_bank$y, predictor = prob_2_step_bank_test, 
    levels = c("no", "yes"), direction = "<", plot = FALSE)
over_hh <- roc(response = test_bank$y, predictor = prob_3_over_test, 
    levels = c("no", "yes"), direction = "<", plot = FALSE)
under_hh <- roc(response = test_bank$y, predictor = prob_4_under_test, 
    levels = c("no", "yes"), direction = "<", plot = FALSE)
both_hh <- roc(response = test_bank$y, predictor = prob_5_both_test, 
    levels = c("no", "yes"), direction = "<", plot = FALSE)
#
cap_hh <- "B36P03"
ttl_hh <- "Bank: ROC: Test"
loc_png <- paste0(.z$PX, "B36P03", "-Bank-Test-ROC", ".png")
auc_hh <- seq(from = 0.15, by = -0.04, length.out = 4)
```

```{r 'B36-ROC-03-Plot', include=FALSE, ref.label=c('B36-ROC-Four')}
#
```

```{r 'B36P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
include_graphics(paste0(".", "/images/", "B36P03", "-Bank-Test-ROC", ".png"))
```

```{r 'B36P0203', echo=FALSE, ref.label=c('B36P02', 'B36P03'), fig.cap="(B36P02 B36P03) Bank: Train & Test"}
#
```

### Code {.unlisted .unnumbered}

```{r 'B36-ROC-02-Set-A', eval=FALSE, ref.label=c('B36-ROC-02-Set')}
#
```

```{r 'B36-ROC-Four', eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh (ROC Plot), over_hh, under_hh, both_hh, auc_hh
if(!file.exists(loc_png)) {
  png(filename = loc_png)
  #dev.control('enable') 
  plot(hh, legacy.axes = TRUE, print.auc = TRUE, print.auc.y = auc_hh[1], col = "#FDE725FF")
  plot(over_hh, print.auc = TRUE, print.auc.y = auc_hh[2], col = "#5DC863FF", add = TRUE)
  plot(under_hh, print.auc = TRUE, print.auc.y = auc_hh[3], col = "#3B528BFF", add = TRUE)
  plot(both_hh, print.auc = TRUE, print.auc.y = auc_hh[4], col = "#440154FF", add = TRUE)
  legend("bottomright", bg ="transparent", 
         legend=c("Imbalanced", "Oversampling", "Undersampling", "Both"), 
         col=c("#FDE725FF", "#5DC863FF", "#3B528BFF","#440154FF"), lwd = 1)
  title(main = ttl_hh, line = 3, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B36 <- recordPlot()
  dev.off()
  assign(cap_hh, B36)
  rm(B36)
  #eval(parse(text = cap_hh))
}
```



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B36-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, auc_hh, both_hh, both_xbank, cap_hh, cmat, cmat_00, cmat_1_glm,
          cmat_2_step_bank_test, cmat_2_step_bank_train, cmat_20, cmat_3_over_test, 
          cmat_3_over_train, cmat_4_under_test, cmat_4_under_train, cmat_5_both_test, 
          cmat_5_both_train, cmat_50, col_hh, dum_xfw, forest_hh, glm_hh, hh, hh_names, idx_xsyw, 
          iin, iio, jjn, jjo, kkn, kko, lln, llo, loc_png, mod_1_glm, mod_1_glm_bank, mod_2_rpart,
          mod_2_step_bank, mod_3_forest, mod_3_glm_over, mod_3_step_over, mod_4_glm_under,
          mod_4_step_under, mod_5_glm_both, mod_5_step_both, over_hh, over_xbank, prob_1_glm,
          prob_2_rpart, prob_2_step_bank_test, prob_2_step_bank_train, prob_3_over_test,
          prob_3_over_train, prob_4_under_test, prob_4_under_train, prob_5_both_test, 
          prob_5_both_train, res_1_glm, res_2_step_bank_test, res_2_step_bank_train, 
          res_3_over_test, res_3_over_train, res_4_under_test, res_4_under_train, res_5_both_test,
          res_5_both_train, rpart_hh, test_bank, test_xfw, train_xfw, ttl_hh, under_hh, 
          under_xbank, xbank, zzB34Bank, loc_rds1, loc_rds2)
```

```{r 'B36-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
