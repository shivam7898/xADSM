# Decision Tree Algorithm (B33, Feb-20) {#b33}

```{r 'B33', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

> "Incomplete"

## Packages

```{r 'B33-Installations', include=FALSE, eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("phangorn", dependencies = TRUE)
  install.packages("ape", dependencies = TRUE)
  install.packages("ROSE", dependencies = TRUE)
}
```


## Review {.tabset .tabset-fade}

- \textcolor{pink}{Question:} In regression, we had to check for normality and multicollinearity (VIF). The KC House data was not normal, so did semi-log transformation and then log-log transformation. Does that not affect model performance in Decision Trees
  - Tree does not assume Normality so there is no requirement of transformation
  - (Aside) 
    - Interaction effect is already assumed in Trees
    - Probably outliers would have low cp value and would be dropped 
    - But some outliers are actually important observations
    - If those have more than 3 z-value, model might drop them

- $\text{\{full, overfit\} } 0 \leq \text{Complexity Parameter (cp)} \leq 1 \text{\{pruned, underfit\}}$
  - A Node would be split if the information gain is more than the cp value, otherwise it would remain as leaf node.

- \textcolor{pink}{Question:} At each split why the split is happening with one variable. Why not more than one variable at same time OR why not split by some other variable 
  - Split is sequential and recursive
  - (Aside)
    - Sequential (step by step, not in one step) : At each step, all variables are evaluated for split and the one with maximum reduction in Gini is selected as the best candidate
    - Recursive : At each subsequent step, again, all variables are evaluated for split and the one with maximum reduction in Gini is selected as the best candidate 
  - (Clarification) If the split happened on variable 'Age', Is it ('Age') is avaiable for further splits in susequent steps 
    - Yes (If the 'cp' criteria allows it)
  - (Clarification) Why the split is happening by 'Age' and not by 'Transmission'
    - On that step 'Age' provides maximum information gain (maximum reduction in Gini) so the split happens on this variable.

### Data {.unlisted .unnumbered}

- [Import Data CarDekho - B26](#set-cardekho-b26 "b26")
- "ForLater" - train() does not take method = 'anova' or 'class' for 'rpart'. (Probably) it handles them based on class of Y but How to perform both explicitly. 

### Previous Lecture Code {.unlisted .unnumbered}

```{r 'B33-GetCarDekho', ref.label=c('B26-GetCarDekho', 'B26-PrepCar', 'B31-Dummies-Car')}
# #xxB26CarDekho, aa, bb, xsyw, xw,xsw, zfw, xnw, znw, 
# #train_xfw, test_xfw, train_xfw_ii, test_xfw_ii
```

```{r 'B33-GetCarDekhoKnit', include=FALSE, eval=TRUE}
xxB26CarDekho <- f_getRDS(xxB26CarDekho)
aa <- xxB26CarDekho
bb <- aa %>% 
  separate(name, c("brand", NA), sep = " ", remove = FALSE, extra = "drop") %>% 
  filter(fuel != "Electric") %>% 
  #mutate(across(where(is.character), factor)) %>% 
  mutate(across(fuel, factor, levels = c("Diesel", "Petrol", "CNG", "LPG"))) %>% 
  mutate(across(transmission, factor, levels = c("Manual", "Automatic"), 
                labels = c("Manual", "Auto"))) %>% 
  mutate(across(owner, factor, 
levels = c("First Owner", "Second Owner", "Third Owner", "Fourth & Above Owner", "Test Drive Car"), 
labels = c("I", "II", "III", "More", "Test"))) %>% 
  mutate(across(seller_type, factor, levels = c("Individual", "Dealer", "Trustmark Dealer"), 
                labels = c("Indiv", "Dealer", "mDealer"))) %>% 
  rename(price = selling_price, km = km_driven, 
         s = seller_type, o = owner, t = transmission, f = fuel) %>% 
  mutate(age = 2022 - year) %>% 
  select(-c(year, name, brand))
# 
xfw <- bb
zfw <- xfw %>% mutate(across(where(is.numeric), ~ as.vector(scale(.)))) 
xnw <- xfw %>% select(where(is.numeric))
znw <- zfw %>% select(where(is.numeric))
#
# #Dummy | Drop First Level i.e. Reference | Drop Selected Columns i.e. Original |
dum_xfw <- xfw %>% dummy_cols(.data = ., 
                  select_columns = c("f", "s", "t", "o"), 
                  remove_first_dummy = TRUE, remove_selected_columns = TRUE)
#
# #Partition Data
set.seed(3)
idx_xfw <- sample.int(n = nrow(dum_xfw), size = floor(0.8 * nrow(dum_xfw)), replace = FALSE)
train_xfw <- dum_xfw[idx_xfw, ]
test_xfw  <- dum_xfw[-idx_xfw, ]
#
# #Decision Trees: Convert Dummy Variables to Factor Variables of two levels "0", "1"
train_xfw_ii <- train_xfw %>% mutate(across(starts_with(c("s_", "f_", "t_", "o_")), factor))
test_xfw_ii  <- test_xfw  %>% mutate(across(starts_with(c("s_", "f_", "t_", "o_")), factor))
```

```{r 'B33-ModelControl'}
cv <- trainControl(method = "repeatedcv", number = 10, repeats = 5, allowParallel = TRUE)
```

### Trees {.unlisted .unnumbered}

```{r 'B33-CV-Car'}
set.seed(3)
# #Basic Regression Tree using Best Tuning Cp Value
mod_tree_anv <- rpart(price ~ ., data = train_xfw_ii, method = 'anova', cp = 0.008528435)
#
# #Formula Interface (Does not match with Basic Regression Tree because of Factors)
mod_cv_form <- suppressWarnings(train(price ~ ., data = train_xfw_ii, 
                                      method = 'rpart', trControl = cv, tuneLength = 10))
#
# #With Non-Formula Interface: Use this if Factors are Present
mod_cv_nonf <- suppressWarnings(train(y = train_xfw_ii$price, 
                                      x = as.data.frame(train_xfw_ii[ , -1]), 
                                      method = 'rpart', trControl = cv, tuneLength = 10))
#
# #Using Formula Interface even with Factors present needs formula() & model.frame()
formula_xfw <- formula(price ~ .)
mod_cv_frame <- suppressWarnings(train(y = model.frame(formula_xfw, train_xfw_ii)[ , 1], 
                                       x = model.frame(formula_xfw, train_xfw_ii)[ , -1], 
                                       method = 'rpart', trControl = cv, tuneLength = 10))
# #Best Tuning Cp value
if(FALSE) mod_cv_frame$bestTune
#
# #Best Model
if(FALSE) mod_cv_frame$finalModel
```

### Simple {.unlisted .unnumbered}

```{r 'B33-PrintTree-Simple-1'}
# #Printing Tree
ii <- as.party(mod_tree_anv)
if(TRUE) length(ii)
if(TRUE) width(ii)
if(TRUE) depth(ii)
if(TRUE) ii
```

### CrossValidated {.unlisted .unnumbered}

```{r 'B33-PrintTree-CV'}
# #Printing Tree
ii <- as.party(mod_cv_frame$finalModel)
if(TRUE) length(ii)
if(TRUE) width(ii)
if(TRUE) depth(ii)
if(TRUE) ii
```

### Factor Problem {.unlisted .unnumbered}

- The problem is that the already converted dummy variables have been further treated as factors with levels and recoded again
  - t_Auto1 itself is a problem and then it is being treated as continuous i.e. split is at 0.5

```{r 'B33-PrintTree-Simple-2'}
# #Printing Tree
ii <- as.party(mod_cv_form$finalModel)
if(TRUE) length(ii)
if(TRUE) width(ii)
if(TRUE) depth(ii)
if(TRUE) ii
```

### Non-Formula {.unlisted .unnumbered}

```{r 'B33-PrintTree-Simple-3'}
# #Printing Tree
ii <- as.party(mod_cv_nonf$finalModel)
if(TRUE) length(ii)
if(TRUE) width(ii)
if(TRUE) depth(ii)
if(TRUE) ii
```



## x

```{r 'B33-Predict-cv-Car', eval=FALSE}
# #Model Validation
# #Class of Both Models are different. rpart() require "vector" but "raw" is used with train()
class(mod_tree)
class(mod_cv)
#
#res_tree <- predict(mod_cv, test_xfw_ii, type  = "raw") 
res_cv <- test_xfw_ii %>% 
  mutate(CalY = predict(mod_cv, ., type = "raw"), Y_Yc = price - CalY, MAPE = Y_Yc / price)
#
ii <- res_cv
summary(ii$Y_Yc)
#
# #RMSE: Root Mean Squared Error
sqrt(mean((ii$Y_Yc)^2))
#
# #MAE: Mean Absolute Error (MAE)
mean(abs(ii$Y_Yc))
#
# #MAPE: Mean Absolute Percentage Error
mean(abs(ii$MAPE))
#
# #Accuracy 
1 - mean(abs(ii$MAPE))
```

## WIP

## Plots

```{r 'B33-PrintTree', include=TRUE, eval=FALSE}
# #Printing Tree
ii <- as.party(hh)
length(ii)
width(ii)
depth(ii)
ii
```

```{r 'B33-Tree-rpart', include=TRUE, eval=FALSE}
# #IN: cap_hh, ttl_hh, loc_png, hh <- mod_tree
#
if(!file.exists(loc_png)|TRUE) {#xxxx
  png(filename = loc_png) 
  dev.control('enable') 
  rpart.plot(hh, extra = 'auto')
  title(main = ttl_hh, line = 2, adj = 0)
  title(sub = cap_hh, line = 4, adj = 1)
  B31 <- recordPlot()
  dev.off()
  assign(cap_hh, B31)
  rm(B31)
}
eval(parse(text = cap_hh))
```


```{r 'B33-Tree-KC-Cp-Set', include=FALSE, eval=FALSE}
# #3 models here
#mod_tree, mod_tree_cp, mod_cv
hh <- mod_tree_cp
#
cap_hh <- "B33P01"
ttl_hh <- "Car: Cp"
loc_png <- paste0(.z$PX, "B33P01", "-Car-cp", ".png")
```

```{r 'B33P03-Save', include=FALSE, ref.label=c('B33-Tree-rpart'), eval=FALSE}
#
```

## Ensemble Methods

- Collection of Models

## Bagging

## Random Forest

```{r 'B33-Forest', eval=FALSE}
# Random Forest is Horribly Slow. Keep it in separate chunk.
# #ntrees = 100, sampsize = 1000, mtry = 4, nodesize = 10 : Var 61.17
# #ntrees = 200, sampsize = 1000, mtry = 4, nodesize = 10 : Var 61.07
# #ntrees = 100, sampsize = 1000, mtry = 8, nodesize = 10 : Var 62.33
# #ntrees = 200, sampsize = 1000, mtry = 8, nodesize = 10 : Var 62.64
# #ntrees = 500, sampsize = 1000, mtry = 8, nodesize = 10 : Var 62.3
# #ntrees = 1000, sampsize = 1000, mtry = 8, nodesize = 10: Var 62.37
# #ntrees = 500, sampsize = 1000, mtry = 10, nodesize = 10: Var 62.37
# #ntrees = 500, sampsize = 1000, mtry = 10, nodesize = 5: 63.52
# #ntrees = 500, sampsize = 1000, mtry = 10, nodesize = 3: 64.39
# #ntrees = 500, sampsize = 1000, mtry = 10, nodesize = 2: 64.53
# #ntrees = 500, sampsize = 2000, mtry = 10, nodesize = 2: 64.88
# #ntrees = 500, sampsize = 3000, mtry = 10, nodesize = 2: 64.41
mod_frst <- randomForest(price ~ ., data = train_xfw_ii, 
                         ntrees = 500, sampsize = 3000, mtry = 10, nodesize = 2)
```

```{r 'B33-Print-Forest', eval=FALSE}
print(mod_frst)
```

## Tune RF

- step factor : how many features it should step through

```{r 'B33-TuneModel', eval=FALSE}
# 1: 4
# 2: 4, 8, 13
# 3: 2, 4, 12
tuneRF(x = train_xfw_ii[, -1], y = train_xfw_ii$price, stepFactor = 10, plot = TRUE, trace = TRUE, 
       ntreeTry = 10, improve = .05)
```



## x



## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B33-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, cv, dum_xfw, formula_xfw, idx_xfw, mod_cv_form, mod_cv_frame,
          mod_cv_nonf, mod_tree_anv, test_xfw, test_xfw_ii, train_xfw, train_xfw_ii, xfw, xnw,
          xxB26CarDekho, zfw, znw)
```

```{r 'B33-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
