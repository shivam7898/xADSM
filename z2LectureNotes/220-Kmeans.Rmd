# K-means (B20, Nov-21) {#b20}

```{r 'B20', include=FALSE, cache=FALSE}
sys.source(paste0(.z$RX, "A99Knitr", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "000Packages", ".R"), envir = knitr::knit_global())
sys.source(paste0(.z$RX, "A00AllUDF", ".R"), envir = knitr::knit_global())
#invisible(lapply(f_getPathR(A09isPrime), knitr::read_chunk))
```

## Overview

- "K-means Cluster Analysis"
  - "ForLater" - The PPT shared in the class was corrupted, need a working file.
  - Refer [Book Merged Here](#c49 "c49")

## Packages

```{r 'B20-Installations', eval=FALSE}
if(FALSE){# #WARNING: Installation may take some time.
  install.packages("factoextra", dependencies = TRUE)
}
```

## Clustering

```{r 'B20D01', comment="", echo=FALSE, results='asis'}
f_getDef("Clustering")
```

```{r 'B20D02', comment="", echo=FALSE, results='asis'}
f_getDef("Cluster")
```

- Clustering is an unsupervised learning technique.

## k-means Clustering

- \textcolor{pink}{Question:} Is it same as k-nearest neighbour
  - No, knn is a classification technique and is supervised. k-means is clustering method and is unsupervised.

- Clustering algorithms seek to construct clusters of records such that the between-cluster variation is large compared to the within-cluster variation.
- The variables needs to be scaled before the eculidean distance can be calculated to identify clusters
- Outliers are also a problem. Normalisation does not help with outliers
- The k-means algorithm can be applied only when the mean of cluster is defined.
  - Thus, the limitation is that we cannot apply k-means to categorical values


```{r 'B20D03', comment="", echo=FALSE, results='asis'}
f_getDef("Euclidean-Distance")
```

- \textcolor{pink}{Question:} Should we need to ensure that in each cluster number of datapoints remain same
  - No

## Algorithm {#k-means-b20}

1. Ask the user how many clusters ${k}$ the data set should be partitioned into. 
    - Ex: $k = 3$
1. Randomly assign ${k}$ records to be the initial cluster center locations. 
1. For each record, find the nearest cluster center. 
    - Thus, in a sense, each cluster center "owns" a subset of the records, thereby representing a partition of the data set. 
    - We therefore have ${k}$ clusters, $\{C_1, C_2, \ldots, C_k\}$
      - Ex: ${C_1 = (3, 9), C_2 = (7, 12), C_3 = (6, 18)}$
    - The "nearest" criterion is usually Euclidean distance
1. For each of the ${k}$ clusters, find the cluster \textcolor{pink}{centroid}, and update the location of each cluster center to the new value of the centroid. 
    - Obviously, centroid location need not to be an actual point within data like mean of a set of values need not to exist within that set itself.
1. Repeat 3-5, until convergence or termination.
    - The algorithm terminates when the centroids no longer change. 
      - In other words, the algorithm terminates when for all clusters $\{C_1, C_2, \ldots, C_k\}$, all the records "owned" by each cluster center remain in that cluster. 
    - Alternatively, the algorithm may terminate when some convergence criterion is met, such as no significant shrinkage in the \textcolor{pink}{mean squared error} $\text{MSE} = \frac{\text{SSE}}{N - k}$, where SSE represents the \textcolor{pink}{sum of squares error}.

- \textcolor{pink}{Question:} Does the number of iterations is a function of initial random number
  - Yes
- \textcolor{pink}{Question:} Would we all, with different initial random number, reach the same cluster solutions
  - Yes

## How many k

- What is the number which would be practically feasible and statistically feasible too
  - 'k' should be the 'best guess' on the number of clusters present in the given data.
    - However, we may not have any idea about the possible number of clusters for high dimensional data and for data that is not scatterplotted 
    - There is NO principled way to know what the value of 'k' ought to be.
    - We may try with successive values of 'k' starting with 2.
  - Within Cluster Sum of Squares \textcolor{pink}{(WSQ)} represents within cluster variation i.e. inside cluster homogeneity.
    - we are expecting low value of WSQ (or MSE or SSE)
  - Between Cluster Sum of Sqares \textcolor{pink}{(BSQ)} represents between cluster variation i.e. between cluster heterogeneity
    - we are expection high value of BSQ (or MSB or SSB)
  - SSE vs. k- looks like Scree Plot and Elbow method can be used to identify the optimum number of k

- The iterative process is stopped when two consecutive 'k' values produce more or less identical results in terms of cluster within and between variances
  - However, it is possible that this 'k' value represents a local minima and not the global minima.

## Data Movies {#set-movies-b20 .tabset .tabset-fade}

\textcolor{pink}{Please import the "B20-movie.csv".}

```{r 'B20-Movies', include=FALSE, eval=FALSE}
# #Path of Object, FileName and MD5Sum tools::md5sum(paste0(.z$XL, "B20-movie.csv"))
xxB20Movies <- f_getObject("xxB20Movies", "B20-movie.csv",
                                "b59d4b0487ce4820ef0893e21b34b417")
```

```{r 'B20-GetMovies', include=FALSE}
xxB20Movies <- f_getRDS(xxB20Movies)
```

- About: [291, 6]
  - Each Row represents a unique customer and the average scores they have given to different movie genres
  - Normalisation
    - (Aside) Normalisation has been done here. However, this data is average rating on a 1-100 scale. So, (probably) it actually does not need the normalisation. "ForLater"

### EDA {.unlisted .unnumbered}

- Refer [Seed for Random Number Generation](#seed-b16 "b16")

```{r 'B20-PrepMovie'}
bb <- aa <- xxB20Movies
# #Drop ID | Scale | 
xw <- aa %>% select(-1) 
zw <- xw %>% mutate(across(everything(), ~ as.vector(scale(.))))
#
summary(xw)
```

## k-means 

- Because I have chosen a different seed than the professor, my algorithm converged through different iterations but to the same clusters. However cluster 1 and cluster 2 got interchanged in the process.
  - In lecture the cluster 1 is cluster 2 here (size 218) and vice-versa.

```{r 'B20-kmeans'}
# #Fix Seed
set.seed(3)
# #Cluster analysis with different k = {2, 3, 4}
k2_zw <- kmeans(zw, centers = 2)
k3_zw <- kmeans(zw, centers = 3)
k4_zw <- kmeans(zw, centers = 4)
#
names(k2_zw)
#
# #Two Clusters
ii <- k2_zw
# #within-cluster sum of squares (Preferred lower value i.e. Homogeneity within cluster)
ii$withinss  
identical(ii$tot.withinss, sum(ii$withinss))
# #between-cluster sum of squares
ii$betweenss 
# #The total sum of squares
ii$totss 
paste0("Between /Total = ",  round(100 * ii$betweenss / ii$totss, 2), "%")
#
# #Members within Clusters
ii$size
#
# #Matrix of cluster centres
round(ii$centers, 3)
#
# #Cluster Membership of each point
str(ii$cluster)
#
# #Save cluster membership of each point back into the dataset
res_movies <- cbind(xw, 
  list(k2 = k2_zw$cluster, k3 = k3_zw$cluster, k4 = k4_zw$cluster)) %>% as_tibble()
```

- Explanation: 
  - In normalised data, average is 0
    - Thus, positive values are above average, negative values are below average
- Two Clusters: Cluster 2 (Size 218) and Cluster 1 (Size 73)
  - Cluster 2 gave Horror & Action movies above average ratings (Favourable)
  - Cluster 2 gave lower than average ratings for Romcom, Comedy, Fantasy (Unfavourable)
  - Behavour of Cluster 1 is completely opposite to Cluster 2
  - However, we cannot make a conclusion here because Between / Total is 47%
    - i.e. Too much heterogeneity within Cluster 2
- Three Clusters: of Sizes 72, 105, 114 with Between /Total = 62% (improved i.e. within reduced)
  - We can analyse these clusters similar to above 
- Four Clusters: of Sizes 73, 51, 69, 98 with Between /Total = 64% 
  - improved i.e. within reduced but not by much 
  - NOTE: Here my cluster sizes are NOT matching with the lecture and the Between /Total is similar but not exactly same.
    - There are 2 reasons for that :
      - I fixed the seed once and then run the commands i.e. (Seed | k=2 | k=3 | k=4). Professor is fixing the seed each time i.e. (Seed | k=2 | Seed | k=3 | Seed | k=4)
      - I used different seed. Effect of starting from different seed is more pronounced as 'k' is increasing

```{r 'B20-kmeans3'}
# #Three Clusters
ii <- k3_zw
ii$size
paste0("Between /Total = ",  round(100 * ii$betweenss / ii$totss, 2), "%")
round(ii$centers, 3)
```

```{r 'B20-kmeans4'}
# #Four Clusters
ii <- k4_zw
ii$size
paste0("Between /Total = ",  round(100 * ii$betweenss / ii$totss, 2), "%")
round(ii$centers, 3)
```

## Elbow Plot of WSS

- \textcolor{pink}{fviz_nbclust()} :
  - Note that it performs the clustering on the original data. It does not take the already created clusters as input.

```{r 'B20-Elbow'}
hh <- zw
cap_hh <- "B20P01"
ttl_hh <- "Movie: Elbow Curve (WSS)"
loc_png <- paste0(.z$PX, "B20P01", "-Movie-Elbow-Wss", ".png")
#
# #factoextra::fviz_nbclust() generates ggplot
# #method = "wss" (for total within sum of square)
B20P01 <- fviz_nbclust(hh, FUNcluster = kmeans, method = "wss") +
  labs(caption = cap_hh, title = ttl_hh)
```

```{r 'B20P01-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B20P01", "-Movie-Elbow-Wss", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B20P01, device = "png", dpi = 144) 
}
```

```{r 'B20P01', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B20P01", "-Movie-Elbow-Wss", ".png"))
```

```{r 'B20-ElbowBase', include=FALSE}
# Total within-groups sum of squares for k = [1, 10]
maxK <- 10L
within_zw <- numeric(maxK)
#
for (ii in seq_along(within_zw)){
  within_zw[ii] <- kmeans(zw, ii)$tot.withinss
}
#
# Plot total within-groups sum of squares
hh <- tibble(xx = seq_along(within_zw), yy = within_zw)
ttl_hh <- "Movie: ElbowPlot for k=[1, 10]"
cap_hh <- "B20P03"
x_hh <- "Number of clusters k"
y_hh <- "Total Within Sum of Square"
#
B20 <- hh %>% { ggplot(., aes(x = xx, y = yy)) + 
    geom_point() +
    geom_line() + 
    scale_x_continuous(breaks = breaks_pretty()) + 
    theme(panel.grid.major = element_blank()) +
    labs(x = x_hh, y = y_hh, 
         subtitle = NULL, caption = cap_hh, title = ttl_hh)
}
assign(cap_hh, B20)
rm(B20)
```

```{r 'B20P03-Save', include=FALSE}
loc_png <- paste0(.z$PX, "B20P03", "-Movie-Elbow-WSS-Base", ".png")
if(!file.exists(loc_png)) {
  ggsave(loc_png, plot = B20P03, device = "png", dpi = 144) 
}
```

```{r 'B20P03', include=FALSE, fig.cap="This-Caption-NOT-Shown"}
knitr::include_graphics(paste0(.z$PX, "B20P03", "-Movie-Elbow-WSS-Base", ".png")) #iiii
```

```{r 'B20P0103', echo=FALSE, ref.label=c('B20P01', 'B20P03'), fig.cap="Movie: Elbow Curve (WSS) in FactoExtra and Base R"}
#
```

## Plot Clusters

```{r 'B20-PlotClust', include=FALSE}
# #
hh <- res_movies %>% select(2:6, Clusters = k3) %>% relocate(Clusters)
ttl_hh <- "Movie: Genres with k=3"
cap_hh <- "B20P02"
loc_png <- paste0(.z$PX, "B20P02", "-Movie-k3", ".png")
#
if(!file.exists(loc_png)) {
  png(filename = loc_png) 
  #dev.control('enable') 
  plot(hh[, 2:ncol(hh)], col = viridis(max(hh$Clusters))[hh$Clusters], main = ttl_hh)
  title(sub = cap_hh, line = 4, adj = 1)
  B20 <- recordPlot()
  dev.off()
  assign(cap_hh, B20)
  rm(B20)
}
```

```{r 'B20P02', echo=FALSE, fig.cap="Movie: Genres with k=3"}
knitr::include_graphics(paste0(.z$PX, "B20P02", "-Movie-k3", ".png"))
```

## Validation {.unlisted .unnumbered .tabset .tabset-fade}

```{r 'B20-Cleanup', include=FALSE, cache=FALSE}
f_rmExist(aa, bb, ii, jj, kk, ll, B20P01, B20P02, B20P03, cap_hh, hh, k2_zw, k3_zw, k4_zw, loc_png,
          res_movies, ttl_hh, xxB20Movies, within_zw, maxK, x_hh, y_hh, xw, zw)
```

```{r 'B20-Validation', include=FALSE, cache=FALSE}
# #SUMMARISED Packages and Objects (BOOK CHECK)
f_()
#
difftime(Sys.time(), k_start)
```

****
